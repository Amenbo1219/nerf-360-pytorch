13
13
13
13
13
13
13
13
13
13
13
13
13
13
13
13
13
13
13
13
13
13
13
13
13
13
13
13
13
13
13
13
13
13
13
13
13
13
13
13
13
13
13
13
13
13
13
13
13
13
13
13
13
13
13
13
13
13
13
13
13
13
13
13
13
13
13
13
13
13
13
13
(72, 3, 4)
Loaded 360 (72, 320, 640, 3) (120, 3, 5) [ 0.13633783  0.02057763 -0.29366428] ./data/indoor/LAB
[ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23
 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47
 48 49 50 51 52 53 54 55 56 57 58]
DEFINING BOUNDS
NEAR FAR 0.0 1.0
hwf [ 0.13633783  0.02057763 -0.29366428]
Found ckpts ['./logs/TUT-LAB-nerf/010000.tar', './logs/TUT-LAB-nerf/020000.tar']
Reloading from ./logs/TUT-LAB-nerf/020000.tar
Not ndc!
Begin
TRAIN views are [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23
 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47
 48 49 50 51 52 53 54 55 56 57 58]
TEST views are [59 60 61 62 63 64 65 66 67 68 69 70 71]
VAL views are [59 60 61 62 63 64 65 66 67 68 69 70 71]
Saved checkpoints at ./logs/TUT-LAB-nerf/020000.tar
[TRAIN] Iter: 20000 Loss: 0.04403498023748398  PSNR: 16.929306030273438
[TRAIN] Iter: 20100 Loss: 0.04491862654685974  PSNR: 16.580713272094727
[TRAIN] Iter: 20200 Loss: 0.04688001424074173  PSNR: 16.22732162475586
[TRAIN] Iter: 20300 Loss: 0.04172515124082565  PSNR: 16.829736709594727
[TRAIN] Iter: 20400 Loss: 0.04226287454366684  PSNR: 16.82784652709961
[TRAIN] Iter: 20500 Loss: 0.04522264748811722  PSNR: 16.56705093383789
[TRAIN] Iter: 20600 Loss: 0.05408927798271179  PSNR: 15.648176193237305
[TRAIN] Iter: 20700 Loss: 0.045492179691791534  PSNR: 16.686845779418945
[TRAIN] Iter: 20800 Loss: 0.042093176394701004  PSNR: 17.018199920654297
[TRAIN] Iter: 20900 Loss: 0.04095138609409332  PSNR: 17.14786720275879
[TRAIN] Iter: 21000 Loss: 0.04831472784280777  PSNR: 16.163837432861328
[TRAIN] Iter: 21100 Loss: 0.039144232869148254  PSNR: 17.209489822387695
[TRAIN] Iter: 21200 Loss: 0.06439775228500366  PSNR: 15.150676727294922
[TRAIN] Iter: 21300 Loss: 0.048293594270944595  PSNR: 16.167139053344727
[TRAIN] Iter: 21400 Loss: 0.04251394420862198  PSNR: 17.07550811767578
[TRAIN] Iter: 21500 Loss: 0.04711758717894554  PSNR: 16.66930389404297
[TRAIN] Iter: 21600 Loss: 0.04057605564594269  PSNR: 17.326242446899414
[TRAIN] Iter: 21700 Loss: 0.04955507442355156  PSNR: 15.941015243530273
[TRAIN] Iter: 21800 Loss: 0.0433705598115921  PSNR: 16.91246223449707
[TRAIN] Iter: 21900 Loss: 0.03922578692436218  PSNR: 17.358869552612305
[TRAIN] Iter: 22000 Loss: 0.043574828654527664  PSNR: 16.763315200805664
[TRAIN] Iter: 22100 Loss: 0.04656795412302017  PSNR: 16.435211181640625
[TRAIN] Iter: 22200 Loss: 0.06605539470911026  PSNR: 14.944270133972168
[TRAIN] Iter: 22300 Loss: 0.050993502140045166  PSNR: 15.699759483337402
[TRAIN] Iter: 22400 Loss: 0.044317107647657394  PSNR: 16.823312759399414
[TRAIN] Iter: 22500 Loss: 0.044620197266340256  PSNR: 16.445907592773438
[TRAIN] Iter: 22600 Loss: 0.04923820495605469  PSNR: 16.097230911254883
[TRAIN] Iter: 22700 Loss: 0.04089194908738136  PSNR: 17.113813400268555
[TRAIN] Iter: 22800 Loss: 0.044825658202171326  PSNR: 16.670791625976562
[TRAIN] Iter: 22900 Loss: 0.048851072788238525  PSNR: 16.210681915283203
[TRAIN] Iter: 23000 Loss: 0.04317019134759903  PSNR: 16.61688995361328
[TRAIN] Iter: 23100 Loss: 0.04977923631668091  PSNR: 16.054393768310547
[TRAIN] Iter: 23200 Loss: 0.04477904364466667  PSNR: 16.527524948120117
[TRAIN] Iter: 23300 Loss: 0.04257996007800102  PSNR: 16.670989990234375
[TRAIN] Iter: 23400 Loss: 0.0558900460600853  PSNR: 15.744750022888184
[TRAIN] Iter: 23500 Loss: 0.03492233529686928  PSNR: 18.01103973388672
[TRAIN] Iter: 23600 Loss: 0.05495728552341461  PSNR: 15.576180458068848
[TRAIN] Iter: 23700 Loss: 0.045285407453775406  PSNR: 16.595773696899414
[TRAIN] Iter: 23800 Loss: 0.04912767559289932  PSNR: 16.071231842041016
[TRAIN] Iter: 23900 Loss: 0.04325123876333237  PSNR: 16.579164505004883
[TRAIN] Iter: 24000 Loss: 0.04746025800704956  PSNR: 16.307415008544922
[TRAIN] Iter: 24100 Loss: 0.04124603420495987  PSNR: 17.10423469543457
[TRAIN] Iter: 24200 Loss: 0.05077097564935684  PSNR: 16.110809326171875
[TRAIN] Iter: 24300 Loss: 0.045097291469573975  PSNR: 16.445165634155273
[TRAIN] Iter: 24400 Loss: 0.03878861665725708  PSNR: 17.226213455200195
[TRAIN] Iter: 24500 Loss: 0.045601703226566315  PSNR: 16.326969146728516
[TRAIN] Iter: 24600 Loss: 0.04049958661198616  PSNR: 17.132287979125977
[TRAIN] Iter: 24700 Loss: 0.044500015676021576  PSNR: 16.728893280029297
[TRAIN] Iter: 24800 Loss: 0.03637317195534706  PSNR: 17.618101119995117
[TRAIN] Iter: 24900 Loss: 0.06043482571840286  PSNR: 15.34743881225586
[TRAIN] Iter: 25000 Loss: 0.03932870924472809  PSNR: 17.25302505493164
[TRAIN] Iter: 25100 Loss: 0.04540708661079407  PSNR: 16.519594192504883
[TRAIN] Iter: 25200 Loss: 0.04543479532003403  PSNR: 16.4382266998291
[TRAIN] Iter: 25300 Loss: 0.04929880425333977  PSNR: 16.017866134643555
[TRAIN] Iter: 25400 Loss: 0.043906331062316895  PSNR: 16.960718154907227
[TRAIN] Iter: 25500 Loss: 0.04229773208498955  PSNR: 16.757625579833984
[TRAIN] Iter: 25600 Loss: 0.04702771455049515  PSNR: 16.336288452148438
[TRAIN] Iter: 25700 Loss: 0.04221571236848831  PSNR: 17.065038681030273
[TRAIN] Iter: 25800 Loss: 0.04529920220375061  PSNR: 16.286409378051758
[TRAIN] Iter: 25900 Loss: 0.04342932254076004  PSNR: 16.58271598815918
[TRAIN] Iter: 26000 Loss: 0.0471196174621582  PSNR: 16.242752075195312
[TRAIN] Iter: 26100 Loss: 0.045542679727077484  PSNR: 16.53584098815918
[TRAIN] Iter: 26200 Loss: 0.03774132952094078  PSNR: 17.403654098510742
[TRAIN] Iter: 26300 Loss: 0.04923471435904503  PSNR: 16.002140045166016
[TRAIN] Iter: 26400 Loss: 0.04377157241106033  PSNR: 16.572927474975586
[TRAIN] Iter: 26500 Loss: 0.040665775537490845  PSNR: 17.07358741760254
[TRAIN] Iter: 26600 Loss: 0.045743875205516815  PSNR: 16.445362091064453
[TRAIN] Iter: 26700 Loss: 0.04494871944189072  PSNR: 16.56890296936035
[TRAIN] Iter: 26800 Loss: 0.03981988504528999  PSNR: 16.95362663269043
[TRAIN] Iter: 26900 Loss: 0.043420933187007904  PSNR: 16.870925903320312
[TRAIN] Iter: 27000 Loss: 0.04435313120484352  PSNR: 16.695371627807617
[TRAIN] Iter: 27100 Loss: 0.03871079534292221  PSNR: 17.456687927246094
[TRAIN] Iter: 27200 Loss: 0.042240723967552185  PSNR: 16.570398330688477
[TRAIN] Iter: 27300 Loss: 0.046085603535175323  PSNR: 16.543901443481445
[TRAIN] Iter: 27400 Loss: 0.042734671384096146  PSNR: 16.581052780151367
[TRAIN] Iter: 27500 Loss: 0.040204007178545  PSNR: 17.03083038330078
[TRAIN] Iter: 27600 Loss: 0.0386606901884079  PSNR: 17.559938430786133
[TRAIN] Iter: 27700 Loss: 0.04090285301208496  PSNR: 16.904300689697266
[TRAIN] Iter: 27800 Loss: 0.04811862111091614  PSNR: 16.2349796295166
[TRAIN] Iter: 27900 Loss: 0.04609687626361847  PSNR: 16.54512596130371
[TRAIN] Iter: 28000 Loss: 0.05765755847096443  PSNR: 15.517168998718262
[TRAIN] Iter: 28100 Loss: 0.039107855409383774  PSNR: 17.341115951538086
[TRAIN] Iter: 28200 Loss: 0.04360909014940262  PSNR: 16.620738983154297
[TRAIN] Iter: 28300 Loss: 0.044752608984708786  PSNR: 16.376136779785156
[TRAIN] Iter: 28400 Loss: 0.04433396831154823  PSNR: 16.565671920776367
[TRAIN] Iter: 28500 Loss: 0.03617764636874199  PSNR: 17.874866485595703
[TRAIN] Iter: 28600 Loss: 0.039989665150642395  PSNR: 17.330453872680664
[TRAIN] Iter: 28700 Loss: 0.0376751571893692  PSNR: 17.50585174560547
[TRAIN] Iter: 28800 Loss: 0.048664484173059464  PSNR: 16.020862579345703
[TRAIN] Iter: 28900 Loss: 0.0427638441324234  PSNR: 16.815580368041992
[TRAIN] Iter: 29000 Loss: 0.04525580257177353  PSNR: 16.47994041442871
[TRAIN] Iter: 29100 Loss: 0.04148166999220848  PSNR: 16.925058364868164
[TRAIN] Iter: 29200 Loss: 0.045588716864585876  PSNR: 16.47736930847168
[TRAIN] Iter: 29300 Loss: 0.038202498108148575  PSNR: 17.38662338256836
[TRAIN] Iter: 29400 Loss: 0.05283757299184799  PSNR: 15.792551040649414
[TRAIN] Iter: 29500 Loss: 0.0454159751534462  PSNR: 16.583566665649414
[TRAIN] Iter: 29600 Loss: 0.04004903882741928  PSNR: 17.276214599609375
[TRAIN] Iter: 29700 Loss: 0.0416976734995842  PSNR: 17.092655181884766
[TRAIN] Iter: 29800 Loss: 0.039124105125665665  PSNR: 17.356731414794922
[TRAIN] Iter: 29900 Loss: 0.0423162579536438  PSNR: 17.138757705688477
Saved checkpoints at ./logs/TUT-LAB-nerf/030000.tar
[TRAIN] Iter: 30000 Loss: 0.042062610387802124  PSNR: 16.80013084411621
[TRAIN] Iter: 30100 Loss: 0.05127760022878647  PSNR: 15.91616439819336
[TRAIN] Iter: 30200 Loss: 0.04424712806940079  PSNR: 16.533170700073242
[TRAIN] Iter: 30300 Loss: 0.04250505566596985  PSNR: 16.834604263305664
[TRAIN] Iter: 30400 Loss: 0.03795675188302994  PSNR: 17.412193298339844
[TRAIN] Iter: 30500 Loss: 0.03399711474776268  PSNR: 17.91863250732422
[TRAIN] Iter: 30600 Loss: 0.04478107765316963  PSNR: 16.50869369506836
[TRAIN] Iter: 30700 Loss: 0.056997835636138916  PSNR: 15.313436508178711
[TRAIN] Iter: 30800 Loss: 0.041281312704086304  PSNR: 16.927217483520508
[TRAIN] Iter: 30900 Loss: 0.04137931019067764  PSNR: 16.65168571472168
[TRAIN] Iter: 31000 Loss: 0.048077914863824844  PSNR: 16.251718521118164
[TRAIN] Iter: 31100 Loss: 0.03902772068977356  PSNR: 17.029315948486328
[TRAIN] Iter: 31200 Loss: 0.04531659185886383  PSNR: 16.347728729248047
[TRAIN] Iter: 31300 Loss: 0.04111887514591217  PSNR: 16.886566162109375
[TRAIN] Iter: 31400 Loss: 0.03617306053638458  PSNR: 17.64390754699707
[TRAIN] Iter: 31500 Loss: 0.036457620561122894  PSNR: 17.555543899536133
[TRAIN] Iter: 31600 Loss: 0.0318090096116066  PSNR: 18.137828826904297
[TRAIN] Iter: 31700 Loss: 0.05647812411189079  PSNR: 15.6127290725708
[TRAIN] Iter: 31800 Loss: 0.042422812432050705  PSNR: 16.917343139648438
[TRAIN] Iter: 31900 Loss: 0.044527504593133926  PSNR: 16.69622039794922
[TRAIN] Iter: 32000 Loss: 0.047133419662714005  PSNR: 16.338533401489258
[TRAIN] Iter: 32100 Loss: 0.03620757907629013  PSNR: 17.632978439331055
[TRAIN] Iter: 32200 Loss: 0.04242515563964844  PSNR: 17.026111602783203
[TRAIN] Iter: 32300 Loss: 0.03335880488157272  PSNR: 18.14296531677246
[TRAIN] Iter: 32400 Loss: 0.04624738544225693  PSNR: 16.457361221313477
[TRAIN] Iter: 32500 Loss: 0.047291647642850876  PSNR: 16.479829788208008
[TRAIN] Iter: 32600 Loss: 0.05797892436385155  PSNR: 15.434428215026855
[TRAIN] Iter: 32700 Loss: 0.04532027244567871  PSNR: 16.52943229675293
[TRAIN] Iter: 32800 Loss: 0.040910378098487854  PSNR: 16.871959686279297
[TRAIN] Iter: 32900 Loss: 0.05403279513120651  PSNR: 15.57486343383789
[TRAIN] Iter: 33000 Loss: 0.050553835928440094  PSNR: 16.319028854370117
[TRAIN] Iter: 33100 Loss: 0.05152241140604019  PSNR: 16.225072860717773
[TRAIN] Iter: 33200 Loss: 0.04283755272626877  PSNR: 16.746492385864258
[TRAIN] Iter: 33300 Loss: 0.0415429025888443  PSNR: 16.97747039794922
[TRAIN] Iter: 33400 Loss: 0.034551721066236496  PSNR: 17.84035873413086
[TRAIN] Iter: 33500 Loss: 0.03961940109729767  PSNR: 17.305952072143555
[TRAIN] Iter: 33600 Loss: 0.04745864123106003  PSNR: 16.561813354492188
[TRAIN] Iter: 33700 Loss: 0.03866198658943176  PSNR: 17.375247955322266
[TRAIN] Iter: 33800 Loss: 0.04472118616104126  PSNR: 16.39680290222168
[TRAIN] Iter: 33900 Loss: 0.04052882269024849  PSNR: 17.220251083374023
[TRAIN] Iter: 34000 Loss: 0.044907256960868835  PSNR: 16.792953491210938
[TRAIN] Iter: 34100 Loss: 0.053325992077589035  PSNR: 16.00062370300293
[TRAIN] Iter: 34200 Loss: 0.03914284706115723  PSNR: 17.145292282104492
[TRAIN] Iter: 34300 Loss: 0.04078972339630127  PSNR: 17.121763229370117
[TRAIN] Iter: 34400 Loss: 0.04536690562963486  PSNR: 16.480072021484375
[TRAIN] Iter: 34500 Loss: 0.037823453545570374  PSNR: 17.46656036376953
[TRAIN] Iter: 34600 Loss: 0.038102541118860245  PSNR: 17.574657440185547
[TRAIN] Iter: 34700 Loss: 0.03195968642830849  PSNR: 18.209209442138672
[TRAIN] Iter: 34800 Loss: 0.05457977205514908  PSNR: 15.799517631530762
[TRAIN] Iter: 34900 Loss: 0.0419774204492569  PSNR: 16.870370864868164
[TRAIN] Iter: 35000 Loss: 0.042159922420978546  PSNR: 16.761354446411133
[TRAIN] Iter: 35100 Loss: 0.04087075591087341  PSNR: 16.94532585144043
[TRAIN] Iter: 35200 Loss: 0.03633381426334381  PSNR: 17.498825073242188
[TRAIN] Iter: 35300 Loss: 0.041561730206012726  PSNR: 17.122722625732422
[TRAIN] Iter: 35400 Loss: 0.034619491547346115  PSNR: 17.8630313873291
[TRAIN] Iter: 35500 Loss: 0.03973860293626785  PSNR: 17.00118064880371
[TRAIN] Iter: 35600 Loss: 0.04263357073068619  PSNR: 16.7352352142334
[TRAIN] Iter: 35700 Loss: 0.05970733240246773  PSNR: 15.402676582336426
[TRAIN] Iter: 35800 Loss: 0.0421515554189682  PSNR: 16.801958084106445
[TRAIN] Iter: 35900 Loss: 0.039069101214408875  PSNR: 17.127182006835938
[TRAIN] Iter: 36000 Loss: 0.04570971801877022  PSNR: 16.366159439086914
[TRAIN] Iter: 36100 Loss: 0.0493813194334507  PSNR: 16.281431198120117
[TRAIN] Iter: 36200 Loss: 0.04188821092247963  PSNR: 16.540668487548828
[TRAIN] Iter: 36300 Loss: 0.04043089598417282  PSNR: 17.03061294555664
[TRAIN] Iter: 36400 Loss: 0.04271838068962097  PSNR: 16.700767517089844
[TRAIN] Iter: 36500 Loss: 0.04193596541881561  PSNR: 16.730377197265625
[TRAIN] Iter: 36600 Loss: 0.042866602540016174  PSNR: 16.679046630859375
[TRAIN] Iter: 36700 Loss: 0.041308172047138214  PSNR: 17.057937622070312
[TRAIN] Iter: 36800 Loss: 0.04550980031490326  PSNR: 16.59850311279297
[TRAIN] Iter: 36900 Loss: 0.038802407681941986  PSNR: 17.0846004486084
[TRAIN] Iter: 37000 Loss: 0.05010590702295303  PSNR: 16.244544982910156
[TRAIN] Iter: 37100 Loss: 0.038039155304431915  PSNR: 17.482397079467773
[TRAIN] Iter: 37200 Loss: 0.03980265185236931  PSNR: 17.20354461669922
[TRAIN] Iter: 37300 Loss: 0.038898348808288574  PSNR: 17.35729217529297
[TRAIN] Iter: 37400 Loss: 0.02827633172273636  PSNR: 18.738765716552734
[TRAIN] Iter: 37500 Loss: 0.04304880648851395  PSNR: 16.47538948059082
[TRAIN] Iter: 37600 Loss: 0.04541298747062683  PSNR: 16.53048324584961
[TRAIN] Iter: 37700 Loss: 0.050410833209753036  PSNR: 16.017372131347656
[TRAIN] Iter: 37800 Loss: 0.049559272825717926  PSNR: 16.337303161621094
[TRAIN] Iter: 37900 Loss: 0.04437675699591637  PSNR: 16.632219314575195
[TRAIN] Iter: 38000 Loss: 0.04275032505393028  PSNR: 16.95542335510254
[TRAIN] Iter: 38100 Loss: 0.04846298694610596  PSNR: 16.224266052246094
[TRAIN] Iter: 38200 Loss: 0.04061604291200638  PSNR: 16.952857971191406
[TRAIN] Iter: 38300 Loss: 0.04667974263429642  PSNR: 16.437538146972656
[TRAIN] Iter: 38400 Loss: 0.033789169043302536  PSNR: 17.93379783630371
[TRAIN] Iter: 38500 Loss: 0.04803047329187393  PSNR: 16.255756378173828
[TRAIN] Iter: 38600 Loss: 0.046354420483112335  PSNR: 16.601726531982422
[TRAIN] Iter: 38700 Loss: 0.03211484104394913  PSNR: 18.26187515258789
[TRAIN] Iter: 38800 Loss: 0.04710802063345909  PSNR: 16.343351364135742
[TRAIN] Iter: 38900 Loss: 0.05467929691076279  PSNR: 15.814887046813965
[TRAIN] Iter: 39000 Loss: 0.042416952550411224  PSNR: 16.689617156982422
[TRAIN] Iter: 39100 Loss: 0.0382559709250927  PSNR: 17.307636260986328
[TRAIN] Iter: 39200 Loss: 0.0381724052131176  PSNR: 17.080097198486328
[TRAIN] Iter: 39300 Loss: 0.045866839587688446  PSNR: 16.225719451904297
[TRAIN] Iter: 39400 Loss: 0.044553883373737335  PSNR: 16.460830688476562
[TRAIN] Iter: 39500 Loss: 0.037597887217998505  PSNR: 17.066020965576172
[TRAIN] Iter: 39600 Loss: 0.050115957856178284  PSNR: 16.31483268737793
[TRAIN] Iter: 39700 Loss: 0.04845587909221649  PSNR: 16.445940017700195
[TRAIN] Iter: 39800 Loss: 0.03633994981646538  PSNR: 17.400951385498047
[TRAIN] Iter: 39900 Loss: 0.0436689555644989  PSNR: 16.680206298828125
Saved checkpoints at ./logs/TUT-LAB-nerf/040000.tar
[TRAIN] Iter: 40000 Loss: 0.04230359196662903  PSNR: 16.75829315185547
[TRAIN] Iter: 40100 Loss: 0.04278235137462616  PSNR: 17.11290740966797
[TRAIN] Iter: 40200 Loss: 0.04105222225189209  PSNR: 17.147790908813477
[TRAIN] Iter: 40300 Loss: 0.04508193954825401  PSNR: 16.83401107788086
[TRAIN] Iter: 40400 Loss: 0.0352180078625679  PSNR: 17.706905364990234
[TRAIN] Iter: 40500 Loss: 0.04163159057497978  PSNR: 16.918880462646484
[TRAIN] Iter: 40600 Loss: 0.04258745536208153  PSNR: 16.780183792114258
[TRAIN] Iter: 40700 Loss: 0.04192585498094559  PSNR: 16.715625762939453
[TRAIN] Iter: 40800 Loss: 0.03820514678955078  PSNR: 17.38701057434082
[TRAIN] Iter: 40900 Loss: 0.044917069375514984  PSNR: 16.64287757873535
[TRAIN] Iter: 41000 Loss: 0.05374857410788536  PSNR: 15.84077262878418
[TRAIN] Iter: 41100 Loss: 0.043184246867895126  PSNR: 16.90522003173828
[TRAIN] Iter: 41200 Loss: 0.03545919060707092  PSNR: 17.639747619628906
[TRAIN] Iter: 41300 Loss: 0.06478223204612732  PSNR: 15.051730155944824
[TRAIN] Iter: 41400 Loss: 0.042225055396556854  PSNR: 16.957780838012695
[TRAIN] Iter: 41500 Loss: 0.04505067318677902  PSNR: 16.4400691986084
[TRAIN] Iter: 41600 Loss: 0.03986828029155731  PSNR: 17.119834899902344
[TRAIN] Iter: 41700 Loss: 0.04837307333946228  PSNR: 16.26190948486328
[TRAIN] Iter: 41800 Loss: 0.03990226611495018  PSNR: 17.1185245513916
[TRAIN] Iter: 41900 Loss: 0.0433230847120285  PSNR: 16.706941604614258
[TRAIN] Iter: 42000 Loss: 0.04345082491636276  PSNR: 16.7253475189209
[TRAIN] Iter: 42100 Loss: 0.04861176386475563  PSNR: 16.48802375793457
[TRAIN] Iter: 42200 Loss: 0.04609496146440506  PSNR: 16.652210235595703
[TRAIN] Iter: 42300 Loss: 0.037139490246772766  PSNR: 17.40689468383789
[TRAIN] Iter: 42400 Loss: 0.03863845393061638  PSNR: 17.487529754638672
[TRAIN] Iter: 42500 Loss: 0.041491977870464325  PSNR: 16.95706558227539
[TRAIN] Iter: 42600 Loss: 0.04603957757353783  PSNR: 16.67792320251465
[TRAIN] Iter: 42700 Loss: 0.041609443724155426  PSNR: 16.937389373779297
[TRAIN] Iter: 42800 Loss: 0.03833047300577164  PSNR: 17.29136085510254
[TRAIN] Iter: 42900 Loss: 0.049338217824697495  PSNR: 16.363622665405273
[TRAIN] Iter: 43000 Loss: 0.04180796444416046  PSNR: 16.854249954223633
[TRAIN] Iter: 43100 Loss: 0.04547889903187752  PSNR: 16.367450714111328
[TRAIN] Iter: 43200 Loss: 0.04512014240026474  PSNR: 16.455032348632812
[TRAIN] Iter: 43300 Loss: 0.04322317987680435  PSNR: 16.742822647094727
[TRAIN] Iter: 43400 Loss: 0.038427308201789856  PSNR: 17.394290924072266
[TRAIN] Iter: 43500 Loss: 0.04413807392120361  PSNR: 16.710556030273438
[TRAIN] Iter: 43600 Loss: 0.0443965345621109  PSNR: 16.757347106933594
[TRAIN] Iter: 43700 Loss: 0.03849324211478233  PSNR: 17.24881362915039
[TRAIN] Iter: 43800 Loss: 0.042944323271512985  PSNR: 16.763071060180664
[TRAIN] Iter: 43900 Loss: 0.04108515381813049  PSNR: 16.969554901123047
[TRAIN] Iter: 44000 Loss: 0.05034993588924408  PSNR: 16.165569305419922
[TRAIN] Iter: 44100 Loss: 0.0418710894882679  PSNR: 16.77553939819336
[TRAIN] Iter: 44200 Loss: 0.047520093619823456  PSNR: 16.366012573242188
[TRAIN] Iter: 44300 Loss: 0.04126574471592903  PSNR: 17.0426025390625
[TRAIN] Iter: 44400 Loss: 0.03819354251027107  PSNR: 17.245756149291992
[TRAIN] Iter: 44500 Loss: 0.04318733513355255  PSNR: 16.804269790649414
[TRAIN] Iter: 44600 Loss: 0.04336351528763771  PSNR: 16.91909408569336
[TRAIN] Iter: 44700 Loss: 0.03970970958471298  PSNR: 17.302608489990234
[TRAIN] Iter: 44800 Loss: 0.04062855616211891  PSNR: 16.818641662597656
[TRAIN] Iter: 44900 Loss: 0.04282933846116066  PSNR: 16.86881446838379
[TRAIN] Iter: 45000 Loss: 0.03959358111023903  PSNR: 17.1198673248291
[TRAIN] Iter: 45100 Loss: 0.04465591907501221  PSNR: 16.642370223999023
[TRAIN] Iter: 45200 Loss: 0.043019868433475494  PSNR: 16.94013786315918
[TRAIN] Iter: 45300 Loss: 0.038352787494659424  PSNR: 17.360946655273438
[TRAIN] Iter: 45400 Loss: 0.04487549513578415  PSNR: 16.58817481994629
[TRAIN] Iter: 45500 Loss: 0.04192007705569267  PSNR: 16.78276824951172
[TRAIN] Iter: 45600 Loss: 0.04381932318210602  PSNR: 16.75452423095703
[TRAIN] Iter: 45700 Loss: 0.04199810326099396  PSNR: 16.790246963500977
[TRAIN] Iter: 45800 Loss: 0.03912722319364548  PSNR: 17.047801971435547
[TRAIN] Iter: 45900 Loss: 0.040797699242830276  PSNR: 16.98396873474121
[TRAIN] Iter: 46000 Loss: 0.04140916466712952  PSNR: 16.787982940673828
[TRAIN] Iter: 46100 Loss: 0.052536606788635254  PSNR: 16.09954071044922
[TRAIN] Iter: 46200 Loss: 0.033252499997615814  PSNR: 18.039520263671875
[TRAIN] Iter: 46300 Loss: 0.04397691786289215  PSNR: 16.505813598632812
[TRAIN] Iter: 46400 Loss: 0.03156900778412819  PSNR: 18.132951736450195
[TRAIN] Iter: 46500 Loss: 0.0405752994120121  PSNR: 16.876419067382812
[TRAIN] Iter: 46600 Loss: 0.04054895415902138  PSNR: 17.033611297607422
[TRAIN] Iter: 46700 Loss: 0.04116814583539963  PSNR: 16.77813720703125
[TRAIN] Iter: 46800 Loss: 0.03959404677152634  PSNR: 17.182472229003906
[TRAIN] Iter: 46900 Loss: 0.04162903502583504  PSNR: 17.137096405029297
[TRAIN] Iter: 47000 Loss: 0.04143630340695381  PSNR: 16.999799728393555
[TRAIN] Iter: 47100 Loss: 0.03959428146481514  PSNR: 17.558025360107422
[TRAIN] Iter: 47200 Loss: 0.03912781924009323  PSNR: 17.21645736694336
[TRAIN] Iter: 47300 Loss: 0.042608216404914856  PSNR: 16.75105857849121
[TRAIN] Iter: 47400 Loss: 0.031617049127817154  PSNR: 18.17123794555664
[TRAIN] Iter: 47500 Loss: 0.04493425041437149  PSNR: 16.5498104095459
[TRAIN] Iter: 47600 Loss: 0.03806225210428238  PSNR: 17.24995231628418
[TRAIN] Iter: 47700 Loss: 0.03696361556649208  PSNR: 17.484548568725586
[TRAIN] Iter: 47800 Loss: 0.030360082164406776  PSNR: 18.286277770996094
[TRAIN] Iter: 47900 Loss: 0.033467426896095276  PSNR: 17.889604568481445
[TRAIN] Iter: 48000 Loss: 0.043346717953681946  PSNR: 16.595914840698242
[TRAIN] Iter: 48100 Loss: 0.036104124039411545  PSNR: 17.636268615722656
[TRAIN] Iter: 48200 Loss: 0.052219223231077194  PSNR: 16.156679153442383
[TRAIN] Iter: 48300 Loss: 0.04018068686127663  PSNR: 17.278635025024414
[TRAIN] Iter: 48400 Loss: 0.0344054251909256  PSNR: 17.745948791503906
[TRAIN] Iter: 48500 Loss: 0.04464263468980789  PSNR: 16.70863151550293
[TRAIN] Iter: 48600 Loss: 0.03727175295352936  PSNR: 17.59221839904785
[TRAIN] Iter: 48700 Loss: 0.04008583724498749  PSNR: 17.0313777923584
[TRAIN] Iter: 48800 Loss: 0.03907928988337517  PSNR: 17.143335342407227
[TRAIN] Iter: 48900 Loss: 0.04186805337667465  PSNR: 16.95989227294922
[TRAIN] Iter: 49000 Loss: 0.038606055080890656  PSNR: 17.05377769470215
[TRAIN] Iter: 49100 Loss: 0.05184335261583328  PSNR: 16.104978561401367
[TRAIN] Iter: 49200 Loss: 0.02874079905450344  PSNR: 18.808910369873047
[TRAIN] Iter: 49300 Loss: 0.04357956722378731  PSNR: 16.990297317504883
[TRAIN] Iter: 49400 Loss: 0.04187403619289398  PSNR: 16.875560760498047
[TRAIN] Iter: 49500 Loss: 0.03707148879766464  PSNR: 17.573518753051758
[TRAIN] Iter: 49600 Loss: 0.0333547368645668  PSNR: 18.063419342041016
[TRAIN] Iter: 49700 Loss: 0.0430755689740181  PSNR: 16.561710357666016
[TRAIN] Iter: 49800 Loss: 0.042037464678287506  PSNR: 16.548322677612305
[TRAIN] Iter: 49900 Loss: 0.043979939073324203  PSNR: 16.68485450744629
Saved checkpoints at ./logs/TUT-LAB-nerf/050000.tar
0 0.0004284381866455078
torch.Size([320, 640, 3]) torch.Size([320, 640])
1 17.816402912139893
2 15.576573848724365
3 17.868834018707275
4 15.545655965805054
5 17.850677251815796
6 15.599488973617554
7 17.697952270507812
8 15.978347778320312
9 17.616642713546753
10 15.834525108337402
11 17.55321979522705
12 15.83801794052124
13 17.54439067840576
14 15.9056715965271
15 17.605671882629395
16 15.771965265274048
17 17.563663244247437
18 15.841651201248169
19 17.559309005737305
20 15.84616732597351
21 17.60909652709961
22 15.88693618774414
23 15.759581089019775
24 17.565965175628662
25 15.811510562896729
26 17.563052654266357
27 15.866975545883179
28 17.60712766647339
29 15.81917428970337
30 17.743656635284424
31 16.06514072418213
32 17.253153324127197
33 15.85027027130127
34 17.391578674316406
35 15.912325143814087
36 17.80015516281128
37 15.790184259414673
38 17.545997381210327
39 15.897613286972046
40 17.645343780517578
41 15.836222171783447
42 17.628321647644043
43 15.763208866119385
44 17.56485605239868
45 15.794768810272217
46 17.55749821662903
47 15.844120502471924
48 17.601250886917114
49 15.834583282470703
50 17.57459330558777
51 15.77375602722168
52 17.76472306251526
53 15.645061731338501
54 17.818804502487183
55 15.677200555801392
56 17.962361335754395
57 16.077765941619873
58 18.29432439804077
59 16.289932250976562
60 18.10175919532776
61 16.544193744659424
62 18.375678300857544
63 15.981705665588379
64 18.15994882583618
65 15.852626323699951
66 18.363291025161743
67 15.619112253189087
68 18.11251187324524
69 15.692335605621338
70 18.361693382263184
71 15.68145751953125
72 18.394121885299683
73 15.795995235443115
74 18.120842695236206
75 15.725625276565552
76 18.09267544746399
77 15.929414749145508
78 18.08205795288086
79 16.145984172821045
80 17.677032470703125
81 16.071675062179565
82 17.850289583206177
83 16.077367782592773
84 17.85890507698059
85 16.062768697738647
86 17.885807752609253
87 16.102056980133057
88 17.899431705474854
89 16.043596744537354
90 17.88180637359619
91 15.969528675079346
92 17.909329652786255
93 16.066124439239502
94 17.870063066482544
95 16.051860570907593
96 17.876035928726196
97 16.04318070411682
98 13.741898775100708
99 16.099412441253662
100 17.835930109024048
101 16.00848937034607
102 17.965622186660767
103 16.030539751052856
104 17.812801361083984
105 16.046178579330444
106 17.866409301757812
107 16.080586671829224
108 17.88887929916382
109 16.04592537879944
110 17.86201310157776
111 16.040017127990723
112 17.829280853271484
113 16.083983421325684
114 17.848363399505615
115 16.059720039367676
116 17.87789225578308
117 15.665570497512817
118 17.511730194091797
119 15.709375619888306
Done, saving (120, 320, 640, 3) (120, 320, 640)
extras:{'raw': tensor([[[-1.6232e+00, -1.2138e+00,  1.4107e-01, -1.1360e+00],
         [-8.4835e-01, -8.3026e-01, -3.2639e-01, -3.9527e+00],
         [-8.1488e-01, -7.8383e-01,  7.3122e-02, -1.5234e+01],
         ...,
         [-8.4522e+00, -6.4290e+00,  5.7955e+00,  4.8746e+01],
         [-9.2443e+00, -7.3548e+00,  4.9446e+00,  5.1919e+01],
         [-8.6851e+00, -6.8033e+00,  5.3590e+00,  5.4633e+01]],

        [[ 1.6375e-01,  1.4168e-01,  2.0047e-01,  3.7240e+00],
         [ 4.9665e-01,  4.2945e-01,  4.4234e-01,  5.2696e+00],
         [ 5.0660e-01,  4.3330e-01,  4.2525e-01,  5.3607e+00],
         ...,
         [ 2.2611e+00,  1.3365e+00,  2.3540e-01,  5.9184e+01],
         [ 2.2370e+00,  1.1451e+00,  2.5419e-02,  6.2156e+01],
         [ 1.9218e+00,  1.0173e+00,  1.5019e-01,  6.3086e+01]],

        [[-6.6445e-02, -7.5927e-02,  8.3859e-02,  1.0690e+01],
         [ 5.6597e-02,  9.9816e-02,  3.1463e-01, -5.4361e-02],
         [-3.0075e-01, -2.8975e-01, -5.3229e-02, -1.1988e+01],
         ...,
         [-5.8882e-01, -1.2861e+00,  3.0446e+00,  3.8094e+02],
         [-6.2447e-01, -1.3361e+00,  3.5620e+00,  3.9545e+02],
         [-3.6353e-01, -1.2570e+00,  3.3966e+00,  3.9963e+02]],

        ...,

        [[-1.1368e-01, -1.1528e-01, -3.6487e-03,  5.7963e+00],
         [-9.4631e-02, -3.8489e-02,  1.9862e-01,  3.0336e+00],
         [-1.9592e-01, -7.5076e-02,  3.3005e-01, -8.0867e+00],
         ...,
         [ 5.9237e-01,  5.8096e-01,  1.2441e-01,  1.7700e+02],
         [ 2.3520e-01,  2.2401e-01,  2.5247e-01,  1.7550e+02],
         [ 2.3184e-01,  2.1715e-01,  3.5439e-01,  1.7608e+02]],

        [[-8.4734e-01, -6.4537e-01, -7.9053e-01, -3.6442e+00],
         [-3.4646e-01, -2.3106e-01,  1.8448e-02, -1.6092e+01],
         [-5.5360e-01, -4.5237e-01,  4.3016e-02, -1.8602e+01],
         ...,
         [-1.3109e+00, -1.1979e+00,  2.6878e+00,  2.7246e+02],
         [-7.9273e-01, -6.7202e-01,  3.2675e+00,  2.7499e+02],
         [-8.7097e-01, -7.0195e-01,  3.1092e+00,  2.7548e+02]],

        [[-9.0953e-02, -9.0994e-02,  1.2936e-01, -8.0983e+00],
         [-3.9950e-01, -3.4754e-01, -2.2243e-01, -9.9226e+00],
         [-3.6023e-01, -3.8846e-01, -3.2164e-01, -1.7935e+01],
         ...,
         [-5.4852e-01, -2.2407e+00, -4.5983e-01,  1.7103e+02],
         [-3.3819e-01, -2.4546e+00, -9.3736e-01,  1.6727e+02],
         [-7.2058e-01, -2.7351e+00, -1.3225e+00,  1.5869e+02]]],
       grad_fn=<ReshapeAliasBackward0>), 'rgb0': tensor([[0.4591, 0.3911, 0.3611],
        [0.6201, 0.6034, 0.5994],
        [0.4619, 0.4778, 0.5380],
        ...,
        [0.3834, 0.3822, 0.3802],
        [0.1712, 0.2138, 0.0980],
        [0.3455, 0.3415, 0.3592]], grad_fn=<ReshapeAliasBackward0>), 'disp0': tensor([21.2583, 22.0948, 16.3767,  ..., 19.2108, 16.7332, 21.1198],
       grad_fn=<ReshapeAliasBackward0>), 'acc0': tensor([1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000],
       grad_fn=<ReshapeAliasBackward0>), 'z_std': tensor([0.0028, 0.0287, 0.0191,  ..., 0.0036, 0.0028, 0.0230])}
0 0.0004928112030029297
torch.Size([320, 640, 3]) torch.Size([320, 640])
1 15.699009418487549
2 17.475138902664185
3 15.655592441558838
4 15.780768871307373
5 17.73128867149353
6 15.583422660827637
7 17.664703130722046
8 15.574065208435059
9 17.641613721847534
10 15.60956621170044
11 17.667750120162964
12 15.414927005767822
13 17.79533076286316
14 15.359606504440308
15 17.67645835876465
16 15.418298482894897
17 17.873769521713257
18 15.355621337890625
19 17.824615716934204
20 15.393269777297974
21 17.783108472824097
22 15.393726825714111
23 17.894991159439087
24 15.325765371322632
25 17.88785433769226
26 15.343887567520142
27 17.868593454360962
28 15.37898564338684
29 17.839126110076904
30 15.43826699256897
31 18.074937343597412
32 15.289707899093628
33 17.915905714035034
34 15.29323935508728
35 17.996070623397827
36 15.20580244064331
37 17.96645188331604
38 15.280246257781982
39 16.166197776794434
40 16.996957063674927
41 16.221179008483887
42 16.96068501472473
43 16.262765645980835
44 16.94292664527893
45 16.199084997177124
46 17.660841464996338
47 15.83055830001831
48 17.06458353996277
49 15.60013461112976
50 17.72602891921997
51 15.66614818572998
52 17.479400873184204
53 15.70735788345337
54 17.52447533607483
55 15.718558549880981
56 17.536845207214355
57 15.722541093826294
58 17.448277950286865
59 15.703714847564697
60 17.518513441085815
61 15.724218845367432
62 17.50138545036316
63 15.688927412033081
64 17.535407304763794
65 15.720556020736694
66 17.47232723236084
67 15.691485166549683
68 17.713372468948364
69 15.808985710144043
70 17.590531826019287
71 15.804393291473389
72 17.417877912521362
73 15.772696256637573
74 17.437139987945557
75 15.784090518951416
76 17.328194618225098
77 15.78780484199524
78 17.419028282165527
79 15.790464878082275
80 15.695333242416382
81 17.317817449569702
82 15.78142523765564
83 17.57144069671631
84 15.527920246124268
85 17.622122049331665
86 15.557809114456177
87 17.85074472427368
88 15.321997165679932
89 17.76221203804016
90 15.434439420700073
91 17.74284029006958
92 15.484930753707886
93 17.761176347732544
94 15.571889638900757
95 15.823871612548828
96 15.252678155899048
97 17.76178002357483
98 15.245465517044067
99 16.740784645080566
100 13.389484405517578
101 13.71635389328003
102 15.810611009597778
103 13.695702314376831
104 15.510440587997437
105 13.724989652633667
106 15.504360914230347
107 13.721646308898926
108 13.705293893814087
109 15.546964168548584
110 13.739843130111694
111 15.508806228637695
112 13.734029531478882
113 15.522703170776367
114 13.734951257705688
115 13.709898948669434
116 15.45968770980835
117 13.746201515197754
118 15.504124879837036
119 13.75167179107666
test poses shape torch.Size([13, 3, 4])
0 0.0006814002990722656
torch.Size([320, 640, 3]) torch.Size([320, 640])
1 13.792439222335815
2 13.803757190704346
3 15.489777088165283
4 13.81661343574524
5 15.584924936294556
6 13.754029273986816
7 15.556752443313599
8 13.733283996582031
9 13.585991859436035
10 15.934322118759155
11 13.804465055465698
12 15.400693893432617
Saved test set
[TRAIN] Iter: 50000 Loss: 0.04110315814614296  PSNR: 16.877117156982422
[TRAIN] Iter: 50100 Loss: 0.03860139846801758  PSNR: 17.23103141784668
[TRAIN] Iter: 50200 Loss: 0.04493667185306549  PSNR: 16.424280166625977
[TRAIN] Iter: 50300 Loss: 0.03991207107901573  PSNR: 17.094858169555664
[TRAIN] Iter: 50400 Loss: 0.04341064393520355  PSNR: 16.510761260986328
[TRAIN] Iter: 50500 Loss: 0.04568902775645256  PSNR: 16.599498748779297
[TRAIN] Iter: 50600 Loss: 0.0366922989487648  PSNR: 17.602312088012695
[TRAIN] Iter: 50700 Loss: 0.045978203415870667  PSNR: 16.50075340270996
[TRAIN] Iter: 50800 Loss: 0.04228498786687851  PSNR: 16.874814987182617
[TRAIN] Iter: 50900 Loss: 0.038472190499305725  PSNR: 17.15226936340332
[TRAIN] Iter: 51000 Loss: 0.04779907315969467  PSNR: 16.234338760375977
[TRAIN] Iter: 51100 Loss: 0.03688439726829529  PSNR: 17.29144859313965
[TRAIN] Iter: 51200 Loss: 0.03400380164384842  PSNR: 17.91037368774414
[TRAIN] Iter: 51300 Loss: 0.038094982504844666  PSNR: 17.438190460205078
[TRAIN] Iter: 51400 Loss: 0.041487667709589005  PSNR: 17.01922035217285
[TRAIN] Iter: 51500 Loss: 0.03182854503393173  PSNR: 18.394147872924805
[TRAIN] Iter: 51600 Loss: 0.04437458515167236  PSNR: 16.667095184326172
[TRAIN] Iter: 51700 Loss: 0.04031053185462952  PSNR: 17.09881591796875
[TRAIN] Iter: 51800 Loss: 0.040674347430467606  PSNR: 17.0721492767334
[TRAIN] Iter: 51900 Loss: 0.04270664602518082  PSNR: 16.664215087890625
[TRAIN] Iter: 52000 Loss: 0.04552606865763664  PSNR: 16.539274215698242
[TRAIN] Iter: 52100 Loss: 0.034043412655591965  PSNR: 17.777488708496094
[TRAIN] Iter: 52200 Loss: 0.044010818004608154  PSNR: 16.703603744506836
[TRAIN] Iter: 52300 Loss: 0.03212052583694458  PSNR: 18.050411224365234
[TRAIN] Iter: 52400 Loss: 0.04547257721424103  PSNR: 16.67848014831543
[TRAIN] Iter: 52500 Loss: 0.040518417954444885  PSNR: 17.026409149169922
[TRAIN] Iter: 52600 Loss: 0.048491984605789185  PSNR: 16.343143463134766
[TRAIN] Iter: 52700 Loss: 0.043341875076293945  PSNR: 17.08768081665039
[TRAIN] Iter: 52800 Loss: 0.041834957897663116  PSNR: 16.95003318786621
[TRAIN] Iter: 52900 Loss: 0.03781197592616081  PSNR: 17.66919708251953
[TRAIN] Iter: 53000 Loss: 0.034575238823890686  PSNR: 18.20682716369629
[TRAIN] Iter: 53100 Loss: 0.038785215467214584  PSNR: 17.1345157623291
[TRAIN] Iter: 53200 Loss: 0.03899475187063217  PSNR: 17.1130428314209
[TRAIN] Iter: 53300 Loss: 0.03780888020992279  PSNR: 17.4470272064209
[TRAIN] Iter: 53400 Loss: 0.04273442551493645  PSNR: 16.774681091308594
[TRAIN] Iter: 53500 Loss: 0.036921851336956024  PSNR: 17.540176391601562
[TRAIN] Iter: 53600 Loss: 0.040358200669288635  PSNR: 17.05989646911621
[TRAIN] Iter: 53700 Loss: 0.048168256878852844  PSNR: 16.42007827758789
[TRAIN] Iter: 53800 Loss: 0.03779342770576477  PSNR: 17.345666885375977
[TRAIN] Iter: 53900 Loss: 0.03887101635336876  PSNR: 17.113203048706055
[TRAIN] Iter: 54000 Loss: 0.03333662822842598  PSNR: 17.881086349487305
[TRAIN] Iter: 54100 Loss: 0.049376606941223145  PSNR: 16.684696197509766
[TRAIN] Iter: 54200 Loss: 0.03967368230223656  PSNR: 17.048362731933594
[TRAIN] Iter: 54300 Loss: 0.035461828112602234  PSNR: 17.802297592163086
[TRAIN] Iter: 54400 Loss: 0.037386346608400345  PSNR: 17.446622848510742
[TRAIN] Iter: 54500 Loss: 0.04538601636886597  PSNR: 16.696069717407227
[TRAIN] Iter: 54600 Loss: 0.03348720073699951  PSNR: 18.045856475830078
[TRAIN] Iter: 54700 Loss: 0.04690680652856827  PSNR: 16.408557891845703
[TRAIN] Iter: 54800 Loss: 0.03673376142978668  PSNR: 17.458799362182617
[TRAIN] Iter: 54900 Loss: 0.041562288999557495  PSNR: 16.894224166870117
[TRAIN] Iter: 55000 Loss: 0.0425497367978096  PSNR: 16.816316604614258
[TRAIN] Iter: 55100 Loss: 0.038897909224033356  PSNR: 17.146635055541992
[TRAIN] Iter: 55200 Loss: 0.03380296379327774  PSNR: 17.84874725341797
[TRAIN] Iter: 55300 Loss: 0.03837807476520538  PSNR: 17.29496192932129
[TRAIN] Iter: 55400 Loss: 0.04302051663398743  PSNR: 16.80803871154785
[TRAIN] Iter: 55500 Loss: 0.03897305205464363  PSNR: 17.005760192871094
[TRAIN] Iter: 55600 Loss: 0.03615310788154602  PSNR: 17.576887130737305
[TRAIN] Iter: 55700 Loss: 0.04026877135038376  PSNR: 17.09172248840332
[TRAIN] Iter: 55800 Loss: 0.040593527257442474  PSNR: 16.896984100341797
[TRAIN] Iter: 55900 Loss: 0.040728192776441574  PSNR: 16.994029998779297
[TRAIN] Iter: 56000 Loss: 0.04377270117402077  PSNR: 16.49146270751953
[TRAIN] Iter: 56100 Loss: 0.03304428234696388  PSNR: 18.180761337280273
[TRAIN] Iter: 56200 Loss: 0.03408879041671753  PSNR: 17.943357467651367
[TRAIN] Iter: 56300 Loss: 0.03641048073768616  PSNR: 17.443498611450195
[TRAIN] Iter: 56400 Loss: 0.03726637363433838  PSNR: 17.53860092163086
[TRAIN] Iter: 56500 Loss: 0.04310569912195206  PSNR: 16.733936309814453
[TRAIN] Iter: 56600 Loss: 0.03173132613301277  PSNR: 18.065805435180664
[TRAIN] Iter: 56700 Loss: 0.040623124688863754  PSNR: 17.028606414794922
[TRAIN] Iter: 56800 Loss: 0.044455766677856445  PSNR: 16.429153442382812
[TRAIN] Iter: 56900 Loss: 0.04260535165667534  PSNR: 16.91675567626953
[TRAIN] Iter: 57000 Loss: 0.03924057260155678  PSNR: 17.394638061523438
[TRAIN] Iter: 57100 Loss: 0.03965821862220764  PSNR: 17.377647399902344
[TRAIN] Iter: 57200 Loss: 0.03901682049036026  PSNR: 17.449989318847656
[TRAIN] Iter: 57300 Loss: 0.03733929991722107  PSNR: 17.26637840270996
[TRAIN] Iter: 57400 Loss: 0.04357334226369858  PSNR: 16.67290687561035
[TRAIN] Iter: 57500 Loss: 0.029532428830862045  PSNR: 18.57549285888672
[TRAIN] Iter: 57600 Loss: 0.04114389792084694  PSNR: 16.929950714111328
[TRAIN] Iter: 57700 Loss: 0.04282383620738983  PSNR: 16.503995895385742
[TRAIN] Iter: 57800 Loss: 0.03131862357258797  PSNR: 18.17589569091797
[TRAIN] Iter: 57900 Loss: 0.03955329209566116  PSNR: 17.182355880737305
[TRAIN] Iter: 58000 Loss: 0.04463362693786621  PSNR: 16.546598434448242
[TRAIN] Iter: 58100 Loss: 0.03456680849194527  PSNR: 17.764442443847656
[TRAIN] Iter: 58200 Loss: 0.033480204641819  PSNR: 18.22600555419922
[TRAIN] Iter: 58300 Loss: 0.04001571983098984  PSNR: 17.00170135498047
[TRAIN] Iter: 58400 Loss: 0.04202455282211304  PSNR: 16.88648796081543
[TRAIN] Iter: 58500 Loss: 0.03864622116088867  PSNR: 17.117252349853516
[TRAIN] Iter: 58600 Loss: 0.03665626049041748  PSNR: 17.514427185058594
[TRAIN] Iter: 58700 Loss: 0.03908228501677513  PSNR: 17.15336036682129
[TRAIN] Iter: 58800 Loss: 0.030427739024162292  PSNR: 18.391141891479492
[TRAIN] Iter: 58900 Loss: 0.044770944863557816  PSNR: 16.834381103515625
[TRAIN] Iter: 59000 Loss: 0.045757513493299484  PSNR: 16.58342170715332
[TRAIN] Iter: 59100 Loss: 0.04245046526193619  PSNR: 16.80881118774414
[TRAIN] Iter: 59200 Loss: 0.04193790256977081  PSNR: 17.02102279663086
[TRAIN] Iter: 59300 Loss: 0.043798789381980896  PSNR: 16.97963523864746
[TRAIN] Iter: 59400 Loss: 0.04289444535970688  PSNR: 16.81024169921875
[TRAIN] Iter: 59500 Loss: 0.04007503390312195  PSNR: 17.079696655273438
[TRAIN] Iter: 59600 Loss: 0.03199738264083862  PSNR: 18.06605339050293
[TRAIN] Iter: 59700 Loss: 0.03602500259876251  PSNR: 17.61959457397461
[TRAIN] Iter: 59800 Loss: 0.03643251210451126  PSNR: 17.79474449157715
[TRAIN] Iter: 59900 Loss: 0.03092903271317482  PSNR: 18.37038803100586
Saved checkpoints at ./logs/TUT-LAB-nerf/060000.tar
[TRAIN] Iter: 60000 Loss: 0.039943188428878784  PSNR: 17.06407356262207
[TRAIN] Iter: 60100 Loss: 0.040164217352867126  PSNR: 17.119983673095703
[TRAIN] Iter: 60200 Loss: 0.03763483464717865  PSNR: 17.365859985351562
[TRAIN] Iter: 60300 Loss: 0.03773229569196701  PSNR: 17.539203643798828
[TRAIN] Iter: 60400 Loss: 0.038510993123054504  PSNR: 17.43770980834961
[TRAIN] Iter: 60500 Loss: 0.03889412432909012  PSNR: 17.397607803344727
[TRAIN] Iter: 60600 Loss: 0.04092470183968544  PSNR: 17.023094177246094
[TRAIN] Iter: 60700 Loss: 0.044163137674331665  PSNR: 16.45535659790039
[TRAIN] Iter: 60800 Loss: 0.03828110173344612  PSNR: 17.222719192504883
[TRAIN] Iter: 60900 Loss: 0.045873790979385376  PSNR: 16.493478775024414
[TRAIN] Iter: 61000 Loss: 0.040323276072740555  PSNR: 17.199708938598633
[TRAIN] Iter: 61100 Loss: 0.04223676025867462  PSNR: 16.874971389770508
[TRAIN] Iter: 61200 Loss: 0.04630725830793381  PSNR: 16.551374435424805
[TRAIN] Iter: 61300 Loss: 0.03827957063913345  PSNR: 17.339147567749023
[TRAIN] Iter: 61400 Loss: 0.03399863839149475  PSNR: 17.92009735107422
[TRAIN] Iter: 61500 Loss: 0.03875855356454849  PSNR: 17.15086555480957
[TRAIN] Iter: 61600 Loss: 0.03506704792380333  PSNR: 17.942035675048828
[TRAIN] Iter: 61700 Loss: 0.041912466287612915  PSNR: 16.90239143371582
[TRAIN] Iter: 61800 Loss: 0.037181079387664795  PSNR: 17.51616668701172
[TRAIN] Iter: 61900 Loss: 0.02630622684955597  PSNR: 19.09162712097168
[TRAIN] Iter: 62000 Loss: 0.03814927488565445  PSNR: 17.325740814208984
[TRAIN] Iter: 62100 Loss: 0.04330192878842354  PSNR: 16.712810516357422
[TRAIN] Iter: 62200 Loss: 0.03155801445245743  PSNR: 18.116880416870117
[TRAIN] Iter: 62300 Loss: 0.044657524675130844  PSNR: 16.768770217895508
[TRAIN] Iter: 62400 Loss: 0.04028516262769699  PSNR: 16.89945411682129
[TRAIN] Iter: 62500 Loss: 0.03998648747801781  PSNR: 17.0282039642334
[TRAIN] Iter: 62600 Loss: 0.040466032922267914  PSNR: 17.079923629760742
[TRAIN] Iter: 62700 Loss: 0.04496551677584648  PSNR: 16.599143981933594
[TRAIN] Iter: 62800 Loss: 0.035951778292655945  PSNR: 17.49502182006836
[TRAIN] Iter: 62900 Loss: 0.03220190852880478  PSNR: 18.152151107788086
[TRAIN] Iter: 63000 Loss: 0.033471427857875824  PSNR: 17.910737991333008
[TRAIN] Iter: 63100 Loss: 0.03856051340699196  PSNR: 17.282503128051758
[TRAIN] Iter: 63200 Loss: 0.03785938769578934  PSNR: 17.301183700561523
[TRAIN] Iter: 63300 Loss: 0.03677324950695038  PSNR: 17.533794403076172
[TRAIN] Iter: 63400 Loss: 0.03899432718753815  PSNR: 17.198619842529297
[TRAIN] Iter: 63500 Loss: 0.047209274023771286  PSNR: 16.44098472595215
[TRAIN] Iter: 63600 Loss: 0.03932494670152664  PSNR: 17.01754379272461
[TRAIN] Iter: 63700 Loss: 0.042295586317777634  PSNR: 16.906253814697266
[TRAIN] Iter: 63800 Loss: 0.042386431246995926  PSNR: 16.826478958129883
[TRAIN] Iter: 63900 Loss: 0.04439535737037659  PSNR: 16.69300651550293
[TRAIN] Iter: 64000 Loss: 0.04321876913309097  PSNR: 16.695581436157227
[TRAIN] Iter: 64100 Loss: 0.04592199623584747  PSNR: 16.522768020629883
[TRAIN] Iter: 64200 Loss: 0.045551545917987823  PSNR: 16.59364891052246
[TRAIN] Iter: 64300 Loss: 0.04192925617098808  PSNR: 16.812406539916992
[TRAIN] Iter: 64400 Loss: 0.03937524929642677  PSNR: 17.041202545166016
[TRAIN] Iter: 64500 Loss: 0.04406766593456268  PSNR: 16.835039138793945
[TRAIN] Iter: 64600 Loss: 0.03911421447992325  PSNR: 17.379844665527344
[TRAIN] Iter: 64700 Loss: 0.040136415511369705  PSNR: 17.096786499023438
[TRAIN] Iter: 64800 Loss: 0.03683587908744812  PSNR: 17.423301696777344
[TRAIN] Iter: 64900 Loss: 0.041153840720653534  PSNR: 16.97783851623535
[TRAIN] Iter: 65000 Loss: 0.03299469128251076  PSNR: 17.94761848449707
[TRAIN] Iter: 65100 Loss: 0.04116606339812279  PSNR: 16.881580352783203
[TRAIN] Iter: 65200 Loss: 0.02975674718618393  PSNR: 18.504491806030273
[TRAIN] Iter: 65300 Loss: 0.04102322459220886  PSNR: 17.018476486206055
[TRAIN] Iter: 65400 Loss: 0.04345215484499931  PSNR: 16.685890197753906
[TRAIN] Iter: 65500 Loss: 0.043739818036556244  PSNR: 16.85984992980957
[TRAIN] Iter: 65600 Loss: 0.038298286497592926  PSNR: 17.109670639038086
[TRAIN] Iter: 65700 Loss: 0.033207282423973083  PSNR: 17.77332878112793
[TRAIN] Iter: 65800 Loss: 0.03410783410072327  PSNR: 17.94894027709961
[TRAIN] Iter: 65900 Loss: 0.047796815633773804  PSNR: 16.022781372070312
[TRAIN] Iter: 66000 Loss: 0.04119439050555229  PSNR: 16.98217010498047
[TRAIN] Iter: 66100 Loss: 0.03752237930893898  PSNR: 17.410228729248047
[TRAIN] Iter: 66200 Loss: 0.029330892488360405  PSNR: 18.51596450805664
[TRAIN] Iter: 66300 Loss: 0.04307616502046585  PSNR: 16.91305160522461
[TRAIN] Iter: 66400 Loss: 0.034717246890068054  PSNR: 17.58810806274414
[TRAIN] Iter: 66500 Loss: 0.0376487597823143  PSNR: 17.37848472595215
[TRAIN] Iter: 66600 Loss: 0.05120803043246269  PSNR: 16.360912322998047
[TRAIN] Iter: 66700 Loss: 0.042869944125413895  PSNR: 17.070892333984375
[TRAIN] Iter: 66800 Loss: 0.03583354875445366  PSNR: 17.57378387451172
[TRAIN] Iter: 66900 Loss: 0.03545993193984032  PSNR: 17.729244232177734
[TRAIN] Iter: 67000 Loss: 0.04065445065498352  PSNR: 17.028085708618164
[TRAIN] Iter: 67100 Loss: 0.03752399981021881  PSNR: 17.47444725036621
[TRAIN] Iter: 67200 Loss: 0.04419218748807907  PSNR: 16.845077514648438
[TRAIN] Iter: 67300 Loss: 0.04368098825216293  PSNR: 16.822507858276367
[TRAIN] Iter: 67400 Loss: 0.038700684905052185  PSNR: 17.094058990478516
[TRAIN] Iter: 67500 Loss: 0.03790948912501335  PSNR: 17.621152877807617
[TRAIN] Iter: 67600 Loss: 0.03169083222746849  PSNR: 18.154870986938477
[TRAIN] Iter: 67700 Loss: 0.03690500557422638  PSNR: 17.30851936340332
[TRAIN] Iter: 67800 Loss: 0.03954658657312393  PSNR: 17.110410690307617
[TRAIN] Iter: 67900 Loss: 0.03544467315077782  PSNR: 17.414817810058594
[TRAIN] Iter: 68000 Loss: 0.03237361088395119  PSNR: 18.020000457763672
[TRAIN] Iter: 68100 Loss: 0.04007629305124283  PSNR: 17.277034759521484
[TRAIN] Iter: 68200 Loss: 0.045772358775138855  PSNR: 16.507173538208008
[TRAIN] Iter: 68300 Loss: 0.04321056604385376  PSNR: 16.904762268066406
[TRAIN] Iter: 68400 Loss: 0.03787597268819809  PSNR: 17.329971313476562
[TRAIN] Iter: 68500 Loss: 0.03431347385048866  PSNR: 17.694143295288086
[TRAIN] Iter: 68600 Loss: 0.042335256934165955  PSNR: 16.900955200195312
[TRAIN] Iter: 68700 Loss: 0.03346503525972366  PSNR: 17.777935028076172
[TRAIN] Iter: 68800 Loss: 0.040255554020404816  PSNR: 17.276859283447266
[TRAIN] Iter: 68900 Loss: 0.04330601915717125  PSNR: 16.79570770263672
[TRAIN] Iter: 69000 Loss: 0.03633668273687363  PSNR: 17.470428466796875
[TRAIN] Iter: 69100 Loss: 0.04055286943912506  PSNR: 17.04899024963379
[TRAIN] Iter: 69200 Loss: 0.04046248272061348  PSNR: 17.614675521850586
[TRAIN] Iter: 69300 Loss: 0.036762773990631104  PSNR: 17.347482681274414
[TRAIN] Iter: 69400 Loss: 0.03449272736907005  PSNR: 17.78325080871582
[TRAIN] Iter: 69500 Loss: 0.04777215048670769  PSNR: 16.40691375732422
[TRAIN] Iter: 69600 Loss: 0.028850169852375984  PSNR: 18.49102783203125
[TRAIN] Iter: 69700 Loss: 0.03223041817545891  PSNR: 18.018827438354492
[TRAIN] Iter: 69800 Loss: 0.043152280151844025  PSNR: 16.75149917602539
[TRAIN] Iter: 69900 Loss: 0.038428228348493576  PSNR: 17.18387794494629
Saved checkpoints at ./logs/TUT-LAB-nerf/070000.tar
[TRAIN] Iter: 70000 Loss: 0.03328488767147064  PSNR: 17.788074493408203
[TRAIN] Iter: 70100 Loss: 0.041334107518196106  PSNR: 17.271425247192383
[TRAIN] Iter: 70200 Loss: 0.04630068689584732  PSNR: 16.742389678955078
[TRAIN] Iter: 70300 Loss: 0.04624161124229431  PSNR: 16.378360748291016
[TRAIN] Iter: 70400 Loss: 0.04110800847411156  PSNR: 17.087940216064453
[TRAIN] Iter: 70500 Loss: 0.032778821885585785  PSNR: 17.9698486328125
[TRAIN] Iter: 70600 Loss: 0.037455685436725616  PSNR: 17.331506729125977
[TRAIN] Iter: 70700 Loss: 0.043173495680093765  PSNR: 16.641433715820312
[TRAIN] Iter: 70800 Loss: 0.035819124430418015  PSNR: 17.508298873901367
[TRAIN] Iter: 70900 Loss: 0.03175380825996399  PSNR: 18.11151695251465
[TRAIN] Iter: 71000 Loss: 0.03951747342944145  PSNR: 17.161087036132812
[TRAIN] Iter: 71100 Loss: 0.0336272306740284  PSNR: 18.160213470458984
[TRAIN] Iter: 71200 Loss: 0.04554826766252518  PSNR: 16.392297744750977
[TRAIN] Iter: 71300 Loss: 0.04763060063123703  PSNR: 16.296524047851562
[TRAIN] Iter: 71400 Loss: 0.037615638226270676  PSNR: 17.37657928466797
[TRAIN] Iter: 71500 Loss: 0.04194623976945877  PSNR: 16.96883201599121
[TRAIN] Iter: 71600 Loss: 0.03879556804895401  PSNR: 17.22856330871582
[TRAIN] Iter: 71700 Loss: 0.029683303087949753  PSNR: 18.522905349731445
[TRAIN] Iter: 71800 Loss: 0.03256692737340927  PSNR: 18.24143409729004
[TRAIN] Iter: 71900 Loss: 0.03767745569348335  PSNR: 17.285526275634766
[TRAIN] Iter: 72000 Loss: 0.038936592638492584  PSNR: 17.122024536132812
[TRAIN] Iter: 72100 Loss: 0.03691413998603821  PSNR: 17.572038650512695
[TRAIN] Iter: 72200 Loss: 0.04136861488223076  PSNR: 16.981592178344727
[TRAIN] Iter: 72300 Loss: 0.032035715878009796  PSNR: 18.340665817260742
[TRAIN] Iter: 72400 Loss: 0.04239178076386452  PSNR: 16.846384048461914
[TRAIN] Iter: 72500 Loss: 0.04129836708307266  PSNR: 17.094913482666016
[TRAIN] Iter: 72600 Loss: 0.044753022491931915  PSNR: 16.512985229492188
[TRAIN] Iter: 72700 Loss: 0.028983458876609802  PSNR: 18.593324661254883
[TRAIN] Iter: 72800 Loss: 0.03961274027824402  PSNR: 17.255451202392578
[TRAIN] Iter: 72900 Loss: 0.04190072417259216  PSNR: 16.94471549987793
[TRAIN] Iter: 73000 Loss: 0.04525306075811386  PSNR: 16.68475914001465
[TRAIN] Iter: 73100 Loss: 0.03904380276799202  PSNR: 17.130983352661133
[TRAIN] Iter: 73200 Loss: 0.040295958518981934  PSNR: 17.06049919128418
[TRAIN] Iter: 73300 Loss: 0.04010897874832153  PSNR: 17.216388702392578
[TRAIN] Iter: 73400 Loss: 0.04290086030960083  PSNR: 16.83574104309082
[TRAIN] Iter: 73500 Loss: 0.040607601404190063  PSNR: 17.05820655822754
[TRAIN] Iter: 73600 Loss: 0.03713436424732208  PSNR: 17.69155502319336
[TRAIN] Iter: 73700 Loss: 0.04031471163034439  PSNR: 17.08133316040039
[TRAIN] Iter: 73800 Loss: 0.0437735915184021  PSNR: 16.745615005493164
[TRAIN] Iter: 73900 Loss: 0.0377807542681694  PSNR: 17.403766632080078
[TRAIN] Iter: 74000 Loss: 0.03901274874806404  PSNR: 17.6334228515625
[TRAIN] Iter: 74100 Loss: 0.04310734197497368  PSNR: 16.9185848236084
[TRAIN] Iter: 74200 Loss: 0.042241375893354416  PSNR: 17.012847900390625
[TRAIN] Iter: 74300 Loss: 0.04081372171640396  PSNR: 17.013233184814453
[TRAIN] Iter: 74400 Loss: 0.037557780742645264  PSNR: 17.503549575805664
[TRAIN] Iter: 74500 Loss: 0.03753318265080452  PSNR: 17.490345001220703
[TRAIN] Iter: 74600 Loss: 0.044016093015670776  PSNR: 16.74565887451172
[TRAIN] Iter: 74700 Loss: 0.036474671214818954  PSNR: 17.58831214904785
[TRAIN] Iter: 74800 Loss: 0.03521249070763588  PSNR: 17.923294067382812
[TRAIN] Iter: 74900 Loss: 0.03775712102651596  PSNR: 17.21358299255371
[TRAIN] Iter: 75000 Loss: 0.04198096692562103  PSNR: 17.066905975341797
[TRAIN] Iter: 75100 Loss: 0.03170447796583176  PSNR: 18.249961853027344
[TRAIN] Iter: 75200 Loss: 0.024818619713187218  PSNR: 19.275386810302734
[TRAIN] Iter: 75300 Loss: 0.02889801375567913  PSNR: 18.48906707763672
[TRAIN] Iter: 75400 Loss: 0.045190684497356415  PSNR: 16.624488830566406
[TRAIN] Iter: 75500 Loss: 0.03525252640247345  PSNR: 17.574689865112305
[TRAIN] Iter: 75600 Loss: 0.03532994166016579  PSNR: 17.64397430419922
[TRAIN] Iter: 75700 Loss: 0.046447835862636566  PSNR: 16.639850616455078
[TRAIN] Iter: 75800 Loss: 0.041424527764320374  PSNR: 16.99848175048828
[TRAIN] Iter: 75900 Loss: 0.040938377380371094  PSNR: 17.309600830078125
[TRAIN] Iter: 76000 Loss: 0.03761600703001022  PSNR: 17.599973678588867
[TRAIN] Iter: 76100 Loss: 0.04526442289352417  PSNR: 16.513530731201172
[TRAIN] Iter: 76200 Loss: 0.03930012881755829  PSNR: 17.419538497924805
[TRAIN] Iter: 76300 Loss: 0.03990285471081734  PSNR: 17.319551467895508
[TRAIN] Iter: 76400 Loss: 0.04156948998570442  PSNR: 16.927202224731445
[TRAIN] Iter: 76500 Loss: 0.03844352066516876  PSNR: 17.168746948242188
[TRAIN] Iter: 76600 Loss: 0.03263484686613083  PSNR: 18.16861915588379
[TRAIN] Iter: 76700 Loss: 0.03809766098856926  PSNR: 17.322275161743164
[TRAIN] Iter: 76800 Loss: 0.029247133061289787  PSNR: 18.53690528869629
[TRAIN] Iter: 76900 Loss: 0.03955744206905365  PSNR: 17.08426284790039
[TRAIN] Iter: 77000 Loss: 0.02785269170999527  PSNR: 18.79682731628418
[TRAIN] Iter: 77100 Loss: 0.03329424560070038  PSNR: 17.959829330444336
[TRAIN] Iter: 77200 Loss: 0.04306827858090401  PSNR: 16.831989288330078
[TRAIN] Iter: 77300 Loss: 0.033470116555690765  PSNR: 17.88605499267578
[TRAIN] Iter: 77400 Loss: 0.045636698603630066  PSNR: 16.52102279663086
[TRAIN] Iter: 77500 Loss: 0.04036824032664299  PSNR: 16.8543758392334
[TRAIN] Iter: 77600 Loss: 0.03238686919212341  PSNR: 17.94045066833496
[TRAIN] Iter: 77700 Loss: 0.03487665206193924  PSNR: 17.679105758666992
[TRAIN] Iter: 77800 Loss: 0.0356057807803154  PSNR: 17.611949920654297
[TRAIN] Iter: 77900 Loss: 0.03669648617506027  PSNR: 17.415849685668945
[TRAIN] Iter: 78000 Loss: 0.041648369282484055  PSNR: 16.990341186523438
[TRAIN] Iter: 78100 Loss: 0.041443634778261185  PSNR: 17.03700065612793
[TRAIN] Iter: 78200 Loss: 0.03601334989070892  PSNR: 17.511959075927734
[TRAIN] Iter: 78300 Loss: 0.033596597611904144  PSNR: 18.03815269470215
[TRAIN] Iter: 78400 Loss: 0.04285978898406029  PSNR: 16.641544342041016
[TRAIN] Iter: 78500 Loss: 0.03456403315067291  PSNR: 17.71275520324707
[TRAIN] Iter: 78600 Loss: 0.0373959057033062  PSNR: 17.49671745300293
[TRAIN] Iter: 78700 Loss: 0.03449271619319916  PSNR: 17.51400375366211
[TRAIN] Iter: 78800 Loss: 0.04417795687913895  PSNR: 16.619054794311523
[TRAIN] Iter: 78900 Loss: 0.03886164724826813  PSNR: 17.171361923217773
[TRAIN] Iter: 79000 Loss: 0.028916412964463234  PSNR: 18.559764862060547
[TRAIN] Iter: 79100 Loss: 0.04625947028398514  PSNR: 16.547800064086914
[TRAIN] Iter: 79200 Loss: 0.02943941205739975  PSNR: 18.654306411743164
[TRAIN] Iter: 79300 Loss: 0.03310326486825943  PSNR: 17.978530883789062
[TRAIN] Iter: 79400 Loss: 0.04039615020155907  PSNR: 17.077037811279297
[TRAIN] Iter: 79500 Loss: 0.036663249135017395  PSNR: 17.472225189208984
[TRAIN] Iter: 79600 Loss: 0.04141863435506821  PSNR: 16.858203887939453
[TRAIN] Iter: 79700 Loss: 0.040478236973285675  PSNR: 17.182031631469727
[TRAIN] Iter: 79800 Loss: 0.0346335805952549  PSNR: 17.4688777923584
[TRAIN] Iter: 79900 Loss: 0.04402424395084381  PSNR: 16.565933227539062
Saved checkpoints at ./logs/TUT-LAB-nerf/080000.tar
[TRAIN] Iter: 80000 Loss: 0.037196867167949677  PSNR: 17.390989303588867
[TRAIN] Iter: 80100 Loss: 0.03544524684548378  PSNR: 17.631040573120117
[TRAIN] Iter: 80200 Loss: 0.035643503069877625  PSNR: 17.80706787109375
[TRAIN] Iter: 80300 Loss: 0.03931473195552826  PSNR: 17.05788230895996
[TRAIN] Iter: 80400 Loss: 0.03611791133880615  PSNR: 17.39822006225586
[TRAIN] Iter: 80500 Loss: 0.035221390426158905  PSNR: 17.87605857849121
[TRAIN] Iter: 80600 Loss: 0.037180136889219284  PSNR: 17.40363883972168
[TRAIN] Iter: 80700 Loss: 0.04031652957201004  PSNR: 17.21535873413086
[TRAIN] Iter: 80800 Loss: 0.03169327974319458  PSNR: 17.983400344848633
[TRAIN] Iter: 80900 Loss: 0.03528019040822983  PSNR: 17.743755340576172
[TRAIN] Iter: 81000 Loss: 0.040708430111408234  PSNR: 17.071449279785156
[TRAIN] Iter: 81100 Loss: 0.046710092574357986  PSNR: 16.53105354309082
[TRAIN] Iter: 81200 Loss: 0.04031059145927429  PSNR: 17.20859146118164
[TRAIN] Iter: 81300 Loss: 0.03310355544090271  PSNR: 17.955745697021484
[TRAIN] Iter: 81400 Loss: 0.032891854643821716  PSNR: 17.62193489074707
[TRAIN] Iter: 81500 Loss: 0.03568092733621597  PSNR: 17.73917579650879
[TRAIN] Iter: 81600 Loss: 0.03803730756044388  PSNR: 17.65906524658203
[TRAIN] Iter: 81700 Loss: 0.0427267849445343  PSNR: 16.97138023376465
[TRAIN] Iter: 81800 Loss: 0.03764171153306961  PSNR: 17.365497589111328
[TRAIN] Iter: 81900 Loss: 0.03280845656991005  PSNR: 17.96286392211914
[TRAIN] Iter: 82000 Loss: 0.04277816414833069  PSNR: 16.832250595092773
[TRAIN] Iter: 82100 Loss: 0.026606455445289612  PSNR: 19.059165954589844
[TRAIN] Iter: 82200 Loss: 0.030780600383877754  PSNR: 18.483789443969727
[TRAIN] Iter: 82300 Loss: 0.03879670053720474  PSNR: 17.33155632019043
[TRAIN] Iter: 82400 Loss: 0.033917877823114395  PSNR: 17.885663986206055
[TRAIN] Iter: 82500 Loss: 0.028615767136216164  PSNR: 18.699241638183594
[TRAIN] Iter: 82600 Loss: 0.03914455324411392  PSNR: 17.103456497192383
[TRAIN] Iter: 82700 Loss: 0.03568992763757706  PSNR: 17.566614151000977
[TRAIN] Iter: 82800 Loss: 0.04009518399834633  PSNR: 17.36224365234375
[TRAIN] Iter: 82900 Loss: 0.03244668245315552  PSNR: 18.135770797729492
[TRAIN] Iter: 83000 Loss: 0.03969615325331688  PSNR: 17.104639053344727
[TRAIN] Iter: 83100 Loss: 0.04163666069507599  PSNR: 16.89881134033203
[TRAIN] Iter: 83200 Loss: 0.036200836300849915  PSNR: 17.47049331665039
[TRAIN] Iter: 83300 Loss: 0.03836697340011597  PSNR: 17.383060455322266
[TRAIN] Iter: 83400 Loss: 0.03910429775714874  PSNR: 17.234189987182617
[TRAIN] Iter: 83500 Loss: 0.046540871262550354  PSNR: 16.64826774597168
[TRAIN] Iter: 83600 Loss: 0.03639623522758484  PSNR: 17.661718368530273
[TRAIN] Iter: 83700 Loss: 0.0433068573474884  PSNR: 17.074398040771484
[TRAIN] Iter: 83800 Loss: 0.03914176672697067  PSNR: 17.370813369750977
[TRAIN] Iter: 83900 Loss: 0.040814127773046494  PSNR: 17.01900291442871
[TRAIN] Iter: 84000 Loss: 0.0341121107339859  PSNR: 17.818998336791992
[TRAIN] Iter: 84100 Loss: 0.03890097141265869  PSNR: 17.13564682006836
[TRAIN] Iter: 84200 Loss: 0.030460672453045845  PSNR: 18.39785385131836
[TRAIN] Iter: 84300 Loss: 0.04024399816989899  PSNR: 17.029010772705078
[TRAIN] Iter: 84400 Loss: 0.051922447979450226  PSNR: 16.004175186157227
[TRAIN] Iter: 84500 Loss: 0.031825773417949677  PSNR: 18.188982009887695
[TRAIN] Iter: 84600 Loss: 0.03912653401494026  PSNR: 17.279367446899414
[TRAIN] Iter: 84700 Loss: 0.03597993403673172  PSNR: 17.4672908782959
[TRAIN] Iter: 84800 Loss: 0.03560955077409744  PSNR: 17.891273498535156
[TRAIN] Iter: 84900 Loss: 0.03579959273338318  PSNR: 17.62198829650879
[TRAIN] Iter: 85000 Loss: 0.034445978701114655  PSNR: 17.791053771972656
[TRAIN] Iter: 85100 Loss: 0.03841499239206314  PSNR: 17.039304733276367
[TRAIN] Iter: 85200 Loss: 0.03777772933244705  PSNR: 17.188648223876953
[TRAIN] Iter: 85300 Loss: 0.03749918192625046  PSNR: 17.70635414123535
[TRAIN] Iter: 85400 Loss: 0.03256675601005554  PSNR: 18.0380859375
[TRAIN] Iter: 85500 Loss: 0.041094738990068436  PSNR: 16.923391342163086
[TRAIN] Iter: 85600 Loss: 0.042654260993003845  PSNR: 16.876924514770508
[TRAIN] Iter: 85700 Loss: 0.038971468806266785  PSNR: 17.272960662841797
[TRAIN] Iter: 85800 Loss: 0.03215274214744568  PSNR: 17.8203125
[TRAIN] Iter: 85900 Loss: 0.028514767065644264  PSNR: 18.67120361328125
[TRAIN] Iter: 86000 Loss: 0.04358623921871185  PSNR: 17.052339553833008
[TRAIN] Iter: 86100 Loss: 0.03746438026428223  PSNR: 17.50920295715332
[TRAIN] Iter: 86200 Loss: 0.035490717738866806  PSNR: 17.622562408447266
[TRAIN] Iter: 86300 Loss: 0.03913838416337967  PSNR: 17.25029182434082
[TRAIN] Iter: 86400 Loss: 0.038266658782958984  PSNR: 17.197900772094727
[TRAIN] Iter: 86500 Loss: 0.03662627562880516  PSNR: 17.435909271240234
[TRAIN] Iter: 86600 Loss: 0.04520101472735405  PSNR: 16.886096954345703
[TRAIN] Iter: 86700 Loss: 0.029982751235365868  PSNR: 18.402482986450195
[TRAIN] Iter: 86800 Loss: 0.04525267705321312  PSNR: 16.655162811279297
[TRAIN] Iter: 86900 Loss: 0.0341290608048439  PSNR: 17.980453491210938
[TRAIN] Iter: 87000 Loss: 0.035102441906929016  PSNR: 17.763525009155273
[TRAIN] Iter: 87100 Loss: 0.03626926243305206  PSNR: 17.412790298461914
[TRAIN] Iter: 87200 Loss: 0.037266649305820465  PSNR: 17.245502471923828
[TRAIN] Iter: 87300 Loss: 0.03752892091870308  PSNR: 17.342365264892578
[TRAIN] Iter: 87400 Loss: 0.03525790199637413  PSNR: 17.48858642578125
[TRAIN] Iter: 87500 Loss: 0.034196190536022186  PSNR: 17.757476806640625
[TRAIN] Iter: 87600 Loss: 0.032609499990940094  PSNR: 18.11798667907715
[TRAIN] Iter: 87700 Loss: 0.03521612286567688  PSNR: 17.647659301757812
[TRAIN] Iter: 87800 Loss: 0.031380511820316315  PSNR: 18.283132553100586
[TRAIN] Iter: 87900 Loss: 0.03855406865477562  PSNR: 17.274625778198242
[TRAIN] Iter: 88000 Loss: 0.0397256538271904  PSNR: 17.275697708129883
[TRAIN] Iter: 88100 Loss: 0.03881964832544327  PSNR: 17.108667373657227
[TRAIN] Iter: 88200 Loss: 0.03854592517018318  PSNR: 17.382217407226562
[TRAIN] Iter: 88300 Loss: 0.036285705864429474  PSNR: 17.576583862304688
[TRAIN] Iter: 88400 Loss: 0.03295151889324188  PSNR: 18.055845260620117
[TRAIN] Iter: 88500 Loss: 0.03854212164878845  PSNR: 17.350297927856445
[TRAIN] Iter: 88600 Loss: 0.028764070942997932  PSNR: 18.391883850097656
[TRAIN] Iter: 88700 Loss: 0.031153077259659767  PSNR: 18.2929744720459
[TRAIN] Iter: 88800 Loss: 0.03531928360462189  PSNR: 17.64102554321289
[TRAIN] Iter: 88900 Loss: 0.03189026191830635  PSNR: 18.043567657470703
[TRAIN] Iter: 89000 Loss: 0.038348086178302765  PSNR: 17.08221435546875
[TRAIN] Iter: 89100 Loss: 0.03649015724658966  PSNR: 17.79924774169922
[TRAIN] Iter: 89200 Loss: 0.04266379028558731  PSNR: 17.017414093017578
[TRAIN] Iter: 89300 Loss: 0.02793925069272518  PSNR: 18.712690353393555
[TRAIN] Iter: 89400 Loss: 0.0339449942111969  PSNR: 17.947237014770508
[TRAIN] Iter: 89500 Loss: 0.03850667551159859  PSNR: 17.42306900024414
[TRAIN] Iter: 89600 Loss: 0.03548639267683029  PSNR: 17.57118034362793
[TRAIN] Iter: 89700 Loss: 0.03421171009540558  PSNR: 17.778642654418945
[TRAIN] Iter: 89800 Loss: 0.039833441376686096  PSNR: 17.14383316040039
[TRAIN] Iter: 89900 Loss: 0.03521724045276642  PSNR: 17.708585739135742
Saved checkpoints at ./logs/TUT-LAB-nerf/090000.tar
[TRAIN] Iter: 90000 Loss: 0.041616905480623245  PSNR: 16.97456932067871
[TRAIN] Iter: 90100 Loss: 0.03089110739529133  PSNR: 18.05411148071289
[TRAIN] Iter: 90200 Loss: 0.0403575524687767  PSNR: 16.937808990478516
[TRAIN] Iter: 90300 Loss: 0.040312085300683975  PSNR: 17.05852699279785
[TRAIN] Iter: 90400 Loss: 0.03508693352341652  PSNR: 17.482275009155273
[TRAIN] Iter: 90500 Loss: 0.037399135529994965  PSNR: 17.318992614746094
[TRAIN] Iter: 90600 Loss: 0.03927809000015259  PSNR: 17.28921890258789
[TRAIN] Iter: 90700 Loss: 0.038830630481243134  PSNR: 17.271745681762695
[TRAIN] Iter: 90800 Loss: 0.02609829232096672  PSNR: 19.088865280151367
[TRAIN] Iter: 90900 Loss: 0.03477008640766144  PSNR: 18.006336212158203
[TRAIN] Iter: 91000 Loss: 0.03460283577442169  PSNR: 17.5639591217041
[TRAIN] Iter: 91100 Loss: 0.039483845233917236  PSNR: 17.202333450317383
[TRAIN] Iter: 91200 Loss: 0.033752623945474625  PSNR: 17.74281120300293
[TRAIN] Iter: 91300 Loss: 0.03536130115389824  PSNR: 17.747604370117188
[TRAIN] Iter: 91400 Loss: 0.046807754784822464  PSNR: 16.462825775146484
[TRAIN] Iter: 91500 Loss: 0.031130727380514145  PSNR: 18.230440139770508
[TRAIN] Iter: 91600 Loss: 0.034001465886831284  PSNR: 17.86113929748535
[TRAIN] Iter: 91700 Loss: 0.03933302313089371  PSNR: 17.415863037109375
[TRAIN] Iter: 91800 Loss: 0.041957177221775055  PSNR: 16.941909790039062
[TRAIN] Iter: 91900 Loss: 0.03877460956573486  PSNR: 17.399940490722656
[TRAIN] Iter: 92000 Loss: 0.03810286894440651  PSNR: 17.347557067871094
[TRAIN] Iter: 92100 Loss: 0.0268915593624115  PSNR: 18.83917999267578
[TRAIN] Iter: 92200 Loss: 0.029879558831453323  PSNR: 18.395660400390625
[TRAIN] Iter: 92300 Loss: 0.03600303828716278  PSNR: 17.62062644958496
[TRAIN] Iter: 92400 Loss: 0.042372606694698334  PSNR: 17.14586067199707
[TRAIN] Iter: 92500 Loss: 0.03619400039315224  PSNR: 17.576202392578125
[TRAIN] Iter: 92600 Loss: 0.03276309370994568  PSNR: 18.01234245300293
[TRAIN] Iter: 92700 Loss: 0.03926404193043709  PSNR: 16.868391036987305
[TRAIN] Iter: 92800 Loss: 0.03214748203754425  PSNR: 18.06067657470703
[TRAIN] Iter: 92900 Loss: 0.03293067589402199  PSNR: 17.91651153564453
[TRAIN] Iter: 93000 Loss: 0.031455911695957184  PSNR: 18.290122985839844
[TRAIN] Iter: 93100 Loss: 0.036236874759197235  PSNR: 17.49526023864746
[TRAIN] Iter: 93200 Loss: 0.03652641177177429  PSNR: 17.352298736572266
[TRAIN] Iter: 93300 Loss: 0.03578173369169235  PSNR: 17.56328582763672
[TRAIN] Iter: 93400 Loss: 0.03728161379694939  PSNR: 17.245359420776367
[TRAIN] Iter: 93500 Loss: 0.04195667803287506  PSNR: 16.995664596557617
[TRAIN] Iter: 93600 Loss: 0.03997950255870819  PSNR: 17.35059928894043
[TRAIN] Iter: 93700 Loss: 0.0416293665766716  PSNR: 17.208845138549805
[TRAIN] Iter: 93800 Loss: 0.03684424236416817  PSNR: 17.579208374023438
[TRAIN] Iter: 93900 Loss: 0.027224745601415634  PSNR: 18.946481704711914
[TRAIN] Iter: 94000 Loss: 0.03899097442626953  PSNR: 17.23383140563965
[TRAIN] Iter: 94100 Loss: 0.03638036549091339  PSNR: 17.63724708557129
[TRAIN] Iter: 94200 Loss: 0.03506885841488838  PSNR: 17.90183448791504
[TRAIN] Iter: 94300 Loss: 0.03580442816019058  PSNR: 17.5330753326416
[TRAIN] Iter: 94400 Loss: 0.0362832136452198  PSNR: 17.593547821044922
[TRAIN] Iter: 94500 Loss: 0.0379023477435112  PSNR: 17.357275009155273
[TRAIN] Iter: 94600 Loss: 0.036641091108322144  PSNR: 17.472476959228516
[TRAIN] Iter: 94700 Loss: 0.02798767015337944  PSNR: 18.797658920288086
[TRAIN] Iter: 94800 Loss: 0.031369999051094055  PSNR: 18.10166358947754
[TRAIN] Iter: 94900 Loss: 0.044660262763500214  PSNR: 16.78416633605957
[TRAIN] Iter: 95000 Loss: 0.03680289536714554  PSNR: 17.477642059326172
[TRAIN] Iter: 95100 Loss: 0.03336642310023308  PSNR: 17.65542221069336
[TRAIN] Iter: 95200 Loss: 0.03681135177612305  PSNR: 17.328901290893555
[TRAIN] Iter: 95300 Loss: 0.03652364760637283  PSNR: 17.76809310913086
[TRAIN] Iter: 95400 Loss: 0.03774428367614746  PSNR: 17.367666244506836
[TRAIN] Iter: 95500 Loss: 0.040009595453739166  PSNR: 17.129037857055664
[TRAIN] Iter: 95600 Loss: 0.03801561892032623  PSNR: 17.33953094482422
[TRAIN] Iter: 95700 Loss: 0.031194334849715233  PSNR: 18.455781936645508
[TRAIN] Iter: 95800 Loss: 0.037441663444042206  PSNR: 17.560806274414062
[TRAIN] Iter: 95900 Loss: 0.034284673631191254  PSNR: 17.928146362304688
[TRAIN] Iter: 96000 Loss: 0.03339814394712448  PSNR: 17.81659507751465
[TRAIN] Iter: 96100 Loss: 0.039548665285110474  PSNR: 17.32349395751953
[TRAIN] Iter: 96200 Loss: 0.03386972099542618  PSNR: 17.95564079284668
[TRAIN] Iter: 96300 Loss: 0.0326567217707634  PSNR: 17.88277244567871
[TRAIN] Iter: 96400 Loss: 0.03805123269557953  PSNR: 17.443986892700195
[TRAIN] Iter: 96500 Loss: 0.04368536174297333  PSNR: 16.834653854370117
[TRAIN] Iter: 96600 Loss: 0.0316547155380249  PSNR: 18.236801147460938
[TRAIN] Iter: 96700 Loss: 0.04360579326748848  PSNR: 16.70610809326172
[TRAIN] Iter: 96800 Loss: 0.02856011688709259  PSNR: 18.63715934753418
[TRAIN] Iter: 96900 Loss: 0.03862087428569794  PSNR: 17.31746482849121
[TRAIN] Iter: 97000 Loss: 0.03676316887140274  PSNR: 17.38043975830078
[TRAIN] Iter: 97100 Loss: 0.030578959733247757  PSNR: 18.46258544921875
[TRAIN] Iter: 97200 Loss: 0.03666391596198082  PSNR: 17.197261810302734
[TRAIN] Iter: 97300 Loss: 0.03162439540028572  PSNR: 18.212942123413086
[TRAIN] Iter: 97400 Loss: 0.034832946956157684  PSNR: 17.727630615234375
[TRAIN] Iter: 97500 Loss: 0.03329633176326752  PSNR: 17.655757904052734
[TRAIN] Iter: 97600 Loss: 0.03252128139138222  PSNR: 18.19841766357422
[TRAIN] Iter: 97700 Loss: 0.03601698949933052  PSNR: 17.53352928161621
[TRAIN] Iter: 97800 Loss: 0.031868644058704376  PSNR: 18.09799575805664
[TRAIN] Iter: 97900 Loss: 0.04243559390306473  PSNR: 16.812150955200195
[TRAIN] Iter: 98000 Loss: 0.03685702010989189  PSNR: 17.617755889892578
[TRAIN] Iter: 98100 Loss: 0.03011939488351345  PSNR: 18.202131271362305
[TRAIN] Iter: 98200 Loss: 0.04437572509050369  PSNR: 16.815114974975586
[TRAIN] Iter: 98300 Loss: 0.03528399020433426  PSNR: 17.813413619995117
[TRAIN] Iter: 98400 Loss: 0.028216473758220673  PSNR: 18.708358764648438
[TRAIN] Iter: 98500 Loss: 0.033547062426805496  PSNR: 17.65226173400879
[TRAIN] Iter: 98600 Loss: 0.0351896733045578  PSNR: 17.562150955200195
[TRAIN] Iter: 98700 Loss: 0.034411001950502396  PSNR: 17.5455265045166
[TRAIN] Iter: 98800 Loss: 0.03309344872832298  PSNR: 18.010019302368164
[TRAIN] Iter: 98900 Loss: 0.0318579226732254  PSNR: 18.4083251953125
[TRAIN] Iter: 99000 Loss: 0.03311295434832573  PSNR: 18.14516830444336
[TRAIN] Iter: 99100 Loss: 0.027182888239622116  PSNR: 18.78668785095215
[TRAIN] Iter: 99200 Loss: 0.02885325998067856  PSNR: 18.638376235961914
[TRAIN] Iter: 99300 Loss: 0.04302072525024414  PSNR: 16.56645393371582
[TRAIN] Iter: 99400 Loss: 0.03689466044306755  PSNR: 17.466657638549805
[TRAIN] Iter: 99500 Loss: 0.036513201892375946  PSNR: 17.33041000366211
[TRAIN] Iter: 99600 Loss: 0.037675388157367706  PSNR: 17.560970306396484
[TRAIN] Iter: 99700 Loss: 0.04160618409514427  PSNR: 16.96319007873535
[TRAIN] Iter: 99800 Loss: 0.03690353408455849  PSNR: 17.427871704101562
[TRAIN] Iter: 99900 Loss: 0.028865616768598557  PSNR: 18.518312454223633
Saved checkpoints at ./logs/TUT-LAB-nerf/100000.tar
0 0.00046825408935546875
torch.Size([320, 640, 3]) torch.Size([320, 640])
1 17.532949686050415
2 15.627327680587769
3 17.55803871154785
4 15.693058490753174
5 15.673289775848389
6 17.566089630126953
7 15.684720039367676
8 17.56165385246277
9 15.575961351394653
10 17.53664755821228
11 15.645863771438599
12 17.7087504863739
13 15.82485318183899
14 17.593751192092896
15 15.672482252120972
16 17.482547998428345
17 15.649039268493652
18 17.524070739746094
19 15.666050672531128
20 17.497594356536865
21 15.529090166091919
22 17.73161482810974
23 15.641734600067139
24 17.55508828163147
25 15.659303665161133
26 17.57938814163208
27 15.53469204902649
28 17.60375666618347
29 15.631759405136108
30 17.625663995742798
31 15.638893127441406
32 17.593127965927124
33 15.581503868103027
34 17.686671257019043
35 15.550347089767456
36 17.569262981414795
37 15.610068082809448
38 15.608267545700073
39 17.604215383529663
40 15.552590608596802
41 17.62901782989502
42 15.571808099746704
43 17.71013832092285
44 15.312673568725586
45 18.18236756324768
46 15.000513315200806
47 18.314510345458984
48 14.948552370071411
49 18.178330421447754
50 14.920389890670776
51 18.48229670524597
52 15.07394814491272
53 18.125365018844604
54 15.084960460662842
55 18.19714879989624
56 15.087776899337769
57 18.036630153656006
58 15.131123542785645
59 18.07476568222046
60 15.209726572036743
61 17.99044156074524
62 15.245153427124023
63 15.711899280548096
64 17.503877639770508
65 15.535316705703735
66 17.71647596359253
67 15.35163426399231
68 18.128803491592407
69 15.519076347351074
70 17.723719120025635
71 15.540938377380371
72 17.661807537078857
73 15.524285793304443
74 17.676244974136353
75 15.509491443634033
76 17.708961248397827
77 15.48694109916687
78 17.743892431259155
79 15.533572673797607
80 17.704182147979736
81 15.5364408493042
82 11.511799097061157
83 17.690715551376343
84 15.51636004447937
85 17.74193525314331
86 15.531833410263062
87 17.70997905731201
88 15.522226333618164
89 17.703832149505615
90 15.449269771575928
91 17.703330993652344
92 15.556396007537842
93 17.70128059387207
94 15.27595067024231
95 18.07228183746338
96 15.657805681228638
97 17.508556842803955
98 15.413514137268066
99 17.768285751342773
100 15.482692003250122
101 17.7489812374115
102 15.474254369735718
103 17.784595489501953
104 15.634520292282104
105 17.83068823814392
106 15.465784072875977
107 17.80760407447815
108 15.342031955718994
109 17.76897621154785
110 15.433351993560791
111 15.662627458572388
112 17.63947892189026
113 15.595309495925903
114 17.769824504852295
115 15.386713981628418
116 18.004504203796387
117 15.38630747795105
118 17.750368118286133
119 15.046550273895264
Done, saving (120, 320, 640, 3) (120, 320, 640)
extras:{'raw': tensor([[[ 1.8717e-01, -2.3032e-02, -2.1277e-01,  2.5222e+01],
         [ 3.7107e-01,  1.1344e-01, -1.2283e-01,  5.9045e+00],
         [ 6.5889e-01,  4.4753e-01,  1.0911e-01, -4.6639e+00],
         ...,
         [ 1.5951e+00, -1.2610e-01, -4.0611e+00,  3.6398e+02],
         [ 1.4311e+00, -9.5652e-02, -3.6295e+00,  3.7180e+02],
         [ 1.5777e+00, -3.0384e-02, -3.7258e+00,  3.7424e+02]],

        [[ 1.5212e-01, -2.9848e-02, -2.1330e-01, -5.7082e+00],
         [-2.9543e-01, -4.0131e-01, -3.6417e-01, -9.8823e+00],
         [-4.9671e-01, -6.8179e-01, -8.5113e-01, -1.5228e+01],
         ...,
         [ 3.3582e+00,  7.8937e-01,  4.1123e-01,  4.1385e+02],
         [ 5.8837e+00,  2.5487e+00,  1.7182e+00,  4.0763e+02],
         [ 7.2390e+00,  3.1230e+00, -9.4820e-02,  4.0010e+02]],

        [[ 1.8813e-01, -1.1289e-02, -1.9902e-01,  1.3250e+01],
         [ 9.4446e-02, -8.7058e-02, -2.3511e-01,  1.1140e+01],
         [ 9.5820e-02, -8.5658e-02, -2.3342e-01,  1.1134e+01],
         ...,
         [ 5.0398e-01,  9.3519e-02, -1.6426e+00,  3.7601e+02],
         [ 3.2615e-01,  4.2366e-02, -1.4265e+00,  3.8846e+02],
         [ 4.6365e-01,  5.2002e-02, -1.6045e+00,  3.8657e+02]],

        ...,

        [[ 8.2466e-02, -1.9450e-01, -3.6810e-01,  1.0539e+01],
         [ 2.1473e-01, -1.4055e-01, -3.0574e-01,  1.2184e+01],
         [ 2.9121e-01, -7.4607e-02, -2.3327e-01,  1.2222e+01],
         ...,
         [-2.0847e+00, -4.8936e+00, -6.4970e+00,  5.9904e+01],
         [-2.7179e+00, -5.4411e+00, -7.1953e+00,  6.3053e+01],
         [-2.5798e+00, -5.5095e+00, -7.5979e+00,  6.0268e+01]],

        [[-1.5093e+00, -1.4223e+00, -1.1760e+00, -5.5166e+00],
         [-2.0925e+00, -2.0198e+00, -1.7619e+00,  8.2018e-01],
         [-1.1405e+00, -1.2103e+00, -1.0625e+00, -1.6177e+01],
         ...,
         [ 2.9155e+00,  1.1498e+00,  7.1769e-01,  1.6020e+02],
         [ 2.8280e+00,  8.7327e-01,  1.5711e-01,  1.7280e+02],
         [-6.6882e-01, -2.2579e+00, -1.6456e+00,  1.6694e+02]],

        [[-1.4777e-01, -3.6291e-01, -2.0156e-01,  4.9995e+00],
         [-1.3421e-01, -4.2354e-01, -4.9673e-01,  9.3522e+00],
         [-1.9269e-01, -4.6600e-01, -5.2711e-01,  9.3460e+00],
         ...,
         [-6.6660e+00, -7.1139e+00, -2.3082e+00,  3.6570e+01],
         [-6.1282e+00, -6.7045e+00, -1.6684e+00,  3.1791e+01],
         [-6.2736e+00, -6.6539e+00, -1.4698e+00,  3.2966e+01]]],
       grad_fn=<ReshapeAliasBackward0>), 'rgb0': tensor([[0.5406, 0.5032, 0.4648],
        [0.7095, 0.6539, 0.5924],
        [0.5530, 0.5065, 0.4546],
        ...,
        [0.5178, 0.4476, 0.4182],
        [0.1156, 0.1335, 0.1687],
        [0.5249, 0.4487, 0.4183]], grad_fn=<ReshapeAliasBackward0>), 'disp0': tensor([ 17.0733,  18.3705,  23.9282,  ..., 130.4404,  26.7452, 132.5007],
       grad_fn=<ReshapeAliasBackward0>), 'acc0': tensor([1., 1., 1.,  ..., 1., 1., 1.], grad_fn=<ReshapeAliasBackward0>), 'z_std': tensor([0.0023, 0.0029, 0.0715,  ..., 0.0051, 0.0045, 0.0028])}
0 0.0004858970642089844
torch.Size([320, 640, 3]) torch.Size([320, 640])
1 14.992498874664307
2 18.077563047409058
3 14.939458131790161
4 18.536742687225342
5 15.13347864151001
6 17.893585920333862
7 14.85893201828003
8 18.310144424438477
9 15.064405918121338
10 18.12691593170166
11 15.146762132644653
12 18.234843969345093
13 15.297863245010376
14 17.898885011672974
15 15.512135028839111
16 17.700854539871216
17 15.532712459564209
18 15.499011278152466
19 17.790517568588257
20 15.501307964324951
21 17.64694571495056
22 15.52814769744873
23 17.693394660949707
24 15.56267786026001
25 17.653945446014404
26 15.502261400222778
27 17.69040083885193
28 15.528557062149048
29 17.7135066986084
30 15.561925411224365
31 17.748559951782227
32 15.541164875030518
33 17.63708233833313
34 15.52981185913086
35 17.688928842544556
36 15.45926570892334
37 17.77383041381836
38 15.465273141860962
39 17.75213360786438
40 15.42275619506836
41 17.80259394645691
42 15.618415832519531
43 17.68644428253174
44 15.499835014343262
45 15.51552414894104
46 17.64485263824463
47 15.508553743362427
48 17.710196256637573
49 15.531108617782593
50 17.745517015457153
51 15.658894538879395
52 17.92918062210083
53 15.426108121871948
54 17.787081003189087
55 15.447941064834595
56 17.814080238342285
57 15.463648557662964
58 17.812588453292847
59 15.44601845741272
60 17.773849487304688
61 15.473115682601929
62 17.772928714752197
63 15.396003723144531
64 17.860190629959106
65 15.405120611190796
66 17.82663059234619
67 15.381438255310059
68 17.843445777893066
69 15.41585636138916
70 17.770456075668335
71 15.759290218353271
72 15.346209049224854
73 17.566967487335205
74 15.236567735671997
75 18.369079113006592
76 15.104851245880127
77 17.949100971221924
78 15.055708885192871
79 16.55027461051941
80 15.01680588722229
81 17.752540588378906
82 14.850777387619019
83 17.180692195892334
84 13.200776815414429
85 13.53624677658081
86 15.905576467514038
87 13.347982168197632
88 16.1589617729187
89 13.082589149475098
90 16.317389249801636
91 13.233214139938354
92 13.42223596572876
93 16.03330135345459
94 13.226159572601318
95 16.342610359191895
96 13.044877767562866
97 13.529000520706177
98 16.146390199661255
99 13.531441926956177
100 15.767584800720215
101 13.576581478118896
102 15.745403289794922
103 13.55112910270691
104 13.539323091506958
105 15.761619329452515
106 13.544142961502075
107 15.785691976547241
108 13.536644697189331
109 15.791723251342773
110 13.574259519577026
111 13.513826608657837
112 15.732643127441406
113 13.531809568405151
114 15.767941951751709
115 13.56844687461853
116 13.538959980010986
117 15.845380306243896
118 13.529176235198975
119 15.782153606414795
test poses shape torch.Size([13, 3, 4])
0 0.00055694580078125
torch.Size([320, 640, 3]) torch.Size([320, 640])
1 15.818192720413208
2 13.619869709014893
3 13.537870645523071
4 15.84590220451355
5 13.576557159423828
6 15.82109022140503
7 13.56157636642456
8 15.899170398712158
9 13.55493974685669
10 13.539599418640137
11 15.980300426483154
12 13.631001949310303
Saved test set
[TRAIN] Iter: 100000 Loss: 0.03288066387176514  PSNR: 17.985801696777344
[TRAIN] Iter: 100100 Loss: 0.02826649509370327  PSNR: 18.792949676513672
[TRAIN] Iter: 100200 Loss: 0.032589104026556015  PSNR: 17.975473403930664
[TRAIN] Iter: 100300 Loss: 0.04402291402220726  PSNR: 16.862024307250977
[TRAIN] Iter: 100400 Loss: 0.03249847888946533  PSNR: 17.692852020263672
[TRAIN] Iter: 100500 Loss: 0.0373232401907444  PSNR: 17.415218353271484
[TRAIN] Iter: 100600 Loss: 0.0353068970143795  PSNR: 17.738697052001953
[TRAIN] Iter: 100700 Loss: 0.034384287893772125  PSNR: 17.955053329467773
[TRAIN] Iter: 100800 Loss: 0.036867015063762665  PSNR: 17.633447647094727
[TRAIN] Iter: 100900 Loss: 0.031671881675720215  PSNR: 18.24048614501953
[TRAIN] Iter: 101000 Loss: 0.04188859090209007  PSNR: 17.064472198486328
[TRAIN] Iter: 101100 Loss: 0.031998179852962494  PSNR: 17.955400466918945
[TRAIN] Iter: 101200 Loss: 0.03520345687866211  PSNR: 17.868925094604492
[TRAIN] Iter: 101300 Loss: 0.03930830955505371  PSNR: 17.29750633239746
[TRAIN] Iter: 101400 Loss: 0.040028076618909836  PSNR: 17.188560485839844
[TRAIN] Iter: 101500 Loss: 0.03681131452322006  PSNR: 17.501008987426758
[TRAIN] Iter: 101600 Loss: 0.03222452849149704  PSNR: 18.02716636657715
[TRAIN] Iter: 101700 Loss: 0.03639984875917435  PSNR: 17.81117057800293
[TRAIN] Iter: 101800 Loss: 0.0381314679980278  PSNR: 17.278148651123047
[TRAIN] Iter: 101900 Loss: 0.026872551068663597  PSNR: 18.999635696411133
[TRAIN] Iter: 102000 Loss: 0.044039275497198105  PSNR: 16.70361328125
[TRAIN] Iter: 102100 Loss: 0.037715718150138855  PSNR: 17.580629348754883
[TRAIN] Iter: 102200 Loss: 0.02916151098906994  PSNR: 18.456005096435547
[TRAIN] Iter: 102300 Loss: 0.03675796836614609  PSNR: 17.681367874145508
[TRAIN] Iter: 102400 Loss: 0.03139998018741608  PSNR: 18.20191192626953
[TRAIN] Iter: 102500 Loss: 0.03687245771288872  PSNR: 17.440784454345703
[TRAIN] Iter: 102600 Loss: 0.0378730446100235  PSNR: 17.37405776977539
[TRAIN] Iter: 102700 Loss: 0.035451434552669525  PSNR: 17.5133113861084
[TRAIN] Iter: 102800 Loss: 0.034110091626644135  PSNR: 17.76043128967285
[TRAIN] Iter: 102900 Loss: 0.04029001295566559  PSNR: 17.166296005249023
[TRAIN] Iter: 103000 Loss: 0.03799830377101898  PSNR: 17.29229736328125
[TRAIN] Iter: 103100 Loss: 0.03172999620437622  PSNR: 18.24443244934082
[TRAIN] Iter: 103200 Loss: 0.04482341185212135  PSNR: 16.795034408569336
[TRAIN] Iter: 103300 Loss: 0.0338718444108963  PSNR: 17.791364669799805
[TRAIN] Iter: 103400 Loss: 0.03681980073451996  PSNR: 17.591184616088867
[TRAIN] Iter: 103500 Loss: 0.03951406851410866  PSNR: 17.16558265686035
[TRAIN] Iter: 103600 Loss: 0.03187219053506851  PSNR: 18.141233444213867
[TRAIN] Iter: 103700 Loss: 0.046528927981853485  PSNR: 16.605175018310547
[TRAIN] Iter: 103800 Loss: 0.04594888910651207  PSNR: 16.592561721801758
[TRAIN] Iter: 103900 Loss: 0.041005395352840424  PSNR: 16.955480575561523
[TRAIN] Iter: 104000 Loss: 0.037728022783994675  PSNR: 17.33501625061035
[TRAIN] Iter: 104100 Loss: 0.029453299939632416  PSNR: 18.474504470825195
[TRAIN] Iter: 104200 Loss: 0.036242663860321045  PSNR: 17.87647819519043
[TRAIN] Iter: 104300 Loss: 0.03297395259141922  PSNR: 17.885953903198242
[TRAIN] Iter: 104400 Loss: 0.036639075726270676  PSNR: 17.599220275878906
[TRAIN] Iter: 104500 Loss: 0.027551330626010895  PSNR: 18.851818084716797
[TRAIN] Iter: 104600 Loss: 0.03301136940717697  PSNR: 17.979825973510742
[TRAIN] Iter: 104700 Loss: 0.0363466702401638  PSNR: 17.7082462310791
[TRAIN] Iter: 104800 Loss: 0.0283057764172554  PSNR: 18.66583251953125
[TRAIN] Iter: 104900 Loss: 0.03663044422864914  PSNR: 17.906808853149414
[TRAIN] Iter: 105000 Loss: 0.03567543998360634  PSNR: 17.942718505859375
[TRAIN] Iter: 105100 Loss: 0.03327732905745506  PSNR: 18.094650268554688
[TRAIN] Iter: 105200 Loss: 0.03543446585536003  PSNR: 18.12942123413086
[TRAIN] Iter: 105300 Loss: 0.032684504985809326  PSNR: 18.115819931030273
[TRAIN] Iter: 105400 Loss: 0.038059014827013016  PSNR: 17.32819175720215
[TRAIN] Iter: 105500 Loss: 0.03846961259841919  PSNR: 17.303327560424805
[TRAIN] Iter: 105600 Loss: 0.0393666997551918  PSNR: 17.249855041503906
[TRAIN] Iter: 105700 Loss: 0.03303002566099167  PSNR: 17.927438735961914
[TRAIN] Iter: 105800 Loss: 0.03386949747800827  PSNR: 17.88064956665039
[TRAIN] Iter: 105900 Loss: 0.035381779074668884  PSNR: 17.553468704223633
[TRAIN] Iter: 106000 Loss: 0.03711222857236862  PSNR: 17.33761215209961
[TRAIN] Iter: 106100 Loss: 0.03868093341588974  PSNR: 17.321670532226562
[TRAIN] Iter: 106200 Loss: 0.02972763031721115  PSNR: 18.597444534301758
[TRAIN] Iter: 106300 Loss: 0.036740824580192566  PSNR: 17.66739845275879
[TRAIN] Iter: 106400 Loss: 0.034840770065784454  PSNR: 17.68164825439453
[TRAIN] Iter: 106500 Loss: 0.03323148563504219  PSNR: 17.600934982299805
[TRAIN] Iter: 106600 Loss: 0.03317476063966751  PSNR: 17.822568893432617
[TRAIN] Iter: 106700 Loss: 0.0450635626912117  PSNR: 16.66488265991211
[TRAIN] Iter: 106800 Loss: 0.035315390676259995  PSNR: 17.632556915283203
[TRAIN] Iter: 106900 Loss: 0.03666907921433449  PSNR: 17.525203704833984
[TRAIN] Iter: 107000 Loss: 0.024715179577469826  PSNR: 19.213359832763672
[TRAIN] Iter: 107100 Loss: 0.031828708946704865  PSNR: 18.35976219177246
[TRAIN] Iter: 107200 Loss: 0.0360187366604805  PSNR: 17.603925704956055
[TRAIN] Iter: 107300 Loss: 0.029160354286432266  PSNR: 18.70130157470703
[TRAIN] Iter: 107400 Loss: 0.02923603355884552  PSNR: 18.759117126464844
[TRAIN] Iter: 107500 Loss: 0.036104850471019745  PSNR: 17.67645263671875
[TRAIN] Iter: 107600 Loss: 0.03867758810520172  PSNR: 17.216962814331055
[TRAIN] Iter: 107700 Loss: 0.042494699358940125  PSNR: 16.944429397583008
[TRAIN] Iter: 107800 Loss: 0.03320321440696716  PSNR: 17.9916934967041
[TRAIN] Iter: 107900 Loss: 0.031622666865587234  PSNR: 18.211668014526367
[TRAIN] Iter: 108000 Loss: 0.026546457782387733  PSNR: 18.987947463989258
[TRAIN] Iter: 108100 Loss: 0.03749867528676987  PSNR: 17.490753173828125
[TRAIN] Iter: 108200 Loss: 0.03138574957847595  PSNR: 18.25904083251953
[TRAIN] Iter: 108300 Loss: 0.03724375367164612  PSNR: 17.784996032714844
[TRAIN] Iter: 108400 Loss: 0.0351201631128788  PSNR: 17.54346466064453
[TRAIN] Iter: 108500 Loss: 0.032046735286712646  PSNR: 17.99528694152832
[TRAIN] Iter: 108600 Loss: 0.031846337020397186  PSNR: 17.94145393371582
[TRAIN] Iter: 108700 Loss: 0.03493762016296387  PSNR: 17.637399673461914
[TRAIN] Iter: 108800 Loss: 0.0353446900844574  PSNR: 17.84745216369629
[TRAIN] Iter: 108900 Loss: 0.03575539588928223  PSNR: 17.53960609436035
[TRAIN] Iter: 109000 Loss: 0.04230885952711105  PSNR: 16.81672477722168
[TRAIN] Iter: 109100 Loss: 0.03858829289674759  PSNR: 17.27346420288086
[TRAIN] Iter: 109200 Loss: 0.04076424241065979  PSNR: 17.16515350341797
[TRAIN] Iter: 109300 Loss: 0.03248090296983719  PSNR: 18.236129760742188
[TRAIN] Iter: 109400 Loss: 0.025494802743196487  PSNR: 19.323514938354492
[TRAIN] Iter: 109500 Loss: 0.030025342479348183  PSNR: 18.33806610107422
[TRAIN] Iter: 109600 Loss: 0.028329331427812576  PSNR: 18.67026710510254
[TRAIN] Iter: 109700 Loss: 0.03698515146970749  PSNR: 17.427650451660156
[TRAIN] Iter: 109800 Loss: 0.033581025898456573  PSNR: 17.622081756591797
[TRAIN] Iter: 109900 Loss: 0.03306380659341812  PSNR: 18.135021209716797
Saved checkpoints at ./logs/TUT-LAB-nerf/110000.tar
[TRAIN] Iter: 110000 Loss: 0.03622714430093765  PSNR: 17.534889221191406
[TRAIN] Iter: 110100 Loss: 0.03480582311749458  PSNR: 17.809215545654297
[TRAIN] Iter: 110200 Loss: 0.02878200076520443  PSNR: 18.646055221557617
[TRAIN] Iter: 110300 Loss: 0.03139151632785797  PSNR: 18.512977600097656
[TRAIN] Iter: 110400 Loss: 0.035366207361221313  PSNR: 17.657560348510742
[TRAIN] Iter: 110500 Loss: 0.04162238538265228  PSNR: 17.052701950073242
[TRAIN] Iter: 110600 Loss: 0.03222373127937317  PSNR: 18.045907974243164
[TRAIN] Iter: 110700 Loss: 0.03509113937616348  PSNR: 17.70143699645996
[TRAIN] Iter: 110800 Loss: 0.025608517229557037  PSNR: 19.15410041809082
[TRAIN] Iter: 110900 Loss: 0.035632744431495667  PSNR: 17.64600372314453
[TRAIN] Iter: 111000 Loss: 0.04168812930583954  PSNR: 17.08614730834961
[TRAIN] Iter: 111100 Loss: 0.03082515299320221  PSNR: 18.087902069091797
[TRAIN] Iter: 111200 Loss: 0.028292763978242874  PSNR: 18.75695037841797
[TRAIN] Iter: 111300 Loss: 0.0286637581884861  PSNR: 18.686290740966797
[TRAIN] Iter: 111400 Loss: 0.019506068900227547  PSNR: 20.320053100585938
[TRAIN] Iter: 111500 Loss: 0.03519018739461899  PSNR: 17.83142852783203
[TRAIN] Iter: 111600 Loss: 0.026247449219226837  PSNR: 19.212017059326172
[TRAIN] Iter: 111700 Loss: 0.03907513618469238  PSNR: 17.228750228881836
[TRAIN] Iter: 111800 Loss: 0.03557353466749191  PSNR: 17.687149047851562
[TRAIN] Iter: 111900 Loss: 0.042296573519706726  PSNR: 16.892250061035156
[TRAIN] Iter: 112000 Loss: 0.03437691181898117  PSNR: 17.438709259033203
[TRAIN] Iter: 112100 Loss: 0.03862626105546951  PSNR: 17.37129783630371
[TRAIN] Iter: 112200 Loss: 0.025393154472112656  PSNR: 19.217355728149414
[TRAIN] Iter: 112300 Loss: 0.03645719960331917  PSNR: 17.691835403442383
[TRAIN] Iter: 112400 Loss: 0.03750289976596832  PSNR: 17.5068359375
[TRAIN] Iter: 112500 Loss: 0.03512771427631378  PSNR: 18.01125144958496
[TRAIN] Iter: 112600 Loss: 0.03905096277594566  PSNR: 17.286895751953125
[TRAIN] Iter: 112700 Loss: 0.039147522300481796  PSNR: 17.417146682739258
[TRAIN] Iter: 112800 Loss: 0.023560259491205215  PSNR: 19.488262176513672
[TRAIN] Iter: 112900 Loss: 0.038271788507699966  PSNR: 17.304859161376953
[TRAIN] Iter: 113000 Loss: 0.03325219824910164  PSNR: 17.943832397460938
[TRAIN] Iter: 113100 Loss: 0.033431217074394226  PSNR: 18.072763442993164
[TRAIN] Iter: 113200 Loss: 0.03450872004032135  PSNR: 17.75058364868164
[TRAIN] Iter: 113300 Loss: 0.037574127316474915  PSNR: 17.376344680786133
[TRAIN] Iter: 113400 Loss: 0.03634801506996155  PSNR: 17.59649085998535
[TRAIN] Iter: 113500 Loss: 0.03550351783633232  PSNR: 17.662851333618164
[TRAIN] Iter: 113600 Loss: 0.024327650666236877  PSNR: 19.348861694335938
[TRAIN] Iter: 113700 Loss: 0.0391266755759716  PSNR: 17.433908462524414
[TRAIN] Iter: 113800 Loss: 0.03413523733615875  PSNR: 17.93686294555664
[TRAIN] Iter: 113900 Loss: 0.03712795302271843  PSNR: 17.375356674194336
[TRAIN] Iter: 114000 Loss: 0.02811186946928501  PSNR: 18.776073455810547
[TRAIN] Iter: 114100 Loss: 0.03771872818470001  PSNR: 17.42228126525879
[TRAIN] Iter: 114200 Loss: 0.029926368966698647  PSNR: 18.400449752807617
[TRAIN] Iter: 114300 Loss: 0.036139506846666336  PSNR: 17.59255027770996
[TRAIN] Iter: 114400 Loss: 0.03354218602180481  PSNR: 17.774574279785156
[TRAIN] Iter: 114500 Loss: 0.03717949986457825  PSNR: 17.487369537353516
[TRAIN] Iter: 114600 Loss: 0.03658901900053024  PSNR: 17.442073822021484
[TRAIN] Iter: 114700 Loss: 0.03357701376080513  PSNR: 17.95615005493164
[TRAIN] Iter: 114800 Loss: 0.03687891364097595  PSNR: 17.580223083496094
[TRAIN] Iter: 114900 Loss: 0.03632640838623047  PSNR: 17.585187911987305
[TRAIN] Iter: 115000 Loss: 0.02945738285779953  PSNR: 18.65061378479004
[TRAIN] Iter: 115100 Loss: 0.03734232485294342  PSNR: 17.483470916748047
[TRAIN] Iter: 115200 Loss: 0.03276597335934639  PSNR: 17.96183967590332
[TRAIN] Iter: 115300 Loss: 0.03338536620140076  PSNR: 17.97037696838379
[TRAIN] Iter: 115400 Loss: 0.026805568486452103  PSNR: 18.860443115234375
[TRAIN] Iter: 115500 Loss: 0.029910828918218613  PSNR: 18.734893798828125
[TRAIN] Iter: 115600 Loss: 0.030593179166316986  PSNR: 18.38762855529785
[TRAIN] Iter: 115700 Loss: 0.03560114651918411  PSNR: 17.83902931213379
[TRAIN] Iter: 115800 Loss: 0.037748485803604126  PSNR: 17.417892456054688
[TRAIN] Iter: 115900 Loss: 0.042247921228408813  PSNR: 16.914493560791016
[TRAIN] Iter: 116000 Loss: 0.02874380350112915  PSNR: 18.608135223388672
[TRAIN] Iter: 116100 Loss: 0.035366691648960114  PSNR: 17.92676544189453
[TRAIN] Iter: 116200 Loss: 0.03302100673317909  PSNR: 17.85129737854004
[TRAIN] Iter: 116300 Loss: 0.03015296161174774  PSNR: 18.199630737304688
[TRAIN] Iter: 116400 Loss: 0.031869515776634216  PSNR: 18.26302146911621
[TRAIN] Iter: 116500 Loss: 0.02517729625105858  PSNR: 19.14939308166504
[TRAIN] Iter: 116600 Loss: 0.042165279388427734  PSNR: 16.83951187133789
[TRAIN] Iter: 116700 Loss: 0.03414960578083992  PSNR: 18.074094772338867
[TRAIN] Iter: 116800 Loss: 0.03126251697540283  PSNR: 18.316377639770508
[TRAIN] Iter: 116900 Loss: 0.030263811349868774  PSNR: 18.41183090209961
[TRAIN] Iter: 117000 Loss: 0.03406012803316116  PSNR: 17.833040237426758
[TRAIN] Iter: 117100 Loss: 0.033570654690265656  PSNR: 17.643381118774414
[TRAIN] Iter: 117200 Loss: 0.03673598915338516  PSNR: 17.29969596862793
[TRAIN] Iter: 117300 Loss: 0.03412613272666931  PSNR: 17.81634521484375
[TRAIN] Iter: 117400 Loss: 0.029781155288219452  PSNR: 18.24669075012207
[TRAIN] Iter: 117500 Loss: 0.03944284841418266  PSNR: 17.14323616027832
[TRAIN] Iter: 117600 Loss: 0.03595024719834328  PSNR: 17.621925354003906
[TRAIN] Iter: 117700 Loss: 0.038899682462215424  PSNR: 17.177642822265625
[TRAIN] Iter: 117800 Loss: 0.03403327241539955  PSNR: 17.756406784057617
[TRAIN] Iter: 117900 Loss: 0.0270150825381279  PSNR: 18.707592010498047
[TRAIN] Iter: 118000 Loss: 0.03216157108545303  PSNR: 18.36217498779297
[TRAIN] Iter: 118100 Loss: 0.03864344209432602  PSNR: 17.38128089904785
[TRAIN] Iter: 118200 Loss: 0.031240561977028847  PSNR: 18.335033416748047
[TRAIN] Iter: 118300 Loss: 0.034165672957897186  PSNR: 17.899974822998047
[TRAIN] Iter: 118400 Loss: 0.033471040427684784  PSNR: 17.65033721923828
[TRAIN] Iter: 118500 Loss: 0.03677625209093094  PSNR: 17.610485076904297
[TRAIN] Iter: 118600 Loss: 0.036535367369651794  PSNR: 17.493553161621094
[TRAIN] Iter: 118700 Loss: 0.03445039317011833  PSNR: 17.82363510131836
[TRAIN] Iter: 118800 Loss: 0.03409063071012497  PSNR: 17.831214904785156
[TRAIN] Iter: 118900 Loss: 0.024851590394973755  PSNR: 19.310277938842773
[TRAIN] Iter: 119000 Loss: 0.041521474719047546  PSNR: 17.328550338745117
[TRAIN] Iter: 119100 Loss: 0.02731965482234955  PSNR: 18.949583053588867
[TRAIN] Iter: 119200 Loss: 0.03287803381681442  PSNR: 17.943798065185547
[TRAIN] Iter: 119300 Loss: 0.03455990180373192  PSNR: 17.890628814697266
[TRAIN] Iter: 119400 Loss: 0.025101805105805397  PSNR: 19.091411590576172
[TRAIN] Iter: 119500 Loss: 0.030327655375003815  PSNR: 18.6292667388916
[TRAIN] Iter: 119600 Loss: 0.04467857629060745  PSNR: 16.844499588012695
[TRAIN] Iter: 119700 Loss: 0.029942236840724945  PSNR: 18.184497833251953
[TRAIN] Iter: 119800 Loss: 0.025680072605609894  PSNR: 19.217370986938477
[TRAIN] Iter: 119900 Loss: 0.032467324286699295  PSNR: 18.077165603637695
Saved checkpoints at ./logs/TUT-LAB-nerf/120000.tar
[TRAIN] Iter: 120000 Loss: 0.026664409786462784  PSNR: 19.006132125854492
[TRAIN] Iter: 120100 Loss: 0.028365425765514374  PSNR: 18.71294403076172
[TRAIN] Iter: 120200 Loss: 0.0311624426394701  PSNR: 18.21208381652832
[TRAIN] Iter: 120300 Loss: 0.03390580415725708  PSNR: 17.841201782226562
[TRAIN] Iter: 120400 Loss: 0.024080919101834297  PSNR: 19.269662857055664
[TRAIN] Iter: 120500 Loss: 0.02379588410258293  PSNR: 19.572668075561523
[TRAIN] Iter: 120600 Loss: 0.042218536138534546  PSNR: 16.930875778198242
[TRAIN] Iter: 120700 Loss: 0.035261161625385284  PSNR: 17.628633499145508
[TRAIN] Iter: 120800 Loss: 0.032516587525606155  PSNR: 17.714275360107422
[TRAIN] Iter: 120900 Loss: 0.03198897838592529  PSNR: 18.44556999206543
[TRAIN] Iter: 121000 Loss: 0.03276248276233673  PSNR: 17.714805603027344
[TRAIN] Iter: 121100 Loss: 0.03941803798079491  PSNR: 17.01080322265625
[TRAIN] Iter: 121200 Loss: 0.03018624149262905  PSNR: 18.243051528930664
[TRAIN] Iter: 121300 Loss: 0.036373645067214966  PSNR: 17.347702026367188
[TRAIN] Iter: 121400 Loss: 0.036405082792043686  PSNR: 17.587831497192383
[TRAIN] Iter: 121500 Loss: 0.034615471959114075  PSNR: 17.57048988342285
[TRAIN] Iter: 121600 Loss: 0.03284498304128647  PSNR: 18.11362075805664
[TRAIN] Iter: 121700 Loss: 0.03326721489429474  PSNR: 18.137357711791992
[TRAIN] Iter: 121800 Loss: 0.030408263206481934  PSNR: 18.304201126098633
[TRAIN] Iter: 121900 Loss: 0.0384097546339035  PSNR: 17.431652069091797
[TRAIN] Iter: 122000 Loss: 0.02957974374294281  PSNR: 18.574867248535156
[TRAIN] Iter: 122100 Loss: 0.030717574059963226  PSNR: 18.31096076965332
[TRAIN] Iter: 122200 Loss: 0.03311248496174812  PSNR: 18.27691650390625
[TRAIN] Iter: 122300 Loss: 0.03291136771440506  PSNR: 17.95387077331543
[TRAIN] Iter: 122400 Loss: 0.03750956058502197  PSNR: 17.45119285583496
[TRAIN] Iter: 122500 Loss: 0.039445579051971436  PSNR: 17.23160171508789
[TRAIN] Iter: 122600 Loss: 0.03362278640270233  PSNR: 17.95316505432129
[TRAIN] Iter: 122700 Loss: 0.031685084104537964  PSNR: 18.14336395263672
[TRAIN] Iter: 122800 Loss: 0.03194441646337509  PSNR: 18.226306915283203
[TRAIN] Iter: 122900 Loss: 0.03420699015259743  PSNR: 17.790740966796875
[TRAIN] Iter: 123000 Loss: 0.029378695413470268  PSNR: 18.6828670501709
[TRAIN] Iter: 123100 Loss: 0.04120079427957535  PSNR: 17.03461456298828
[TRAIN] Iter: 123200 Loss: 0.03096047416329384  PSNR: 18.181026458740234
[TRAIN] Iter: 123300 Loss: 0.03140043839812279  PSNR: 18.00337791442871
[TRAIN] Iter: 123400 Loss: 0.0385761633515358  PSNR: 17.509662628173828
[TRAIN] Iter: 123500 Loss: 0.03435719385743141  PSNR: 17.888172149658203
[TRAIN] Iter: 123600 Loss: 0.029995447024703026  PSNR: 18.340478897094727
[TRAIN] Iter: 123700 Loss: 0.034434907138347626  PSNR: 17.875200271606445
[TRAIN] Iter: 123800 Loss: 0.03137053921818733  PSNR: 17.95990753173828
[TRAIN] Iter: 123900 Loss: 0.039029717445373535  PSNR: 17.22739028930664
[TRAIN] Iter: 124000 Loss: 0.03006897307932377  PSNR: 18.623153686523438
[TRAIN] Iter: 124100 Loss: 0.027222760021686554  PSNR: 18.90252113342285
[TRAIN] Iter: 124200 Loss: 0.03487727791070938  PSNR: 17.714277267456055
[TRAIN] Iter: 124300 Loss: 0.03616100549697876  PSNR: 17.297632217407227
[TRAIN] Iter: 124400 Loss: 0.03242047131061554  PSNR: 18.055198669433594
[TRAIN] Iter: 124500 Loss: 0.031605884432792664  PSNR: 17.914745330810547
[TRAIN] Iter: 124600 Loss: 0.028809010982513428  PSNR: 18.698036193847656
[TRAIN] Iter: 124700 Loss: 0.03868667036294937  PSNR: 17.229480743408203
[TRAIN] Iter: 124800 Loss: 0.03121640346944332  PSNR: 18.081098556518555
[TRAIN] Iter: 124900 Loss: 0.03488297387957573  PSNR: 17.83837890625
[TRAIN] Iter: 125000 Loss: 0.03948095813393593  PSNR: 17.097572326660156
[TRAIN] Iter: 125100 Loss: 0.036855701357126236  PSNR: 17.581485748291016
[TRAIN] Iter: 125200 Loss: 0.02897399663925171  PSNR: 18.488319396972656
[TRAIN] Iter: 125300 Loss: 0.03242182359099388  PSNR: 18.039552688598633
[TRAIN] Iter: 125400 Loss: 0.03210007771849632  PSNR: 18.034177780151367
[TRAIN] Iter: 125500 Loss: 0.037535130977630615  PSNR: 18.168123245239258
[TRAIN] Iter: 125600 Loss: 0.03307110816240311  PSNR: 18.19405746459961
[TRAIN] Iter: 125700 Loss: 0.0326433926820755  PSNR: 18.034515380859375
[TRAIN] Iter: 125800 Loss: 0.03138948976993561  PSNR: 18.122827529907227
[TRAIN] Iter: 125900 Loss: 0.03230297565460205  PSNR: 18.19598960876465
[TRAIN] Iter: 126000 Loss: 0.031608980149030685  PSNR: 18.424283981323242
[TRAIN] Iter: 126100 Loss: 0.030580846592783928  PSNR: 17.901071548461914
[TRAIN] Iter: 126200 Loss: 0.03748038411140442  PSNR: 17.46828842163086
[TRAIN] Iter: 126300 Loss: 0.02461370825767517  PSNR: 19.2676944732666
[TRAIN] Iter: 126400 Loss: 0.025634195655584335  PSNR: 18.912254333496094
[TRAIN] Iter: 126500 Loss: 0.029453016817569733  PSNR: 18.41811180114746
[TRAIN] Iter: 126600 Loss: 0.03037211485207081  PSNR: 18.52846336364746
[TRAIN] Iter: 126700 Loss: 0.03035873919725418  PSNR: 18.194732666015625
[TRAIN] Iter: 126800 Loss: 0.03138500824570656  PSNR: 18.075029373168945
[TRAIN] Iter: 126900 Loss: 0.03311222419142723  PSNR: 18.0354061126709
[TRAIN] Iter: 127000 Loss: 0.03538203984498978  PSNR: 17.720827102661133
[TRAIN] Iter: 127100 Loss: 0.02898973599076271  PSNR: 18.59612274169922
[TRAIN] Iter: 127200 Loss: 0.026237038895487785  PSNR: 19.002370834350586
[TRAIN] Iter: 127300 Loss: 0.026260899379849434  PSNR: 18.95009994506836
[TRAIN] Iter: 127400 Loss: 0.035084594041109085  PSNR: 17.723255157470703
[TRAIN] Iter: 127500 Loss: 0.036947883665561676  PSNR: 17.46510887145996
[TRAIN] Iter: 127600 Loss: 0.033845268189907074  PSNR: 17.948347091674805
[TRAIN] Iter: 127700 Loss: 0.0339677631855011  PSNR: 18.027286529541016
[TRAIN] Iter: 127800 Loss: 0.03482132777571678  PSNR: 17.76211166381836
[TRAIN] Iter: 127900 Loss: 0.038528189063072205  PSNR: 17.329021453857422
[TRAIN] Iter: 128000 Loss: 0.03749513626098633  PSNR: 17.43136978149414
[TRAIN] Iter: 128100 Loss: 0.03385940566658974  PSNR: 17.894533157348633
[TRAIN] Iter: 128200 Loss: 0.034855179488658905  PSNR: 17.82068634033203
[TRAIN] Iter: 128300 Loss: 0.031903207302093506  PSNR: 18.220855712890625
[TRAIN] Iter: 128400 Loss: 0.03495470806956291  PSNR: 17.787843704223633
[TRAIN] Iter: 128500 Loss: 0.03845195099711418  PSNR: 17.23210334777832
[TRAIN] Iter: 128600 Loss: 0.0353541374206543  PSNR: 17.913719177246094
[TRAIN] Iter: 128700 Loss: 0.03546556457877159  PSNR: 17.524093627929688
[TRAIN] Iter: 128800 Loss: 0.031086672097444534  PSNR: 18.2824764251709
[TRAIN] Iter: 128900 Loss: 0.03592978045344353  PSNR: 17.556106567382812
[TRAIN] Iter: 129000 Loss: 0.032716795802116394  PSNR: 17.919719696044922
[TRAIN] Iter: 129100 Loss: 0.03365505486726761  PSNR: 18.04667091369629
[TRAIN] Iter: 129200 Loss: 0.0311533622443676  PSNR: 18.153854370117188
[TRAIN] Iter: 129300 Loss: 0.035409413278102875  PSNR: 17.461620330810547
[TRAIN] Iter: 129400 Loss: 0.04010050371289253  PSNR: 17.175209045410156
[TRAIN] Iter: 129500 Loss: 0.03373274207115173  PSNR: 17.64388656616211
[TRAIN] Iter: 129600 Loss: 0.028497032821178436  PSNR: 18.476436614990234
[TRAIN] Iter: 129700 Loss: 0.03248094022274017  PSNR: 18.14470672607422
[TRAIN] Iter: 129800 Loss: 0.032794829457998276  PSNR: 18.005203247070312
[TRAIN] Iter: 129900 Loss: 0.03256746381521225  PSNR: 18.17397689819336
Saved checkpoints at ./logs/TUT-LAB-nerf/130000.tar
[TRAIN] Iter: 130000 Loss: 0.04085621237754822  PSNR: 17.015785217285156
[TRAIN] Iter: 130100 Loss: 0.03558258339762688  PSNR: 17.76395034790039
[TRAIN] Iter: 130200 Loss: 0.03518853709101677  PSNR: 17.726118087768555
[TRAIN] Iter: 130300 Loss: 0.028438977897167206  PSNR: 18.73393440246582
[TRAIN] Iter: 130400 Loss: 0.025986438617110252  PSNR: 19.045427322387695
[TRAIN] Iter: 130500 Loss: 0.037753473967313766  PSNR: 17.45054817199707
[TRAIN] Iter: 130600 Loss: 0.044068172574043274  PSNR: 16.830839157104492
[TRAIN] Iter: 130700 Loss: 0.03161302208900452  PSNR: 17.995716094970703
[TRAIN] Iter: 130800 Loss: 0.03383433073759079  PSNR: 18.023578643798828
[TRAIN] Iter: 130900 Loss: 0.030084598809480667  PSNR: 18.44817543029785
[TRAIN] Iter: 131000 Loss: 0.03962642326951027  PSNR: 17.169031143188477
[TRAIN] Iter: 131100 Loss: 0.0316697433590889  PSNR: 18.126157760620117
[TRAIN] Iter: 131200 Loss: 0.025743653997778893  PSNR: 18.997215270996094
[TRAIN] Iter: 131300 Loss: 0.034072503447532654  PSNR: 17.828996658325195
[TRAIN] Iter: 131400 Loss: 0.03550739958882332  PSNR: 17.694604873657227
[TRAIN] Iter: 131500 Loss: 0.03776000812649727  PSNR: 17.329710006713867
[TRAIN] Iter: 131600 Loss: 0.03667464107275009  PSNR: 17.48614501953125
[TRAIN] Iter: 131700 Loss: 0.0313301607966423  PSNR: 18.44757843017578
[TRAIN] Iter: 131800 Loss: 0.030832260847091675  PSNR: 18.2669677734375
[TRAIN] Iter: 131900 Loss: 0.03276524692773819  PSNR: 18.12430191040039
[TRAIN] Iter: 132000 Loss: 0.030190303921699524  PSNR: 18.48100471496582
[TRAIN] Iter: 132100 Loss: 0.03350457549095154  PSNR: 17.77750587463379
[TRAIN] Iter: 132200 Loss: 0.031120987609028816  PSNR: 18.514156341552734
[TRAIN] Iter: 132300 Loss: 0.027804750949144363  PSNR: 18.431232452392578
[TRAIN] Iter: 132400 Loss: 0.030577776953577995  PSNR: 18.455120086669922
[TRAIN] Iter: 132500 Loss: 0.027285447344183922  PSNR: 18.964317321777344
[TRAIN] Iter: 132600 Loss: 0.03684524446725845  PSNR: 17.605424880981445
[TRAIN] Iter: 132700 Loss: 0.038399599492549896  PSNR: 17.45369529724121
[TRAIN] Iter: 132800 Loss: 0.029274020344018936  PSNR: 18.328134536743164
[TRAIN] Iter: 132900 Loss: 0.033391792327165604  PSNR: 18.023929595947266
[TRAIN] Iter: 133000 Loss: 0.03165866434574127  PSNR: 18.072723388671875
[TRAIN] Iter: 133100 Loss: 0.03275696188211441  PSNR: 18.031463623046875
[TRAIN] Iter: 133200 Loss: 0.03373831510543823  PSNR: 17.904308319091797
[TRAIN] Iter: 133300 Loss: 0.03575165197253227  PSNR: 17.68158531188965
[TRAIN] Iter: 133400 Loss: 0.03494618460536003  PSNR: 17.887765884399414
[TRAIN] Iter: 133500 Loss: 0.03261403739452362  PSNR: 18.13697624206543
[TRAIN] Iter: 133600 Loss: 0.03371388465166092  PSNR: 17.799121856689453
[TRAIN] Iter: 133700 Loss: 0.030745260417461395  PSNR: 18.556827545166016
[TRAIN] Iter: 133800 Loss: 0.032612137496471405  PSNR: 18.235300064086914
[TRAIN] Iter: 133900 Loss: 0.03714122995734215  PSNR: 17.681486129760742
[TRAIN] Iter: 134000 Loss: 0.026672612875699997  PSNR: 18.810880661010742
[TRAIN] Iter: 134100 Loss: 0.03605262562632561  PSNR: 17.901884078979492
[TRAIN] Iter: 134200 Loss: 0.03967161476612091  PSNR: 17.252159118652344
[TRAIN] Iter: 134300 Loss: 0.03285457566380501  PSNR: 18.118467330932617
[TRAIN] Iter: 134400 Loss: 0.037154167890548706  PSNR: 17.597320556640625
[TRAIN] Iter: 134500 Loss: 0.02820448763668537  PSNR: 18.660444259643555
[TRAIN] Iter: 134600 Loss: 0.03420967608690262  PSNR: 17.81081199645996
[TRAIN] Iter: 134700 Loss: 0.03387444466352463  PSNR: 17.64798355102539
[TRAIN] Iter: 134800 Loss: 0.03414522111415863  PSNR: 17.731285095214844
[TRAIN] Iter: 134900 Loss: 0.035613708198070526  PSNR: 17.99287986755371
[TRAIN] Iter: 135000 Loss: 0.033568356186151505  PSNR: 17.7548828125
[TRAIN] Iter: 135100 Loss: 0.02595537155866623  PSNR: 19.05902862548828
[TRAIN] Iter: 135200 Loss: 0.03289789706468582  PSNR: 18.034425735473633
[TRAIN] Iter: 135300 Loss: 0.027489155530929565  PSNR: 18.615612030029297
[TRAIN] Iter: 135400 Loss: 0.039313074201345444  PSNR: 17.618635177612305
[TRAIN] Iter: 135500 Loss: 0.02816087007522583  PSNR: 18.976831436157227
[TRAIN] Iter: 135600 Loss: 0.027915071696043015  PSNR: 18.540178298950195
[TRAIN] Iter: 135700 Loss: 0.03207283467054367  PSNR: 18.215057373046875
[TRAIN] Iter: 135800 Loss: 0.031198889017105103  PSNR: 18.5002498626709
[TRAIN] Iter: 135900 Loss: 0.03616371005773544  PSNR: 17.597999572753906
[TRAIN] Iter: 136000 Loss: 0.03206811845302582  PSNR: 17.856340408325195
[TRAIN] Iter: 136100 Loss: 0.029679376631975174  PSNR: 18.205339431762695
[TRAIN] Iter: 136200 Loss: 0.037125252187252045  PSNR: 17.53571128845215
[TRAIN] Iter: 136300 Loss: 0.0329187847673893  PSNR: 17.780765533447266
[TRAIN] Iter: 136400 Loss: 0.02574535831809044  PSNR: 19.19776153564453
[TRAIN] Iter: 136500 Loss: 0.02745242603123188  PSNR: 18.826461791992188
[TRAIN] Iter: 136600 Loss: 0.024427110329270363  PSNR: 19.130617141723633
[TRAIN] Iter: 136700 Loss: 0.027028154581785202  PSNR: 18.953508377075195
[TRAIN] Iter: 136800 Loss: 0.036195993423461914  PSNR: 17.5880184173584
[TRAIN] Iter: 136900 Loss: 0.03718557581305504  PSNR: 17.624576568603516
[TRAIN] Iter: 137000 Loss: 0.029837407171726227  PSNR: 17.98836898803711
[TRAIN] Iter: 137100 Loss: 0.029115336015820503  PSNR: 18.382640838623047
[TRAIN] Iter: 137200 Loss: 0.027704862877726555  PSNR: 18.544919967651367
[TRAIN] Iter: 137300 Loss: 0.027531255036592484  PSNR: 18.87123680114746
[TRAIN] Iter: 137400 Loss: 0.028253652155399323  PSNR: 18.713848114013672
[TRAIN] Iter: 137500 Loss: 0.0358094722032547  PSNR: 17.652912139892578
[TRAIN] Iter: 137600 Loss: 0.037885285913944244  PSNR: 17.43778419494629
[TRAIN] Iter: 137700 Loss: 0.02937600575387478  PSNR: 18.316287994384766
[TRAIN] Iter: 137800 Loss: 0.0320199579000473  PSNR: 18.338483810424805
[TRAIN] Iter: 137900 Loss: 0.028072414919734  PSNR: 18.78070068359375
[TRAIN] Iter: 138000 Loss: 0.03523295372724533  PSNR: 18.000198364257812
[TRAIN] Iter: 138100 Loss: 0.03292149677872658  PSNR: 17.941238403320312
[TRAIN] Iter: 138200 Loss: 0.03589905798435211  PSNR: 17.66507911682129
[TRAIN] Iter: 138300 Loss: 0.035520635545253754  PSNR: 17.604217529296875
[TRAIN] Iter: 138400 Loss: 0.03360479325056076  PSNR: 17.965490341186523
[TRAIN] Iter: 138500 Loss: 0.0291983000934124  PSNR: 18.335569381713867
[TRAIN] Iter: 138600 Loss: 0.03670061379671097  PSNR: 17.506120681762695
[TRAIN] Iter: 138700 Loss: 0.03757983446121216  PSNR: 17.59937858581543
[TRAIN] Iter: 138800 Loss: 0.03638231381773949  PSNR: 17.64310646057129
[TRAIN] Iter: 138900 Loss: 0.03449245169758797  PSNR: 17.7862548828125
[TRAIN] Iter: 139000 Loss: 0.02561090886592865  PSNR: 19.083162307739258
[TRAIN] Iter: 139100 Loss: 0.02623935602605343  PSNR: 19.056612014770508
[TRAIN] Iter: 139200 Loss: 0.033539146184921265  PSNR: 18.21591567993164
[TRAIN] Iter: 139300 Loss: 0.027715696021914482  PSNR: 18.755144119262695
[TRAIN] Iter: 139400 Loss: 0.031131906434893608  PSNR: 18.296030044555664
[TRAIN] Iter: 139500 Loss: 0.03475217893719673  PSNR: 17.739377975463867
[TRAIN] Iter: 139600 Loss: 0.03223677724599838  PSNR: 18.218477249145508
[TRAIN] Iter: 139700 Loss: 0.03633727878332138  PSNR: 17.52292251586914
[TRAIN] Iter: 139800 Loss: 0.03382764011621475  PSNR: 18.13072395324707
[TRAIN] Iter: 139900 Loss: 0.0274105966091156  PSNR: 18.973968505859375
Saved checkpoints at ./logs/TUT-LAB-nerf/140000.tar
[TRAIN] Iter: 140000 Loss: 0.0248310174793005  PSNR: 19.228239059448242
[TRAIN] Iter: 140100 Loss: 0.030539512634277344  PSNR: 18.484577178955078
[TRAIN] Iter: 140200 Loss: 0.03553546965122223  PSNR: 17.60015296936035
[TRAIN] Iter: 140300 Loss: 0.03512683883309364  PSNR: 17.764978408813477
[TRAIN] Iter: 140400 Loss: 0.026061758399009705  PSNR: 18.92027473449707
[TRAIN] Iter: 140500 Loss: 0.026197321712970734  PSNR: 19.057790756225586
[TRAIN] Iter: 140600 Loss: 0.029532359912991524  PSNR: 18.51974105834961
[TRAIN] Iter: 140700 Loss: 0.027398131787776947  PSNR: 18.881847381591797
[TRAIN] Iter: 140800 Loss: 0.036606140434741974  PSNR: 17.55070686340332
[TRAIN] Iter: 140900 Loss: 0.03404909372329712  PSNR: 18.105497360229492
[TRAIN] Iter: 141000 Loss: 0.028960134834051132  PSNR: 18.345176696777344
[TRAIN] Iter: 141100 Loss: 0.03199135512113571  PSNR: 18.105308532714844
[TRAIN] Iter: 141200 Loss: 0.02465089224278927  PSNR: 19.240787506103516
[TRAIN] Iter: 141300 Loss: 0.03508920967578888  PSNR: 17.577241897583008
[TRAIN] Iter: 141400 Loss: 0.04056991636753082  PSNR: 17.10527992248535
[TRAIN] Iter: 141500 Loss: 0.029811300337314606  PSNR: 18.47552490234375
[TRAIN] Iter: 141600 Loss: 0.02487478405237198  PSNR: 19.18407440185547
[TRAIN] Iter: 141700 Loss: 0.03716370835900307  PSNR: 17.603303909301758
[TRAIN] Iter: 141800 Loss: 0.03176350146532059  PSNR: 18.092308044433594
[TRAIN] Iter: 141900 Loss: 0.035071536898612976  PSNR: 17.63089370727539
[TRAIN] Iter: 142000 Loss: 0.03257979452610016  PSNR: 18.198495864868164
[TRAIN] Iter: 142100 Loss: 0.033257871866226196  PSNR: 17.88187599182129
[TRAIN] Iter: 142200 Loss: 0.024233978241682053  PSNR: 19.442962646484375
[TRAIN] Iter: 142300 Loss: 0.03830450773239136  PSNR: 17.294971466064453
[TRAIN] Iter: 142400 Loss: 0.030903011560440063  PSNR: 18.00977325439453
[TRAIN] Iter: 142500 Loss: 0.03676231950521469  PSNR: 17.568143844604492
[TRAIN] Iter: 142600 Loss: 0.0422007218003273  PSNR: 17.00166893005371
[TRAIN] Iter: 142700 Loss: 0.03464137017726898  PSNR: 17.706600189208984
[TRAIN] Iter: 142800 Loss: 0.026984188705682755  PSNR: 19.336088180541992
[TRAIN] Iter: 142900 Loss: 0.026712525635957718  PSNR: 18.971200942993164
[TRAIN] Iter: 143000 Loss: 0.037095725536346436  PSNR: 17.55459976196289
[TRAIN] Iter: 143100 Loss: 0.03959932550787926  PSNR: 17.080415725708008
[TRAIN] Iter: 143200 Loss: 0.027748137712478638  PSNR: 18.711610794067383
[TRAIN] Iter: 143300 Loss: 0.03386387974023819  PSNR: 17.800254821777344
[TRAIN] Iter: 143400 Loss: 0.028426796197891235  PSNR: 18.59626007080078
[TRAIN] Iter: 143500 Loss: 0.03147854655981064  PSNR: 18.26770782470703
[TRAIN] Iter: 143600 Loss: 0.035709843039512634  PSNR: 17.559906005859375
[TRAIN] Iter: 143700 Loss: 0.03576912730932236  PSNR: 17.452335357666016
[TRAIN] Iter: 143800 Loss: 0.030393105000257492  PSNR: 18.328134536743164
[TRAIN] Iter: 143900 Loss: 0.029226934537291527  PSNR: 18.68524742126465
[TRAIN] Iter: 144000 Loss: 0.04266548156738281  PSNR: 17.19803810119629
[TRAIN] Iter: 144100 Loss: 0.033887021243572235  PSNR: 17.771038055419922
[TRAIN] Iter: 144200 Loss: 0.025744706392288208  PSNR: 19.095529556274414
[TRAIN] Iter: 144300 Loss: 0.04131808876991272  PSNR: 17.048978805541992
[TRAIN] Iter: 144400 Loss: 0.0339963361620903  PSNR: 18.000085830688477
[TRAIN] Iter: 144500 Loss: 0.03920251131057739  PSNR: 17.260313034057617
[TRAIN] Iter: 144600 Loss: 0.03333213925361633  PSNR: 18.18770408630371
[TRAIN] Iter: 144700 Loss: 0.027945920825004578  PSNR: 18.593461990356445
[TRAIN] Iter: 144800 Loss: 0.03287000209093094  PSNR: 18.257856369018555
[TRAIN] Iter: 144900 Loss: 0.030213311314582825  PSNR: 18.307519912719727
[TRAIN] Iter: 145000 Loss: 0.02643137238919735  PSNR: 19.07599449157715
[TRAIN] Iter: 145100 Loss: 0.027421820908784866  PSNR: 18.733081817626953
[TRAIN] Iter: 145200 Loss: 0.036715537309646606  PSNR: 17.451234817504883
[TRAIN] Iter: 145300 Loss: 0.03389640152454376  PSNR: 18.11481285095215
[TRAIN] Iter: 145400 Loss: 0.029177740216255188  PSNR: 18.704748153686523
[TRAIN] Iter: 145500 Loss: 0.029488442465662956  PSNR: 18.59124183654785
[TRAIN] Iter: 145600 Loss: 0.03216825798153877  PSNR: 18.34678840637207
[TRAIN] Iter: 145700 Loss: 0.03364761918783188  PSNR: 17.820201873779297
[TRAIN] Iter: 145800 Loss: 0.03328470140695572  PSNR: 17.812089920043945
[TRAIN] Iter: 145900 Loss: 0.03301069140434265  PSNR: 18.06665802001953
[TRAIN] Iter: 146000 Loss: 0.03707345575094223  PSNR: 17.4765625
[TRAIN] Iter: 146100 Loss: 0.036229051649570465  PSNR: 17.539714813232422
[TRAIN] Iter: 146200 Loss: 0.031012728810310364  PSNR: 18.28175926208496
[TRAIN] Iter: 146300 Loss: 0.031698308885097504  PSNR: 18.221351623535156
[TRAIN] Iter: 146400 Loss: 0.035867057740688324  PSNR: 17.678119659423828
[TRAIN] Iter: 146500 Loss: 0.03953273966908455  PSNR: 17.298120498657227
[TRAIN] Iter: 146600 Loss: 0.03170464187860489  PSNR: 18.319604873657227
[TRAIN] Iter: 146700 Loss: 0.02822285145521164  PSNR: 18.60578727722168
[TRAIN] Iter: 146800 Loss: 0.03289642557501793  PSNR: 18.051599502563477
[TRAIN] Iter: 146900 Loss: 0.03793523460626602  PSNR: 17.31937599182129
[TRAIN] Iter: 147000 Loss: 0.03419754281640053  PSNR: 17.81305694580078
[TRAIN] Iter: 147100 Loss: 0.03517002612352371  PSNR: 18.031387329101562
[TRAIN] Iter: 147200 Loss: 0.02970706671476364  PSNR: 18.572092056274414
[TRAIN] Iter: 147300 Loss: 0.03353501856327057  PSNR: 17.83612823486328
[TRAIN] Iter: 147400 Loss: 0.03571290150284767  PSNR: 17.614261627197266
[TRAIN] Iter: 147500 Loss: 0.026675455272197723  PSNR: 18.91045570373535
[TRAIN] Iter: 147600 Loss: 0.029383502900600433  PSNR: 18.060461044311523
[TRAIN] Iter: 147700 Loss: 0.03301099315285683  PSNR: 18.03398895263672
[TRAIN] Iter: 147800 Loss: 0.03202267736196518  PSNR: 17.96622085571289
[TRAIN] Iter: 147900 Loss: 0.028506122529506683  PSNR: 18.780244827270508
[TRAIN] Iter: 148000 Loss: 0.03297867253422737  PSNR: 18.11699676513672
[TRAIN] Iter: 148100 Loss: 0.029769493266940117  PSNR: 18.564075469970703
[TRAIN] Iter: 148200 Loss: 0.037325672805309296  PSNR: 17.51056671142578
[TRAIN] Iter: 148300 Loss: 0.029128992930054665  PSNR: 18.694541931152344
[TRAIN] Iter: 148400 Loss: 0.03434745967388153  PSNR: 17.843856811523438
[TRAIN] Iter: 148500 Loss: 0.026418576017022133  PSNR: 19.035247802734375
[TRAIN] Iter: 148600 Loss: 0.030870869755744934  PSNR: 18.512454986572266
[TRAIN] Iter: 148700 Loss: 0.03325618430972099  PSNR: 17.832378387451172
[TRAIN] Iter: 148800 Loss: 0.033565931022167206  PSNR: 17.940982818603516
[TRAIN] Iter: 148900 Loss: 0.03225500136613846  PSNR: 18.306110382080078
[TRAIN] Iter: 149000 Loss: 0.029043851420283318  PSNR: 18.6820068359375
[TRAIN] Iter: 149100 Loss: 0.03670470416545868  PSNR: 17.63279151916504
[TRAIN] Iter: 149200 Loss: 0.03551375865936279  PSNR: 17.686180114746094
[TRAIN] Iter: 149300 Loss: 0.02745484560728073  PSNR: 18.70701789855957
[TRAIN] Iter: 149400 Loss: 0.03327921777963638  PSNR: 18.07480239868164
[TRAIN] Iter: 149500 Loss: 0.03149299696087837  PSNR: 18.25300407409668
[TRAIN] Iter: 149600 Loss: 0.026797600090503693  PSNR: 18.959890365600586
[TRAIN] Iter: 149700 Loss: 0.031040899455547333  PSNR: 18.46660041809082
[TRAIN] Iter: 149800 Loss: 0.02186964452266693  PSNR: 19.865886688232422
[TRAIN] Iter: 149900 Loss: 0.026048101484775543  PSNR: 18.99701690673828
Saved checkpoints at ./logs/TUT-LAB-nerf/150000.tar
0 0.0003478527069091797
torch.Size([320, 640, 3]) torch.Size([320, 640])
1 15.120850086212158
2 18.2413387298584
3 15.101696491241455
4 18.175124645233154
5 15.008827209472656
6 18.268408060073853
7 14.991847515106201
8 18.317548036575317
9 14.95625901222229
10 18.369335651397705
11 14.977426528930664
12 18.318227767944336
13 15.045491456985474
14 15.539638757705688
15 17.77927565574646
16 15.374315977096558
17 17.93128275871277
18 15.244755983352661
19 18.11519169807434
20 15.091692924499512
21 18.345208883285522
22 15.377259969711304
23 17.853992223739624
24 15.380837440490723
25 17.900962829589844
26 15.396582841873169
27 17.901548624038696
28 15.27344298362732
29 18.06632971763611
30 15.205392599105835
31 18.08397126197815
32 15.316705703735352
33 17.885849237442017
34 15.637994289398193
35 15.378443956375122
36 17.662311553955078
37 15.54236626625061
38 17.866753578186035
39 15.425781011581421
40 17.910199880599976
41 15.245774030685425
42 17.910301208496094
43 15.184974670410156
44 18.15799117088318
45 15.0702965259552
46 18.178795337677002
47 14.932021141052246
48 18.409401655197144
49 14.959229230880737
50 18.18419623374939
51 15.36368727684021
52 17.89329767227173
53 15.360525131225586
54 17.912373304367065
55 15.360497951507568
56 15.355185270309448
57 17.89457058906555
58 15.40990400314331
59 17.952754259109497
60 15.293846130371094
61 17.913875818252563
62 15.36207890510559
63 17.928114652633667
64 15.338489532470703
65 17.93413805961609
66 11.410546779632568
67 17.910558700561523
68 15.348058938980103
69 17.988992929458618
70 15.345508575439453
71 15.357030391693115
72 17.96097207069397
73 15.273897171020508
74 17.82998776435852
75 15.260449647903442
76 17.99239730834961
77 15.210292339324951
78 18.21159338951111
79 15.38875675201416
80 17.90433645248413
81 15.281700134277344
82 17.913066864013672
83 15.31113862991333
84 17.94001817703247
85 15.321596622467041
86 17.941277980804443
87 15.308407306671143
88 17.977631092071533
89 15.511656761169434
90 18.01429271697998
91 15.340218782424927
92 15.350916147232056
93 17.897895097732544
94 15.289961576461792
95 17.951470375061035
96 15.28084945678711
97 18.163137435913086
98 15.144893407821655
99 18.360740423202515
100 15.043800830841064
101 18.178322553634644
102 15.072084188461304
103 18.211187839508057
104 15.052890300750732
105 18.380503177642822
106 15.25772738456726
107 17.844544172286987
108 15.227606773376465
109 18.01874279975891
110 15.299768924713135
111 17.924229383468628
112 15.354851961135864
113 15.07688307762146
114 18.25019097328186
115 15.096726179122925
116 18.138487339019775
117 15.025705575942993
118 18.307720184326172
119 15.002421140670776
Done, saving (120, 320, 640, 3) (120, 320, 640)
extras:{'raw': tensor([[[-1.5766e+00, -1.4541e+00, -1.3356e+00, -1.2194e+01],
         [-1.0468e+00, -1.0799e+00, -1.1892e+00, -1.7056e+01],
         [-1.0086e+00, -1.1456e+00, -1.3891e+00, -2.3837e+01],
         ...,
         [ 2.4977e+00,  8.4504e-01,  7.6035e+00,  1.6987e+02],
         [ 1.3686e+00, -1.4888e-01,  6.7309e+00,  1.7212e+02],
         [ 1.6274e+00,  1.2456e-01,  6.3814e+00,  1.6995e+02]],

        [[ 4.2381e-01,  2.4071e-01, -1.0796e-02, -1.2955e+00],
         [ 5.7384e-01,  4.0760e-01,  2.2364e-01,  2.8601e+00],
         [ 5.7914e-01,  3.9790e-01,  1.7213e-01,  4.6187e+00],
         ...,
         [ 1.3584e+01,  1.2533e+01,  1.4592e+01,  8.9818e+01],
         [ 1.4874e+01,  1.3442e+01,  1.5222e+01,  1.0460e+02],
         [ 1.4603e+01,  1.2789e+01,  1.4050e+01,  1.0290e+02]],

        [[-1.4602e+00, -1.4137e+00, -9.1184e-01, -2.0107e+01],
         [-1.3647e-01, -5.4262e-01, -6.0887e-01, -1.6210e+01],
         [-1.3969e-01, -5.0755e-01, -6.0896e-01, -1.6257e+01],
         ...,
         [-1.2750e+01, -9.8249e+00, -5.4565e+00,  3.0031e+01],
         [-1.2311e+01, -9.5499e+00, -5.5593e+00,  1.8699e+01],
         [-9.6223e+00, -7.7837e+00, -4.7511e+00,  1.0827e+01]],

        ...,

        [[ 2.5766e-01,  9.4572e-02, -1.4778e-01,  1.6991e+01],
         [ 2.8610e-01,  3.6206e-02, -2.6638e-01,  9.4320e+00],
         [ 9.1093e-01,  7.8289e-01,  9.4942e-01, -3.5304e+01],
         ...,
         [ 2.6574e+01,  2.1747e+01,  1.7830e+01,  1.5902e+02],
         [ 2.7915e+01,  2.2962e+01,  1.9106e+01,  1.7536e+02],
         [ 2.8642e+01,  2.3518e+01,  1.9395e+01,  1.8306e+02]],

        [[-1.1528e+00, -1.1215e+00, -1.1617e+00, -2.4254e+01],
         [-5.9101e-01, -6.5585e-01, -7.0414e-01, -2.5862e+01],
         [-1.2694e+00, -1.0483e+00, -1.3331e+00, -1.7763e+01],
         ...,
         [-5.7380e+00, -6.6832e+00, -9.1005e+00,  4.3763e+02],
         [-5.5672e+00, -6.5010e+00, -8.9183e+00,  4.4136e+02],
         [-5.8786e+00, -6.8040e+00, -9.3667e+00,  4.5526e+02]],

        [[-6.7192e-01, -8.7665e-01, -9.5854e-01, -2.1591e+01],
         [-1.1225e+00, -1.1527e+00, -9.6018e-01, -1.5826e+01],
         [-1.8694e+00, -1.8009e+00, -1.7107e+00, -2.0396e+01],
         ...,
         [-2.6952e+00, -2.0932e+00,  2.7996e-01,  5.6578e+00],
         [-5.2468e-01, -9.3265e-01,  3.0435e-01, -5.9148e-01],
         [-2.7771e-01, -7.8878e-01,  3.7230e-01,  4.5323e-01]]],
       grad_fn=<ReshapeAliasBackward0>), 'rgb0': tensor([[0.3609, 0.3741, 0.4669],
        [0.6343, 0.5828, 0.5333],
        [0.1590, 0.1916, 0.2703],
        ...,
        [0.5926, 0.5521, 0.4977],
        [0.0657, 0.0895, 0.0414],
        [0.6138, 0.5286, 0.5616]], grad_fn=<ReshapeAliasBackward0>), 'disp0': tensor([ 23.9554, 243.0372,  21.2658,  ..., 374.6960,  16.6466,  21.3185],
       grad_fn=<ReshapeAliasBackward0>), 'acc0': tensor([1., 1., 1.,  ..., 1., 1., 1.], grad_fn=<ReshapeAliasBackward0>), 'z_std': tensor([0.0021, 0.0246, 0.0022,  ..., 0.2675, 0.0029, 0.0020])}
0 0.000484466552734375
torch.Size([320, 640, 3]) torch.Size([320, 640])
1 14.999412536621094
2 18.349060773849487
3 14.977943181991577
4 18.30124282836914
5 14.92017650604248
6 18.40511655807495
7 14.966047763824463
8 18.439772605895996
9 14.97416067123413
10 18.174397468566895
11 14.947286605834961
12 18.178930282592773
13 15.833279609680176
14 15.53174638748169
15 17.06741189956665
16 15.092604160308838
17 18.97287654876709
18 15.11254596710205
19 17.495831966400146
20 15.305497169494629
21 18.092264652252197
22 15.176512718200684
23 18.087512731552124
24 15.267182111740112
25 17.903490781784058
26 15.340513706207275
27 17.874439239501953
28 15.388774394989014
29 17.902347803115845
30 15.366375207901001
31 17.906421899795532
32 15.387205123901367
33 17.861761331558228
34 15.399437665939331
35 15.556661367416382
36 18.070419549942017
37 15.364029169082642
38 17.896040678024292
39 15.356689453125
40 17.93728280067444
41 15.392837047576904
42 17.86528730392456
43 15.34611463546753
44 17.926157474517822
45 15.37891149520874
46 17.936567068099976
47 15.37152361869812
48 17.954149961471558
49 15.27476692199707
50 17.927276134490967
51 15.333762645721436
52 17.972463846206665
53 15.258705139160156
54 18.12324333190918
55 15.570679903030396
56 17.55979299545288
57 15.583044290542603
58 15.105468034744263
59 18.126868724822998
60 15.02457880973816
61 18.348859310150146
62 14.961918830871582
63 16.431562423706055
64 14.83854603767395
65 18.24310851097107
66 15.051771879196167
67 16.835373401641846
68 13.30984115600586
69 13.215191841125488
70 16.13428235054016
71 13.068332195281982
72 16.487510442733765
73 13.08555793762207
74 16.189792156219482
75 13.32640290260315
76 13.175147533416748
77 16.389824151992798
78 13.027763843536377
79 16.396172285079956
80 13.144195795059204
81 13.232775688171387
82 16.344701528549194
83 13.06803560256958
84 16.423004865646362
85 13.064035654067993
86 13.3869788646698
87 16.22900700569153
88 13.079635858535767
89 16.290264129638672
90 12.958484411239624
91 16.424038410186768
92 14.202656030654907
93 15.6631178855896
94 13.86327862739563
95 13.367043256759644
96 15.012125968933105
97 13.2712984085083
98 15.946983098983765
99 13.507388353347778
100 13.619930505752563
101 15.976311922073364
102 13.766786813735962
103 15.228198766708374
104 13.661548376083374
105 15.554877519607544
106 13.81121277809143
107 13.815063953399658
108 15.388441562652588
109 13.66242527961731
110 15.462698221206665
111 13.624113082885742
112 15.537913799285889
113 13.684004545211792
114 13.775136470794678
115 15.551077365875244
116 13.783990621566772
117 15.461408138275146
118 13.801698207855225
119 15.372007369995117
test poses shape torch.Size([13, 3, 4])
0 0.0006244182586669922
torch.Size([320, 640, 3]) torch.Size([320, 640])
1 13.828886985778809
2 15.435917139053345
3 13.733901023864746
4 15.56925368309021
5 13.742460012435913
6 15.739543437957764
7 13.514173030853271
8 13.829686164855957
9 15.502158880233765
10 13.79313588142395
11 15.596059560775757
12 13.462098360061646
Saved test set
[TRAIN] Iter: 150000 Loss: 0.03223005682229996  PSNR: 18.495046615600586
[TRAIN] Iter: 150100 Loss: 0.03664346784353256  PSNR: 17.450719833374023
[TRAIN] Iter: 150200 Loss: 0.027085548266768456  PSNR: 18.7247371673584
[TRAIN] Iter: 150300 Loss: 0.0347030907869339  PSNR: 17.889362335205078
[TRAIN] Iter: 150400 Loss: 0.02449610084295273  PSNR: 19.4000301361084
[TRAIN] Iter: 150500 Loss: 0.02883205935359001  PSNR: 18.71914291381836
[TRAIN] Iter: 150600 Loss: 0.032893694937229156  PSNR: 17.967063903808594
[TRAIN] Iter: 150700 Loss: 0.03636490926146507  PSNR: 17.53006935119629
[TRAIN] Iter: 150800 Loss: 0.028906360268592834  PSNR: 18.704042434692383
[TRAIN] Iter: 150900 Loss: 0.03643424063920975  PSNR: 17.569124221801758
[TRAIN] Iter: 151000 Loss: 0.03085225075483322  PSNR: 18.566822052001953
[TRAIN] Iter: 151100 Loss: 0.03056127391755581  PSNR: 18.225675582885742
[TRAIN] Iter: 151200 Loss: 0.03075448051095009  PSNR: 18.672290802001953
[TRAIN] Iter: 151300 Loss: 0.03405001014471054  PSNR: 17.910797119140625
[TRAIN] Iter: 151400 Loss: 0.03627826273441315  PSNR: 17.417253494262695
[TRAIN] Iter: 151500 Loss: 0.02688400261104107  PSNR: 18.663257598876953
[TRAIN] Iter: 151600 Loss: 0.030963892117142677  PSNR: 18.00347328186035
[TRAIN] Iter: 151700 Loss: 0.033186592161655426  PSNR: 17.75786590576172
[TRAIN] Iter: 151800 Loss: 0.029911842197179794  PSNR: 18.746719360351562
[TRAIN] Iter: 151900 Loss: 0.026607586070895195  PSNR: 18.975635528564453
[TRAIN] Iter: 152000 Loss: 0.031047387048602104  PSNR: 18.567981719970703
[TRAIN] Iter: 152100 Loss: 0.029552053660154343  PSNR: 18.463850021362305
[TRAIN] Iter: 152200 Loss: 0.038886286318302155  PSNR: 17.110647201538086
[TRAIN] Iter: 152300 Loss: 0.03623946011066437  PSNR: 17.48775863647461
[TRAIN] Iter: 152400 Loss: 0.02889775112271309  PSNR: 18.753652572631836
[TRAIN] Iter: 152500 Loss: 0.025184351950883865  PSNR: 19.08730697631836
[TRAIN] Iter: 152600 Loss: 0.0335296206176281  PSNR: 18.121694564819336
[TRAIN] Iter: 152700 Loss: 0.03315892815589905  PSNR: 18.00177574157715
[TRAIN] Iter: 152800 Loss: 0.033895764499902725  PSNR: 18.07080078125
[TRAIN] Iter: 152900 Loss: 0.029486071318387985  PSNR: 18.622962951660156
[TRAIN] Iter: 153000 Loss: 0.02946903184056282  PSNR: 18.489980697631836
[TRAIN] Iter: 153100 Loss: 0.03168319910764694  PSNR: 18.290996551513672
[TRAIN] Iter: 153200 Loss: 0.03339449316263199  PSNR: 17.867076873779297
[TRAIN] Iter: 153300 Loss: 0.032908760011196136  PSNR: 18.122175216674805
[TRAIN] Iter: 153400 Loss: 0.03262365609407425  PSNR: 17.99102020263672
[TRAIN] Iter: 153500 Loss: 0.027993710711598396  PSNR: 18.53744888305664
[TRAIN] Iter: 153600 Loss: 0.03142337501049042  PSNR: 18.13982391357422
[TRAIN] Iter: 153700 Loss: 0.0355621799826622  PSNR: 17.876270294189453
[TRAIN] Iter: 153800 Loss: 0.029629308730363846  PSNR: 18.56037139892578
[TRAIN] Iter: 153900 Loss: 0.0319083034992218  PSNR: 17.972244262695312
[TRAIN] Iter: 154000 Loss: 0.03013889491558075  PSNR: 18.27583885192871
[TRAIN] Iter: 154100 Loss: 0.02549891732633114  PSNR: 19.221071243286133
[TRAIN] Iter: 154200 Loss: 0.03826476261019707  PSNR: 17.321392059326172
[TRAIN] Iter: 154300 Loss: 0.02663312293589115  PSNR: 18.90086555480957
[TRAIN] Iter: 154400 Loss: 0.03513677790760994  PSNR: 18.00029182434082
[TRAIN] Iter: 154500 Loss: 0.028863023966550827  PSNR: 18.891773223876953
[TRAIN] Iter: 154600 Loss: 0.035612091422080994  PSNR: 17.666976928710938
[TRAIN] Iter: 154700 Loss: 0.030834032222628593  PSNR: 18.297828674316406
[TRAIN] Iter: 154800 Loss: 0.03206644952297211  PSNR: 18.10666275024414
[TRAIN] Iter: 154900 Loss: 0.024637475609779358  PSNR: 19.39069366455078
[TRAIN] Iter: 155000 Loss: 0.03316202387213707  PSNR: 17.755430221557617
[TRAIN] Iter: 155100 Loss: 0.02787007950246334  PSNR: 18.988468170166016
[TRAIN] Iter: 155200 Loss: 0.031676728278398514  PSNR: 18.23211097717285
[TRAIN] Iter: 155300 Loss: 0.03677302598953247  PSNR: 17.459814071655273
[TRAIN] Iter: 155400 Loss: 0.03371051698923111  PSNR: 17.744808197021484
[TRAIN] Iter: 155500 Loss: 0.025242380797863007  PSNR: 19.066600799560547
[TRAIN] Iter: 155600 Loss: 0.03428696095943451  PSNR: 17.538469314575195
[TRAIN] Iter: 155700 Loss: 0.030962221324443817  PSNR: 18.230831146240234
[TRAIN] Iter: 155800 Loss: 0.04542170464992523  PSNR: 16.68228530883789
[TRAIN] Iter: 155900 Loss: 0.031545490026474  PSNR: 18.063152313232422
[TRAIN] Iter: 156000 Loss: 0.037812404334545135  PSNR: 17.280012130737305
[TRAIN] Iter: 156100 Loss: 0.026723921298980713  PSNR: 19.156417846679688
[TRAIN] Iter: 156200 Loss: 0.034498296678066254  PSNR: 17.6805477142334
[TRAIN] Iter: 156300 Loss: 0.03198035806417465  PSNR: 18.05738067626953
[TRAIN] Iter: 156400 Loss: 0.03134119510650635  PSNR: 18.165746688842773
[TRAIN] Iter: 156500 Loss: 0.03265629708766937  PSNR: 18.319236755371094
[TRAIN] Iter: 156600 Loss: 0.033605292439460754  PSNR: 17.829063415527344
[TRAIN] Iter: 156700 Loss: 0.03310582786798477  PSNR: 18.019094467163086
[TRAIN] Iter: 156800 Loss: 0.0294695645570755  PSNR: 18.72003936767578
[TRAIN] Iter: 156900 Loss: 0.03458388149738312  PSNR: 17.87495231628418
[TRAIN] Iter: 157000 Loss: 0.032529257237911224  PSNR: 18.120803833007812
[TRAIN] Iter: 157100 Loss: 0.032126203179359436  PSNR: 18.145429611206055
[TRAIN] Iter: 157200 Loss: 0.028956923633813858  PSNR: 18.691368103027344
[TRAIN] Iter: 157300 Loss: 0.029798384755849838  PSNR: 18.795114517211914
[TRAIN] Iter: 157400 Loss: 0.03026708960533142  PSNR: 18.356332778930664
[TRAIN] Iter: 157500 Loss: 0.04141099750995636  PSNR: 17.029232025146484
[TRAIN] Iter: 157600 Loss: 0.025063060224056244  PSNR: 19.545175552368164
[TRAIN] Iter: 157700 Loss: 0.03249644488096237  PSNR: 18.101289749145508
[TRAIN] Iter: 157800 Loss: 0.029764845967292786  PSNR: 18.70635986328125
[TRAIN] Iter: 157900 Loss: 0.03172637149691582  PSNR: 18.352672576904297
[TRAIN] Iter: 158000 Loss: 0.02557620033621788  PSNR: 18.96135902404785
[TRAIN] Iter: 158100 Loss: 0.02887272834777832  PSNR: 18.58318328857422
[TRAIN] Iter: 158200 Loss: 0.03516621142625809  PSNR: 17.786088943481445
[TRAIN] Iter: 158300 Loss: 0.02753843553364277  PSNR: 18.74317169189453
[TRAIN] Iter: 158400 Loss: 0.027391720563173294  PSNR: 18.32107925415039
[TRAIN] Iter: 158500 Loss: 0.03373875468969345  PSNR: 17.958311080932617
[TRAIN] Iter: 158600 Loss: 0.0336553230881691  PSNR: 17.613574981689453
[TRAIN] Iter: 158700 Loss: 0.028027309104800224  PSNR: 18.79364013671875
[TRAIN] Iter: 158800 Loss: 0.03434571996331215  PSNR: 17.82711410522461
[TRAIN] Iter: 158900 Loss: 0.0325593575835228  PSNR: 17.93198585510254
[TRAIN] Iter: 159000 Loss: 0.02714325301349163  PSNR: 18.672700881958008
[TRAIN] Iter: 159100 Loss: 0.03498022258281708  PSNR: 17.79448890686035
[TRAIN] Iter: 159200 Loss: 0.03372572734951973  PSNR: 17.69009780883789
[TRAIN] Iter: 159300 Loss: 0.02812705561518669  PSNR: 18.76740264892578
[TRAIN] Iter: 159400 Loss: 0.027792325243353844  PSNR: 18.533885955810547
[TRAIN] Iter: 159500 Loss: 0.03596640005707741  PSNR: 17.606765747070312
[TRAIN] Iter: 159600 Loss: 0.033313214778900146  PSNR: 18.029708862304688
[TRAIN] Iter: 159700 Loss: 0.033282242715358734  PSNR: 17.9444580078125
[TRAIN] Iter: 159800 Loss: 0.033448830246925354  PSNR: 17.960058212280273
[TRAIN] Iter: 159900 Loss: 0.03063810244202614  PSNR: 18.156187057495117
Saved checkpoints at ./logs/TUT-LAB-nerf/160000.tar
[TRAIN] Iter: 160000 Loss: 0.033427800983190536  PSNR: 17.764219284057617
[TRAIN] Iter: 160100 Loss: 0.03302852809429169  PSNR: 18.146881103515625
[TRAIN] Iter: 160200 Loss: 0.03307908773422241  PSNR: 17.827743530273438
[TRAIN] Iter: 160300 Loss: 0.030401356518268585  PSNR: 18.383403778076172
[TRAIN] Iter: 160400 Loss: 0.0367717370390892  PSNR: 17.57282066345215
[TRAIN] Iter: 160500 Loss: 0.03479864448308945  PSNR: 17.905691146850586
[TRAIN] Iter: 160600 Loss: 0.03529127687215805  PSNR: 17.835874557495117
[TRAIN] Iter: 160700 Loss: 0.02985052764415741  PSNR: 18.604455947875977
[TRAIN] Iter: 160800 Loss: 0.027116678655147552  PSNR: 18.806360244750977
[TRAIN] Iter: 160900 Loss: 0.029326237738132477  PSNR: 18.60374641418457
[TRAIN] Iter: 161000 Loss: 0.03617504984140396  PSNR: 17.865333557128906
[TRAIN] Iter: 161100 Loss: 0.033990733325481415  PSNR: 17.90013313293457
[TRAIN] Iter: 161200 Loss: 0.02964283898472786  PSNR: 18.638063430786133
[TRAIN] Iter: 161300 Loss: 0.02675076201558113  PSNR: 18.504976272583008
[TRAIN] Iter: 161400 Loss: 0.024802012369036674  PSNR: 19.256589889526367
[TRAIN] Iter: 161500 Loss: 0.03186498582363129  PSNR: 18.649057388305664
[TRAIN] Iter: 161600 Loss: 0.03088565170764923  PSNR: 18.293636322021484
[TRAIN] Iter: 161700 Loss: 0.03559483587741852  PSNR: 17.852142333984375
[TRAIN] Iter: 161800 Loss: 0.03778979182243347  PSNR: 17.39550018310547
[TRAIN] Iter: 161900 Loss: 0.03459371253848076  PSNR: 17.671600341796875
[TRAIN] Iter: 162000 Loss: 0.03478076308965683  PSNR: 17.91353988647461
[TRAIN] Iter: 162100 Loss: 0.023728929460048676  PSNR: 19.368297576904297
[TRAIN] Iter: 162200 Loss: 0.0335637703537941  PSNR: 17.886838912963867
[TRAIN] Iter: 162300 Loss: 0.039730045944452286  PSNR: 17.316747665405273
[TRAIN] Iter: 162400 Loss: 0.029619211331009865  PSNR: 18.404399871826172
[TRAIN] Iter: 162500 Loss: 0.02563663199543953  PSNR: 19.064041137695312
[TRAIN] Iter: 162600 Loss: 0.029694033786654472  PSNR: 18.44432830810547
[TRAIN] Iter: 162700 Loss: 0.02198088727891445  PSNR: 19.813268661499023
[TRAIN] Iter: 162800 Loss: 0.037964511662721634  PSNR: 17.39967918395996
[TRAIN] Iter: 162900 Loss: 0.024273328483104706  PSNR: 19.464962005615234
[TRAIN] Iter: 163000 Loss: 0.030661441385746002  PSNR: 18.36894416809082
[TRAIN] Iter: 163100 Loss: 0.035918161273002625  PSNR: 17.607521057128906
[TRAIN] Iter: 163200 Loss: 0.03236750513315201  PSNR: 17.99768829345703
[TRAIN] Iter: 163300 Loss: 0.0334465354681015  PSNR: 17.810123443603516
[TRAIN] Iter: 163400 Loss: 0.035097021609544754  PSNR: 17.645708084106445
[TRAIN] Iter: 163500 Loss: 0.03688978776335716  PSNR: 17.396991729736328
[TRAIN] Iter: 163600 Loss: 0.03650650754570961  PSNR: 17.542211532592773
[TRAIN] Iter: 163700 Loss: 0.034098852425813675  PSNR: 17.939598083496094
[TRAIN] Iter: 163800 Loss: 0.033908192068338394  PSNR: 17.937772750854492
[TRAIN] Iter: 163900 Loss: 0.03119935654103756  PSNR: 18.348358154296875
[TRAIN] Iter: 164000 Loss: 0.02893637865781784  PSNR: 18.44552230834961
[TRAIN] Iter: 164100 Loss: 0.030758565291762352  PSNR: 18.61378288269043
[TRAIN] Iter: 164200 Loss: 0.03725573793053627  PSNR: 17.530498504638672
[TRAIN] Iter: 164300 Loss: 0.03041331097483635  PSNR: 18.22865104675293
[TRAIN] Iter: 164400 Loss: 0.034403327852487564  PSNR: 17.63274383544922
[TRAIN] Iter: 164500 Loss: 0.029201596975326538  PSNR: 18.27351188659668
[TRAIN] Iter: 164600 Loss: 0.035470034927129745  PSNR: 17.73015594482422
[TRAIN] Iter: 164700 Loss: 0.03679673373699188  PSNR: 17.69607925415039
[TRAIN] Iter: 164800 Loss: 0.042208850383758545  PSNR: 17.10615348815918
[TRAIN] Iter: 164900 Loss: 0.025529546663165092  PSNR: 19.26522445678711
[TRAIN] Iter: 165000 Loss: 0.02894662134349346  PSNR: 18.676048278808594
[TRAIN] Iter: 165100 Loss: 0.03344804048538208  PSNR: 18.134037017822266
[TRAIN] Iter: 165200 Loss: 0.02810858190059662  PSNR: 18.719573974609375
[TRAIN] Iter: 165300 Loss: 0.025906015187501907  PSNR: 19.318645477294922
[TRAIN] Iter: 165400 Loss: 0.02885819971561432  PSNR: 18.52312469482422
[TRAIN] Iter: 165500 Loss: 0.025290336459875107  PSNR: 19.1834659576416
[TRAIN] Iter: 165600 Loss: 0.03357427939772606  PSNR: 17.77986717224121
[TRAIN] Iter: 165700 Loss: 0.028558770194649696  PSNR: 18.633337020874023
[TRAIN] Iter: 165800 Loss: 0.023600470274686813  PSNR: 19.187955856323242
[TRAIN] Iter: 165900 Loss: 0.032912589609622955  PSNR: 18.185312271118164
[TRAIN] Iter: 166000 Loss: 0.03073226287961006  PSNR: 18.324844360351562
[TRAIN] Iter: 166100 Loss: 0.02879112772643566  PSNR: 18.545392990112305
[TRAIN] Iter: 166200 Loss: 0.0265337023884058  PSNR: 19.115291595458984
[TRAIN] Iter: 166300 Loss: 0.030572308227419853  PSNR: 18.521699905395508
[TRAIN] Iter: 166400 Loss: 0.034485429525375366  PSNR: 17.874473571777344
[TRAIN] Iter: 166500 Loss: 0.03259879723191261  PSNR: 18.342632293701172
[TRAIN] Iter: 166600 Loss: 0.023041969165205956  PSNR: 19.700206756591797
[TRAIN] Iter: 166700 Loss: 0.02616759203374386  PSNR: 19.13703727722168
[TRAIN] Iter: 166800 Loss: 0.032814886420965195  PSNR: 18.060741424560547
[TRAIN] Iter: 166900 Loss: 0.03394114226102829  PSNR: 17.8406925201416
[TRAIN] Iter: 167000 Loss: 0.02741868421435356  PSNR: 18.74679183959961
[TRAIN] Iter: 167100 Loss: 0.03401554003357887  PSNR: 17.88678550720215
[TRAIN] Iter: 167200 Loss: 0.022005237638950348  PSNR: 19.728328704833984
[TRAIN] Iter: 167300 Loss: 0.026455191895365715  PSNR: 18.589738845825195
[TRAIN] Iter: 167400 Loss: 0.03371312841773033  PSNR: 17.90673828125
[TRAIN] Iter: 167500 Loss: 0.03270377218723297  PSNR: 18.169540405273438
[TRAIN] Iter: 167600 Loss: 0.02582400292158127  PSNR: 19.178911209106445
[TRAIN] Iter: 167700 Loss: 0.02857096679508686  PSNR: 18.747716903686523
[TRAIN] Iter: 167800 Loss: 0.02637571655213833  PSNR: 19.017547607421875
[TRAIN] Iter: 167900 Loss: 0.021313851699233055  PSNR: 19.965574264526367
[TRAIN] Iter: 168000 Loss: 0.025099195539951324  PSNR: 19.045194625854492
[TRAIN] Iter: 168100 Loss: 0.030064277350902557  PSNR: 18.44548797607422
[TRAIN] Iter: 168200 Loss: 0.033516108989715576  PSNR: 17.973827362060547
[TRAIN] Iter: 168300 Loss: 0.039922989904880524  PSNR: 17.487337112426758
[TRAIN] Iter: 168400 Loss: 0.028319992125034332  PSNR: 18.7697696685791
[TRAIN] Iter: 168500 Loss: 0.02753840759396553  PSNR: 18.76827621459961
[TRAIN] Iter: 168600 Loss: 0.036286480724811554  PSNR: 17.66667366027832
[TRAIN] Iter: 168700 Loss: 0.03684242069721222  PSNR: 17.57686996459961
[TRAIN] Iter: 168800 Loss: 0.039725013077259064  PSNR: 17.47483253479004
[TRAIN] Iter: 168900 Loss: 0.03028622269630432  PSNR: 18.191375732421875
[TRAIN] Iter: 169000 Loss: 0.02906806580722332  PSNR: 18.55681800842285
[TRAIN] Iter: 169100 Loss: 0.03249237686395645  PSNR: 18.18332290649414
[TRAIN] Iter: 169200 Loss: 0.030476976186037064  PSNR: 18.480148315429688
[TRAIN] Iter: 169300 Loss: 0.024052130058407784  PSNR: 19.29945945739746
[TRAIN] Iter: 169400 Loss: 0.025655286386609077  PSNR: 19.289417266845703
[TRAIN] Iter: 169500 Loss: 0.02914939448237419  PSNR: 18.573652267456055
[TRAIN] Iter: 169600 Loss: 0.0350845530629158  PSNR: 17.69115447998047
[TRAIN] Iter: 169700 Loss: 0.03094100020825863  PSNR: 18.28167152404785
[TRAIN] Iter: 169800 Loss: 0.03546294569969177  PSNR: 17.667325973510742
[TRAIN] Iter: 169900 Loss: 0.030344415456056595  PSNR: 18.405502319335938
Saved checkpoints at ./logs/TUT-LAB-nerf/170000.tar
[TRAIN] Iter: 170000 Loss: 0.03301376849412918  PSNR: 17.98696517944336
[TRAIN] Iter: 170100 Loss: 0.03034147620201111  PSNR: 18.54289436340332
[TRAIN] Iter: 170200 Loss: 0.03659532591700554  PSNR: 17.588518142700195
[TRAIN] Iter: 170300 Loss: 0.03377583622932434  PSNR: 18.0543270111084
[TRAIN] Iter: 170400 Loss: 0.02738131396472454  PSNR: 18.615375518798828
[TRAIN] Iter: 170500 Loss: 0.032677359879016876  PSNR: 17.79475212097168
[TRAIN] Iter: 170600 Loss: 0.025047313421964645  PSNR: 19.117605209350586
[TRAIN] Iter: 170700 Loss: 0.03009854443371296  PSNR: 18.505613327026367
[TRAIN] Iter: 170800 Loss: 0.023000873625278473  PSNR: 19.668909072875977
[TRAIN] Iter: 170900 Loss: 0.03167624771595001  PSNR: 18.395536422729492
[TRAIN] Iter: 171000 Loss: 0.033669862896203995  PSNR: 18.251676559448242
[TRAIN] Iter: 171100 Loss: 0.036311712116003036  PSNR: 17.60133934020996
[TRAIN] Iter: 171200 Loss: 0.03321225196123123  PSNR: 18.021188735961914
[TRAIN] Iter: 171300 Loss: 0.036550551652908325  PSNR: 17.644479751586914
[TRAIN] Iter: 171400 Loss: 0.039654262363910675  PSNR: 17.259864807128906
[TRAIN] Iter: 171500 Loss: 0.03772188723087311  PSNR: 17.312604904174805
[TRAIN] Iter: 171600 Loss: 0.03145403414964676  PSNR: 18.157363891601562
[TRAIN] Iter: 171700 Loss: 0.032764919102191925  PSNR: 17.817811965942383
[TRAIN] Iter: 171800 Loss: 0.028845980763435364  PSNR: 19.04817008972168
[TRAIN] Iter: 171900 Loss: 0.03499168902635574  PSNR: 17.725610733032227
[TRAIN] Iter: 172000 Loss: 0.028568755835294724  PSNR: 18.45616912841797
[TRAIN] Iter: 172100 Loss: 0.02697387896478176  PSNR: 19.008136749267578
[TRAIN] Iter: 172200 Loss: 0.0336923711001873  PSNR: 18.031909942626953
[TRAIN] Iter: 172300 Loss: 0.026463914662599564  PSNR: 18.995149612426758
[TRAIN] Iter: 172400 Loss: 0.0293391365557909  PSNR: 18.516069412231445
[TRAIN] Iter: 172500 Loss: 0.027971208095550537  PSNR: 18.27580451965332
[TRAIN] Iter: 172600 Loss: 0.022825585678219795  PSNR: 19.71407127380371
[TRAIN] Iter: 172700 Loss: 0.037467315793037415  PSNR: 17.349346160888672
[TRAIN] Iter: 172800 Loss: 0.034411367028951645  PSNR: 17.776464462280273
[TRAIN] Iter: 172900 Loss: 0.030287068337202072  PSNR: 18.621082305908203
[TRAIN] Iter: 173000 Loss: 0.03830583021044731  PSNR: 17.414226531982422
[TRAIN] Iter: 173100 Loss: 0.026360590010881424  PSNR: 19.068513870239258
[TRAIN] Iter: 173200 Loss: 0.0252075158059597  PSNR: 19.28390884399414
[TRAIN] Iter: 173300 Loss: 0.028601868078112602  PSNR: 18.739137649536133
[TRAIN] Iter: 173400 Loss: 0.037691906094551086  PSNR: 17.353425979614258
[TRAIN] Iter: 173500 Loss: 0.02667253464460373  PSNR: 19.068988800048828
[TRAIN] Iter: 173600 Loss: 0.026573123410344124  PSNR: 19.075695037841797
[TRAIN] Iter: 173700 Loss: 0.03185254707932472  PSNR: 18.21076774597168
[TRAIN] Iter: 173800 Loss: 0.03196165710687637  PSNR: 18.249399185180664
[TRAIN] Iter: 173900 Loss: 0.023724820464849472  PSNR: 19.57823944091797
[TRAIN] Iter: 174000 Loss: 0.026921870186924934  PSNR: 18.86652183532715
[TRAIN] Iter: 174100 Loss: 0.02889106050133705  PSNR: 18.95685386657715
[TRAIN] Iter: 174200 Loss: 0.025644952431321144  PSNR: 19.093503952026367
[TRAIN] Iter: 174300 Loss: 0.02822946384549141  PSNR: 18.63740348815918
[TRAIN] Iter: 174400 Loss: 0.03086317703127861  PSNR: 17.988908767700195
[TRAIN] Iter: 174500 Loss: 0.027682354673743248  PSNR: 18.757299423217773
[TRAIN] Iter: 174600 Loss: 0.025929179042577744  PSNR: 19.16454315185547
[TRAIN] Iter: 174700 Loss: 0.02653711661696434  PSNR: 18.948486328125
[TRAIN] Iter: 174800 Loss: 0.030347077175974846  PSNR: 18.26475715637207
[TRAIN] Iter: 174900 Loss: 0.0366097129881382  PSNR: 17.52183723449707
[TRAIN] Iter: 175000 Loss: 0.02109762653708458  PSNR: 20.098215103149414
[TRAIN] Iter: 175100 Loss: 0.032915499061346054  PSNR: 18.26450538635254
[TRAIN] Iter: 175200 Loss: 0.03527754917740822  PSNR: 17.590471267700195
[TRAIN] Iter: 175300 Loss: 0.030080562457442284  PSNR: 18.24993133544922
[TRAIN] Iter: 175400 Loss: 0.03446062654256821  PSNR: 17.692960739135742
[TRAIN] Iter: 175500 Loss: 0.022687027230858803  PSNR: 19.667192459106445
[TRAIN] Iter: 175600 Loss: 0.03688686341047287  PSNR: 17.876144409179688
[TRAIN] Iter: 175700 Loss: 0.035254620015621185  PSNR: 17.68105125427246
[TRAIN] Iter: 175800 Loss: 0.0338180810213089  PSNR: 17.82640838623047
[TRAIN] Iter: 175900 Loss: 0.03076203167438507  PSNR: 18.23292350769043
[TRAIN] Iter: 176000 Loss: 0.03847246617078781  PSNR: 17.211101531982422
[TRAIN] Iter: 176100 Loss: 0.030264591798186302  PSNR: 18.212221145629883
[TRAIN] Iter: 176200 Loss: 0.026586612686514854  PSNR: 18.941421508789062
[TRAIN] Iter: 176300 Loss: 0.027161508798599243  PSNR: 18.850238800048828
[TRAIN] Iter: 176400 Loss: 0.03561396524310112  PSNR: 17.522605895996094
[TRAIN] Iter: 176500 Loss: 0.029144585132598877  PSNR: 18.54043197631836
[TRAIN] Iter: 176600 Loss: 0.038488734513521194  PSNR: 17.726598739624023
[TRAIN] Iter: 176700 Loss: 0.029822351410984993  PSNR: 18.491872787475586
[TRAIN] Iter: 176800 Loss: 0.03210725262761116  PSNR: 18.001808166503906
[TRAIN] Iter: 176900 Loss: 0.02562659978866577  PSNR: 19.342092514038086
[TRAIN] Iter: 177000 Loss: 0.03308403491973877  PSNR: 18.01534080505371
[TRAIN] Iter: 177100 Loss: 0.03063981607556343  PSNR: 18.36034393310547
[TRAIN] Iter: 177200 Loss: 0.03398478403687477  PSNR: 17.860870361328125
[TRAIN] Iter: 177300 Loss: 0.024302922189235687  PSNR: 19.287160873413086
[TRAIN] Iter: 177400 Loss: 0.030227161943912506  PSNR: 18.70461082458496
[TRAIN] Iter: 177500 Loss: 0.025262342765927315  PSNR: 19.043672561645508
[TRAIN] Iter: 177600 Loss: 0.026599759235978127  PSNR: 18.95748519897461
[TRAIN] Iter: 177700 Loss: 0.030070215463638306  PSNR: 18.420597076416016
[TRAIN] Iter: 177800 Loss: 0.027142666280269623  PSNR: 18.89697265625
[TRAIN] Iter: 177900 Loss: 0.028016164898872375  PSNR: 18.352985382080078
[TRAIN] Iter: 178000 Loss: 0.03093724139034748  PSNR: 18.45471954345703
[TRAIN] Iter: 178100 Loss: 0.032599642872810364  PSNR: 17.974462509155273
[TRAIN] Iter: 178200 Loss: 0.02695746161043644  PSNR: 18.640670776367188
[TRAIN] Iter: 178300 Loss: 0.028852691873908043  PSNR: 18.86709976196289
[TRAIN] Iter: 178400 Loss: 0.029676586389541626  PSNR: 18.2053279876709
[TRAIN] Iter: 178500 Loss: 0.03557099029421806  PSNR: 18.032854080200195
[TRAIN] Iter: 178600 Loss: 0.03158888220787048  PSNR: 18.207691192626953
[TRAIN] Iter: 178700 Loss: 0.034891948103904724  PSNR: 17.90359878540039
[TRAIN] Iter: 178800 Loss: 0.037322863936424255  PSNR: 17.340349197387695
[TRAIN] Iter: 178900 Loss: 0.03291741758584976  PSNR: 17.983787536621094
[TRAIN] Iter: 179000 Loss: 0.027215631678700447  PSNR: 18.750629425048828
[TRAIN] Iter: 179100 Loss: 0.03464451804757118  PSNR: 18.053359985351562
[TRAIN] Iter: 179200 Loss: 0.03028600849211216  PSNR: 18.67268943786621
[TRAIN] Iter: 179300 Loss: 0.033724479377269745  PSNR: 17.95100212097168
[TRAIN] Iter: 179400 Loss: 0.02808009833097458  PSNR: 19.198881149291992
[TRAIN] Iter: 179500 Loss: 0.02884330227971077  PSNR: 18.723011016845703
[TRAIN] Iter: 179600 Loss: 0.030158162117004395  PSNR: 18.548982620239258
[TRAIN] Iter: 179700 Loss: 0.03684109076857567  PSNR: 17.934377670288086
[TRAIN] Iter: 179800 Loss: 0.0331619530916214  PSNR: 18.031667709350586
[TRAIN] Iter: 179900 Loss: 0.03840608522295952  PSNR: 17.464120864868164
Saved checkpoints at ./logs/TUT-LAB-nerf/180000.tar
[TRAIN] Iter: 180000 Loss: 0.03482130542397499  PSNR: 17.781639099121094
[TRAIN] Iter: 180100 Loss: 0.022143494337797165  PSNR: 19.71405029296875
[TRAIN] Iter: 180200 Loss: 0.03271489590406418  PSNR: 18.118776321411133
[TRAIN] Iter: 180300 Loss: 0.032085057348012924  PSNR: 18.229999542236328
[TRAIN] Iter: 180400 Loss: 0.02674499899148941  PSNR: 18.981754302978516
[TRAIN] Iter: 180500 Loss: 0.03015030361711979  PSNR: 18.392290115356445
[TRAIN] Iter: 180600 Loss: 0.035515401512384415  PSNR: 17.834531784057617
[TRAIN] Iter: 180700 Loss: 0.036472637206315994  PSNR: 17.517189025878906
[TRAIN] Iter: 180800 Loss: 0.028665371239185333  PSNR: 18.646228790283203
[TRAIN] Iter: 180900 Loss: 0.0310496985912323  PSNR: 18.27204704284668
[TRAIN] Iter: 181000 Loss: 0.03225645050406456  PSNR: 18.190162658691406
[TRAIN] Iter: 181100 Loss: 0.027349188923835754  PSNR: 18.852218627929688
[TRAIN] Iter: 181200 Loss: 0.03668087348341942  PSNR: 17.502180099487305
[TRAIN] Iter: 181300 Loss: 0.029418017715215683  PSNR: 18.55164909362793
[TRAIN] Iter: 181400 Loss: 0.03421951085329056  PSNR: 17.883007049560547
[TRAIN] Iter: 181500 Loss: 0.029730314388871193  PSNR: 18.229881286621094
[TRAIN] Iter: 181600 Loss: 0.03374519944190979  PSNR: 18.093074798583984
[TRAIN] Iter: 181700 Loss: 0.029741838574409485  PSNR: 18.479755401611328
[TRAIN] Iter: 181800 Loss: 0.024532590061426163  PSNR: 18.991413116455078
[TRAIN] Iter: 181900 Loss: 0.024841496720910072  PSNR: 19.126508712768555
[TRAIN] Iter: 182000 Loss: 0.028808053582906723  PSNR: 18.434202194213867
[TRAIN] Iter: 182100 Loss: 0.03170514106750488  PSNR: 18.238136291503906
[TRAIN] Iter: 182200 Loss: 0.030097372829914093  PSNR: 18.463499069213867
[TRAIN] Iter: 182300 Loss: 0.031176259741187096  PSNR: 18.599756240844727
[TRAIN] Iter: 182400 Loss: 0.03387216478586197  PSNR: 18.03826904296875
[TRAIN] Iter: 182500 Loss: 0.03121229261159897  PSNR: 18.314952850341797
[TRAIN] Iter: 182600 Loss: 0.03197659179568291  PSNR: 18.173545837402344
[TRAIN] Iter: 182700 Loss: 0.027687229216098785  PSNR: 18.7012996673584
[TRAIN] Iter: 182800 Loss: 0.037161074578762054  PSNR: 17.426856994628906
[TRAIN] Iter: 182900 Loss: 0.026421234011650085  PSNR: 18.76393699645996
[TRAIN] Iter: 183000 Loss: 0.02719257026910782  PSNR: 18.79533576965332
[TRAIN] Iter: 183100 Loss: 0.0378609374165535  PSNR: 17.41510772705078
[TRAIN] Iter: 183200 Loss: 0.035619549453258514  PSNR: 17.639245986938477
[TRAIN] Iter: 183300 Loss: 0.023942137137055397  PSNR: 19.31298065185547
[TRAIN] Iter: 183400 Loss: 0.02458767034113407  PSNR: 19.269763946533203
[TRAIN] Iter: 183500 Loss: 0.03480767458677292  PSNR: 17.671369552612305
[TRAIN] Iter: 183600 Loss: 0.024828074499964714  PSNR: 19.23508071899414
[TRAIN] Iter: 183700 Loss: 0.030355708673596382  PSNR: 18.411195755004883
[TRAIN] Iter: 183800 Loss: 0.032046839594841  PSNR: 17.938682556152344
[TRAIN] Iter: 183900 Loss: 0.0399181991815567  PSNR: 17.048879623413086
[TRAIN] Iter: 184000 Loss: 0.03180791437625885  PSNR: 18.178998947143555
[TRAIN] Iter: 184100 Loss: 0.020666278898715973  PSNR: 20.002887725830078
[TRAIN] Iter: 184200 Loss: 0.031084846705198288  PSNR: 18.37095069885254
[TRAIN] Iter: 184300 Loss: 0.034325383603572845  PSNR: 17.97276496887207
[TRAIN] Iter: 184400 Loss: 0.025848597288131714  PSNR: 19.071250915527344
[TRAIN] Iter: 184500 Loss: 0.031217701733112335  PSNR: 18.369226455688477
[TRAIN] Iter: 184600 Loss: 0.028696713969111443  PSNR: 18.72854232788086
[TRAIN] Iter: 184700 Loss: 0.030984532088041306  PSNR: 18.186513900756836
[TRAIN] Iter: 184800 Loss: 0.02847432717680931  PSNR: 18.65355682373047
[TRAIN] Iter: 184900 Loss: 0.024308007210493088  PSNR: 19.446969985961914
[TRAIN] Iter: 185000 Loss: 0.03801561892032623  PSNR: 17.30001449584961
[TRAIN] Iter: 185100 Loss: 0.03418339043855667  PSNR: 17.805389404296875
[TRAIN] Iter: 185200 Loss: 0.02794676274061203  PSNR: 19.07570457458496
[TRAIN] Iter: 185300 Loss: 0.034425172954797745  PSNR: 17.831621170043945
[TRAIN] Iter: 185400 Loss: 0.0274262186139822  PSNR: 18.675479888916016
[TRAIN] Iter: 185500 Loss: 0.03336741775274277  PSNR: 17.86437225341797
[TRAIN] Iter: 185600 Loss: 0.031111355870962143  PSNR: 18.340150833129883
[TRAIN] Iter: 185700 Loss: 0.026215026155114174  PSNR: 19.17351531982422
[TRAIN] Iter: 185800 Loss: 0.029681934043765068  PSNR: 18.55743980407715
[TRAIN] Iter: 185900 Loss: 0.03284380957484245  PSNR: 17.979572296142578
[TRAIN] Iter: 186000 Loss: 0.03442933410406113  PSNR: 17.830852508544922
[TRAIN] Iter: 186100 Loss: 0.028682686388492584  PSNR: 18.4427547454834
[TRAIN] Iter: 186200 Loss: 0.031746067106723785  PSNR: 18.142375946044922
[TRAIN] Iter: 186300 Loss: 0.036885712295770645  PSNR: 17.467628479003906
[TRAIN] Iter: 186400 Loss: 0.031474534422159195  PSNR: 18.130502700805664
[TRAIN] Iter: 186500 Loss: 0.027445748448371887  PSNR: 18.851560592651367
[TRAIN] Iter: 186600 Loss: 0.03179381787776947  PSNR: 18.339448928833008
[TRAIN] Iter: 186700 Loss: 0.02536729909479618  PSNR: 19.217496871948242
[TRAIN] Iter: 186800 Loss: 0.030648723244667053  PSNR: 18.371448516845703
[TRAIN] Iter: 186900 Loss: 0.028028998523950577  PSNR: 18.772192001342773
[TRAIN] Iter: 187000 Loss: 0.03450966626405716  PSNR: 17.801509857177734
[TRAIN] Iter: 187100 Loss: 0.027463320642709732  PSNR: 18.743642807006836
[TRAIN] Iter: 187200 Loss: 0.03126603737473488  PSNR: 18.251638412475586
[TRAIN] Iter: 187300 Loss: 0.03434481471776962  PSNR: 17.738264083862305
[TRAIN] Iter: 187400 Loss: 0.03195120394229889  PSNR: 18.193288803100586
[TRAIN] Iter: 187500 Loss: 0.030712172389030457  PSNR: 18.54506492614746
[TRAIN] Iter: 187600 Loss: 0.028644414618611336  PSNR: 18.43624496459961
[TRAIN] Iter: 187700 Loss: 0.03160922974348068  PSNR: 18.15837860107422
[TRAIN] Iter: 187800 Loss: 0.03299249708652496  PSNR: 17.903316497802734
[TRAIN] Iter: 187900 Loss: 0.02759980782866478  PSNR: 18.758737564086914
[TRAIN] Iter: 188000 Loss: 0.03418860584497452  PSNR: 17.979042053222656
[TRAIN] Iter: 188100 Loss: 0.03175075352191925  PSNR: 18.14308738708496
[TRAIN] Iter: 188200 Loss: 0.026407994329929352  PSNR: 18.645349502563477
[TRAIN] Iter: 188300 Loss: 0.03471390902996063  PSNR: 17.81992530822754
[TRAIN] Iter: 188400 Loss: 0.024658335372805595  PSNR: 19.180347442626953
[TRAIN] Iter: 188500 Loss: 0.02926676906645298  PSNR: 18.511327743530273
[TRAIN] Iter: 188600 Loss: 0.02791709080338478  PSNR: 18.964580535888672
[TRAIN] Iter: 188700 Loss: 0.03564136102795601  PSNR: 17.728591918945312
[TRAIN] Iter: 188800 Loss: 0.02532206103205681  PSNR: 19.37148094177246
[TRAIN] Iter: 188900 Loss: 0.031123515218496323  PSNR: 18.173885345458984
[TRAIN] Iter: 189000 Loss: 0.03036092035472393  PSNR: 18.483640670776367
[TRAIN] Iter: 189100 Loss: 0.03459805250167847  PSNR: 17.898948669433594
[TRAIN] Iter: 189200 Loss: 0.028201907873153687  PSNR: 18.804210662841797
[TRAIN] Iter: 189300 Loss: 0.030277948826551437  PSNR: 18.457916259765625
[TRAIN] Iter: 189400 Loss: 0.029084553942084312  PSNR: 18.432613372802734
[TRAIN] Iter: 189500 Loss: 0.02859627455472946  PSNR: 18.429889678955078
[TRAIN] Iter: 189600 Loss: 0.029799330979585648  PSNR: 18.451818466186523
[TRAIN] Iter: 189700 Loss: 0.03488805145025253  PSNR: 17.662315368652344
[TRAIN] Iter: 189800 Loss: 0.025699321180582047  PSNR: 19.13410186767578
[TRAIN] Iter: 189900 Loss: 0.03813793510198593  PSNR: 17.512706756591797
Saved checkpoints at ./logs/TUT-LAB-nerf/190000.tar
[TRAIN] Iter: 190000 Loss: 0.03350813686847687  PSNR: 17.948646545410156
[TRAIN] Iter: 190100 Loss: 0.02674410678446293  PSNR: 19.16387939453125
[TRAIN] Iter: 190200 Loss: 0.02667134255170822  PSNR: 18.843734741210938
[TRAIN] Iter: 190300 Loss: 0.030835984274744987  PSNR: 18.277301788330078
[TRAIN] Iter: 190400 Loss: 0.027048662304878235  PSNR: 19.027606964111328
[TRAIN] Iter: 190500 Loss: 0.02914339303970337  PSNR: 18.525331497192383
[TRAIN] Iter: 190600 Loss: 0.029966194182634354  PSNR: 18.59075355529785
[TRAIN] Iter: 190700 Loss: 0.026041753590106964  PSNR: 19.16335105895996
[TRAIN] Iter: 190800 Loss: 0.025722524151206017  PSNR: 19.101600646972656
[TRAIN] Iter: 190900 Loss: 0.033511027693748474  PSNR: 18.009252548217773
[TRAIN] Iter: 191000 Loss: 0.03222406655550003  PSNR: 18.087425231933594
[TRAIN] Iter: 191100 Loss: 0.021584581583738327  PSNR: 19.865201950073242
[TRAIN] Iter: 191200 Loss: 0.02592032216489315  PSNR: 19.139339447021484
[TRAIN] Iter: 191300 Loss: 0.024958375841379166  PSNR: 19.081024169921875
[TRAIN] Iter: 191400 Loss: 0.02823544479906559  PSNR: 18.72313117980957
[TRAIN] Iter: 191500 Loss: 0.024729836732149124  PSNR: 19.121753692626953
[TRAIN] Iter: 191600 Loss: 0.031208422034978867  PSNR: 18.25868034362793
[TRAIN] Iter: 191700 Loss: 0.028482411056756973  PSNR: 18.722543716430664
[TRAIN] Iter: 191800 Loss: 0.029329413548111916  PSNR: 18.435239791870117
[TRAIN] Iter: 191900 Loss: 0.033610403537750244  PSNR: 17.841684341430664
[TRAIN] Iter: 192000 Loss: 0.032035745680332184  PSNR: 18.04129409790039
[TRAIN] Iter: 192100 Loss: 0.022431276738643646  PSNR: 19.760398864746094
[TRAIN] Iter: 192200 Loss: 0.03155450150370598  PSNR: 18.25217056274414
[TRAIN] Iter: 192300 Loss: 0.031326670199632645  PSNR: 18.181396484375
[TRAIN] Iter: 192400 Loss: 0.028967639431357384  PSNR: 18.526390075683594
[TRAIN] Iter: 192500 Loss: 0.031785786151885986  PSNR: 18.1451358795166
[TRAIN] Iter: 192600 Loss: 0.02582566812634468  PSNR: 19.005346298217773
[TRAIN] Iter: 192700 Loss: 0.028169283643364906  PSNR: 18.4864501953125
[TRAIN] Iter: 192800 Loss: 0.029400929808616638  PSNR: 18.3594913482666
[TRAIN] Iter: 192900 Loss: 0.02985949069261551  PSNR: 18.46892738342285
[TRAIN] Iter: 193000 Loss: 0.027941349893808365  PSNR: 18.350341796875
[TRAIN] Iter: 193100 Loss: 0.02957235649228096  PSNR: 18.596237182617188
[TRAIN] Iter: 193200 Loss: 0.0300900861620903  PSNR: 18.195505142211914
[TRAIN] Iter: 193300 Loss: 0.029040751978754997  PSNR: 18.735946655273438
[TRAIN] Iter: 193400 Loss: 0.03196897357702255  PSNR: 18.182188034057617
[TRAIN] Iter: 193500 Loss: 0.02930362895131111  PSNR: 18.721649169921875
[TRAIN] Iter: 193600 Loss: 0.03664126619696617  PSNR: 17.577722549438477
[TRAIN] Iter: 193700 Loss: 0.026628635823726654  PSNR: 19.113121032714844
[TRAIN] Iter: 193800 Loss: 0.036395154893398285  PSNR: 17.765430450439453
[TRAIN] Iter: 193900 Loss: 0.03187309950590134  PSNR: 18.399288177490234
[TRAIN] Iter: 194000 Loss: 0.030953647568821907  PSNR: 17.917570114135742
[TRAIN] Iter: 194100 Loss: 0.031315214931964874  PSNR: 18.466379165649414
[TRAIN] Iter: 194200 Loss: 0.03202752023935318  PSNR: 18.071115493774414
[TRAIN] Iter: 194300 Loss: 0.02738761715590954  PSNR: 18.767742156982422
[TRAIN] Iter: 194400 Loss: 0.03750243037939072  PSNR: 17.21579360961914
[TRAIN] Iter: 194500 Loss: 0.024462629109621048  PSNR: 19.116249084472656
[TRAIN] Iter: 194600 Loss: 0.03169751912355423  PSNR: 18.260860443115234
[TRAIN] Iter: 194700 Loss: 0.031232096254825592  PSNR: 18.12925910949707
[TRAIN] Iter: 194800 Loss: 0.035455696284770966  PSNR: 17.575824737548828
[TRAIN] Iter: 194900 Loss: 0.02807866409420967  PSNR: 19.063335418701172
[TRAIN] Iter: 195000 Loss: 0.02209317684173584  PSNR: 19.889097213745117
[TRAIN] Iter: 195100 Loss: 0.03406153619289398  PSNR: 17.900075912475586
[TRAIN] Iter: 195200 Loss: 0.031486742198467255  PSNR: 18.231382369995117
[TRAIN] Iter: 195300 Loss: 0.03142012655735016  PSNR: 17.98900604248047
[TRAIN] Iter: 195400 Loss: 0.026951177045702934  PSNR: 19.047718048095703
[TRAIN] Iter: 195500 Loss: 0.030566250905394554  PSNR: 18.196218490600586
[TRAIN] Iter: 195600 Loss: 0.0288529135286808  PSNR: 18.519962310791016
[TRAIN] Iter: 195700 Loss: 0.03294160217046738  PSNR: 17.972450256347656
[TRAIN] Iter: 195800 Loss: 0.03347926214337349  PSNR: 17.888551712036133
[TRAIN] Iter: 195900 Loss: 0.02909117192029953  PSNR: 18.510746002197266
[TRAIN] Iter: 196000 Loss: 0.03396247327327728  PSNR: 17.922073364257812
[TRAIN] Iter: 196100 Loss: 0.025432592257857323  PSNR: 19.343034744262695
[TRAIN] Iter: 196200 Loss: 0.024272017180919647  PSNR: 19.313335418701172
[TRAIN] Iter: 196300 Loss: 0.026981225237250328  PSNR: 19.056074142456055
[TRAIN] Iter: 196400 Loss: 0.025120433419942856  PSNR: 19.17070198059082
[TRAIN] Iter: 196500 Loss: 0.03194316476583481  PSNR: 18.190351486206055
[TRAIN] Iter: 196600 Loss: 0.03321970999240875  PSNR: 17.83395004272461
[TRAIN] Iter: 196700 Loss: 0.029185231775045395  PSNR: 18.453922271728516
[TRAIN] Iter: 196800 Loss: 0.02278968319296837  PSNR: 19.703981399536133
[TRAIN] Iter: 196900 Loss: 0.034709326922893524  PSNR: 18.08334732055664
[TRAIN] Iter: 197000 Loss: 0.027698922902345657  PSNR: 18.54454803466797
[TRAIN] Iter: 197100 Loss: 0.03499334305524826  PSNR: 17.679426193237305
[TRAIN] Iter: 197200 Loss: 0.02882123738527298  PSNR: 18.438871383666992
[TRAIN] Iter: 197300 Loss: 0.030160032212734222  PSNR: 18.400650024414062
[TRAIN] Iter: 197400 Loss: 0.02818182483315468  PSNR: 18.768213272094727
[TRAIN] Iter: 197500 Loss: 0.028551334515213966  PSNR: 18.955272674560547
[TRAIN] Iter: 197600 Loss: 0.03143472224473953  PSNR: 18.30238151550293
[TRAIN] Iter: 197700 Loss: 0.02334640733897686  PSNR: 19.700546264648438
[TRAIN] Iter: 197800 Loss: 0.030315779149532318  PSNR: 18.733657836914062
[TRAIN] Iter: 197900 Loss: 0.03143327683210373  PSNR: 18.254859924316406
[TRAIN] Iter: 198000 Loss: 0.030550291761755943  PSNR: 18.505613327026367
[TRAIN] Iter: 198100 Loss: 0.03312121331691742  PSNR: 17.89958381652832
[TRAIN] Iter: 198200 Loss: 0.029990127310156822  PSNR: 18.481725692749023
[TRAIN] Iter: 198300 Loss: 0.026508299633860588  PSNR: 18.974380493164062
[TRAIN] Iter: 198400 Loss: 0.031034452840685844  PSNR: 18.423826217651367
[TRAIN] Iter: 198500 Loss: 0.021414022892713547  PSNR: 19.88797378540039
[TRAIN] Iter: 198600 Loss: 0.03152794390916824  PSNR: 18.203262329101562
[TRAIN] Iter: 198700 Loss: 0.031009012833237648  PSNR: 18.422225952148438
[TRAIN] Iter: 198800 Loss: 0.02585795894265175  PSNR: 19.138202667236328
[TRAIN] Iter: 198900 Loss: 0.03512882441282272  PSNR: 17.98145866394043
[TRAIN] Iter: 199000 Loss: 0.033010028302669525  PSNR: 18.040565490722656
[TRAIN] Iter: 199100 Loss: 0.025440968573093414  PSNR: 18.768198013305664
[TRAIN] Iter: 199200 Loss: 0.027616173028945923  PSNR: 18.488920211791992
[TRAIN] Iter: 199300 Loss: 0.02820710465312004  PSNR: 18.74147605895996
[TRAIN] Iter: 199400 Loss: 0.028861168771982193  PSNR: 18.903854370117188
[TRAIN] Iter: 199500 Loss: 0.03500045835971832  PSNR: 17.604734420776367
[TRAIN] Iter: 199600 Loss: 0.029774053022265434  PSNR: 18.51065444946289
[TRAIN] Iter: 199700 Loss: 0.03361591696739197  PSNR: 17.98368263244629
[TRAIN] Iter: 199800 Loss: 0.023131798952817917  PSNR: 19.680856704711914
[TRAIN] Iter: 199900 Loss: 0.03639005497097969  PSNR: 17.612178802490234
Saved checkpoints at ./logs/TUT-LAB-nerf/200000.tar
0 0.00040078163146972656
torch.Size([320, 640, 3]) torch.Size([320, 640])
1 18.222461223602295
2 15.268335580825806
3 18.816909790039062
4 15.77458930015564
5 16.886950969696045
6 15.265297174453735
7 18.38999032974243
8 15.269593238830566
9 18.3324613571167
10 15.338720321655273
11 18.307369709014893
12 15.29463005065918
13 18.232755661010742
14 15.239569425582886
15 18.27360963821411
16 15.2738778591156
17 18.23397707939148
18 15.251711368560791
19 18.149416208267212
20 15.328415870666504
21 18.102628469467163
22 15.450665950775146
23 17.884965896606445
24 15.50375485420227
25 15.517883777618408
26 17.996631622314453
27 15.540898561477661
28 17.970216751098633
29 15.542623043060303
30 17.957282543182373
31 15.52004599571228
32 17.99705934524536
33 15.568769693374634
34 17.988215446472168
35 15.43474006652832
36 17.83496403694153
37 15.261455535888672
38 18.24556064605713
39 15.270525693893433
40 18.346861124038696
41 15.31735110282898
42 18.869157791137695
43 15.885732412338257
44 16.794644117355347
45 15.320863485336304
46 18.37645387649536
47 15.657312154769897
48 17.704424142837524
49 12.938946723937988
50 15.146714210510254
51 17.799293518066406
52 15.626627445220947
53 18.005717277526855
54 15.322269439697266
55 18.211763620376587
56 15.411078929901123
57 18.463829278945923
58 16.238843202590942
59 18.573885440826416
60 16.260531425476074
61 18.806235313415527
62 15.91125774383545
63 18.903655767440796
64 15.852235794067383
65 18.65697145462036
66 15.975006103515625
67 18.845367908477783
68 15.992023706436157
69 18.720777988433838
70 15.797017097473145
71 18.95108699798584
72 15.825797319412231
73 18.65635919570923
74 15.87811017036438
75 18.73579692840576
76 16.092843770980835
77 18.972034454345703
78 16.32358169555664
79 18.203643798828125
80 16.329289436340332
81 18.27522921562195
82 18.43946599960327
83 16.510494709014893
84 18.45381236076355
85 16.455442905426025
86 18.354832410812378
87 16.40982222557068
88 18.38499116897583
89 16.379339694976807
90 18.41581654548645
91 16.09778356552124
92 18.089241981506348
93 16.06433343887329
94 18.12686848640442
95 16.063265085220337
96 18.14999008178711
97 16.085373640060425
98 18.13236713409424
99 16.110447645187378
100 18.151607513427734
101 16.0946364402771
102 18.116057872772217
103 16.05877923965454
104 17.98173451423645
105 15.964400291442871
106 18.330480575561523
107 16.070988655090332
108 18.2651469707489
109 16.10230827331543
110 18.06405544281006
111 15.85756778717041
112 18.516358613967896
113 16.053226232528687
114 18.158998727798462
115 15.957542896270752
116 15.925423622131348
117 18.349287033081055
118 15.86162805557251
119 18.310810089111328
Done, saving (120, 320, 640, 3) (120, 320, 640)
extras:{'raw': tensor([[[-2.7740e-01, -3.2656e-01,  2.8165e-02, -4.5033e+00],
         [-1.2677e+00, -7.0241e-01,  7.7693e-01, -1.2006e+01],
         [-7.6955e-01, -7.4625e-01, -4.8771e-01, -3.1091e+01],
         ...,
         [-3.2107e+00, -2.2922e+00, -1.7075e-01,  3.9115e+00],
         [-2.9798e+00, -2.2429e+00, -2.9012e-01, -3.4293e+00],
         [-3.0472e+00, -2.5294e+00, -5.4599e-01, -5.6491e+00]],

        [[-1.0315e+00, -9.2480e-01, -1.4698e+00, -6.4137e-01],
         [-1.0404e+00, -9.2593e-01, -1.4425e+00, -1.1067e+00],
         [-1.0645e+00, -9.4635e-01, -1.4658e+00, -1.8611e+00],
         ...,
         [ 5.2652e+00,  3.3352e+00,  3.4640e-01,  1.3408e+02],
         [ 4.8526e+00,  2.7581e+00, -4.5728e-01,  1.4613e+02],
         [-3.8196e-01, -1.8113e+00, -4.6637e+00,  1.6117e+02]],

        [[ 7.3018e-01,  6.7154e-01,  7.5175e-01, -1.3674e+00],
         [ 3.5686e-01,  3.6102e-01,  5.4141e-01,  8.3728e+00],
         [ 7.5170e-01,  7.2684e-01,  7.3086e-01, -1.4194e+00],
         ...,
         [-4.4763e+00, -9.3876e+00, -8.1242e+00,  6.2330e+02],
         [-3.1892e+00, -8.4928e+00, -8.1368e+00,  5.8935e+02],
         [-4.3480e+00, -9.3442e+00, -9.3505e+00,  6.0725e+02]],

        ...,

        [[-1.2192e+00, -1.1998e+00, -1.1894e+00, -1.5063e+00],
         [ 2.1658e+00,  1.9724e+00,  2.1819e+00, -2.5658e+01],
         [ 3.8009e-01,  4.3151e-01,  7.2909e-01, -3.4791e+01],
         ...,
         [ 1.9196e+00,  8.6966e-01, -1.6890e+00,  1.7268e+02],
         [ 2.6741e+00,  1.5641e+00, -9.3692e-01,  1.8036e+02],
         [ 2.3640e+00,  1.2393e+00, -1.4280e+00,  1.7990e+02]],

        [[ 8.7139e-02, -1.2274e-02, -3.0679e-01,  4.9468e+00],
         [ 1.6370e-01,  8.6227e-02, -9.0798e-03,  1.6830e+01],
         [ 1.3092e-01,  5.8061e-02, -3.1587e-02,  1.6813e+01],
         ...,
         [ 1.2780e+00,  2.4736e+00,  6.0456e+00,  1.6963e+02],
         [ 1.9658e+00,  3.0935e+00,  6.6466e+00,  1.7003e+02],
         [ 2.3881e+00,  3.4683e+00,  6.9057e+00,  1.7139e+02]],

        [[ 5.3450e-02, -1.4291e-01, -3.8592e-01,  2.0564e+01],
         [-6.9679e-02, -2.4968e-01, -5.0003e-01,  1.5650e+01],
         [ 1.3696e+00,  1.1147e+00,  7.9982e-01, -1.1316e+01],
         ...,
         [-4.0775e-01, -9.9889e-01, -3.3394e+00,  1.9422e+02],
         [-2.0916e-01, -8.0880e-01, -3.1087e+00,  1.9770e+02],
         [-5.2442e-01, -1.1300e+00, -3.5353e+00,  2.1242e+02]]],
       grad_fn=<ReshapeAliasBackward0>), 'rgb0': tensor([[0.2111, 0.3242, 0.6526],
        [0.2722, 0.2741, 0.1795],
        [0.6863, 0.6859, 0.7186],
        ...,
        [0.1808, 0.2000, 0.1917],
        [0.5187, 0.5023, 0.4550],
        [0.5331, 0.5009, 0.4807]], grad_fn=<ReshapeAliasBackward0>), 'disp0': tensor([ 17.3306, 103.5888,  22.8576,  ...,  16.2624, 153.4143,  83.3452],
       grad_fn=<ReshapeAliasBackward0>), 'acc0': tensor([1., 1., 1.,  ..., 1., 1., 1.], grad_fn=<ReshapeAliasBackward0>), 'z_std': tensor([0.0035, 0.0124, 0.0026,  ..., 0.0031, 0.0021, 0.0024])}
0 0.0005795955657958984
torch.Size([320, 640, 3]) torch.Size([320, 640])
1 15.836328029632568
2 15.656010389328003
3 18.65430974960327
4 15.555971145629883
5 18.74856948852539
6 15.50772500038147
7 18.7889666557312
8 15.413210153579712
9 18.89365315437317
10 15.398511171340942
11 18.93569850921631
12 15.400006771087646
13 18.847569465637207
14 15.333358764648438
15 19.19262409210205
16 15.403064489364624
17 18.56188941001892
18 15.338968276977539
19 18.99710726737976
20 15.36532473564148
21 18.927225351333618
22 15.43251371383667
23 18.76267409324646
24 15.023863554000854
25 18.577666521072388
26 15.597232341766357
27 17.856168508529663
28 15.242488145828247
29 17.756868362426758
30 15.488120317459106
31 18.06397795677185
32 15.435564041137695
33 15.114433765411377
34 18.54036808013916
35 15.227747917175293
36 18.223693370819092
37 15.114362955093384
38 18.290208339691162
39 15.170484781265259
40 18.297348499298096
41 15.218814611434937
42 18.291322946548462
43 15.3066987991333
44 18.25117826461792
45 14.900641918182373
46 17.40920114517212
47 15.094501495361328
48 18.20276427268982
49 15.12413501739502
50 16.39602279663086
51 13.489631175994873
52 13.459597826004028
53 16.335114002227783
54 13.369173526763916
55 16.361231327056885
56 13.368374109268188
57 13.539715766906738
58 16.2485568523407
59 13.77133560180664
60 16.94987726211548
61 13.98041296005249
62 16.86048698425293
63 13.919261693954468
64 14.398081541061401
65 16.761022090911865
66 14.052990913391113
67 16.97614312171936
68 14.027819156646729
69 16.928566217422485
70 13.906158208847046
71 16.901899576187134
72 13.948921203613281
73 14.113978385925293
74 16.79112696647644
75 13.843372583389282
76 17.0938663482666
77 13.852112293243408
78 16.95398259162903
79 13.9151930809021
80 13.81839632987976
81 16.90557098388672
82 13.842129468917847
83 17.189286708831787
84 13.917726039886475
85 17.186711311340332
86 13.934041500091553
87 13.762877941131592
88 17.37026333808899
89 13.497193098068237
90 17.00115442276001
91 13.48563814163208
92 16.95953893661499
93 13.865541458129883
94 13.645829677581787
95 17.03908944129944
96 13.58056902885437
97 17.191709756851196
98 14.077636003494263
99 16.02378749847412
100 13.683053255081177
101 16.90268325805664
102 14.102329015731812
103 14.077446937561035
104 17.236111164093018
105 13.789604425430298
106 17.279621601104736
107 13.679162740707397
108 16.976484775543213
109 14.23114013671875
110 14.418370485305786
111 17.01370072364807
112 14.228918552398682
113 16.906955003738403
114 14.20230221748352
115 16.861323833465576
116 14.257538080215454
117 16.879814863204956
118 14.276021480560303
119 14.211663484573364
test poses shape torch.Size([13, 3, 4])
0 0.0005199909210205078
torch.Size([320, 640, 3]) torch.Size([320, 640])
1 14.2840735912323
2 16.92901110649109
3 14.24874234199524
4 16.93404197692871
5 14.122823715209961
6 16.784430980682373
7 14.137205123901367
8 14.102314710617065
9 16.771206855773926
10 13.971137762069702
11 16.85058355331421
12 13.733453750610352
Saved test set
[TRAIN] Iter: 200000 Loss: 0.02869848720729351  PSNR: 18.549030303955078
[TRAIN] Iter: 200100 Loss: 0.031416572630405426  PSNR: 18.110477447509766
[TRAIN] Iter: 200200 Loss: 0.03552760183811188  PSNR: 17.76861572265625
[TRAIN] Iter: 200300 Loss: 0.026410741731524467  PSNR: 19.06481170654297
[TRAIN] Iter: 200400 Loss: 0.031253084540367126  PSNR: 18.314260482788086
[TRAIN] Iter: 200500 Loss: 0.026573047041893005  PSNR: 18.881465911865234
[TRAIN] Iter: 200600 Loss: 0.03652917593717575  PSNR: 17.69107437133789
[TRAIN] Iter: 200700 Loss: 0.03353126719594002  PSNR: 17.7758731842041
[TRAIN] Iter: 200800 Loss: 0.023508895188570023  PSNR: 19.43122100830078
[TRAIN] Iter: 200900 Loss: 0.0321388840675354  PSNR: 18.215839385986328
[TRAIN] Iter: 201000 Loss: 0.03680688887834549  PSNR: 17.463205337524414
[TRAIN] Iter: 201100 Loss: 0.027941331267356873  PSNR: 18.64423370361328
[TRAIN] Iter: 201200 Loss: 0.03331928700208664  PSNR: 17.931264877319336
[TRAIN] Iter: 201300 Loss: 0.02549378015100956  PSNR: 19.062936782836914
[TRAIN] Iter: 201400 Loss: 0.02793923206627369  PSNR: 18.865686416625977
[TRAIN] Iter: 201500 Loss: 0.025644581764936447  PSNR: 19.247238159179688
[TRAIN] Iter: 201600 Loss: 0.02901173010468483  PSNR: 18.45193862915039
[TRAIN] Iter: 201700 Loss: 0.03609782084822655  PSNR: 17.573301315307617
[TRAIN] Iter: 201800 Loss: 0.027549652382731438  PSNR: 18.570629119873047
[TRAIN] Iter: 201900 Loss: 0.02585170790553093  PSNR: 19.084827423095703
[TRAIN] Iter: 202000 Loss: 0.03081541880965233  PSNR: 18.305158615112305
[TRAIN] Iter: 202100 Loss: 0.02668837457895279  PSNR: 18.783815383911133
[TRAIN] Iter: 202200 Loss: 0.036195188760757446  PSNR: 17.592483520507812
[TRAIN] Iter: 202300 Loss: 0.02514263615012169  PSNR: 19.338937759399414
[TRAIN] Iter: 202400 Loss: 0.03414788469672203  PSNR: 17.830821990966797
[TRAIN] Iter: 202500 Loss: 0.034403495490550995  PSNR: 17.91053581237793
[TRAIN] Iter: 202600 Loss: 0.027979250997304916  PSNR: 18.49991226196289
[TRAIN] Iter: 202700 Loss: 0.0264214389026165  PSNR: 19.12296485900879
[TRAIN] Iter: 202800 Loss: 0.03163015469908714  PSNR: 18.319852828979492
[TRAIN] Iter: 202900 Loss: 0.02549649029970169  PSNR: 19.22142791748047
[TRAIN] Iter: 203000 Loss: 0.021534958854317665  PSNR: 19.867504119873047
[TRAIN] Iter: 203100 Loss: 0.023252008482813835  PSNR: 19.64063835144043
[TRAIN] Iter: 203200 Loss: 0.03584756702184677  PSNR: 17.69881820678711
[TRAIN] Iter: 203300 Loss: 0.028882015496492386  PSNR: 18.54267692565918
[TRAIN] Iter: 203400 Loss: 0.029684392735362053  PSNR: 18.618013381958008
[TRAIN] Iter: 203500 Loss: 0.03129372000694275  PSNR: 18.136877059936523
[TRAIN] Iter: 203600 Loss: 0.028139133006334305  PSNR: 18.599302291870117
[TRAIN] Iter: 203700 Loss: 0.03399500250816345  PSNR: 17.925048828125
[TRAIN] Iter: 203800 Loss: 0.035893842577934265  PSNR: 17.635257720947266
[TRAIN] Iter: 203900 Loss: 0.028257181867957115  PSNR: 18.661184310913086
[TRAIN] Iter: 204000 Loss: 0.025158386677503586  PSNR: 18.980262756347656
[TRAIN] Iter: 204100 Loss: 0.025598069652915  PSNR: 19.108633041381836
[TRAIN] Iter: 204200 Loss: 0.023703716695308685  PSNR: 19.389118194580078
[TRAIN] Iter: 204300 Loss: 0.020789949223399162  PSNR: 20.078327178955078
[TRAIN] Iter: 204400 Loss: 0.027801048010587692  PSNR: 18.944089889526367
[TRAIN] Iter: 204500 Loss: 0.029162384569644928  PSNR: 18.676761627197266
[TRAIN] Iter: 204600 Loss: 0.029314208775758743  PSNR: 18.651165008544922
[TRAIN] Iter: 204700 Loss: 0.029071008786559105  PSNR: 18.461036682128906
[TRAIN] Iter: 204800 Loss: 0.02928793802857399  PSNR: 18.542083740234375
[TRAIN] Iter: 204900 Loss: 0.022618144750595093  PSNR: 19.629375457763672
[TRAIN] Iter: 205000 Loss: 0.033677082508802414  PSNR: 17.8858699798584
[TRAIN] Iter: 205100 Loss: 0.029883062466979027  PSNR: 18.533283233642578
[TRAIN] Iter: 205200 Loss: 0.030613191425800323  PSNR: 18.600770950317383
[TRAIN] Iter: 205300 Loss: 0.03229120746254921  PSNR: 18.047897338867188
[TRAIN] Iter: 205400 Loss: 0.030052311718463898  PSNR: 18.33344268798828
[TRAIN] Iter: 205500 Loss: 0.032082367688417435  PSNR: 18.044919967651367
[TRAIN] Iter: 205600 Loss: 0.028239049017429352  PSNR: 18.177780151367188
[TRAIN] Iter: 205700 Loss: 0.028996000066399574  PSNR: 18.69036293029785
[TRAIN] Iter: 205800 Loss: 0.031271956861019135  PSNR: 18.724367141723633
[TRAIN] Iter: 205900 Loss: 0.031360894441604614  PSNR: 18.172903060913086
[TRAIN] Iter: 206000 Loss: 0.02983184903860092  PSNR: 18.477890014648438
[TRAIN] Iter: 206100 Loss: 0.030438778921961784  PSNR: 18.609262466430664
[TRAIN] Iter: 206200 Loss: 0.025508929044008255  PSNR: 19.07575035095215
[TRAIN] Iter: 206300 Loss: 0.031922221183776855  PSNR: 18.144020080566406
[TRAIN] Iter: 206400 Loss: 0.031215783208608627  PSNR: 18.167078018188477
[TRAIN] Iter: 206500 Loss: 0.028024984523653984  PSNR: 18.503950119018555
[TRAIN] Iter: 206600 Loss: 0.028630562126636505  PSNR: 18.617494583129883
[TRAIN] Iter: 206700 Loss: 0.034317824989557266  PSNR: 17.90911102294922
[TRAIN] Iter: 206800 Loss: 0.024600373581051826  PSNR: 19.393535614013672
[TRAIN] Iter: 206900 Loss: 0.030558699741959572  PSNR: 18.371530532836914
[TRAIN] Iter: 207000 Loss: 0.03235907480120659  PSNR: 18.16238784790039
[TRAIN] Iter: 207100 Loss: 0.026632070541381836  PSNR: 18.99785614013672
[TRAIN] Iter: 207200 Loss: 0.03128534555435181  PSNR: 18.387537002563477
[TRAIN] Iter: 207300 Loss: 0.028009068220853806  PSNR: 18.743427276611328
[TRAIN] Iter: 207400 Loss: 0.029434338212013245  PSNR: 18.592275619506836
[TRAIN] Iter: 207500 Loss: 0.03257451951503754  PSNR: 17.96924591064453
[TRAIN] Iter: 207600 Loss: 0.025248242542147636  PSNR: 19.31367301940918
[TRAIN] Iter: 207700 Loss: 0.03296110779047012  PSNR: 17.959897994995117
[TRAIN] Iter: 207800 Loss: 0.02600129507482052  PSNR: 18.767799377441406
[TRAIN] Iter: 207900 Loss: 0.03476009890437126  PSNR: 17.652372360229492
[TRAIN] Iter: 208000 Loss: 0.027468163520097733  PSNR: 18.85909652709961
[TRAIN] Iter: 208100 Loss: 0.028811980038881302  PSNR: 18.631784439086914
[TRAIN] Iter: 208200 Loss: 0.03274852782487869  PSNR: 18.004709243774414
[TRAIN] Iter: 208300 Loss: 0.025616567581892014  PSNR: 18.834182739257812
[TRAIN] Iter: 208400 Loss: 0.028365090489387512  PSNR: 19.014009475708008
[TRAIN] Iter: 208500 Loss: 0.02621794492006302  PSNR: 19.08446502685547
[TRAIN] Iter: 208600 Loss: 0.0314825177192688  PSNR: 18.294574737548828
[TRAIN] Iter: 208700 Loss: 0.029085058718919754  PSNR: 18.60163116455078
[TRAIN] Iter: 208800 Loss: 0.02121114172041416  PSNR: 19.977060317993164
[TRAIN] Iter: 208900 Loss: 0.02888977713882923  PSNR: 18.401206970214844
[TRAIN] Iter: 209000 Loss: 0.03233574330806732  PSNR: 17.87755584716797
[TRAIN] Iter: 209100 Loss: 0.03135033696889877  PSNR: 18.30605125427246
[TRAIN] Iter: 209200 Loss: 0.03176487982273102  PSNR: 18.30132293701172
[TRAIN] Iter: 209300 Loss: 0.026632023975253105  PSNR: 18.67884063720703
[TRAIN] Iter: 209400 Loss: 0.030311545357108116  PSNR: 18.418598175048828
[TRAIN] Iter: 209500 Loss: 0.03247382864356041  PSNR: 18.005563735961914
[TRAIN] Iter: 209600 Loss: 0.03375426307320595  PSNR: 18.45806312561035
[TRAIN] Iter: 209700 Loss: 0.03101985529065132  PSNR: 18.36150550842285
[TRAIN] Iter: 209800 Loss: 0.027125351130962372  PSNR: 18.75756072998047
[TRAIN] Iter: 209900 Loss: 0.028824279084801674  PSNR: 18.58526039123535
Saved checkpoints at ./logs/TUT-LAB-nerf/210000.tar
[TRAIN] Iter: 210000 Loss: 0.03281564638018608  PSNR: 18.257518768310547
[TRAIN] Iter: 210100 Loss: 0.026313776150345802  PSNR: 18.85544204711914
[TRAIN] Iter: 210200 Loss: 0.034606657922267914  PSNR: 17.633092880249023
[TRAIN] Iter: 210300 Loss: 0.02160317450761795  PSNR: 19.946456909179688
[TRAIN] Iter: 210400 Loss: 0.02797274850308895  PSNR: 18.839599609375
[TRAIN] Iter: 210500 Loss: 0.030512001365423203  PSNR: 18.41501808166504
[TRAIN] Iter: 210600 Loss: 0.029362931847572327  PSNR: 18.54526710510254
[TRAIN] Iter: 210700 Loss: 0.028171081095933914  PSNR: 18.95953941345215
[TRAIN] Iter: 210800 Loss: 0.03279895335435867  PSNR: 18.085168838500977
[TRAIN] Iter: 210900 Loss: 0.03254988044500351  PSNR: 18.094274520874023
[TRAIN] Iter: 211000 Loss: 0.02828344702720642  PSNR: 18.847810745239258
[TRAIN] Iter: 211100 Loss: 0.029143380001187325  PSNR: 18.633371353149414
[TRAIN] Iter: 211200 Loss: 0.029148126021027565  PSNR: 18.48764991760254
[TRAIN] Iter: 211300 Loss: 0.029124850407242775  PSNR: 18.339101791381836
[TRAIN] Iter: 211400 Loss: 0.030522968620061874  PSNR: 18.359527587890625
[TRAIN] Iter: 211500 Loss: 0.03454550355672836  PSNR: 17.73493194580078
[TRAIN] Iter: 211600 Loss: 0.029400702565908432  PSNR: 18.51985740661621
[TRAIN] Iter: 211700 Loss: 0.03119877353310585  PSNR: 18.307872772216797
[TRAIN] Iter: 211800 Loss: 0.02491459622979164  PSNR: 19.24350357055664
[TRAIN] Iter: 211900 Loss: 0.021772747859358788  PSNR: 19.803218841552734
[TRAIN] Iter: 212000 Loss: 0.028478000313043594  PSNR: 18.637989044189453
[TRAIN] Iter: 212100 Loss: 0.025141127407550812  PSNR: 19.265424728393555
[TRAIN] Iter: 212200 Loss: 0.024643531069159508  PSNR: 19.223392486572266
[TRAIN] Iter: 212300 Loss: 0.03275609016418457  PSNR: 17.975624084472656
[TRAIN] Iter: 212400 Loss: 0.02578815631568432  PSNR: 19.106252670288086
[TRAIN] Iter: 212500 Loss: 0.026181837543845177  PSNR: 18.827043533325195
[TRAIN] Iter: 212600 Loss: 0.035438477993011475  PSNR: 17.841848373413086
[TRAIN] Iter: 212700 Loss: 0.02186300978064537  PSNR: 19.721168518066406
[TRAIN] Iter: 212800 Loss: 0.034655362367630005  PSNR: 17.761262893676758
[TRAIN] Iter: 212900 Loss: 0.03682435303926468  PSNR: 17.639427185058594
[TRAIN] Iter: 213000 Loss: 0.03387641906738281  PSNR: 18.02935028076172
[TRAIN] Iter: 213100 Loss: 0.03345480561256409  PSNR: 17.81372833251953
[TRAIN] Iter: 213200 Loss: 0.02387913316488266  PSNR: 19.510042190551758
[TRAIN] Iter: 213300 Loss: 0.034046366810798645  PSNR: 17.744842529296875
[TRAIN] Iter: 213400 Loss: 0.026291975751519203  PSNR: 18.84807586669922
[TRAIN] Iter: 213500 Loss: 0.026100430637598038  PSNR: 19.035993576049805
[TRAIN] Iter: 213600 Loss: 0.031750261783599854  PSNR: 18.02236557006836
[TRAIN] Iter: 213700 Loss: 0.028653942048549652  PSNR: 18.92903709411621
[TRAIN] Iter: 213800 Loss: 0.03193505480885506  PSNR: 18.193206787109375
[TRAIN] Iter: 213900 Loss: 0.02510419674217701  PSNR: 19.17761993408203
[TRAIN] Iter: 214000 Loss: 0.02877352386713028  PSNR: 18.390716552734375
[TRAIN] Iter: 214100 Loss: 0.031211916357278824  PSNR: 18.217411041259766
[TRAIN] Iter: 214200 Loss: 0.02914927899837494  PSNR: 18.560216903686523
[TRAIN] Iter: 214300 Loss: 0.02521098032593727  PSNR: 18.81678009033203
[TRAIN] Iter: 214400 Loss: 0.030257074162364006  PSNR: 18.52153778076172
[TRAIN] Iter: 214500 Loss: 0.030755383893847466  PSNR: 18.278545379638672
[TRAIN] Iter: 214600 Loss: 0.026554791256785393  PSNR: 18.737903594970703
[TRAIN] Iter: 214700 Loss: 0.024995308369398117  PSNR: 19.264083862304688
[TRAIN] Iter: 214800 Loss: 0.03397750481963158  PSNR: 17.94939422607422
[TRAIN] Iter: 214900 Loss: 0.02854492887854576  PSNR: 18.63764190673828
[TRAIN] Iter: 215000 Loss: 0.021011747419834137  PSNR: 19.982749938964844
[TRAIN] Iter: 215100 Loss: 0.025355860590934753  PSNR: 18.903051376342773
[TRAIN] Iter: 215200 Loss: 0.027198433876037598  PSNR: 18.4504337310791
[TRAIN] Iter: 215300 Loss: 0.03682195022702217  PSNR: 17.51544189453125
[TRAIN] Iter: 215400 Loss: 0.03751501068472862  PSNR: 17.39837074279785
[TRAIN] Iter: 215500 Loss: 0.034113809466362  PSNR: 17.64588737487793
[TRAIN] Iter: 215600 Loss: 0.02385600656270981  PSNR: 19.38905143737793
[TRAIN] Iter: 215700 Loss: 0.032405368983745575  PSNR: 18.20839500427246
[TRAIN] Iter: 215800 Loss: 0.02332240715622902  PSNR: 19.80156707763672
[TRAIN] Iter: 215900 Loss: 0.02960311435163021  PSNR: 18.032745361328125
[TRAIN] Iter: 216000 Loss: 0.03462839871644974  PSNR: 17.792879104614258
[TRAIN] Iter: 216100 Loss: 0.02838476374745369  PSNR: 18.713642120361328
[TRAIN] Iter: 216200 Loss: 0.0334622785449028  PSNR: 17.98113441467285
[TRAIN] Iter: 216300 Loss: 0.03466226905584335  PSNR: 17.830339431762695
[TRAIN] Iter: 216400 Loss: 0.0300399549305439  PSNR: 18.455474853515625
[TRAIN] Iter: 216500 Loss: 0.031395792961120605  PSNR: 18.27721405029297
[TRAIN] Iter: 216600 Loss: 0.03092772327363491  PSNR: 18.564943313598633
[TRAIN] Iter: 216700 Loss: 0.025469817221164703  PSNR: 18.963176727294922
[TRAIN] Iter: 216800 Loss: 0.028493542224168777  PSNR: 18.710710525512695
[TRAIN] Iter: 216900 Loss: 0.027050388976931572  PSNR: 18.654943466186523
[TRAIN] Iter: 217000 Loss: 0.028130697086453438  PSNR: 19.08625602722168
[TRAIN] Iter: 217100 Loss: 0.028923235833644867  PSNR: 18.510820388793945
[TRAIN] Iter: 217200 Loss: 0.03231704235076904  PSNR: 18.019569396972656
[TRAIN] Iter: 217300 Loss: 0.02836965024471283  PSNR: 18.693880081176758
[TRAIN] Iter: 217400 Loss: 0.032015927135944366  PSNR: 18.198978424072266
[TRAIN] Iter: 217500 Loss: 0.025554150342941284  PSNR: 19.082542419433594
[TRAIN] Iter: 217600 Loss: 0.022062700241804123  PSNR: 19.432575225830078
[TRAIN] Iter: 217700 Loss: 0.025274651125073433  PSNR: 19.180566787719727
[TRAIN] Iter: 217800 Loss: 0.02894841693341732  PSNR: 18.65477180480957
[TRAIN] Iter: 217900 Loss: 0.02997066080570221  PSNR: 18.557735443115234
[TRAIN] Iter: 218000 Loss: 0.0333574116230011  PSNR: 17.855409622192383
[TRAIN] Iter: 218100 Loss: 0.0276923980563879  PSNR: 18.62228012084961
[TRAIN] Iter: 218200 Loss: 0.026736542582511902  PSNR: 18.962833404541016
[TRAIN] Iter: 218300 Loss: 0.024900486692786217  PSNR: 19.466842651367188
[TRAIN] Iter: 218400 Loss: 0.03313680365681648  PSNR: 17.99462890625
[TRAIN] Iter: 218500 Loss: 0.02917742170393467  PSNR: 18.609956741333008
[TRAIN] Iter: 218600 Loss: 0.033929627388715744  PSNR: 17.8933162689209
[TRAIN] Iter: 218700 Loss: 0.030588457360863686  PSNR: 18.335729598999023
[TRAIN] Iter: 218800 Loss: 0.02590996026992798  PSNR: 19.006887435913086
[TRAIN] Iter: 218900 Loss: 0.03104427456855774  PSNR: 18.182228088378906
[TRAIN] Iter: 219000 Loss: 0.031641144305467606  PSNR: 18.261924743652344
[TRAIN] Iter: 219100 Loss: 0.03207070380449295  PSNR: 18.245140075683594
[TRAIN] Iter: 219200 Loss: 0.02883303537964821  PSNR: 18.511638641357422
[TRAIN] Iter: 219300 Loss: 0.026253217831254005  PSNR: 19.13498306274414
[TRAIN] Iter: 219400 Loss: 0.02922671101987362  PSNR: 18.82223892211914
[TRAIN] Iter: 219500 Loss: 0.032540615648031235  PSNR: 18.042709350585938
[TRAIN] Iter: 219600 Loss: 0.03471408784389496  PSNR: 17.801189422607422
[TRAIN] Iter: 219700 Loss: 0.02692604437470436  PSNR: 19.04035186767578
[TRAIN] Iter: 219800 Loss: 0.025197753682732582  PSNR: 19.189470291137695
[TRAIN] Iter: 219900 Loss: 0.030485253781080246  PSNR: 18.420997619628906
Saved checkpoints at ./logs/TUT-LAB-nerf/220000.tar
[TRAIN] Iter: 220000 Loss: 0.03676138445734978  PSNR: 17.49642562866211
[TRAIN] Iter: 220100 Loss: 0.025212643668055534  PSNR: 19.50062370300293
[TRAIN] Iter: 220200 Loss: 0.025996973738074303  PSNR: 19.086700439453125
[TRAIN] Iter: 220300 Loss: 0.029413387179374695  PSNR: 18.451946258544922
[TRAIN] Iter: 220400 Loss: 0.03113538958132267  PSNR: 18.03691291809082
[TRAIN] Iter: 220500 Loss: 0.032191161066293716  PSNR: 18.186784744262695
[TRAIN] Iter: 220600 Loss: 0.024075252935290337  PSNR: 19.400129318237305
[TRAIN] Iter: 220700 Loss: 0.027289651334285736  PSNR: 18.927425384521484
[TRAIN] Iter: 220800 Loss: 0.02852822281420231  PSNR: 18.458969116210938
[TRAIN] Iter: 220900 Loss: 0.0363488495349884  PSNR: 17.627283096313477
[TRAIN] Iter: 221000 Loss: 0.03040214441716671  PSNR: 18.364681243896484
[TRAIN] Iter: 221100 Loss: 0.031186960637569427  PSNR: 18.28389549255371
[TRAIN] Iter: 221200 Loss: 0.026630792766809464  PSNR: 18.836990356445312
[TRAIN] Iter: 221300 Loss: 0.030668985098600388  PSNR: 18.603776931762695
[TRAIN] Iter: 221400 Loss: 0.029976455494761467  PSNR: 18.74261474609375
[TRAIN] Iter: 221500 Loss: 0.028520014137029648  PSNR: 18.887781143188477
[TRAIN] Iter: 221600 Loss: 0.02935177832841873  PSNR: 18.292686462402344
[TRAIN] Iter: 221700 Loss: 0.02795933000743389  PSNR: 18.812715530395508
[TRAIN] Iter: 221800 Loss: 0.0347805991768837  PSNR: 17.698198318481445
[TRAIN] Iter: 221900 Loss: 0.029348209500312805  PSNR: 18.62384033203125
[TRAIN] Iter: 222000 Loss: 0.03176970034837723  PSNR: 18.017362594604492
[TRAIN] Iter: 222100 Loss: 0.02957814186811447  PSNR: 18.596271514892578
[TRAIN] Iter: 222200 Loss: 0.024070167914032936  PSNR: 19.359291076660156
[TRAIN] Iter: 222300 Loss: 0.02715574949979782  PSNR: 18.594383239746094
[TRAIN] Iter: 222400 Loss: 0.02843036875128746  PSNR: 18.925220489501953
[TRAIN] Iter: 222500 Loss: 0.025816163048148155  PSNR: 19.145132064819336
[TRAIN] Iter: 222600 Loss: 0.02515757456421852  PSNR: 19.235963821411133
[TRAIN] Iter: 222700 Loss: 0.0212840773165226  PSNR: 19.92472267150879
[TRAIN] Iter: 222800 Loss: 0.027970153838396072  PSNR: 18.923110961914062
[TRAIN] Iter: 222900 Loss: 0.03415481746196747  PSNR: 17.85545539855957
[TRAIN] Iter: 223000 Loss: 0.025213927030563354  PSNR: 19.13010597229004
[TRAIN] Iter: 223100 Loss: 0.029387492686510086  PSNR: 18.268815994262695
[TRAIN] Iter: 223200 Loss: 0.021491393446922302  PSNR: 19.885976791381836
[TRAIN] Iter: 223300 Loss: 0.0215873084962368  PSNR: 19.71921730041504
[TRAIN] Iter: 223400 Loss: 0.026611831039190292  PSNR: 18.855953216552734
[TRAIN] Iter: 223500 Loss: 0.02773725986480713  PSNR: 18.541257858276367
[TRAIN] Iter: 223600 Loss: 0.033258602023124695  PSNR: 17.977014541625977
[TRAIN] Iter: 223700 Loss: 0.03191012889146805  PSNR: 18.047529220581055
[TRAIN] Iter: 223800 Loss: 0.025528069585561752  PSNR: 19.203147888183594
[TRAIN] Iter: 223900 Loss: 0.029951896518468857  PSNR: 18.586790084838867
[TRAIN] Iter: 224000 Loss: 0.03229992091655731  PSNR: 18.02549934387207
[TRAIN] Iter: 224100 Loss: 0.03486005961894989  PSNR: 17.827932357788086
[TRAIN] Iter: 224200 Loss: 0.034017935395240784  PSNR: 17.873170852661133
[TRAIN] Iter: 224300 Loss: 0.03356392681598663  PSNR: 18.114961624145508
[TRAIN] Iter: 224400 Loss: 0.02142968587577343  PSNR: 19.864002227783203
[TRAIN] Iter: 224500 Loss: 0.02367531880736351  PSNR: 19.12853240966797
[TRAIN] Iter: 224600 Loss: 0.024211043491959572  PSNR: 19.355173110961914
[TRAIN] Iter: 224700 Loss: 0.035566963255405426  PSNR: 17.73740005493164
[TRAIN] Iter: 224800 Loss: 0.02676590345799923  PSNR: 19.027393341064453
[TRAIN] Iter: 224900 Loss: 0.027813296765089035  PSNR: 18.77796745300293
[TRAIN] Iter: 225000 Loss: 0.03250259906053543  PSNR: 18.04600715637207
[TRAIN] Iter: 225100 Loss: 0.026809459552168846  PSNR: 18.57023811340332
[TRAIN] Iter: 225200 Loss: 0.02857634425163269  PSNR: 18.330097198486328
[TRAIN] Iter: 225300 Loss: 0.0272023044526577  PSNR: 19.04579734802246
[TRAIN] Iter: 225400 Loss: 0.03600902855396271  PSNR: 17.6131591796875
[TRAIN] Iter: 225500 Loss: 0.027404386550188065  PSNR: 18.62249755859375
[TRAIN] Iter: 225600 Loss: 0.02496739849448204  PSNR: 18.860565185546875
[TRAIN] Iter: 225700 Loss: 0.03432084992527962  PSNR: 17.810901641845703
[TRAIN] Iter: 225800 Loss: 0.030477462336421013  PSNR: 18.42748260498047
[TRAIN] Iter: 225900 Loss: 0.02845187298953533  PSNR: 18.815576553344727
[TRAIN] Iter: 226000 Loss: 0.029877938330173492  PSNR: 18.46361541748047
[TRAIN] Iter: 226100 Loss: 0.03018154390156269  PSNR: 18.589221954345703
[TRAIN] Iter: 226200 Loss: 0.028797615319490433  PSNR: 18.739164352416992
[TRAIN] Iter: 226300 Loss: 0.029664674773812294  PSNR: 18.377639770507812
[TRAIN] Iter: 226400 Loss: 0.02270491048693657  PSNR: 19.54834747314453
[TRAIN] Iter: 226500 Loss: 0.02601075917482376  PSNR: 19.381290435791016
[TRAIN] Iter: 226600 Loss: 0.029597552493214607  PSNR: 18.577932357788086
[TRAIN] Iter: 226700 Loss: 0.02675042860209942  PSNR: 18.94974708557129
[TRAIN] Iter: 226800 Loss: 0.027415389195084572  PSNR: 18.60256576538086
[TRAIN] Iter: 226900 Loss: 0.029999755322933197  PSNR: 18.375572204589844
[TRAIN] Iter: 227000 Loss: 0.0239485464990139  PSNR: 19.08974838256836
[TRAIN] Iter: 227100 Loss: 0.02557399496436119  PSNR: 19.10673713684082
[TRAIN] Iter: 227200 Loss: 0.023051340132951736  PSNR: 19.49230194091797
[TRAIN] Iter: 227300 Loss: 0.03327692672610283  PSNR: 17.964597702026367
[TRAIN] Iter: 227400 Loss: 0.02694518305361271  PSNR: 19.0288028717041
[TRAIN] Iter: 227500 Loss: 0.03647322952747345  PSNR: 17.45889663696289
[TRAIN] Iter: 227600 Loss: 0.02883484959602356  PSNR: 18.643985748291016
[TRAIN] Iter: 227700 Loss: 0.026358921080827713  PSNR: 18.704364776611328
[TRAIN] Iter: 227800 Loss: 0.029998358339071274  PSNR: 18.54731559753418
[TRAIN] Iter: 227900 Loss: 0.034276075661182404  PSNR: 17.812833786010742
[TRAIN] Iter: 228000 Loss: 0.03507842868566513  PSNR: 17.90569305419922
[TRAIN] Iter: 228100 Loss: 0.0247807614505291  PSNR: 19.36928367614746
[TRAIN] Iter: 228200 Loss: 0.03401806578040123  PSNR: 17.78018569946289
[TRAIN] Iter: 228300 Loss: 0.032992441207170486  PSNR: 18.076339721679688
[TRAIN] Iter: 228400 Loss: 0.021193500608205795  PSNR: 19.804157257080078
[TRAIN] Iter: 228500 Loss: 0.02319744974374771  PSNR: 19.563974380493164
[TRAIN] Iter: 228600 Loss: 0.031280018389225006  PSNR: 18.3729305267334
[TRAIN] Iter: 228700 Loss: 0.032383132725954056  PSNR: 18.082120895385742
[TRAIN] Iter: 228800 Loss: 0.030365794897079468  PSNR: 18.3509578704834
[TRAIN] Iter: 228900 Loss: 0.029467729851603508  PSNR: 18.365713119506836
[TRAIN] Iter: 229000 Loss: 0.033658016473054886  PSNR: 17.88780975341797
[TRAIN] Iter: 229100 Loss: 0.027630923315882683  PSNR: 18.64980125427246
[TRAIN] Iter: 229200 Loss: 0.0283343642950058  PSNR: 18.798921585083008
[TRAIN] Iter: 229300 Loss: 0.03643427789211273  PSNR: 17.68166732788086
[TRAIN] Iter: 229400 Loss: 0.028391262516379356  PSNR: 18.26110076904297
[TRAIN] Iter: 229500 Loss: 0.035405077040195465  PSNR: 17.883684158325195
[TRAIN] Iter: 229600 Loss: 0.031565144658088684  PSNR: 18.245573043823242
[TRAIN] Iter: 229700 Loss: 0.02308860421180725  PSNR: 19.65314292907715
[TRAIN] Iter: 229800 Loss: 0.032943401485681534  PSNR: 17.957542419433594
[TRAIN] Iter: 229900 Loss: 0.026452891528606415  PSNR: 18.922117233276367
Saved checkpoints at ./logs/TUT-LAB-nerf/230000.tar
[TRAIN] Iter: 230000 Loss: 0.029087688773870468  PSNR: 18.77798843383789
[TRAIN] Iter: 230100 Loss: 0.03753524273633957  PSNR: 17.588003158569336
[TRAIN] Iter: 230200 Loss: 0.0326274037361145  PSNR: 18.147891998291016
[TRAIN] Iter: 230300 Loss: 0.033786989748477936  PSNR: 17.946470260620117
[TRAIN] Iter: 230400 Loss: 0.02980457991361618  PSNR: 18.799407958984375
[TRAIN] Iter: 230500 Loss: 0.023475734516978264  PSNR: 19.448442459106445
[TRAIN] Iter: 230600 Loss: 0.031217064708471298  PSNR: 18.19355010986328
[TRAIN] Iter: 230700 Loss: 0.03035534918308258  PSNR: 18.419790267944336
[TRAIN] Iter: 230800 Loss: 0.034985486418008804  PSNR: 17.670215606689453
[TRAIN] Iter: 230900 Loss: 0.033262334764003754  PSNR: 18.22610855102539
[TRAIN] Iter: 231000 Loss: 0.029532987624406815  PSNR: 18.466154098510742
[TRAIN] Iter: 231100 Loss: 0.029075665399432182  PSNR: 18.769588470458984
[TRAIN] Iter: 231200 Loss: 0.032912805676460266  PSNR: 17.911060333251953
[TRAIN] Iter: 231300 Loss: 0.036198075860738754  PSNR: 17.500246047973633
[TRAIN] Iter: 231400 Loss: 0.027649343013763428  PSNR: 18.954418182373047
[TRAIN] Iter: 231500 Loss: 0.028281472623348236  PSNR: 18.497541427612305
[TRAIN] Iter: 231600 Loss: 0.03377021849155426  PSNR: 18.040260314941406
[TRAIN] Iter: 231700 Loss: 0.025282524526119232  PSNR: 19.109628677368164
[TRAIN] Iter: 231800 Loss: 0.027619771659374237  PSNR: 18.922760009765625
[TRAIN] Iter: 231900 Loss: 0.03223476558923721  PSNR: 18.229305267333984
[TRAIN] Iter: 232000 Loss: 0.026492692530155182  PSNR: 18.919471740722656
[TRAIN] Iter: 232100 Loss: 0.02649572491645813  PSNR: 18.957185745239258
[TRAIN] Iter: 232200 Loss: 0.030686024576425552  PSNR: 18.321353912353516
[TRAIN] Iter: 232300 Loss: 0.02805108018219471  PSNR: 18.724225997924805
[TRAIN] Iter: 232400 Loss: 0.03188636153936386  PSNR: 18.088886260986328
[TRAIN] Iter: 232500 Loss: 0.023898152634501457  PSNR: 19.593664169311523
[TRAIN] Iter: 232600 Loss: 0.02778506651520729  PSNR: 18.377235412597656
[TRAIN] Iter: 232700 Loss: 0.03297678381204605  PSNR: 17.884723663330078
[TRAIN] Iter: 232800 Loss: 0.02546807751059532  PSNR: 18.736221313476562
[TRAIN] Iter: 232900 Loss: 0.03071899339556694  PSNR: 18.52726173400879
[TRAIN] Iter: 233000 Loss: 0.035004518926143646  PSNR: 17.774559020996094
[TRAIN] Iter: 233100 Loss: 0.02811034768819809  PSNR: 18.86250877380371
[TRAIN] Iter: 233200 Loss: 0.04016340523958206  PSNR: 17.438232421875
[TRAIN] Iter: 233300 Loss: 0.03225015103816986  PSNR: 18.143762588500977
[TRAIN] Iter: 233400 Loss: 0.030602287501096725  PSNR: 18.32009506225586
[TRAIN] Iter: 233500 Loss: 0.032175496220588684  PSNR: 18.093097686767578
[TRAIN] Iter: 233600 Loss: 0.03146626055240631  PSNR: 18.327205657958984
[TRAIN] Iter: 233700 Loss: 0.02569699101150036  PSNR: 19.211042404174805
[TRAIN] Iter: 233800 Loss: 0.02764045260846615  PSNR: 18.761323928833008
[TRAIN] Iter: 233900 Loss: 0.027698922902345657  PSNR: 18.66099739074707
[TRAIN] Iter: 234000 Loss: 0.030032243579626083  PSNR: 18.629024505615234
[TRAIN] Iter: 234100 Loss: 0.024817712604999542  PSNR: 19.36054229736328
[TRAIN] Iter: 234200 Loss: 0.03662702441215515  PSNR: 17.57146453857422
[TRAIN] Iter: 234300 Loss: 0.025418799370527267  PSNR: 19.08570098876953
[TRAIN] Iter: 234400 Loss: 0.025702141225337982  PSNR: 18.74107551574707
[TRAIN] Iter: 234500 Loss: 0.033705636858940125  PSNR: 17.790546417236328
[TRAIN] Iter: 234600 Loss: 0.023868290707468987  PSNR: 19.451379776000977
[TRAIN] Iter: 234700 Loss: 0.024057358503341675  PSNR: 19.558931350708008
[TRAIN] Iter: 234800 Loss: 0.03021317534148693  PSNR: 18.340953826904297
[TRAIN] Iter: 234900 Loss: 0.025142189115285873  PSNR: 19.191125869750977
[TRAIN] Iter: 235000 Loss: 0.027431895956397057  PSNR: 18.59535026550293
[TRAIN] Iter: 235100 Loss: 0.026879683136940002  PSNR: 19.241716384887695
[TRAIN] Iter: 235200 Loss: 0.0314980186522007  PSNR: 18.205129623413086
[TRAIN] Iter: 235300 Loss: 0.030371537432074547  PSNR: 18.226856231689453
[TRAIN] Iter: 235400 Loss: 0.03114195540547371  PSNR: 18.278169631958008
[TRAIN] Iter: 235500 Loss: 0.02850201725959778  PSNR: 18.691333770751953
[TRAIN] Iter: 235600 Loss: 0.02783156931400299  PSNR: 18.783458709716797
[TRAIN] Iter: 235700 Loss: 0.024569418281316757  PSNR: 19.22847557067871
[TRAIN] Iter: 235800 Loss: 0.032996997237205505  PSNR: 17.95340347290039
[TRAIN] Iter: 235900 Loss: 0.027326179668307304  PSNR: 18.99317741394043
[TRAIN] Iter: 236000 Loss: 0.028580760583281517  PSNR: 18.71477699279785
[TRAIN] Iter: 236100 Loss: 0.031014185398817062  PSNR: 18.296167373657227
[TRAIN] Iter: 236200 Loss: 0.027520088478922844  PSNR: 18.770322799682617
[TRAIN] Iter: 236300 Loss: 0.03017045184969902  PSNR: 18.615116119384766
[TRAIN] Iter: 236400 Loss: 0.033330805599689484  PSNR: 18.227048873901367
[TRAIN] Iter: 236500 Loss: 0.029708102345466614  PSNR: 18.54936408996582
[TRAIN] Iter: 236600 Loss: 0.02926332876086235  PSNR: 18.693483352661133
[TRAIN] Iter: 236700 Loss: 0.026966065168380737  PSNR: 18.946123123168945
[TRAIN] Iter: 236800 Loss: 0.025286749005317688  PSNR: 19.47599220275879
[TRAIN] Iter: 236900 Loss: 0.02017509564757347  PSNR: 19.981616973876953
[TRAIN] Iter: 237000 Loss: 0.027955226600170135  PSNR: 18.66886329650879
[TRAIN] Iter: 237100 Loss: 0.02717464603483677  PSNR: 18.642826080322266
[TRAIN] Iter: 237200 Loss: 0.03025929629802704  PSNR: 18.374713897705078
[TRAIN] Iter: 237300 Loss: 0.03192361071705818  PSNR: 18.05039405822754
[TRAIN] Iter: 237400 Loss: 0.02986961603164673  PSNR: 18.653364181518555
[TRAIN] Iter: 237500 Loss: 0.021435707807540894  PSNR: 19.96034049987793
[TRAIN] Iter: 237600 Loss: 0.023170581087470055  PSNR: 19.617490768432617
[TRAIN] Iter: 237700 Loss: 0.029374465346336365  PSNR: 18.65314483642578
[TRAIN] Iter: 237800 Loss: 0.0210118405520916  PSNR: 19.872591018676758
[TRAIN] Iter: 237900 Loss: 0.027005886659026146  PSNR: 19.231019973754883
[TRAIN] Iter: 238000 Loss: 0.026424720883369446  PSNR: 18.873516082763672
[TRAIN] Iter: 238100 Loss: 0.025951065123081207  PSNR: 18.723037719726562
[TRAIN] Iter: 238200 Loss: 0.025629926472902298  PSNR: 18.974905014038086
[TRAIN] Iter: 238300 Loss: 0.032750409096479416  PSNR: 18.034257888793945
[TRAIN] Iter: 238400 Loss: 0.02817799150943756  PSNR: 18.764036178588867
[TRAIN] Iter: 238500 Loss: 0.021193552762269974  PSNR: 20.09752655029297
[TRAIN] Iter: 238600 Loss: 0.02879517897963524  PSNR: 18.541475296020508
[TRAIN] Iter: 238700 Loss: 0.02792542427778244  PSNR: 18.784717559814453
[TRAIN] Iter: 238800 Loss: 0.019720397889614105  PSNR: 20.383098602294922
[TRAIN] Iter: 238900 Loss: 0.02771613933146  PSNR: 18.810758590698242
[TRAIN] Iter: 239000 Loss: 0.02828236110508442  PSNR: 18.494792938232422
[TRAIN] Iter: 239100 Loss: 0.02438751421868801  PSNR: 19.542373657226562
[TRAIN] Iter: 239200 Loss: 0.02576579712331295  PSNR: 19.038297653198242
[TRAIN] Iter: 239300 Loss: 0.026873257011175156  PSNR: 19.070140838623047
[TRAIN] Iter: 239400 Loss: 0.032563675194978714  PSNR: 18.163925170898438
[TRAIN] Iter: 239500 Loss: 0.034299980849027634  PSNR: 17.94463539123535
[TRAIN] Iter: 239600 Loss: 0.025986190885305405  PSNR: 18.92319107055664
[TRAIN] Iter: 239700 Loss: 0.029824886471033096  PSNR: 18.445911407470703
[TRAIN] Iter: 239800 Loss: 0.028192510828375816  PSNR: 18.51082420349121
[TRAIN] Iter: 239900 Loss: 0.026040207594633102  PSNR: 18.912696838378906
Saved checkpoints at ./logs/TUT-LAB-nerf/240000.tar
[TRAIN] Iter: 240000 Loss: 0.02868109568953514  PSNR: 18.7650146484375
[TRAIN] Iter: 240100 Loss: 0.03183659166097641  PSNR: 18.219234466552734
[TRAIN] Iter: 240200 Loss: 0.030543319880962372  PSNR: 18.3560791015625
[TRAIN] Iter: 240300 Loss: 0.02228061482310295  PSNR: 19.800874710083008
[TRAIN] Iter: 240400 Loss: 0.030559523031115532  PSNR: 18.38133430480957
[TRAIN] Iter: 240500 Loss: 0.028155043721199036  PSNR: 18.73403549194336
[TRAIN] Iter: 240600 Loss: 0.029349258169531822  PSNR: 18.439390182495117
[TRAIN] Iter: 240700 Loss: 0.02689422108232975  PSNR: 18.8208065032959
[TRAIN] Iter: 240800 Loss: 0.02718394249677658  PSNR: 18.927961349487305
[TRAIN] Iter: 240900 Loss: 0.032587312161922455  PSNR: 18.004926681518555
[TRAIN] Iter: 241000 Loss: 0.031187720596790314  PSNR: 18.366884231567383
[TRAIN] Iter: 241100 Loss: 0.023129893466830254  PSNR: 19.251708984375
[TRAIN] Iter: 241200 Loss: 0.0285653043538332  PSNR: 18.660654067993164
[TRAIN] Iter: 241300 Loss: 0.024265993386507034  PSNR: 19.067190170288086
[TRAIN] Iter: 241400 Loss: 0.02579784393310547  PSNR: 19.03006935119629
[TRAIN] Iter: 241500 Loss: 0.034198254346847534  PSNR: 17.919198989868164
[TRAIN] Iter: 241600 Loss: 0.032134100794792175  PSNR: 18.0970516204834
[TRAIN] Iter: 241700 Loss: 0.027680784463882446  PSNR: 18.92908477783203
[TRAIN] Iter: 241800 Loss: 0.02536284551024437  PSNR: 19.271678924560547
[TRAIN] Iter: 241900 Loss: 0.03589741885662079  PSNR: 17.6214542388916
[TRAIN] Iter: 242000 Loss: 0.031217552721500397  PSNR: 18.101747512817383
[TRAIN] Iter: 242100 Loss: 0.0218290276825428  PSNR: 19.82242202758789
[TRAIN] Iter: 242200 Loss: 0.03194766491651535  PSNR: 18.042631149291992
[TRAIN] Iter: 242300 Loss: 0.02389991097152233  PSNR: 19.36723518371582
[TRAIN] Iter: 242400 Loss: 0.026865355670452118  PSNR: 19.0323486328125
[TRAIN] Iter: 242500 Loss: 0.026753708720207214  PSNR: 18.97439956665039
[TRAIN] Iter: 242600 Loss: 0.03326324373483658  PSNR: 17.981752395629883
[TRAIN] Iter: 242700 Loss: 0.026172544807195663  PSNR: 19.112524032592773
[TRAIN] Iter: 242800 Loss: 0.027550209313631058  PSNR: 18.8900146484375
[TRAIN] Iter: 242900 Loss: 0.027071721851825714  PSNR: 18.976234436035156
[TRAIN] Iter: 243000 Loss: 0.02458200417459011  PSNR: 19.409006118774414
[TRAIN] Iter: 243100 Loss: 0.023835960775613785  PSNR: 19.48715591430664
[TRAIN] Iter: 243200 Loss: 0.02564600668847561  PSNR: 19.42611312866211
[TRAIN] Iter: 243300 Loss: 0.03480606526136398  PSNR: 17.837862014770508
[TRAIN] Iter: 243400 Loss: 0.0330653041601181  PSNR: 17.99306869506836
[TRAIN] Iter: 243500 Loss: 0.028239626437425613  PSNR: 18.310306549072266
[TRAIN] Iter: 243600 Loss: 0.03149867802858353  PSNR: 18.196500778198242
[TRAIN] Iter: 243700 Loss: 0.02608315274119377  PSNR: 18.98565101623535
[TRAIN] Iter: 243800 Loss: 0.027978073805570602  PSNR: 18.500242233276367
[TRAIN] Iter: 243900 Loss: 0.023602066561579704  PSNR: 19.454833984375
[TRAIN] Iter: 244000 Loss: 0.0318048819899559  PSNR: 18.195226669311523
[TRAIN] Iter: 244100 Loss: 0.02992318943142891  PSNR: 18.41265296936035
[TRAIN] Iter: 244200 Loss: 0.03141174837946892  PSNR: 18.269861221313477
[TRAIN] Iter: 244300 Loss: 0.02786075696349144  PSNR: 18.886812210083008
[TRAIN] Iter: 244400 Loss: 0.029719633981585503  PSNR: 18.501535415649414
[TRAIN] Iter: 244500 Loss: 0.03118124231696129  PSNR: 18.06180191040039
[TRAIN] Iter: 244600 Loss: 0.031004518270492554  PSNR: 18.30953025817871
[TRAIN] Iter: 244700 Loss: 0.028197165578603745  PSNR: 18.738157272338867
[TRAIN] Iter: 244800 Loss: 0.03165813535451889  PSNR: 18.078948974609375
[TRAIN] Iter: 244900 Loss: 0.025154119357466698  PSNR: 18.998029708862305
[TRAIN] Iter: 245000 Loss: 0.0266350619494915  PSNR: 19.121152877807617
[TRAIN] Iter: 245100 Loss: 0.02733418345451355  PSNR: 18.8636417388916
[TRAIN] Iter: 245200 Loss: 0.02947898581624031  PSNR: 18.321945190429688
[TRAIN] Iter: 245300 Loss: 0.02942758798599243  PSNR: 18.487897872924805
[TRAIN] Iter: 245400 Loss: 0.03326630964875221  PSNR: 17.859432220458984
[TRAIN] Iter: 245500 Loss: 0.027444764971733093  PSNR: 18.753395080566406
[TRAIN] Iter: 245600 Loss: 0.027993502095341682  PSNR: 18.866928100585938
[TRAIN] Iter: 245700 Loss: 0.026154330000281334  PSNR: 19.17753028869629
[TRAIN] Iter: 245800 Loss: 0.025220785290002823  PSNR: 19.276836395263672
[TRAIN] Iter: 245900 Loss: 0.0322018638253212  PSNR: 18.123559951782227
[TRAIN] Iter: 246000 Loss: 0.029416296631097794  PSNR: 18.55745506286621
[TRAIN] Iter: 246100 Loss: 0.02973504364490509  PSNR: 18.482418060302734
[TRAIN] Iter: 246200 Loss: 0.02650648169219494  PSNR: 19.03861427307129
[TRAIN] Iter: 246300 Loss: 0.03255029022693634  PSNR: 17.963687896728516
[TRAIN] Iter: 246400 Loss: 0.022938668727874756  PSNR: 19.686182022094727
[TRAIN] Iter: 246500 Loss: 0.035655029118061066  PSNR: 17.74251937866211
[TRAIN] Iter: 246600 Loss: 0.03169548138976097  PSNR: 18.259414672851562
[TRAIN] Iter: 246700 Loss: 0.03183119744062424  PSNR: 18.07002067565918
[TRAIN] Iter: 246800 Loss: 0.027349062263965607  PSNR: 18.519521713256836
[TRAIN] Iter: 246900 Loss: 0.02738029696047306  PSNR: 18.971418380737305
[TRAIN] Iter: 247000 Loss: 0.024761822074651718  PSNR: 19.533153533935547
[TRAIN] Iter: 247100 Loss: 0.025345465168356895  PSNR: 19.248672485351562
[TRAIN] Iter: 247200 Loss: 0.03144991025328636  PSNR: 18.11213493347168
[TRAIN] Iter: 247300 Loss: 0.02658824436366558  PSNR: 18.99366569519043
[TRAIN] Iter: 247400 Loss: 0.03675713390111923  PSNR: 17.51324462890625
[TRAIN] Iter: 247500 Loss: 0.030855780467391014  PSNR: 18.283540725708008
[TRAIN] Iter: 247600 Loss: 0.029578275978565216  PSNR: 18.385513305664062
[TRAIN] Iter: 247700 Loss: 0.01934051141142845  PSNR: 20.311487197875977
[TRAIN] Iter: 247800 Loss: 0.032509833574295044  PSNR: 17.99177360534668
[TRAIN] Iter: 247900 Loss: 0.021911468356847763  PSNR: 19.587806701660156
[TRAIN] Iter: 248000 Loss: 0.033003535121679306  PSNR: 18.04106330871582
[TRAIN] Iter: 248100 Loss: 0.02315141074359417  PSNR: 19.368879318237305
[TRAIN] Iter: 248200 Loss: 0.03257054090499878  PSNR: 17.984943389892578
[TRAIN] Iter: 248300 Loss: 0.023362794890999794  PSNR: 19.331707000732422
[TRAIN] Iter: 248400 Loss: 0.03090735524892807  PSNR: 18.420978546142578
[TRAIN] Iter: 248500 Loss: 0.026516621932387352  PSNR: 18.767637252807617
[TRAIN] Iter: 248600 Loss: 0.026080545037984848  PSNR: 19.097049713134766
[TRAIN] Iter: 248700 Loss: 0.029967863112688065  PSNR: 18.458200454711914
[TRAIN] Iter: 248800 Loss: 0.024440770968794823  PSNR: 19.200637817382812
[TRAIN] Iter: 248900 Loss: 0.029167311266064644  PSNR: 18.585969924926758
[TRAIN] Iter: 249000 Loss: 0.029014721512794495  PSNR: 18.278087615966797
[TRAIN] Iter: 249100 Loss: 0.025966009125113487  PSNR: 18.885831832885742
[TRAIN] Iter: 249200 Loss: 0.033804815262556076  PSNR: 17.821203231811523
[TRAIN] Iter: 249300 Loss: 0.027055567130446434  PSNR: 18.70880699157715
[TRAIN] Iter: 249400 Loss: 0.031154971569776535  PSNR: 18.245763778686523
[TRAIN] Iter: 249500 Loss: 0.030241133645176888  PSNR: 18.22435188293457
[TRAIN] Iter: 249600 Loss: 0.029229506850242615  PSNR: 18.64263916015625
[TRAIN] Iter: 249700 Loss: 0.037619657814502716  PSNR: 17.571155548095703
[TRAIN] Iter: 249800 Loss: 0.02933015674352646  PSNR: 18.512344360351562
[TRAIN] Iter: 249900 Loss: 0.030542928725481033  PSNR: 18.359861373901367
Saved checkpoints at ./logs/TUT-LAB-nerf/250000.tar
0 0.0003876686096191406
torch.Size([320, 640, 3]) torch.Size([320, 640])
1 15.709134340286255
2 18.916835069656372
3 15.355954647064209
4 18.818954467773438
5 15.769662380218506
6 18.54415225982666
7 15.894726037979126
8 18.6164710521698
9 15.751431226730347
10 18.548271656036377
11 15.807202339172363
12 18.344829559326172
13 16.120618104934692
14 18.75884485244751
15 15.91451907157898
16 18.479175806045532
17 15.661291360855103
18 15.689303398132324
19 18.45737338066101
20 15.92018747329712
21 18.908565998077393
22 15.896982431411743
23 18.38114094734192
24 15.721195936203003
25 18.4818274974823
26 15.938509225845337
27 18.76694893836975
28 16.01142191886902
29 18.494564533233643
30 15.887357711791992
31 15.816161632537842
32 13.960924625396729
33 18.76698088645935
34 15.96261715888977
35 18.70360541343689
36 15.752408504486084
37 18.810694217681885
38 15.978558540344238
39 18.87409257888794
40 15.690179824829102
41 18.743679523468018
42 15.878413677215576
43 18.55933952331543
44 15.821152210235596
45 18.75876808166504
46 15.74913501739502
47 18.71760082244873
48 15.717057704925537
49 18.610562801361084
50 15.506235122680664
51 18.636287450790405
52 15.710822582244873
53 18.56626582145691
54 16.26738142967224
55 15.70789361000061
56 18.006031036376953
57 15.673134803771973
58 20.089206218719482
59 15.773326635360718
60 17.55888295173645
61 15.556494235992432
62 19.863749980926514
63 15.845605373382568
64 18.290400505065918
65 15.835833311080933
66 19.684786319732666
67 15.82375955581665
68 19.21300959587097
69 15.894654512405396
70 19.395482063293457
71 15.80385136604309
72 18.899637937545776
73 15.787223100662231
74 19.738378763198853
75 16.094165563583374
76 19.082043409347534
77 15.762286901473999
78 18.802518844604492
79 16.643779754638672
80 18.244640111923218
81 15.681830644607544
82 19.30061364173889
83 15.780442476272583
84 18.940065383911133
85 15.522878170013428
86 18.731922149658203
87 15.482115983963013
88 18.695693731307983
89 15.54911184310913
90 18.663487911224365
91 15.502975463867188
92 18.797025203704834
93 15.398398637771606
94 18.76938271522522
95 15.441956520080566
96 18.74278211593628
97 15.500005960464478
98 18.724270820617676
99 15.592019319534302
100 18.708041191101074
101 15.49004602432251
102 18.704405546188354
103 15.48612928390503
104 18.63535189628601
105 15.6352219581604
106 18.676448345184326
107 15.580013513565063
108 15.528645038604736
109 18.599735736846924
110 15.545764684677124
111 18.715060472488403
112 15.602023601531982
113 19.009014129638672
114 15.440818786621094
115 18.736668348312378
116 15.58525013923645
117 18.710023403167725
118 15.810227394104004
119 19.023755311965942
Done, saving (120, 320, 640, 3) (120, 320, 640)
extras:{'raw': tensor([[[-6.4943e-02, -2.8514e-01, -3.3958e-01, -5.2848e+00],
         [ 2.4580e-01,  8.6877e-03, -1.4350e-01,  2.8080e+00],
         [ 1.8117e-01, -2.2280e-02, -1.6600e-01,  2.1488e+00],
         ...,
         [ 2.0117e+01,  1.8498e+01,  1.9119e+01,  2.4843e+02],
         [ 2.0743e+01,  1.8880e+01,  1.9215e+01,  2.4255e+02],
         [ 2.1220e+01,  1.9426e+01,  2.0010e+01,  2.3871e+02]],

        [[ 1.9003e-01,  3.6568e-02,  4.7068e-02, -4.5316e+00],
         [-2.7145e-03, -5.1345e-02, -2.7027e-02,  5.8112e+00],
         [ 7.0071e-02,  3.2087e-02,  3.4586e-02,  2.0807e+01],
         ...,
         [ 1.3299e+01,  1.1403e+01,  1.0245e+01,  9.0723e+01],
         [ 1.2784e+01,  1.0919e+01,  9.6833e+00,  8.5453e+01],
         [ 1.2064e+01,  1.0157e+01,  8.6891e+00,  7.4190e+01]],

        [[ 1.0559e-01,  2.0790e-01,  5.3532e-01, -1.8842e+00],
         [-7.3569e-01, -4.4225e-01, -2.3619e-01, -1.1992e+01],
         [-1.1135e-01,  6.2450e-02,  8.4101e-02, -7.1449e+00],
         ...,
         [-9.2463e+00, -1.1670e+01, -1.7492e+01,  7.2300e+02],
         [-8.4604e+00, -1.0971e+01, -1.7123e+01,  7.1865e+02],
         [-8.3205e+00, -1.0804e+01, -1.6916e+01,  7.1644e+02]],

        ...,

        [[-4.8027e-01, -4.9790e-01, -5.0255e-01, -7.4242e+00],
         [-2.3822e-01, -2.9675e-01, -2.2879e-01,  8.1527e+00],
         [-2.7383e-01, -3.3660e-01, -4.1769e-01,  6.1044e+00],
         ...,
         [ 1.1193e+01,  1.0522e+01,  1.1764e+01,  6.7061e+01],
         [ 1.0598e+01,  9.9967e+00,  1.1161e+01,  5.8291e+01],
         [ 1.3018e+01,  1.1923e+01,  1.2579e+01,  3.6121e+01]],

        [[ 2.3813e-01,  5.3897e-02, -6.3867e-02, -1.0514e+01],
         [ 3.6001e-02, -1.6486e-01, -3.2104e-01,  2.8644e+00],
         [ 4.9275e-02, -1.5215e-01, -3.0834e-01,  3.4257e+00],
         ...,
         [ 5.1176e-01,  8.4958e-02, -3.1430e+00,  3.6845e+02],
         [-2.2613e-01, -5.9121e-01, -3.8143e+00,  3.6267e+02],
         [ 2.4106e-01, -6.3542e-02, -3.1207e+00,  3.5256e+02]],

        [[ 7.4858e-01,  5.6692e-01,  5.0241e-01, -8.4002e+00],
         [ 8.4486e-01,  6.5409e-01,  6.5325e-01, -5.5108e+00],
         [ 5.9703e-01,  4.5417e-01,  5.3076e-01, -2.4982e+00],
         ...,
         [ 1.2493e+01,  1.1460e+01,  1.2023e+01,  1.4031e+02],
         [ 1.2963e+01,  1.1911e+01,  1.2531e+01,  1.4021e+02],
         [ 1.2775e+01,  1.1533e+01,  1.1599e+01,  1.3715e+02]]],
       grad_fn=<ReshapeAliasBackward0>), 'rgb0': tensor([[0.5854, 0.5302, 0.4948],
        [0.4375, 0.4231, 0.3959],
        [0.3788, 0.4462, 0.4327],
        ...,
        [0.5031, 0.4896, 0.4676],
        [0.5345, 0.4741, 0.4329],
        [0.6959, 0.6570, 0.6407]], grad_fn=<ReshapeAliasBackward0>), 'disp0': tensor([142.1701,  15.2451,  31.3093,  ...,  15.1227,  85.5394,  61.9549],
       grad_fn=<ReshapeAliasBackward0>), 'acc0': tensor([1., 1., 1.,  ..., 1., 1., 1.], grad_fn=<ReshapeAliasBackward0>), 'z_std': tensor([0.0021, 0.0074, 0.0023,  ..., 0.0028, 0.0020, 0.0030])}
0 0.0005252361297607422
torch.Size([320, 640, 3]) torch.Size([320, 640])
1 19.14816951751709
2 15.773921489715576
3 19.20891523361206
4 15.599878311157227
5 19.323238372802734
6 15.875614643096924
7 18.459088802337646
8 15.58633542060852
9 20.245600938796997
10 15.802450180053711
11 17.623029708862305
12 15.70079779624939
13 20.207947492599487
14 15.74996304512024
15 17.885780096054077
16 15.500122785568237
17 19.32244038581848
18 15.511759519577026
19 19.18976092338562
20 15.501752138137817
21 18.78826904296875
22 15.55734133720398
23 18.265931129455566
24 15.246488332748413
25 18.68008279800415
26 15.65686821937561
27 18.386333227157593
28 12.651994943618774
29 15.386802434921265
30 18.3751962184906
31 15.531985521316528
32 17.233635663986206
33 13.685696840286255
34 16.557231903076172
35 13.792686462402344
36 16.609123945236206
37 13.748846292495728
38 13.87086296081543
39 16.629178047180176
40 13.820958852767944
41 16.59761333465576
42 13.70714259147644
43 16.347541570663452
44 13.75567364692688
45 13.745978116989136
46 16.25425124168396
47 13.51770806312561
48 17.071539402008057
49 13.609702825546265
50 13.558327436447144
51 16.930376291275024
52 13.561387538909912
53 16.674148559570312
54 13.675580024719238
55 16.895994901657104
56 13.518743753433228
57 13.816840648651123
58 16.74476671218872
59 13.774266958236694
60 16.839317321777344
61 13.768376350402832
62 16.54876399040222
63 13.667899131774902
64 13.658615589141846
65 16.738436460494995
66 13.67925477027893
67 16.58913540840149
68 13.7615807056427
69 16.822113275527954
70 13.801896095275879
71 13.688371896743774
72 16.82271957397461
73 13.413373470306396
74 16.494748830795288
75 13.394426822662354
76 13.680098056793213
77 16.88409185409546
78 13.43976354598999
79 17.127449989318848
80 13.241340160369873
81 16.68259596824646
82 14.033406734466553
83 13.532315492630005
84 16.191012144088745
85 13.487293004989624
86 17.68808078765869
87 14.34258508682251
88 15.729430437088013
89 13.281213521957397
90 13.367495775222778
91 17.83894371986389
92 13.668656587600708
93 15.213589906692505
94 13.385823011398315
95 17.137954473495483
96 14.218075037002563
97 13.54502558708191
98 16.028788805007935
99 13.587703466415405
100 17.41344690322876
101 13.58497929573059
102 16.562000513076782
103 13.555443286895752
104 13.57664942741394
105 17.237204551696777
106 13.714776754379272
107 16.919280529022217
108 13.632376194000244
109 13.495736360549927
110 17.287632942199707
111 13.783095598220825
112 16.555880784988403
113 13.592928647994995
114 16.426490783691406
115 13.663837432861328
116 13.606528282165527
117 16.536765336990356
118 13.837692499160767
119 16.55642318725586
test poses shape torch.Size([13, 3, 4])
0 0.0007672309875488281
torch.Size([320, 640, 3]) torch.Size([320, 640])
1 16.52803945541382
2 13.588021278381348
3 13.743979930877686
4 16.539541721343994
5 13.806713819503784
6 16.69645643234253
7 13.863329887390137
8 16.51670002937317
9 13.48189902305603
10 13.487581968307495
11 16.94904589653015
12 13.54380488395691
Saved test set
[TRAIN] Iter: 250000 Loss: 0.029125958681106567  PSNR: 18.49831199645996
[TRAIN] Iter: 250100 Loss: 0.02652309089899063  PSNR: 18.887784957885742
[TRAIN] Iter: 250200 Loss: 0.029864221811294556  PSNR: 18.51301383972168
[TRAIN] Iter: 250300 Loss: 0.02738424390554428  PSNR: 18.820714950561523
[TRAIN] Iter: 250400 Loss: 0.03566080331802368  PSNR: 17.67464256286621
[TRAIN] Iter: 250500 Loss: 0.028441770002245903  PSNR: 18.674898147583008
[TRAIN] Iter: 250600 Loss: 0.026962926611304283  PSNR: 18.893726348876953
[TRAIN] Iter: 250700 Loss: 0.03349535912275314  PSNR: 17.891826629638672
[TRAIN] Iter: 250800 Loss: 0.022509846836328506  PSNR: 19.648653030395508
[TRAIN] Iter: 250900 Loss: 0.025791306048631668  PSNR: 19.535274505615234
[TRAIN] Iter: 251000 Loss: 0.02628154680132866  PSNR: 19.14215087890625
[TRAIN] Iter: 251100 Loss: 0.025566697120666504  PSNR: 18.942201614379883
[TRAIN] Iter: 251200 Loss: 0.03192821890115738  PSNR: 18.028902053833008
[TRAIN] Iter: 251300 Loss: 0.02727428264915943  PSNR: 18.74679946899414
[TRAIN] Iter: 251400 Loss: 0.031339216977357864  PSNR: 18.285165786743164
[TRAIN] Iter: 251500 Loss: 0.031166523694992065  PSNR: 18.10579490661621
[TRAIN] Iter: 251600 Loss: 0.02359701134264469  PSNR: 19.329668045043945
[TRAIN] Iter: 251700 Loss: 0.02732699364423752  PSNR: 18.709880828857422
[TRAIN] Iter: 251800 Loss: 0.027431195601820946  PSNR: 18.770050048828125
[TRAIN] Iter: 251900 Loss: 0.03058527410030365  PSNR: 18.35274887084961
[TRAIN] Iter: 252000 Loss: 0.029018975794315338  PSNR: 18.65826416015625
[TRAIN] Iter: 252100 Loss: 0.0282805897295475  PSNR: 18.475576400756836
[TRAIN] Iter: 252200 Loss: 0.028358187526464462  PSNR: 18.686279296875
[TRAIN] Iter: 252300 Loss: 0.025734324008226395  PSNR: 19.101224899291992
[TRAIN] Iter: 252400 Loss: 0.024808304384350777  PSNR: 19.175281524658203
[TRAIN] Iter: 252500 Loss: 0.0349687784910202  PSNR: 18.277820587158203
[TRAIN] Iter: 252600 Loss: 0.03137149661779404  PSNR: 18.2976016998291
[TRAIN] Iter: 252700 Loss: 0.027236534282565117  PSNR: 18.757156372070312
[TRAIN] Iter: 252800 Loss: 0.03087056800723076  PSNR: 18.293468475341797
[TRAIN] Iter: 252900 Loss: 0.0309993177652359  PSNR: 18.263660430908203
[TRAIN] Iter: 253000 Loss: 0.030183373019099236  PSNR: 18.28584098815918
[TRAIN] Iter: 253100 Loss: 0.03263959288597107  PSNR: 18.098758697509766
[TRAIN] Iter: 253200 Loss: 0.029820047318935394  PSNR: 18.454090118408203
[TRAIN] Iter: 253300 Loss: 0.03255061060190201  PSNR: 18.028522491455078
[TRAIN] Iter: 253400 Loss: 0.033191949129104614  PSNR: 18.07236671447754
[TRAIN] Iter: 253500 Loss: 0.025230061262845993  PSNR: 19.169584274291992
[TRAIN] Iter: 253600 Loss: 0.021739158779382706  PSNR: 19.80367660522461
[TRAIN] Iter: 253700 Loss: 0.03677421808242798  PSNR: 17.592267990112305
[TRAIN] Iter: 253800 Loss: 0.027607470750808716  PSNR: 18.804048538208008
[TRAIN] Iter: 253900 Loss: 0.03178144246339798  PSNR: 18.253442764282227
[TRAIN] Iter: 254000 Loss: 0.026181811466813087  PSNR: 19.13435173034668
[TRAIN] Iter: 254100 Loss: 0.03840958699584007  PSNR: 17.430936813354492
[TRAIN] Iter: 254200 Loss: 0.03155653551220894  PSNR: 18.20705795288086
[TRAIN] Iter: 254300 Loss: 0.03210517391562462  PSNR: 18.177133560180664
[TRAIN] Iter: 254400 Loss: 0.02856791764497757  PSNR: 18.682161331176758
[TRAIN] Iter: 254500 Loss: 0.02545912377536297  PSNR: 18.99936866760254
[TRAIN] Iter: 254600 Loss: 0.031165558844804764  PSNR: 18.370515823364258
[TRAIN] Iter: 254700 Loss: 0.023133136332035065  PSNR: 19.446392059326172
[TRAIN] Iter: 254800 Loss: 0.032802969217300415  PSNR: 17.97235107421875
[TRAIN] Iter: 254900 Loss: 0.030318889766931534  PSNR: 18.505624771118164
[TRAIN] Iter: 255000 Loss: 0.03207467496395111  PSNR: 18.330801010131836
[TRAIN] Iter: 255100 Loss: 0.02458801306784153  PSNR: 19.293851852416992
[TRAIN] Iter: 255200 Loss: 0.02850041165947914  PSNR: 18.645517349243164
[TRAIN] Iter: 255300 Loss: 0.032606691122055054  PSNR: 18.07379913330078
[TRAIN] Iter: 255400 Loss: 0.028364013880491257  PSNR: 18.660568237304688
[TRAIN] Iter: 255500 Loss: 0.02309255674481392  PSNR: 19.365312576293945
[TRAIN] Iter: 255600 Loss: 0.028244806453585625  PSNR: 18.671716690063477
[TRAIN] Iter: 255700 Loss: 0.030785489827394485  PSNR: 18.316162109375
[TRAIN] Iter: 255800 Loss: 0.028419870883226395  PSNR: 18.700237274169922
[TRAIN] Iter: 255900 Loss: 0.027563534677028656  PSNR: 18.875263214111328
[TRAIN] Iter: 256000 Loss: 0.021863441914319992  PSNR: 19.726491928100586
[TRAIN] Iter: 256100 Loss: 0.02800854481756687  PSNR: 18.64290428161621
[TRAIN] Iter: 256200 Loss: 0.031135819852352142  PSNR: 18.288698196411133
[TRAIN] Iter: 256300 Loss: 0.030524078756570816  PSNR: 18.35610580444336
[TRAIN] Iter: 256400 Loss: 0.02552591636776924  PSNR: 19.093692779541016
[TRAIN] Iter: 256500 Loss: 0.022777009755373  PSNR: 19.706832885742188
[TRAIN] Iter: 256600 Loss: 0.03415895253419876  PSNR: 17.79345703125
[TRAIN] Iter: 256700 Loss: 0.026456160470843315  PSNR: 19.423635482788086
[TRAIN] Iter: 256800 Loss: 0.029287150129675865  PSNR: 18.378442764282227
[TRAIN] Iter: 256900 Loss: 0.031559184193611145  PSNR: 18.165910720825195
[TRAIN] Iter: 257000 Loss: 0.027706503868103027  PSNR: 18.76706886291504
[TRAIN] Iter: 257100 Loss: 0.029959335923194885  PSNR: 18.365379333496094
[TRAIN] Iter: 257200 Loss: 0.02525796927511692  PSNR: 19.2573184967041
[TRAIN] Iter: 257300 Loss: 0.026225006207823753  PSNR: 18.817684173583984
[TRAIN] Iter: 257400 Loss: 0.027872171252965927  PSNR: 19.026302337646484
[TRAIN] Iter: 257500 Loss: 0.02580970712006092  PSNR: 18.923112869262695
[TRAIN] Iter: 257600 Loss: 0.028678052127361298  PSNR: 18.580753326416016
[TRAIN] Iter: 257700 Loss: 0.022185834124684334  PSNR: 19.67264175415039
[TRAIN] Iter: 257800 Loss: 0.02181464247405529  PSNR: 19.860069274902344
[TRAIN] Iter: 257900 Loss: 0.032098572701215744  PSNR: 17.950069427490234
[TRAIN] Iter: 258000 Loss: 0.026371952146291733  PSNR: 18.578859329223633
[TRAIN] Iter: 258100 Loss: 0.02599889412522316  PSNR: 18.687026977539062
[TRAIN] Iter: 258200 Loss: 0.025043098255991936  PSNR: 19.365663528442383
[TRAIN] Iter: 258300 Loss: 0.02092619612812996  PSNR: 19.828317642211914
[TRAIN] Iter: 258400 Loss: 0.030710555613040924  PSNR: 18.344512939453125
[TRAIN] Iter: 258500 Loss: 0.03308636695146561  PSNR: 17.984981536865234
[TRAIN] Iter: 258600 Loss: 0.022745613008737564  PSNR: 19.689884185791016
[TRAIN] Iter: 258700 Loss: 0.02945244312286377  PSNR: 18.375762939453125
[TRAIN] Iter: 258800 Loss: 0.028991391882300377  PSNR: 18.304393768310547
[TRAIN] Iter: 258900 Loss: 0.029219185933470726  PSNR: 18.75343894958496
[TRAIN] Iter: 259000 Loss: 0.025234829634428024  PSNR: 19.101369857788086
[TRAIN] Iter: 259100 Loss: 0.026680132374167442  PSNR: 18.895488739013672
[TRAIN] Iter: 259200 Loss: 0.034046322107315063  PSNR: 17.86653709411621
[TRAIN] Iter: 259300 Loss: 0.02986634522676468  PSNR: 18.55961799621582
[TRAIN] Iter: 259400 Loss: 0.0312645398080349  PSNR: 18.22127342224121
[TRAIN] Iter: 259500 Loss: 0.025682128965854645  PSNR: 19.123937606811523
[TRAIN] Iter: 259600 Loss: 0.03378891944885254  PSNR: 17.773218154907227
[TRAIN] Iter: 259700 Loss: 0.027030542492866516  PSNR: 18.797731399536133
[TRAIN] Iter: 259800 Loss: 0.021000899374485016  PSNR: 19.967802047729492
[TRAIN] Iter: 259900 Loss: 0.02891632355749607  PSNR: 18.746379852294922
Saved checkpoints at ./logs/TUT-LAB-nerf/260000.tar
[TRAIN] Iter: 260000 Loss: 0.03337416052818298  PSNR: 18.168447494506836
[TRAIN] Iter: 260100 Loss: 0.03128005936741829  PSNR: 18.294410705566406
[TRAIN] Iter: 260200 Loss: 0.024708261713385582  PSNR: 19.113445281982422
[TRAIN] Iter: 260300 Loss: 0.024247393012046814  PSNR: 19.338733673095703
[TRAIN] Iter: 260400 Loss: 0.02865496464073658  PSNR: 18.56540870666504
[TRAIN] Iter: 260500 Loss: 0.025034833699464798  PSNR: 19.257080078125
[TRAIN] Iter: 260600 Loss: 0.028683394193649292  PSNR: 18.42997169494629
[TRAIN] Iter: 260700 Loss: 0.022927414625883102  PSNR: 19.602785110473633
[TRAIN] Iter: 260800 Loss: 0.029697533696889877  PSNR: 18.45086097717285
[TRAIN] Iter: 260900 Loss: 0.033590175211429596  PSNR: 18.177349090576172
[TRAIN] Iter: 261000 Loss: 0.02970672957599163  PSNR: 18.193817138671875
[TRAIN] Iter: 261100 Loss: 0.024413473904132843  PSNR: 19.208690643310547
[TRAIN] Iter: 261200 Loss: 0.028432633727788925  PSNR: 18.585050582885742
[TRAIN] Iter: 261300 Loss: 0.026223275810480118  PSNR: 19.0908145904541
[TRAIN] Iter: 261400 Loss: 0.02358226478099823  PSNR: 19.5426082611084
[TRAIN] Iter: 261500 Loss: 0.024832040071487427  PSNR: 19.12501335144043
[TRAIN] Iter: 261600 Loss: 0.02607453241944313  PSNR: 19.03885269165039
[TRAIN] Iter: 261700 Loss: 0.031862035393714905  PSNR: 18.3377628326416
[TRAIN] Iter: 261800 Loss: 0.03212718665599823  PSNR: 18.16831398010254
[TRAIN] Iter: 261900 Loss: 0.025958601385354996  PSNR: 19.174644470214844
[TRAIN] Iter: 262000 Loss: 0.032109618186950684  PSNR: 18.149259567260742
[TRAIN] Iter: 262100 Loss: 0.02034584805369377  PSNR: 20.17037582397461
[TRAIN] Iter: 262200 Loss: 0.028493504971265793  PSNR: 18.632415771484375
[TRAIN] Iter: 262300 Loss: 0.03509259968996048  PSNR: 17.636119842529297
[TRAIN] Iter: 262400 Loss: 0.023817993700504303  PSNR: 19.35991668701172
[TRAIN] Iter: 262500 Loss: 0.025170335546135902  PSNR: 19.26140022277832
[TRAIN] Iter: 262600 Loss: 0.03176650032401085  PSNR: 18.060096740722656
[TRAIN] Iter: 262700 Loss: 0.023594778031110764  PSNR: 19.504636764526367
[TRAIN] Iter: 262800 Loss: 0.02960234135389328  PSNR: 18.59525489807129
[TRAIN] Iter: 262900 Loss: 0.03385530412197113  PSNR: 17.893856048583984
[TRAIN] Iter: 263000 Loss: 0.025282539427280426  PSNR: 19.159076690673828
[TRAIN] Iter: 263100 Loss: 0.028140055015683174  PSNR: 18.748714447021484
[TRAIN] Iter: 263200 Loss: 0.02715536765754223  PSNR: 18.72186851501465
[TRAIN] Iter: 263300 Loss: 0.02428116649389267  PSNR: 19.170799255371094
[TRAIN] Iter: 263400 Loss: 0.030293986201286316  PSNR: 18.37981605529785
[TRAIN] Iter: 263500 Loss: 0.033130817115306854  PSNR: 18.085750579833984
[TRAIN] Iter: 263600 Loss: 0.03627612441778183  PSNR: 17.58489418029785
[TRAIN] Iter: 263700 Loss: 0.023754194378852844  PSNR: 19.450225830078125
[TRAIN] Iter: 263800 Loss: 0.02605735883116722  PSNR: 18.974365234375
[TRAIN] Iter: 263900 Loss: 0.026231400668621063  PSNR: 19.36522102355957
[TRAIN] Iter: 264000 Loss: 0.026485417038202286  PSNR: 19.12080192565918
[TRAIN] Iter: 264100 Loss: 0.03063110262155533  PSNR: 18.212312698364258
[TRAIN] Iter: 264200 Loss: 0.02974182926118374  PSNR: 18.53059196472168
[TRAIN] Iter: 264300 Loss: 0.028821660205721855  PSNR: 18.368497848510742
[TRAIN] Iter: 264400 Loss: 0.035265032202005386  PSNR: 17.671314239501953
[TRAIN] Iter: 264500 Loss: 0.03245863690972328  PSNR: 18.112957000732422
[TRAIN] Iter: 264600 Loss: 0.021649936214089394  PSNR: 20.035367965698242
[TRAIN] Iter: 264700 Loss: 0.030560262501239777  PSNR: 18.222991943359375
[TRAIN] Iter: 264800 Loss: 0.02786070480942726  PSNR: 18.731740951538086
[TRAIN] Iter: 264900 Loss: 0.029653972014784813  PSNR: 18.42917823791504
[TRAIN] Iter: 265000 Loss: 0.03168901056051254  PSNR: 18.29032325744629
[TRAIN] Iter: 265100 Loss: 0.027738703414797783  PSNR: 19.067790985107422
[TRAIN] Iter: 265200 Loss: 0.02903350070118904  PSNR: 18.602853775024414
[TRAIN] Iter: 265300 Loss: 0.026032589375972748  PSNR: 18.77350425720215
[TRAIN] Iter: 265400 Loss: 0.026949051767587662  PSNR: 18.882509231567383
[TRAIN] Iter: 265500 Loss: 0.0293722003698349  PSNR: 18.37794303894043
[TRAIN] Iter: 265600 Loss: 0.0322243794798851  PSNR: 17.900203704833984
[TRAIN] Iter: 265700 Loss: 0.03209603205323219  PSNR: 18.13932991027832
[TRAIN] Iter: 265800 Loss: 0.03193099796772003  PSNR: 18.202428817749023
[TRAIN] Iter: 265900 Loss: 0.028224259614944458  PSNR: 18.58188247680664
[TRAIN] Iter: 266000 Loss: 0.032648567110300064  PSNR: 18.11563491821289
[TRAIN] Iter: 266100 Loss: 0.026331910863518715  PSNR: 18.847774505615234
[TRAIN] Iter: 266200 Loss: 0.03203023225069046  PSNR: 18.347963333129883
[TRAIN] Iter: 266300 Loss: 0.026248782873153687  PSNR: 19.221181869506836
[TRAIN] Iter: 266400 Loss: 0.024983376264572144  PSNR: 19.053808212280273
[TRAIN] Iter: 266500 Loss: 0.030787359923124313  PSNR: 18.19891929626465
[TRAIN] Iter: 266600 Loss: 0.031365811824798584  PSNR: 18.261302947998047
[TRAIN] Iter: 266700 Loss: 0.027575641870498657  PSNR: 18.723905563354492
[TRAIN] Iter: 266800 Loss: 0.023771092295646667  PSNR: 19.52589988708496
[TRAIN] Iter: 266900 Loss: 0.033428315073251724  PSNR: 18.05954360961914
[TRAIN] Iter: 267000 Loss: 0.02355325035750866  PSNR: 19.309940338134766
[TRAIN] Iter: 267100 Loss: 0.0223662368953228  PSNR: 19.631759643554688
[TRAIN] Iter: 267200 Loss: 0.028938716277480125  PSNR: 18.667139053344727
[TRAIN] Iter: 267300 Loss: 0.029388656839728355  PSNR: 18.550094604492188
[TRAIN] Iter: 267400 Loss: 0.03419862315058708  PSNR: 17.800628662109375
[TRAIN] Iter: 267500 Loss: 0.024390066042542458  PSNR: 19.38442039489746
[TRAIN] Iter: 267600 Loss: 0.0326387956738472  PSNR: 18.07525634765625
[TRAIN] Iter: 267700 Loss: 0.028614996001124382  PSNR: 18.716527938842773
[TRAIN] Iter: 267800 Loss: 0.03317609429359436  PSNR: 17.941164016723633
[TRAIN] Iter: 267900 Loss: 0.029610708355903625  PSNR: 18.57451057434082
[TRAIN] Iter: 268000 Loss: 0.029159851372241974  PSNR: 18.424436569213867
[TRAIN] Iter: 268100 Loss: 0.02703351341187954  PSNR: 19.480310440063477
[TRAIN] Iter: 268200 Loss: 0.032360922545194626  PSNR: 17.984844207763672
[TRAIN] Iter: 268300 Loss: 0.019970569759607315  PSNR: 20.09430694580078
[TRAIN] Iter: 268400 Loss: 0.032103799283504486  PSNR: 18.050376892089844
[TRAIN] Iter: 268500 Loss: 0.031253375113010406  PSNR: 18.221538543701172
[TRAIN] Iter: 268600 Loss: 0.03401888906955719  PSNR: 17.844514846801758
[TRAIN] Iter: 268700 Loss: 0.031127501279115677  PSNR: 18.3330135345459
[TRAIN] Iter: 268800 Loss: 0.026818130165338516  PSNR: 18.965312957763672
[TRAIN] Iter: 268900 Loss: 0.03393518179655075  PSNR: 17.91372299194336
[TRAIN] Iter: 269000 Loss: 0.027502302080392838  PSNR: 18.638429641723633
[TRAIN] Iter: 269100 Loss: 0.030896537005901337  PSNR: 18.42604637145996
[TRAIN] Iter: 269200 Loss: 0.0315563827753067  PSNR: 18.33976936340332
[TRAIN] Iter: 269300 Loss: 0.028016265481710434  PSNR: 18.803050994873047
[TRAIN] Iter: 269400 Loss: 0.02906755544245243  PSNR: 18.288415908813477
[TRAIN] Iter: 269500 Loss: 0.025349531322717667  PSNR: 19.07490348815918
[TRAIN] Iter: 269600 Loss: 0.028456134721636772  PSNR: 18.673690795898438
[TRAIN] Iter: 269700 Loss: 0.028180893510580063  PSNR: 18.80242156982422
[TRAIN] Iter: 269800 Loss: 0.026158109307289124  PSNR: 19.136241912841797
[TRAIN] Iter: 269900 Loss: 0.032945550978183746  PSNR: 18.046995162963867
Saved checkpoints at ./logs/TUT-LAB-nerf/270000.tar
[TRAIN] Iter: 270000 Loss: 0.03203761577606201  PSNR: 18.163408279418945
[TRAIN] Iter: 270100 Loss: 0.022572986781597137  PSNR: 19.837020874023438
[TRAIN] Iter: 270200 Loss: 0.032585762441158295  PSNR: 18.07404136657715
[TRAIN] Iter: 270300 Loss: 0.03388532996177673  PSNR: 17.91193199157715
[TRAIN] Iter: 270400 Loss: 0.02948237583041191  PSNR: 18.414308547973633
[TRAIN] Iter: 270500 Loss: 0.02296186238527298  PSNR: 19.627784729003906
[TRAIN] Iter: 270600 Loss: 0.02681928128004074  PSNR: 18.979551315307617
[TRAIN] Iter: 270700 Loss: 0.037648070603609085  PSNR: 17.42083168029785
[TRAIN] Iter: 270800 Loss: 0.0246183630079031  PSNR: 19.373910903930664
[TRAIN] Iter: 270900 Loss: 0.027618689462542534  PSNR: 18.648962020874023
[TRAIN] Iter: 271000 Loss: 0.023259973153471947  PSNR: 19.495634078979492
[TRAIN] Iter: 271100 Loss: 0.02552328258752823  PSNR: 19.194751739501953
[TRAIN] Iter: 271200 Loss: 0.02523593045771122  PSNR: 19.071744918823242
[TRAIN] Iter: 271300 Loss: 0.030917709693312645  PSNR: 18.420793533325195
[TRAIN] Iter: 271400 Loss: 0.03084670938551426  PSNR: 18.486177444458008
[TRAIN] Iter: 271500 Loss: 0.03041907399892807  PSNR: 18.43541717529297
[TRAIN] Iter: 271600 Loss: 0.029188310727477074  PSNR: 18.553485870361328
[TRAIN] Iter: 271700 Loss: 0.02752162702381611  PSNR: 18.86623191833496
[TRAIN] Iter: 271800 Loss: 0.03126411512494087  PSNR: 18.382579803466797
[TRAIN] Iter: 271900 Loss: 0.028734704479575157  PSNR: 18.666316986083984
[TRAIN] Iter: 272000 Loss: 0.0306151881814003  PSNR: 18.468761444091797
[TRAIN] Iter: 272100 Loss: 0.027134064584970474  PSNR: 19.1185245513916
[TRAIN] Iter: 272200 Loss: 0.032353341579437256  PSNR: 18.147876739501953
[TRAIN] Iter: 272300 Loss: 0.028905397281050682  PSNR: 19.063562393188477
[TRAIN] Iter: 272400 Loss: 0.026118909940123558  PSNR: 18.918590545654297
[TRAIN] Iter: 272500 Loss: 0.025882866233587265  PSNR: 19.302526473999023
[TRAIN] Iter: 272600 Loss: 0.031125612556934357  PSNR: 18.307859420776367
[TRAIN] Iter: 272700 Loss: 0.02673264779150486  PSNR: 18.70740509033203
[TRAIN] Iter: 272800 Loss: 0.02726645953953266  PSNR: 18.769840240478516
[TRAIN] Iter: 272900 Loss: 0.03141240030527115  PSNR: 18.164363861083984
[TRAIN] Iter: 273000 Loss: 0.026620961725711823  PSNR: 18.91385269165039
[TRAIN] Iter: 273100 Loss: 0.032619357109069824  PSNR: 18.000520706176758
[TRAIN] Iter: 273200 Loss: 0.028165288269519806  PSNR: 18.62126350402832
[TRAIN] Iter: 273300 Loss: 0.032873209565877914  PSNR: 17.852066040039062
[TRAIN] Iter: 273400 Loss: 0.021471310406923294  PSNR: 19.74671173095703
[TRAIN] Iter: 273500 Loss: 0.0228547602891922  PSNR: 19.72442054748535
[TRAIN] Iter: 273600 Loss: 0.021906744688749313  PSNR: 19.665508270263672
[TRAIN] Iter: 273700 Loss: 0.027982767671346664  PSNR: 18.734468460083008
[TRAIN] Iter: 273800 Loss: 0.03186126798391342  PSNR: 18.14307403564453
[TRAIN] Iter: 273900 Loss: 0.03421420603990555  PSNR: 17.829519271850586
[TRAIN] Iter: 274000 Loss: 0.0260836910456419  PSNR: 19.369234085083008
[TRAIN] Iter: 274100 Loss: 0.024263042956590652  PSNR: 19.29034996032715
[TRAIN] Iter: 274200 Loss: 0.029820160940289497  PSNR: 18.664859771728516
[TRAIN] Iter: 274300 Loss: 0.025449948385357857  PSNR: 19.41516876220703
[TRAIN] Iter: 274400 Loss: 0.023996641859412193  PSNR: 19.416873931884766
[TRAIN] Iter: 274500 Loss: 0.0335005484521389  PSNR: 17.9434871673584
[TRAIN] Iter: 274600 Loss: 0.028901025652885437  PSNR: 18.554237365722656
[TRAIN] Iter: 274700 Loss: 0.024550095200538635  PSNR: 19.153060913085938
[TRAIN] Iter: 274800 Loss: 0.03164108097553253  PSNR: 18.282135009765625
[TRAIN] Iter: 274900 Loss: 0.03140059486031532  PSNR: 18.370206832885742
[TRAIN] Iter: 275000 Loss: 0.03181580454111099  PSNR: 18.11023712158203
[TRAIN] Iter: 275100 Loss: 0.027642156928777695  PSNR: 18.877347946166992
[TRAIN] Iter: 275200 Loss: 0.025190092623233795  PSNR: 19.078182220458984
[TRAIN] Iter: 275300 Loss: 0.02598625421524048  PSNR: 19.10309600830078
[TRAIN] Iter: 275400 Loss: 0.022914409637451172  PSNR: 19.673036575317383
[TRAIN] Iter: 275500 Loss: 0.03299526870250702  PSNR: 17.912851333618164
[TRAIN] Iter: 275600 Loss: 0.022755272686481476  PSNR: 19.659408569335938
[TRAIN] Iter: 275700 Loss: 0.03008178249001503  PSNR: 18.39537811279297
[TRAIN] Iter: 275800 Loss: 0.027502814307808876  PSNR: 18.562458038330078
[TRAIN] Iter: 275900 Loss: 0.02645554393529892  PSNR: 19.02890396118164
[TRAIN] Iter: 276000 Loss: 0.028278721496462822  PSNR: 18.798381805419922
[TRAIN] Iter: 276100 Loss: 0.03310476616024971  PSNR: 17.972333908081055
[TRAIN] Iter: 276200 Loss: 0.024709615856409073  PSNR: 18.917272567749023
[TRAIN] Iter: 276300 Loss: 0.028065644204616547  PSNR: 18.757061004638672
[TRAIN] Iter: 276400 Loss: 0.027912743389606476  PSNR: 18.699445724487305
[TRAIN] Iter: 276500 Loss: 0.02994070202112198  PSNR: 18.722850799560547
[TRAIN] Iter: 276600 Loss: 0.03039075993001461  PSNR: 18.386476516723633
[TRAIN] Iter: 276700 Loss: 0.03155260533094406  PSNR: 18.1678466796875
[TRAIN] Iter: 276800 Loss: 0.030181506648659706  PSNR: 18.37019157409668
[TRAIN] Iter: 276900 Loss: 0.023594887927174568  PSNR: 19.431800842285156
[TRAIN] Iter: 277000 Loss: 0.02743103913962841  PSNR: 18.8542423248291
[TRAIN] Iter: 277100 Loss: 0.02619306556880474  PSNR: 18.752912521362305
[TRAIN] Iter: 277200 Loss: 0.028490496799349785  PSNR: 18.337907791137695
[TRAIN] Iter: 277300 Loss: 0.029860444366931915  PSNR: 18.317407608032227
[TRAIN] Iter: 277400 Loss: 0.03349164128303528  PSNR: 18.02846336364746
[TRAIN] Iter: 277500 Loss: 0.024944940581917763  PSNR: 18.85732078552246
[TRAIN] Iter: 277600 Loss: 0.02657967619597912  PSNR: 19.05409049987793
[TRAIN] Iter: 277700 Loss: 0.03087584301829338  PSNR: 18.240772247314453
[TRAIN] Iter: 277800 Loss: 0.023947790265083313  PSNR: 19.664337158203125
[TRAIN] Iter: 277900 Loss: 0.027784764766693115  PSNR: 18.96745491027832
[TRAIN] Iter: 278000 Loss: 0.02652868628501892  PSNR: 18.992704391479492
[TRAIN] Iter: 278100 Loss: 0.028614487498998642  PSNR: 18.598386764526367
[TRAIN] Iter: 278200 Loss: 0.02706853300333023  PSNR: 18.505102157592773
[TRAIN] Iter: 278300 Loss: 0.02199704386293888  PSNR: 19.66575050354004
[TRAIN] Iter: 278400 Loss: 0.02464243955910206  PSNR: 19.181684494018555
[TRAIN] Iter: 278500 Loss: 0.033896248787641525  PSNR: 17.871904373168945
[TRAIN] Iter: 278600 Loss: 0.031372781842947006  PSNR: 18.1845760345459
[TRAIN] Iter: 278700 Loss: 0.03314526379108429  PSNR: 18.105899810791016
[TRAIN] Iter: 278800 Loss: 0.026452435180544853  PSNR: 18.923171997070312
[TRAIN] Iter: 278900 Loss: 0.02985120750963688  PSNR: 18.44579315185547
[TRAIN] Iter: 279000 Loss: 0.026456037536263466  PSNR: 19.284425735473633
[TRAIN] Iter: 279100 Loss: 0.027400050312280655  PSNR: 18.986583709716797
[TRAIN] Iter: 279200 Loss: 0.02673930861055851  PSNR: 18.818788528442383
[TRAIN] Iter: 279300 Loss: 0.030368035659193993  PSNR: 18.35501480102539
[TRAIN] Iter: 279400 Loss: 0.025887973606586456  PSNR: 19.01259422302246
[TRAIN] Iter: 279500 Loss: 0.031539201736450195  PSNR: 18.322113037109375
[TRAIN] Iter: 279600 Loss: 0.02703937701880932  PSNR: 19.062061309814453
[TRAIN] Iter: 279700 Loss: 0.021849561482667923  PSNR: 19.775157928466797
[TRAIN] Iter: 279800 Loss: 0.033324114978313446  PSNR: 17.867599487304688
[TRAIN] Iter: 279900 Loss: 0.031172551214694977  PSNR: 18.251859664916992
Saved checkpoints at ./logs/TUT-LAB-nerf/280000.tar
[TRAIN] Iter: 280000 Loss: 0.031060315668582916  PSNR: 18.342357635498047
[TRAIN] Iter: 280100 Loss: 0.03259262442588806  PSNR: 18.052461624145508
[TRAIN] Iter: 280200 Loss: 0.019793465733528137  PSNR: 20.14753532409668
[TRAIN] Iter: 280300 Loss: 0.0313221737742424  PSNR: 18.147624969482422
[TRAIN] Iter: 280400 Loss: 0.030929313972592354  PSNR: 18.35125732421875
[TRAIN] Iter: 280500 Loss: 0.024954963475465775  PSNR: 19.192684173583984
[TRAIN] Iter: 280600 Loss: 0.03087300807237625  PSNR: 18.461944580078125
[TRAIN] Iter: 280700 Loss: 0.03079104796051979  PSNR: 18.35146141052246
[TRAIN] Iter: 280800 Loss: 0.025423865765333176  PSNR: 18.932903289794922
[TRAIN] Iter: 280900 Loss: 0.02483580820262432  PSNR: 19.47542381286621
[TRAIN] Iter: 281000 Loss: 0.035378359258174896  PSNR: 17.707311630249023
[TRAIN] Iter: 281100 Loss: 0.02366716042160988  PSNR: 19.36051368713379
[TRAIN] Iter: 281200 Loss: 0.02928810566663742  PSNR: 18.083528518676758
[TRAIN] Iter: 281300 Loss: 0.02749834954738617  PSNR: 18.505651473999023
[TRAIN] Iter: 281400 Loss: 0.029885921627283096  PSNR: 18.695178985595703
[TRAIN] Iter: 281500 Loss: 0.03249422460794449  PSNR: 18.296791076660156
[TRAIN] Iter: 281600 Loss: 0.02819381281733513  PSNR: 18.816211700439453
[TRAIN] Iter: 281700 Loss: 0.025449691340327263  PSNR: 19.360647201538086
[TRAIN] Iter: 281800 Loss: 0.030472109094262123  PSNR: 18.32903480529785
[TRAIN] Iter: 281900 Loss: 0.028922636061906815  PSNR: 18.750036239624023
[TRAIN] Iter: 282000 Loss: 0.03019249252974987  PSNR: 18.29867935180664
[TRAIN] Iter: 282100 Loss: 0.03153374791145325  PSNR: 18.149600982666016
[TRAIN] Iter: 282200 Loss: 0.025894267484545708  PSNR: 19.081806182861328
[TRAIN] Iter: 282300 Loss: 0.027645718306303024  PSNR: 18.303699493408203
[TRAIN] Iter: 282400 Loss: 0.029469726607203484  PSNR: 18.6922664642334
[TRAIN] Iter: 282500 Loss: 0.0325663797557354  PSNR: 17.995718002319336
[TRAIN] Iter: 282600 Loss: 0.02564527466893196  PSNR: 19.42677116394043
[TRAIN] Iter: 282700 Loss: 0.036033134907484055  PSNR: 17.54559898376465
[TRAIN] Iter: 282800 Loss: 0.023518914356827736  PSNR: 19.457887649536133
[TRAIN] Iter: 282900 Loss: 0.02692243456840515  PSNR: 19.219318389892578
[TRAIN] Iter: 283000 Loss: 0.02406151220202446  PSNR: 19.274566650390625
[TRAIN] Iter: 283100 Loss: 0.028653908520936966  PSNR: 18.677289962768555
[TRAIN] Iter: 283200 Loss: 0.02856728993356228  PSNR: 18.68355941772461
[TRAIN] Iter: 283300 Loss: 0.03220736235380173  PSNR: 18.181673049926758
[TRAIN] Iter: 283400 Loss: 0.02389523945748806  PSNR: 19.43779945373535
[TRAIN] Iter: 283500 Loss: 0.03159163147211075  PSNR: 18.08191680908203
[TRAIN] Iter: 283600 Loss: 0.021109111607074738  PSNR: 20.067235946655273
[TRAIN] Iter: 283700 Loss: 0.028828270733356476  PSNR: 18.526586532592773
[TRAIN] Iter: 283800 Loss: 0.033408600836992264  PSNR: 18.049819946289062
[TRAIN] Iter: 283900 Loss: 0.024761177599430084  PSNR: 19.32587432861328
[TRAIN] Iter: 284000 Loss: 0.025418102741241455  PSNR: 19.168718338012695
[TRAIN] Iter: 284100 Loss: 0.025452537462115288  PSNR: 19.010726928710938
[TRAIN] Iter: 284200 Loss: 0.02131776139140129  PSNR: 19.987468719482422
[TRAIN] Iter: 284300 Loss: 0.03262767195701599  PSNR: 18.090049743652344
[TRAIN] Iter: 284400 Loss: 0.03416178748011589  PSNR: 17.818584442138672
[TRAIN] Iter: 284500 Loss: 0.03631777688860893  PSNR: 17.521812438964844
[TRAIN] Iter: 284600 Loss: 0.02617763727903366  PSNR: 18.852214813232422
[TRAIN] Iter: 284700 Loss: 0.02538997307419777  PSNR: 19.36357307434082
[TRAIN] Iter: 284800 Loss: 0.022410381585359573  PSNR: 19.756710052490234
[TRAIN] Iter: 284900 Loss: 0.02611001580953598  PSNR: 19.14020538330078
[TRAIN] Iter: 285000 Loss: 0.028113607317209244  PSNR: 18.799209594726562
[TRAIN] Iter: 285100 Loss: 0.02585676498711109  PSNR: 19.059707641601562
[TRAIN] Iter: 285200 Loss: 0.02459567040205002  PSNR: 19.35597038269043
[TRAIN] Iter: 285300 Loss: 0.02696032077074051  PSNR: 18.94704818725586
[TRAIN] Iter: 285400 Loss: 0.0313003808259964  PSNR: 18.16773223876953
[TRAIN] Iter: 285500 Loss: 0.03172311186790466  PSNR: 18.151100158691406
[TRAIN] Iter: 285600 Loss: 0.025788864120841026  PSNR: 19.16124153137207
[TRAIN] Iter: 285700 Loss: 0.025477362796664238  PSNR: 19.08650779724121
[TRAIN] Iter: 285800 Loss: 0.02696685492992401  PSNR: 18.975812911987305
[TRAIN] Iter: 285900 Loss: 0.027886580675840378  PSNR: 18.754287719726562
[TRAIN] Iter: 286000 Loss: 0.02754221297800541  PSNR: 18.535249710083008
[TRAIN] Iter: 286100 Loss: 0.030995383858680725  PSNR: 18.320066452026367
[TRAIN] Iter: 286200 Loss: 0.03328210860490799  PSNR: 17.952632904052734
[TRAIN] Iter: 286300 Loss: 0.02600700594484806  PSNR: 19.10087776184082
[TRAIN] Iter: 286400 Loss: 0.023971229791641235  PSNR: 19.48241424560547
[TRAIN] Iter: 286500 Loss: 0.03419821709394455  PSNR: 17.821550369262695
[TRAIN] Iter: 286600 Loss: 0.031030144542455673  PSNR: 18.30071449279785
[TRAIN] Iter: 286700 Loss: 0.025589769706130028  PSNR: 19.226604461669922
[TRAIN] Iter: 286800 Loss: 0.032495737075805664  PSNR: 18.015962600708008
[TRAIN] Iter: 286900 Loss: 0.030300531536340714  PSNR: 18.483386993408203
[TRAIN] Iter: 287000 Loss: 0.02597203105688095  PSNR: 19.109960556030273
[TRAIN] Iter: 287100 Loss: 0.033079877495765686  PSNR: 18.05198860168457
[TRAIN] Iter: 287200 Loss: 0.027895916253328323  PSNR: 19.11631965637207
[TRAIN] Iter: 287300 Loss: 0.029283225536346436  PSNR: 18.499412536621094
[TRAIN] Iter: 287400 Loss: 0.027896150946617126  PSNR: 18.72228240966797
[TRAIN] Iter: 287500 Loss: 0.033125266432762146  PSNR: 18.048114776611328
[TRAIN] Iter: 287600 Loss: 0.025284498929977417  PSNR: 19.172666549682617
[TRAIN] Iter: 287700 Loss: 0.029884841293096542  PSNR: 18.673816680908203
[TRAIN] Iter: 287800 Loss: 0.030723340809345245  PSNR: 18.707237243652344
[TRAIN] Iter: 287900 Loss: 0.027774140238761902  PSNR: 18.87681770324707
[TRAIN] Iter: 288000 Loss: 0.031061779707670212  PSNR: 18.178842544555664
[TRAIN] Iter: 288100 Loss: 0.026682578027248383  PSNR: 19.05490493774414
[TRAIN] Iter: 288200 Loss: 0.03357831761240959  PSNR: 17.90555763244629
[TRAIN] Iter: 288300 Loss: 0.026407703757286072  PSNR: 19.143722534179688
[TRAIN] Iter: 288400 Loss: 0.032478295266628265  PSNR: 18.066659927368164
[TRAIN] Iter: 288500 Loss: 0.03245512396097183  PSNR: 18.00370216369629
[TRAIN] Iter: 288600 Loss: 0.022154390811920166  PSNR: 19.506690979003906
[TRAIN] Iter: 288700 Loss: 0.023361949250102043  PSNR: 19.40068244934082
[TRAIN] Iter: 288800 Loss: 0.031806085258722305  PSNR: 18.331220626831055
[TRAIN] Iter: 288900 Loss: 0.03159095346927643  PSNR: 18.426143646240234
[TRAIN] Iter: 289000 Loss: 0.027467623353004456  PSNR: 18.803924560546875
[TRAIN] Iter: 289100 Loss: 0.031011655926704407  PSNR: 18.097543716430664
[TRAIN] Iter: 289200 Loss: 0.027795761823654175  PSNR: 18.812414169311523
[TRAIN] Iter: 289300 Loss: 0.03189869225025177  PSNR: 18.07370948791504
[TRAIN] Iter: 289400 Loss: 0.030646968632936478  PSNR: 18.315786361694336
[TRAIN] Iter: 289500 Loss: 0.034219879657030106  PSNR: 17.777332305908203
[TRAIN] Iter: 289600 Loss: 0.02016439288854599  PSNR: 20.30179214477539
[TRAIN] Iter: 289700 Loss: 0.02457684651017189  PSNR: 19.119447708129883
[TRAIN] Iter: 289800 Loss: 0.021394675597548485  PSNR: 19.86263084411621
[TRAIN] Iter: 289900 Loss: 0.02509351447224617  PSNR: 19.225908279418945
Saved checkpoints at ./logs/TUT-LAB-nerf/290000.tar
[TRAIN] Iter: 290000 Loss: 0.02588394097983837  PSNR: 18.82285499572754
[TRAIN] Iter: 290100 Loss: 0.027462001889944077  PSNR: 18.657995223999023
[TRAIN] Iter: 290200 Loss: 0.028670094907283783  PSNR: 18.593746185302734
[TRAIN] Iter: 290300 Loss: 0.03233112767338753  PSNR: 18.02555274963379
[TRAIN] Iter: 290400 Loss: 0.020611539483070374  PSNR: 20.209339141845703
[TRAIN] Iter: 290500 Loss: 0.0327158123254776  PSNR: 17.82085418701172
[TRAIN] Iter: 290600 Loss: 0.026423625648021698  PSNR: 19.052478790283203
[TRAIN] Iter: 290700 Loss: 0.033318258821964264  PSNR: 17.846860885620117
[TRAIN] Iter: 290800 Loss: 0.02374059148132801  PSNR: 18.974750518798828
[TRAIN] Iter: 290900 Loss: 0.031762510538101196  PSNR: 18.1452579498291
[TRAIN] Iter: 291000 Loss: 0.03238686919212341  PSNR: 18.220905303955078
[TRAIN] Iter: 291100 Loss: 0.029081642627716064  PSNR: 18.545700073242188
[TRAIN] Iter: 291200 Loss: 0.03417862951755524  PSNR: 17.794153213500977
[TRAIN] Iter: 291300 Loss: 0.025134243071079254  PSNR: 19.240276336669922
[TRAIN] Iter: 291400 Loss: 0.02495875395834446  PSNR: 19.532777786254883
[TRAIN] Iter: 291500 Loss: 0.03393873572349548  PSNR: 17.792455673217773
[TRAIN] Iter: 291600 Loss: 0.021895600482821465  PSNR: 19.821874618530273
[TRAIN] Iter: 291700 Loss: 0.027544526383280754  PSNR: 18.848222732543945
[TRAIN] Iter: 291800 Loss: 0.029986713081598282  PSNR: 18.399690628051758
[TRAIN] Iter: 291900 Loss: 0.020809505134820938  PSNR: 20.022624969482422
[TRAIN] Iter: 292000 Loss: 0.02998371049761772  PSNR: 18.3360652923584
[TRAIN] Iter: 292100 Loss: 0.03429453447461128  PSNR: 18.103713989257812
[TRAIN] Iter: 292200 Loss: 0.02983933873474598  PSNR: 18.453454971313477
[TRAIN] Iter: 292300 Loss: 0.02515282854437828  PSNR: 18.98131561279297
[TRAIN] Iter: 292400 Loss: 0.02487674169242382  PSNR: 19.506460189819336
[TRAIN] Iter: 292500 Loss: 0.031353939324617386  PSNR: 18.218936920166016
[TRAIN] Iter: 292600 Loss: 0.025176677852869034  PSNR: 19.079227447509766
[TRAIN] Iter: 292700 Loss: 0.029041174799203873  PSNR: 18.513216018676758
[TRAIN] Iter: 292800 Loss: 0.0263346079736948  PSNR: 19.04469108581543
[TRAIN] Iter: 292900 Loss: 0.027872003614902496  PSNR: 18.801443099975586
[TRAIN] Iter: 293000 Loss: 0.02451311983168125  PSNR: 18.911724090576172
[TRAIN] Iter: 293100 Loss: 0.02760796621441841  PSNR: 18.493425369262695
[TRAIN] Iter: 293200 Loss: 0.023889970034360886  PSNR: 19.278520584106445
[TRAIN] Iter: 293300 Loss: 0.028445567935705185  PSNR: 18.674602508544922
[TRAIN] Iter: 293400 Loss: 0.02559845708310604  PSNR: 19.227214813232422
[TRAIN] Iter: 293500 Loss: 0.02858424000442028  PSNR: 18.70965576171875
[TRAIN] Iter: 293600 Loss: 0.026599109172821045  PSNR: 18.93269157409668
[TRAIN] Iter: 293700 Loss: 0.021731175482273102  PSNR: 19.902963638305664
[TRAIN] Iter: 293800 Loss: 0.029140304774045944  PSNR: 18.537006378173828
[TRAIN] Iter: 293900 Loss: 0.029674282297492027  PSNR: 18.254343032836914
[TRAIN] Iter: 294000 Loss: 0.029378313571214676  PSNR: 18.726804733276367
[TRAIN] Iter: 294100 Loss: 0.02945207990705967  PSNR: 18.14603614807129
[TRAIN] Iter: 294200 Loss: 0.03041452169418335  PSNR: 18.340797424316406
[TRAIN] Iter: 294300 Loss: 0.031874895095825195  PSNR: 18.13416290283203
[TRAIN] Iter: 294400 Loss: 0.025795772671699524  PSNR: 19.090240478515625
[TRAIN] Iter: 294500 Loss: 0.02300468645989895  PSNR: 19.783639907836914
[TRAIN] Iter: 294600 Loss: 0.022661546245217323  PSNR: 19.449613571166992
[TRAIN] Iter: 294700 Loss: 0.030378390103578568  PSNR: 18.261625289916992
[TRAIN] Iter: 294800 Loss: 0.029711466282606125  PSNR: 18.500585556030273
[TRAIN] Iter: 294900 Loss: 0.03233734518289566  PSNR: 18.13189125061035
[TRAIN] Iter: 295000 Loss: 0.028630249202251434  PSNR: 18.634571075439453
[TRAIN] Iter: 295100 Loss: 0.029404476284980774  PSNR: 18.766704559326172
[TRAIN] Iter: 295200 Loss: 0.029533959925174713  PSNR: 18.57415771484375
[TRAIN] Iter: 295300 Loss: 0.03303947299718857  PSNR: 17.96271514892578
[TRAIN] Iter: 295400 Loss: 0.024189665913581848  PSNR: 19.382736206054688
[TRAIN] Iter: 295500 Loss: 0.0293685682117939  PSNR: 18.646923065185547
[TRAIN] Iter: 295600 Loss: 0.027819855138659477  PSNR: 18.94682502746582
[TRAIN] Iter: 295700 Loss: 0.028154706582427025  PSNR: 18.456016540527344
[TRAIN] Iter: 295800 Loss: 0.02804829739034176  PSNR: 18.765331268310547
[TRAIN] Iter: 295900 Loss: 0.031838588416576385  PSNR: 18.25409507751465
[TRAIN] Iter: 296000 Loss: 0.025201626121997833  PSNR: 19.279888153076172
[TRAIN] Iter: 296100 Loss: 0.03395196795463562  PSNR: 17.869413375854492
[TRAIN] Iter: 296200 Loss: 0.02330215834081173  PSNR: 19.71109962463379
[TRAIN] Iter: 296300 Loss: 0.03151687979698181  PSNR: 18.14557456970215
[TRAIN] Iter: 296400 Loss: 0.022466370835900307  PSNR: 19.70720100402832
[TRAIN] Iter: 296500 Loss: 0.037900641560554504  PSNR: 17.308147430419922
[TRAIN] Iter: 296600 Loss: 0.024853762239217758  PSNR: 19.22894287109375
[TRAIN] Iter: 296700 Loss: 0.028698507696390152  PSNR: 18.660736083984375
[TRAIN] Iter: 296800 Loss: 0.025842875242233276  PSNR: 19.112478256225586
[TRAIN] Iter: 296900 Loss: 0.02847343310713768  PSNR: 18.538389205932617
[TRAIN] Iter: 297000 Loss: 0.030849851667881012  PSNR: 18.30719566345215
[TRAIN] Iter: 297100 Loss: 0.032457537949085236  PSNR: 18.109952926635742
[TRAIN] Iter: 297200 Loss: 0.030203022062778473  PSNR: 18.368955612182617
[TRAIN] Iter: 297300 Loss: 0.028499208390712738  PSNR: 18.78150749206543
[TRAIN] Iter: 297400 Loss: 0.028064701706171036  PSNR: 18.6024112701416
[TRAIN] Iter: 297500 Loss: 0.03468118607997894  PSNR: 17.826431274414062
[TRAIN] Iter: 297600 Loss: 0.032606348395347595  PSNR: 17.95428466796875
[TRAIN] Iter: 297700 Loss: 0.029548611491918564  PSNR: 18.513147354125977
[TRAIN] Iter: 297800 Loss: 0.02693338319659233  PSNR: 18.68369483947754
[TRAIN] Iter: 297900 Loss: 0.027784040197730064  PSNR: 18.877412796020508
[TRAIN] Iter: 298000 Loss: 0.02268746681511402  PSNR: 19.370101928710938
[TRAIN] Iter: 298100 Loss: 0.02971522882580757  PSNR: 18.805700302124023
[TRAIN] Iter: 298200 Loss: 0.028693892061710358  PSNR: 18.43441390991211
[TRAIN] Iter: 298300 Loss: 0.02927173301577568  PSNR: 18.643142700195312
[TRAIN] Iter: 298400 Loss: 0.023792199790477753  PSNR: 19.40985107421875
[TRAIN] Iter: 298500 Loss: 0.025622643530368805  PSNR: 19.125539779663086
[TRAIN] Iter: 298600 Loss: 0.024409383535385132  PSNR: 19.19793128967285
[TRAIN] Iter: 298700 Loss: 0.026177145540714264  PSNR: 19.170360565185547
[TRAIN] Iter: 298800 Loss: 0.027850577607750893  PSNR: 18.680145263671875
[TRAIN] Iter: 298900 Loss: 0.02902901917695999  PSNR: 18.577713012695312
[TRAIN] Iter: 299000 Loss: 0.031517188996076584  PSNR: 18.425195693969727
[TRAIN] Iter: 299100 Loss: 0.03198904171586037  PSNR: 18.18815040588379
[TRAIN] Iter: 299200 Loss: 0.029331665486097336  PSNR: 18.2418270111084
[TRAIN] Iter: 299300 Loss: 0.024702681228518486  PSNR: 19.23137664794922
[TRAIN] Iter: 299400 Loss: 0.02017120085656643  PSNR: 20.23440170288086
[TRAIN] Iter: 299500 Loss: 0.03063158690929413  PSNR: 18.352279663085938
[TRAIN] Iter: 299600 Loss: 0.03167835250496864  PSNR: 18.302167892456055
[TRAIN] Iter: 299700 Loss: 0.02935914695262909  PSNR: 18.5219783782959
[TRAIN] Iter: 299800 Loss: 0.029095206409692764  PSNR: 18.56902503967285
[TRAIN] Iter: 299900 Loss: 0.02964901551604271  PSNR: 18.580303192138672
Saved checkpoints at ./logs/TUT-LAB-nerf/300000.tar
0 0.0004088878631591797
torch.Size([320, 640, 3]) torch.Size([320, 640])
1 18.106513023376465
2 15.149973630905151
3 15.120582342147827
4 18.014155864715576
5 14.83380913734436
6 18.350590467453003
7 14.900297403335571
8 18.313634157180786
9 14.851895332336426
10 18.184159517288208
11 14.888905048370361
12 18.206576824188232
13 14.987310409545898
14 18.178582906723022
15 10.07527494430542
16 14.780369758605957
17 18.28513813018799
18 14.818977117538452
19 18.259546279907227
20 14.864580154418945
21 18.28903865814209
22 14.800573110580444
23 18.228894472122192
24 14.906345844268799
25 14.801156759262085
26 18.245009899139404
27 14.802144289016724
28 18.3238627910614
29 14.789296388626099
30 18.312249898910522
31 14.797230005264282
32 18.310874462127686
33 14.880374431610107
34 18.30138850212097
35 14.779757499694824
36 18.22117781639099
37 14.879940032958984
38 14.778003692626953
39 18.347364902496338
40 14.842837810516357
41 18.372809171676636
42 14.713991403579712
43 18.383584022521973
44 14.787810564041138
45 18.333879470825195
46 14.834590196609497
47 18.294186115264893
48 14.896823167800903
49 18.17324709892273
50 15.026278495788574
51 14.831417798995972
52 18.233652591705322
53 14.840413093566895
54 18.372734785079956
55 14.842863082885742
56 18.03469705581665
57 14.882628440856934
58 18.294936656951904
59 15.048823595046997
60 17.881619215011597
61 14.906919002532959
62 18.29224705696106
63 15.430127143859863
64 15.162261009216309
65 17.780473232269287
66 14.730902433395386
67 18.75225281715393
68 14.814014911651611
69 17.919804573059082
70 14.833618402481079
71 18.53817391395569
72 15.086710214614868
73 17.89877414703369
74 15.003914833068848
75 18.080496549606323
76 15.001306533813477
77 18.063605070114136
78 15.065711975097656
79 15.043139457702637
80 18.006961584091187
81 14.78855586051941
82 18.377188205718994
83 14.818954944610596
84 18.249796628952026
85 14.879899978637695
86 18.16956377029419
87 14.867898941040039
88 18.194676160812378
89 14.925588130950928
90 18.187952280044556
91 14.944345235824585
92 14.833208084106445
93 18.321577548980713
94 14.781934022903442
95 18.30965542793274
96 14.80562686920166
97 18.40281367301941
98 14.761611700057983
99 18.26332712173462
100 14.86117696762085
101 18.29129457473755
102 14.872517585754395
103 18.25785183906555
104 14.903299331665039
105 14.77897334098816
106 18.295585870742798
107 14.782436847686768
108 18.336084127426147
109 14.821383237838745
110 18.341105937957764
111 14.829074144363403
112 18.247310876846313
113 14.820037841796875
114 18.28175687789917
115 14.823887825012207
116 18.3571195602417
117 14.880964279174805
118 14.765146970748901
119 18.343239545822144
Done, saving (120, 320, 640, 3) (120, 320, 640)
extras:{'raw': tensor([[[-1.1104e-01, -9.5983e-02, -5.5221e-02, -2.0217e+01],
         [-5.2195e-01, -4.0248e-01, -4.2609e-01, -1.5251e+01],
         [-4.6905e-01, -4.1689e-01, -4.5299e-01, -4.6404e+00],
         ...,
         [ 8.2775e+00,  4.4814e+00, -7.4154e-01,  3.0476e+02],
         [ 7.4538e+00,  3.8722e+00, -9.3884e-01,  3.0708e+02],
         [ 9.1644e+00,  5.0027e+00, -6.6671e-01,  3.1906e+02]],

        [[ 1.0229e-01, -1.1300e-01, -2.8627e-01,  1.8065e+01],
         [ 2.1106e-01, -2.6316e-02, -1.6962e-01,  2.9738e+00],
         [ 1.8363e-01,  5.9151e-02, -4.6907e-02, -3.7234e+00],
         ...,
         [ 2.6520e+01,  2.3673e+01,  2.6228e+01,  2.1001e+02],
         [ 2.7338e+01,  2.4530e+01,  2.7162e+01,  2.1501e+02],
         [ 2.8712e+01,  2.6081e+01,  2.9390e+01,  2.1337e+02]],

        [[-9.3206e-01, -8.4475e-01, -9.6039e-01, -2.4449e+01],
         [-1.1273e+00, -1.1288e+00, -1.4653e+00, -2.6144e+00],
         [-1.0794e+00, -1.0843e+00, -1.4188e+00, -1.7454e+00],
         ...,
         [ 4.4188e+00,  2.7565e+00,  5.7859e-01,  1.6068e+02],
         [ 5.1266e+00,  3.7464e+00,  2.2386e+00,  1.6474e+02],
         [ 4.1683e+00,  2.6048e+00,  6.5643e-01,  1.3543e+02]],

        ...,

        [[ 3.0029e-01,  1.5162e-01,  5.4938e-02,  1.7555e+01],
         [ 7.7211e-02, -4.1972e-02, -1.5090e-01,  5.3367e+00],
         [-1.1798e-03, -1.7012e-01, -3.1579e-01,  3.8458e+00],
         ...,
         [ 1.6784e+01,  1.4059e+01,  1.1898e+01,  1.8384e+02],
         [ 1.7074e+01,  1.3819e+01,  1.0641e+01,  1.8009e+02],
         [ 1.7937e+01,  1.4533e+01,  1.1165e+01,  1.7458e+02]],

        [[-1.4558e-01, -7.0298e-01, -1.4113e+00, -2.3429e+01],
         [-1.9184e-01, -4.9948e-01, -5.4939e-01, -1.1402e+01],
         [ 2.3601e-02, -9.8354e-02,  3.2797e-01, -1.8548e+01],
         ...,
         [-1.7787e+00, -9.8694e-01,  1.3852e+00, -5.8592e+00],
         [-1.7763e+00, -1.5794e+00, -9.0003e-01, -7.6694e+00],
         [-6.6242e-01, -1.2724e+00, -1.6065e+00,  8.4405e+00]],

        [[-1.5376e+00, -1.5375e+00, -9.8176e-01, -4.7598e+01],
         [-1.2036e+00, -1.4139e+00, -1.2456e+00, -3.9514e+01],
         [-5.5466e-01, -7.1125e-01, -4.7036e-01, -2.0381e+01],
         ...,
         [-6.5179e-01, -1.5591e-01,  1.3011e+00, -1.9754e+00],
         [-1.0820e-01,  2.0888e-01,  1.5603e+00,  7.2900e+00],
         [-3.1755e-01,  8.4041e-02,  9.7396e-01,  5.4395e+00]]],
       grad_fn=<ReshapeAliasBackward0>), 'rgb0': tensor([[0.4511, 0.4933, 0.5168],
        [0.4971, 0.4520, 0.4275],
        [0.2180, 0.2643, 0.1739],
        ...,
        [0.5854, 0.5505, 0.5290],
        [0.5538, 0.4138, 0.3178],
        [0.3753, 0.3917, 0.6250]], grad_fn=<ReshapeAliasBackward0>), 'disp0': tensor([ 30.0274, 277.0935,  86.7559,  ..., 298.2884,  16.7563,  11.7500],
       grad_fn=<ReshapeAliasBackward0>), 'acc0': tensor([1., 1., 1.,  ..., 1., 1., 1.], grad_fn=<ReshapeAliasBackward0>), 'z_std': tensor([0.0717, 0.2663, 0.0039,  ..., 0.3003, 0.0024, 0.0023])}
0 0.0004303455352783203
torch.Size([320, 640, 3]) torch.Size([320, 640])
1 18.38503074645996
2 14.797422885894775
3 18.350646495819092
4 14.794905424118042
5 18.38922619819641
6 14.954911231994629
7 18.135682821273804
8 15.02268648147583
9 18.072250843048096
10 15.066734552383423
11 17.963518857955933
12 14.988045454025269
13 12.077142715454102
14 18.27275276184082
15 14.892071008682251
16 17.934313535690308
17 14.450342416763306
18 16.421842098236084
19 13.28537130355835
20 13.024619340896606
21 16.428715467453003
22 13.123251914978027
23 16.427167654037476
24 13.225366115570068
25 13.032302141189575
26 16.553958892822266
27 13.075467109680176
28 16.40718126296997
29 13.19110107421875
30 13.042348384857178
31 16.598512411117554
32 13.20177674293518
33 16.302276849746704
34 13.23072624206543
35 16.264776945114136
36 13.215086936950684
37 13.246920585632324
38 16.18664836883545
39 13.038447380065918
40 16.54041600227356
41 13.218844652175903
42 13.150137424468994
43 16.445971488952637
44 13.101216316223145
45 16.43739104270935
46 13.120083808898926
47 13.059251546859741
48 16.507765769958496
49 13.097679138183594
50 16.497905731201172
51 13.152825117111206
52 13.083324909210205
53 16.55535578727722
54 13.050840854644775
55 16.506048440933228
56 13.091870069503784
57 13.040018558502197
58 16.541022062301636
59 13.044560670852661
60 16.51454758644104
61 13.112079858779907
62 16.49181628227234
63 13.19146466255188
64 13.054327249526978
65 16.570223569869995
66 13.06856918334961
67 16.4713191986084
68 13.173192262649536
69 13.084734201431274
70 16.588769674301147
71 13.09086275100708
72 16.534645080566406
73 13.11170744895935
74 13.076297521591187
75 16.663591384887695
76 13.052157878875732
77 16.520056009292603
78 13.118257761001587
79 13.072359561920166
80 16.60356879234314
81 13.048230171203613
82 16.53991198539734
83 13.092465162277222
84 13.038136005401611
85 16.624120950698853
86 13.070883989334106
87 16.542872667312622
88 13.165300607681274
89 13.045188426971436
90 16.61694073677063
91 13.099361419677734
92 16.407970666885376
93 13.21769404411316
94 16.306987047195435
95 13.263270139694214
96 13.065219640731812
97 16.50131630897522
98 13.142874479293823
99 16.238465070724487
100 13.235727787017822
101 13.055599689483643
102 16.591371774673462
103 13.176161289215088
104 16.306235313415527
105 13.164936065673828
106 13.069485902786255
107 16.57455015182495
108 13.142104864120483
109 16.371191263198853
110 13.191452026367188
111 13.04161548614502
112 13.603650331497192
113 19.504391193389893
114 15.001632452011108
115 12.225301265716553
116 13.599773645401001
117 16.097206592559814
118 13.716960430145264
119 13.248563766479492
test poses shape torch.Size([13, 3, 4])
0 0.0007030963897705078
torch.Size([320, 640, 3]) torch.Size([320, 640])
1 13.372678279876709
2 16.426927089691162
3 13.314347982406616
4 15.552314758300781
5 13.629811525344849
6 13.329551458358765
7 16.459275245666504
8 13.333929061889648
9 15.933549880981445
10 13.362565040588379
11 15.7704496383667
12 13.882589340209961
Saved test set
[TRAIN] Iter: 300000 Loss: 0.029916061088442802  PSNR: 18.414878845214844
[TRAIN] Iter: 300100 Loss: 0.030089164152741432  PSNR: 18.30257225036621
[TRAIN] Iter: 300200 Loss: 0.026115238666534424  PSNR: 18.938167572021484
[TRAIN] Iter: 300300 Loss: 0.031040430068969727  PSNR: 18.28647804260254
[TRAIN] Iter: 300400 Loss: 0.026463061571121216  PSNR: 19.118568420410156
[TRAIN] Iter: 300500 Loss: 0.024807652458548546  PSNR: 18.808795928955078
[TRAIN] Iter: 300600 Loss: 0.024974757805466652  PSNR: 19.399566650390625
[TRAIN] Iter: 300700 Loss: 0.03346722573041916  PSNR: 17.955812454223633
[TRAIN] Iter: 300800 Loss: 0.034732885658741  PSNR: 17.691404342651367
[TRAIN] Iter: 300900 Loss: 0.0308816060423851  PSNR: 18.34027862548828
[TRAIN] Iter: 301000 Loss: 0.034635771065950394  PSNR: 17.715377807617188
[TRAIN] Iter: 301100 Loss: 0.024655772373080254  PSNR: 19.037137985229492
[TRAIN] Iter: 301200 Loss: 0.030461035668849945  PSNR: 18.348176956176758
[TRAIN] Iter: 301300 Loss: 0.029420774430036545  PSNR: 18.206329345703125
[TRAIN] Iter: 301400 Loss: 0.029774462804198265  PSNR: 18.71840476989746
[TRAIN] Iter: 301500 Loss: 0.03405093401670456  PSNR: 17.901683807373047
[TRAIN] Iter: 301600 Loss: 0.024721471592783928  PSNR: 19.430343627929688
[TRAIN] Iter: 301700 Loss: 0.03205977380275726  PSNR: 18.000869750976562
[TRAIN] Iter: 301800 Loss: 0.022569485008716583  PSNR: 19.711292266845703
[TRAIN] Iter: 301900 Loss: 0.02870294637978077  PSNR: 18.716571807861328
[TRAIN] Iter: 302000 Loss: 0.02376512810587883  PSNR: 19.451313018798828
[TRAIN] Iter: 302100 Loss: 0.028054777532815933  PSNR: 18.566444396972656
[TRAIN] Iter: 302200 Loss: 0.02753939852118492  PSNR: 18.775768280029297
[TRAIN] Iter: 302300 Loss: 0.029134541749954224  PSNR: 18.450407028198242
[TRAIN] Iter: 302400 Loss: 0.033610280603170395  PSNR: 17.943361282348633
[TRAIN] Iter: 302500 Loss: 0.026850460097193718  PSNR: 19.03803253173828
[TRAIN] Iter: 302600 Loss: 0.025155404582619667  PSNR: 19.174577713012695
[TRAIN] Iter: 302700 Loss: 0.02651260606944561  PSNR: 18.666996002197266
[TRAIN] Iter: 302800 Loss: 0.028067994862794876  PSNR: 18.74103355407715
[TRAIN] Iter: 302900 Loss: 0.03136990964412689  PSNR: 18.185121536254883
[TRAIN] Iter: 303000 Loss: 0.03160183131694794  PSNR: 18.080434799194336
[TRAIN] Iter: 303100 Loss: 0.025043180212378502  PSNR: 19.111783981323242
[TRAIN] Iter: 303200 Loss: 0.022407906129956245  PSNR: 19.779996871948242
[TRAIN] Iter: 303300 Loss: 0.024782715365290642  PSNR: 19.217336654663086
[TRAIN] Iter: 303400 Loss: 0.021711092442274094  PSNR: 19.683547973632812
[TRAIN] Iter: 303500 Loss: 0.021442122757434845  PSNR: 19.983901977539062
[TRAIN] Iter: 303600 Loss: 0.025613199919462204  PSNR: 19.221452713012695
[TRAIN] Iter: 303700 Loss: 0.02547343447804451  PSNR: 19.13102912902832
[TRAIN] Iter: 303800 Loss: 0.03567963093519211  PSNR: 17.640695571899414
[TRAIN] Iter: 303900 Loss: 0.02714730240404606  PSNR: 18.91535758972168
[TRAIN] Iter: 304000 Loss: 0.036060184240341187  PSNR: 17.527029037475586
[TRAIN] Iter: 304100 Loss: 0.026935866102576256  PSNR: 18.87400245666504
[TRAIN] Iter: 304200 Loss: 0.031268276274204254  PSNR: 18.216140747070312
[TRAIN] Iter: 304300 Loss: 0.029177026823163033  PSNR: 18.707382202148438
[TRAIN] Iter: 304400 Loss: 0.030850837007164955  PSNR: 18.36682891845703
[TRAIN] Iter: 304500 Loss: 0.031059356406331062  PSNR: 18.183055877685547
[TRAIN] Iter: 304600 Loss: 0.027989760041236877  PSNR: 18.53297996520996
[TRAIN] Iter: 304700 Loss: 0.021726300939917564  PSNR: 19.757780075073242
[TRAIN] Iter: 304800 Loss: 0.025001248344779015  PSNR: 19.426809310913086
[TRAIN] Iter: 304900 Loss: 0.029138736426830292  PSNR: 18.625629425048828
[TRAIN] Iter: 305000 Loss: 0.023524140939116478  PSNR: 19.48661231994629
[TRAIN] Iter: 305100 Loss: 0.02908838726580143  PSNR: 18.356721878051758
[TRAIN] Iter: 305200 Loss: 0.03290008008480072  PSNR: 18.202518463134766
[TRAIN] Iter: 305300 Loss: 0.030508849769830704  PSNR: 18.403566360473633
[TRAIN] Iter: 305400 Loss: 0.022540610283613205  PSNR: 19.43355369567871
[TRAIN] Iter: 305500 Loss: 0.030609099194407463  PSNR: 18.225608825683594
[TRAIN] Iter: 305600 Loss: 0.024626944214105606  PSNR: 19.138439178466797
[TRAIN] Iter: 305700 Loss: 0.030537014827132225  PSNR: 18.149599075317383
[TRAIN] Iter: 305800 Loss: 0.03024902194738388  PSNR: 18.50597381591797
[TRAIN] Iter: 305900 Loss: 0.023698441684246063  PSNR: 19.35014533996582
[TRAIN] Iter: 306000 Loss: 0.02206883206963539  PSNR: 19.705551147460938
[TRAIN] Iter: 306100 Loss: 0.020955847576260567  PSNR: 19.9757080078125
[TRAIN] Iter: 306200 Loss: 0.026115691289305687  PSNR: 19.32843017578125
[TRAIN] Iter: 306300 Loss: 0.023124899715185165  PSNR: 19.6787166595459
[TRAIN] Iter: 306400 Loss: 0.03248624876141548  PSNR: 18.123294830322266
[TRAIN] Iter: 306500 Loss: 0.034687645733356476  PSNR: 17.855894088745117
[TRAIN] Iter: 306600 Loss: 0.024273734539747238  PSNR: 19.150306701660156
[TRAIN] Iter: 306700 Loss: 0.027811631560325623  PSNR: 18.674278259277344
[TRAIN] Iter: 306800 Loss: 0.025963090360164642  PSNR: 19.10654640197754
[TRAIN] Iter: 306900 Loss: 0.03204509988427162  PSNR: 18.191926956176758
[TRAIN] Iter: 307000 Loss: 0.02407781034708023  PSNR: 19.36939811706543
[TRAIN] Iter: 307100 Loss: 0.0338999405503273  PSNR: 17.939760208129883
[TRAIN] Iter: 307200 Loss: 0.030552275478839874  PSNR: 18.34885597229004
[TRAIN] Iter: 307300 Loss: 0.03317919746041298  PSNR: 17.983354568481445
[TRAIN] Iter: 307400 Loss: 0.023847226053476334  PSNR: 19.88179588317871
[TRAIN] Iter: 307500 Loss: 0.03163314610719681  PSNR: 18.111543655395508
[TRAIN] Iter: 307600 Loss: 0.027534255757927895  PSNR: 18.901126861572266
[TRAIN] Iter: 307700 Loss: 0.02704853005707264  PSNR: 18.706043243408203
[TRAIN] Iter: 307800 Loss: 0.026585420593619347  PSNR: 18.70790672302246
[TRAIN] Iter: 307900 Loss: 0.023880349472165108  PSNR: 19.61467933654785
[TRAIN] Iter: 308000 Loss: 0.0337400883436203  PSNR: 17.975414276123047
[TRAIN] Iter: 308100 Loss: 0.030946964398026466  PSNR: 18.24271583557129
[TRAIN] Iter: 308200 Loss: 0.02765427529811859  PSNR: 18.825593948364258
[TRAIN] Iter: 308300 Loss: 0.029573697596788406  PSNR: 18.436328887939453
[TRAIN] Iter: 308400 Loss: 0.02579702064394951  PSNR: 18.93639373779297
[TRAIN] Iter: 308500 Loss: 0.02740374580025673  PSNR: 18.718358993530273
[TRAIN] Iter: 308600 Loss: 0.03373616188764572  PSNR: 17.784605026245117
[TRAIN] Iter: 308700 Loss: 0.023688964545726776  PSNR: 19.5151309967041
[TRAIN] Iter: 308800 Loss: 0.02666379138827324  PSNR: 18.69801902770996
[TRAIN] Iter: 308900 Loss: 0.026139220222830772  PSNR: 19.155433654785156
[TRAIN] Iter: 309000 Loss: 0.02483077347278595  PSNR: 19.075855255126953
[TRAIN] Iter: 309100 Loss: 0.034054502844810486  PSNR: 17.902828216552734
[TRAIN] Iter: 309200 Loss: 0.030668681487441063  PSNR: 18.246618270874023
[TRAIN] Iter: 309300 Loss: 0.01860513724386692  PSNR: 20.557119369506836
[TRAIN] Iter: 309400 Loss: 0.026073358952999115  PSNR: 19.14575958251953
[TRAIN] Iter: 309500 Loss: 0.028672020882368088  PSNR: 18.576257705688477
[TRAIN] Iter: 309600 Loss: 0.026448426768183708  PSNR: 19.012948989868164
[TRAIN] Iter: 309700 Loss: 0.025100253522396088  PSNR: 19.158985137939453
[TRAIN] Iter: 309800 Loss: 0.02449953556060791  PSNR: 19.376344680786133
[TRAIN] Iter: 309900 Loss: 0.02132982388138771  PSNR: 19.818431854248047
Saved checkpoints at ./logs/TUT-LAB-nerf/310000.tar
[TRAIN] Iter: 310000 Loss: 0.02895759604871273  PSNR: 18.69831657409668
[TRAIN] Iter: 310100 Loss: 0.029305126518011093  PSNR: 18.538103103637695
[TRAIN] Iter: 310200 Loss: 0.029103925451636314  PSNR: 18.465839385986328
[TRAIN] Iter: 310300 Loss: 0.024389145895838737  PSNR: 19.454795837402344
[TRAIN] Iter: 310400 Loss: 0.03303166478872299  PSNR: 17.99247932434082
[TRAIN] Iter: 310500 Loss: 0.02510470524430275  PSNR: 19.264860153198242
[TRAIN] Iter: 310600 Loss: 0.030062410980463028  PSNR: 18.440343856811523
[TRAIN] Iter: 310700 Loss: 0.02559226006269455  PSNR: 18.916526794433594
[TRAIN] Iter: 310800 Loss: 0.02778371423482895  PSNR: 18.914825439453125
[TRAIN] Iter: 310900 Loss: 0.035490505397319794  PSNR: 17.681474685668945
[TRAIN] Iter: 311000 Loss: 0.030687935650348663  PSNR: 18.236801147460938
[TRAIN] Iter: 311100 Loss: 0.022171393036842346  PSNR: 19.72572135925293
[TRAIN] Iter: 311200 Loss: 0.026147985830903053  PSNR: 18.903026580810547
[TRAIN] Iter: 311300 Loss: 0.032956819981336594  PSNR: 17.9660701751709
[TRAIN] Iter: 311400 Loss: 0.023574594408273697  PSNR: 19.277467727661133
[TRAIN] Iter: 311500 Loss: 0.02841823361814022  PSNR: 18.637596130371094
[TRAIN] Iter: 311600 Loss: 0.02919088862836361  PSNR: 18.456504821777344
[TRAIN] Iter: 311700 Loss: 0.02530008554458618  PSNR: 19.066190719604492
[TRAIN] Iter: 311800 Loss: 0.02392599545419216  PSNR: 19.43435287475586
[TRAIN] Iter: 311900 Loss: 0.027081705629825592  PSNR: 18.796340942382812
[TRAIN] Iter: 312000 Loss: 0.03195594251155853  PSNR: 18.177846908569336
[TRAIN] Iter: 312100 Loss: 0.02771163359284401  PSNR: 18.984397888183594
[TRAIN] Iter: 312200 Loss: 0.02941984497010708  PSNR: 18.591014862060547
[TRAIN] Iter: 312300 Loss: 0.03225257620215416  PSNR: 17.951208114624023
[TRAIN] Iter: 312400 Loss: 0.0274815671145916  PSNR: 18.682432174682617
[TRAIN] Iter: 312500 Loss: 0.02297283336520195  PSNR: 19.794658660888672
[TRAIN] Iter: 312600 Loss: 0.02427586168050766  PSNR: 19.21426010131836
[TRAIN] Iter: 312700 Loss: 0.02743002027273178  PSNR: 18.819686889648438
[TRAIN] Iter: 312800 Loss: 0.028804849833250046  PSNR: 18.74850082397461
[TRAIN] Iter: 312900 Loss: 0.023899834603071213  PSNR: 19.064678192138672
[TRAIN] Iter: 313000 Loss: 0.030219189822673798  PSNR: 18.4868221282959
[TRAIN] Iter: 313100 Loss: 0.024277273565530777  PSNR: 19.472103118896484
[TRAIN] Iter: 313200 Loss: 0.027983171865344048  PSNR: 18.76691436767578
[TRAIN] Iter: 313300 Loss: 0.024865558370947838  PSNR: 19.302034378051758
[TRAIN] Iter: 313400 Loss: 0.0247407928109169  PSNR: 18.88701820373535
[TRAIN] Iter: 313500 Loss: 0.032976873219013214  PSNR: 18.12041664123535
[TRAIN] Iter: 313600 Loss: 0.03280799835920334  PSNR: 17.96912956237793
[TRAIN] Iter: 313700 Loss: 0.02559760957956314  PSNR: 19.019100189208984
[TRAIN] Iter: 313800 Loss: 0.019574381411075592  PSNR: 20.279613494873047
[TRAIN] Iter: 313900 Loss: 0.03224930539727211  PSNR: 18.157943725585938
[TRAIN] Iter: 314000 Loss: 0.024150490760803223  PSNR: 19.486907958984375
[TRAIN] Iter: 314100 Loss: 0.02762293815612793  PSNR: 18.810653686523438
[TRAIN] Iter: 314200 Loss: 0.024993058294057846  PSNR: 19.532148361206055
[TRAIN] Iter: 314300 Loss: 0.02758088894188404  PSNR: 19.081228256225586
[TRAIN] Iter: 314400 Loss: 0.02709747850894928  PSNR: 18.99773406982422
[TRAIN] Iter: 314500 Loss: 0.02981218509376049  PSNR: 18.45505714416504
[TRAIN] Iter: 314600 Loss: 0.032307304441928864  PSNR: 18.108802795410156
[TRAIN] Iter: 314700 Loss: 0.024383705109357834  PSNR: 19.380897521972656
[TRAIN] Iter: 314800 Loss: 0.0326288640499115  PSNR: 18.051931381225586
[TRAIN] Iter: 314900 Loss: 0.027976615354418755  PSNR: 18.50957679748535
[TRAIN] Iter: 315000 Loss: 0.02558296173810959  PSNR: 19.212020874023438
[TRAIN] Iter: 315100 Loss: 0.02765028364956379  PSNR: 18.822744369506836
[TRAIN] Iter: 315200 Loss: 0.022710200399160385  PSNR: 19.52312660217285
[TRAIN] Iter: 315300 Loss: 0.029711563140153885  PSNR: 18.487504959106445
[TRAIN] Iter: 315400 Loss: 0.02536017820239067  PSNR: 19.121810913085938
[TRAIN] Iter: 315500 Loss: 0.022176899015903473  PSNR: 19.631860733032227
[TRAIN] Iter: 315600 Loss: 0.022713756188750267  PSNR: 19.671924591064453
[TRAIN] Iter: 315700 Loss: 0.025385800749063492  PSNR: 19.094022750854492
[TRAIN] Iter: 315800 Loss: 0.027550972998142242  PSNR: 18.756694793701172
[TRAIN] Iter: 315900 Loss: 0.02602440118789673  PSNR: 19.15219497680664
[TRAIN] Iter: 316000 Loss: 0.030390555039048195  PSNR: 18.820743560791016
[TRAIN] Iter: 316100 Loss: 0.025105448439717293  PSNR: 19.07259750366211
[TRAIN] Iter: 316200 Loss: 0.02689705416560173  PSNR: 18.913288116455078
[TRAIN] Iter: 316300 Loss: 0.02728354185819626  PSNR: 18.849458694458008
[TRAIN] Iter: 316400 Loss: 0.02716575190424919  PSNR: 18.88313102722168
[TRAIN] Iter: 316500 Loss: 0.02656613662838936  PSNR: 19.035852432250977
[TRAIN] Iter: 316600 Loss: 0.024737901985645294  PSNR: 19.141813278198242
[TRAIN] Iter: 316700 Loss: 0.02601715177297592  PSNR: 18.881080627441406
[TRAIN] Iter: 316800 Loss: 0.024127289652824402  PSNR: 19.408733367919922
[TRAIN] Iter: 316900 Loss: 0.021274929866194725  PSNR: 19.942325592041016
[TRAIN] Iter: 317000 Loss: 0.032646551728248596  PSNR: 17.986255645751953
[TRAIN] Iter: 317100 Loss: 0.02708025835454464  PSNR: 18.969261169433594
[TRAIN] Iter: 317200 Loss: 0.02605295553803444  PSNR: 18.837507247924805
[TRAIN] Iter: 317300 Loss: 0.02506493590772152  PSNR: 19.420011520385742
[TRAIN] Iter: 317400 Loss: 0.028036314994096756  PSNR: 18.7260684967041
[TRAIN] Iter: 317500 Loss: 0.02658691629767418  PSNR: 18.92700958251953
[TRAIN] Iter: 317600 Loss: 0.03324494510889053  PSNR: 17.956472396850586
[TRAIN] Iter: 317700 Loss: 0.022012975066900253  PSNR: 19.94734764099121
[TRAIN] Iter: 317800 Loss: 0.024918600916862488  PSNR: 19.098527908325195
[TRAIN] Iter: 317900 Loss: 0.030325021594762802  PSNR: 18.29606819152832
[TRAIN] Iter: 318000 Loss: 0.030267568305134773  PSNR: 18.410009384155273
[TRAIN] Iter: 318100 Loss: 0.030274275690317154  PSNR: 18.333478927612305
[TRAIN] Iter: 318200 Loss: 0.02274775505065918  PSNR: 19.469396591186523
[TRAIN] Iter: 318300 Loss: 0.030260726809501648  PSNR: 18.422605514526367
[TRAIN] Iter: 318400 Loss: 0.02589438483119011  PSNR: 19.22976303100586
[TRAIN] Iter: 318500 Loss: 0.029081963002681732  PSNR: 18.97644805908203
[TRAIN] Iter: 318600 Loss: 0.0304301455616951  PSNR: 18.57476043701172
[TRAIN] Iter: 318700 Loss: 0.024886351078748703  PSNR: 19.442752838134766
[TRAIN] Iter: 318800 Loss: 0.02686912566423416  PSNR: 18.969694137573242
[TRAIN] Iter: 318900 Loss: 0.02306951954960823  PSNR: 19.549346923828125
[TRAIN] Iter: 319000 Loss: 0.029160328209400177  PSNR: 18.259626388549805
[TRAIN] Iter: 319100 Loss: 0.02312925085425377  PSNR: 19.58257484436035
[TRAIN] Iter: 319200 Loss: 0.027510080486536026  PSNR: 18.928993225097656
[TRAIN] Iter: 319300 Loss: 0.026188712567090988  PSNR: 19.13407325744629
[TRAIN] Iter: 319400 Loss: 0.02378755807876587  PSNR: 19.465307235717773
[TRAIN] Iter: 319500 Loss: 0.03573870658874512  PSNR: 17.65885353088379
[TRAIN] Iter: 319600 Loss: 0.031163230538368225  PSNR: 18.31884765625
[TRAIN] Iter: 319700 Loss: 0.024808650836348534  PSNR: 19.264745712280273
[TRAIN] Iter: 319800 Loss: 0.0297414418309927  PSNR: 18.563295364379883
[TRAIN] Iter: 319900 Loss: 0.02225673943758011  PSNR: 19.659025192260742
Saved checkpoints at ./logs/TUT-LAB-nerf/320000.tar
[TRAIN] Iter: 320000 Loss: 0.025588996708393097  PSNR: 18.97102928161621
[TRAIN] Iter: 320100 Loss: 0.02764653041958809  PSNR: 18.57737159729004
[TRAIN] Iter: 320200 Loss: 0.027484890073537827  PSNR: 18.773937225341797
[TRAIN] Iter: 320300 Loss: 0.02704836055636406  PSNR: 18.941146850585938
[TRAIN] Iter: 320400 Loss: 0.02220919355750084  PSNR: 19.61825180053711
[TRAIN] Iter: 320500 Loss: 0.026457346975803375  PSNR: 18.875028610229492
[TRAIN] Iter: 320600 Loss: 0.02687881886959076  PSNR: 19.071712493896484
[TRAIN] Iter: 320700 Loss: 0.030764063820242882  PSNR: 18.225921630859375
[TRAIN] Iter: 320800 Loss: 0.034410130232572556  PSNR: 17.826011657714844
[TRAIN] Iter: 320900 Loss: 0.026578180491924286  PSNR: 18.791109085083008
[TRAIN] Iter: 321000 Loss: 0.0224065613001585  PSNR: 19.625328063964844
[TRAIN] Iter: 321100 Loss: 0.02970896288752556  PSNR: 18.522153854370117
[TRAIN] Iter: 321200 Loss: 0.0315849632024765  PSNR: 18.30694007873535
[TRAIN] Iter: 321300 Loss: 0.02430053986608982  PSNR: 19.1241397857666
[TRAIN] Iter: 321400 Loss: 0.02618013508617878  PSNR: 18.68450927734375
[TRAIN] Iter: 321500 Loss: 0.030539344996213913  PSNR: 18.57847023010254
[TRAIN] Iter: 321600 Loss: 0.02559330314397812  PSNR: 19.20534324645996
[TRAIN] Iter: 321700 Loss: 0.026095377281308174  PSNR: 19.150588989257812
[TRAIN] Iter: 321800 Loss: 0.03137298673391342  PSNR: 18.08012580871582
[TRAIN] Iter: 321900 Loss: 0.03281792625784874  PSNR: 17.980504989624023
[TRAIN] Iter: 322000 Loss: 0.033399585634469986  PSNR: 17.884124755859375
[TRAIN] Iter: 322100 Loss: 0.022814273834228516  PSNR: 19.79549217224121
[TRAIN] Iter: 322200 Loss: 0.03185657411813736  PSNR: 18.273406982421875
[TRAIN] Iter: 322300 Loss: 0.02742800861597061  PSNR: 19.088891983032227
[TRAIN] Iter: 322400 Loss: 0.033515602350234985  PSNR: 17.934446334838867
[TRAIN] Iter: 322500 Loss: 0.02932184934616089  PSNR: 18.53911018371582
[TRAIN] Iter: 322600 Loss: 0.03262198716402054  PSNR: 18.21497344970703
[TRAIN] Iter: 322700 Loss: 0.026136381551623344  PSNR: 18.738515853881836
[TRAIN] Iter: 322800 Loss: 0.027154941111803055  PSNR: 18.750179290771484
[TRAIN] Iter: 322900 Loss: 0.029942210763692856  PSNR: 18.67734718322754
[TRAIN] Iter: 323000 Loss: 0.026066934689879417  PSNR: 19.01363754272461
[TRAIN] Iter: 323100 Loss: 0.025948256254196167  PSNR: 19.259035110473633
[TRAIN] Iter: 323200 Loss: 0.028067149221897125  PSNR: 18.55476951599121
[TRAIN] Iter: 323300 Loss: 0.029426751658320427  PSNR: 18.43315315246582
[TRAIN] Iter: 323400 Loss: 0.029371477663517  PSNR: 18.526689529418945
[TRAIN] Iter: 323500 Loss: 0.026943601667881012  PSNR: 18.891193389892578
[TRAIN] Iter: 323600 Loss: 0.02411689981818199  PSNR: 19.279943466186523
[TRAIN] Iter: 323700 Loss: 0.026433628052473068  PSNR: 19.23183250427246
[TRAIN] Iter: 323800 Loss: 0.032714806497097015  PSNR: 17.995336532592773
[TRAIN] Iter: 323900 Loss: 0.02929103747010231  PSNR: 18.374778747558594
[TRAIN] Iter: 324000 Loss: 0.03216296434402466  PSNR: 17.984943389892578
[TRAIN] Iter: 324100 Loss: 0.029572967439889908  PSNR: 18.484939575195312
[TRAIN] Iter: 324200 Loss: 0.03258165717124939  PSNR: 18.052783966064453
[TRAIN] Iter: 324300 Loss: 0.02574896812438965  PSNR: 19.329530715942383
[TRAIN] Iter: 324400 Loss: 0.022953234612941742  PSNR: 19.55803680419922
[TRAIN] Iter: 324500 Loss: 0.027184123173356056  PSNR: 18.596439361572266
[TRAIN] Iter: 324600 Loss: 0.02208239957690239  PSNR: 19.652984619140625
[TRAIN] Iter: 324700 Loss: 0.02501075528562069  PSNR: 19.313913345336914
[TRAIN] Iter: 324800 Loss: 0.02512080781161785  PSNR: 19.248361587524414
[TRAIN] Iter: 324900 Loss: 0.023133188486099243  PSNR: 19.433238983154297
[TRAIN] Iter: 325000 Loss: 0.026675695553421974  PSNR: 18.839324951171875
[TRAIN] Iter: 325100 Loss: 0.024825837463140488  PSNR: 18.958162307739258
[TRAIN] Iter: 325200 Loss: 0.03468753397464752  PSNR: 17.833768844604492
[TRAIN] Iter: 325300 Loss: 0.02653193101286888  PSNR: 19.111379623413086
[TRAIN] Iter: 325400 Loss: 0.0285878274589777  PSNR: 18.61994743347168
[TRAIN] Iter: 325500 Loss: 0.030773820355534554  PSNR: 18.302520751953125
[TRAIN] Iter: 325600 Loss: 0.029125262051820755  PSNR: 18.68911361694336
[TRAIN] Iter: 325700 Loss: 0.02947867289185524  PSNR: 18.62993621826172
[TRAIN] Iter: 325800 Loss: 0.024653218686580658  PSNR: 18.663270950317383
[TRAIN] Iter: 325900 Loss: 0.026539873331785202  PSNR: 19.35474967956543
[TRAIN] Iter: 326000 Loss: 0.022005310282111168  PSNR: 19.5532283782959
[TRAIN] Iter: 326100 Loss: 0.033097729086875916  PSNR: 17.985994338989258
[TRAIN] Iter: 326200 Loss: 0.027035066857933998  PSNR: 18.684465408325195
[TRAIN] Iter: 326300 Loss: 0.02984676882624626  PSNR: 18.48492431640625
[TRAIN] Iter: 326400 Loss: 0.026382558047771454  PSNR: 19.039106369018555
[TRAIN] Iter: 326500 Loss: 0.032356247305870056  PSNR: 18.09037971496582
[TRAIN] Iter: 326600 Loss: 0.02393030934035778  PSNR: 19.22609519958496
[TRAIN] Iter: 326700 Loss: 0.02656906098127365  PSNR: 19.02374267578125
[TRAIN] Iter: 326800 Loss: 0.023136503994464874  PSNR: 19.249895095825195
[TRAIN] Iter: 326900 Loss: 0.034849874675273895  PSNR: 17.650571823120117
[TRAIN] Iter: 327000 Loss: 0.024353325366973877  PSNR: 19.494464874267578
[TRAIN] Iter: 327100 Loss: 0.026650413870811462  PSNR: 18.859695434570312
[TRAIN] Iter: 327200 Loss: 0.03268679231405258  PSNR: 18.11050033569336
[TRAIN] Iter: 327300 Loss: 0.026528727263212204  PSNR: 18.868986129760742
[TRAIN] Iter: 327400 Loss: 0.03215564414858818  PSNR: 18.098922729492188
[TRAIN] Iter: 327500 Loss: 0.03303491324186325  PSNR: 18.071958541870117
[TRAIN] Iter: 327600 Loss: 0.025107575580477715  PSNR: 19.29156494140625
[TRAIN] Iter: 327700 Loss: 0.02633492276072502  PSNR: 19.29591178894043
[TRAIN] Iter: 327800 Loss: 0.023589810356497765  PSNR: 19.37531280517578
[TRAIN] Iter: 327900 Loss: 0.03072332963347435  PSNR: 18.397781372070312
[TRAIN] Iter: 328000 Loss: 0.022245977073907852  PSNR: 19.501129150390625
[TRAIN] Iter: 328100 Loss: 0.02913694828748703  PSNR: 18.62262535095215
[TRAIN] Iter: 328200 Loss: 0.025489572435617447  PSNR: 19.199214935302734
[TRAIN] Iter: 328300 Loss: 0.03177720308303833  PSNR: 18.127994537353516
[TRAIN] Iter: 328400 Loss: 0.02974860370159149  PSNR: 18.51057243347168
[TRAIN] Iter: 328500 Loss: 0.028902923688292503  PSNR: 18.441869735717773
[TRAIN] Iter: 328600 Loss: 0.02718561887741089  PSNR: 19.38450813293457
[TRAIN] Iter: 328700 Loss: 0.026843084022402763  PSNR: 19.17842674255371
[TRAIN] Iter: 328800 Loss: 0.02502702921628952  PSNR: 19.18720054626465
[TRAIN] Iter: 328900 Loss: 0.02181920036673546  PSNR: 19.58840560913086
[TRAIN] Iter: 329000 Loss: 0.02435540035367012  PSNR: 19.195993423461914
[TRAIN] Iter: 329100 Loss: 0.0327630415558815  PSNR: 18.011491775512695
[TRAIN] Iter: 329200 Loss: 0.025859752669930458  PSNR: 19.055700302124023
[TRAIN] Iter: 329300 Loss: 0.028596892952919006  PSNR: 18.380067825317383
[TRAIN] Iter: 329400 Loss: 0.029114816337823868  PSNR: 18.595130920410156
[TRAIN] Iter: 329500 Loss: 0.02325601503252983  PSNR: 19.667043685913086
[TRAIN] Iter: 329600 Loss: 0.022664770483970642  PSNR: 19.644607543945312
[TRAIN] Iter: 329700 Loss: 0.028522390872240067  PSNR: 18.461984634399414
[TRAIN] Iter: 329800 Loss: 0.029006190598011017  PSNR: 18.502193450927734
[TRAIN] Iter: 329900 Loss: 0.02390180714428425  PSNR: 19.18607521057129
Saved checkpoints at ./logs/TUT-LAB-nerf/330000.tar
[TRAIN] Iter: 330000 Loss: 0.024592403322458267  PSNR: 19.344934463500977
[TRAIN] Iter: 330100 Loss: 0.03069164976477623  PSNR: 18.25857925415039
[TRAIN] Iter: 330200 Loss: 0.029576735571026802  PSNR: 18.523792266845703
[TRAIN] Iter: 330300 Loss: 0.024087443947792053  PSNR: 19.405189514160156
[TRAIN] Iter: 330400 Loss: 0.03179240971803665  PSNR: 18.036266326904297
[TRAIN] Iter: 330500 Loss: 0.0273633673787117  PSNR: 18.815595626831055
[TRAIN] Iter: 330600 Loss: 0.028554854914546013  PSNR: 18.701183319091797
[TRAIN] Iter: 330700 Loss: 0.02623094990849495  PSNR: 19.075428009033203
[TRAIN] Iter: 330800 Loss: 0.028002403676509857  PSNR: 18.8242244720459
[TRAIN] Iter: 330900 Loss: 0.027959510684013367  PSNR: 18.87213706970215
[TRAIN] Iter: 331000 Loss: 0.028700249269604683  PSNR: 18.55662727355957
[TRAIN] Iter: 331100 Loss: 0.026081211864948273  PSNR: 18.863250732421875
[TRAIN] Iter: 331200 Loss: 0.022660627961158752  PSNR: 19.638948440551758
[TRAIN] Iter: 331300 Loss: 0.018417485058307648  PSNR: 20.5839786529541
[TRAIN] Iter: 331400 Loss: 0.03135740011930466  PSNR: 18.25647735595703
[TRAIN] Iter: 331500 Loss: 0.02811342664062977  PSNR: 18.633358001708984
[TRAIN] Iter: 331600 Loss: 0.02055862732231617  PSNR: 20.252321243286133
[TRAIN] Iter: 331700 Loss: 0.025179803371429443  PSNR: 19.134071350097656
[TRAIN] Iter: 331800 Loss: 0.03212553635239601  PSNR: 18.1533145904541
[TRAIN] Iter: 331900 Loss: 0.029517725110054016  PSNR: 18.678874969482422
[TRAIN] Iter: 332000 Loss: 0.027083158493041992  PSNR: 19.009408950805664
[TRAIN] Iter: 332100 Loss: 0.030028996989130974  PSNR: 18.37114906311035
[TRAIN] Iter: 332200 Loss: 0.028024401515722275  PSNR: 18.763301849365234
[TRAIN] Iter: 332300 Loss: 0.028751730918884277  PSNR: 18.88443374633789
[TRAIN] Iter: 332400 Loss: 0.03451307862997055  PSNR: 17.807355880737305
[TRAIN] Iter: 332500 Loss: 0.02961871586740017  PSNR: 18.55265998840332
[TRAIN] Iter: 332600 Loss: 0.02749842032790184  PSNR: 18.63813591003418
[TRAIN] Iter: 332700 Loss: 0.024684347212314606  PSNR: 19.207727432250977
[TRAIN] Iter: 332800 Loss: 0.023777278140187263  PSNR: 19.454801559448242
[TRAIN] Iter: 332900 Loss: 0.0341503843665123  PSNR: 17.942428588867188
[TRAIN] Iter: 333000 Loss: 0.027876071631908417  PSNR: 18.733068466186523
[TRAIN] Iter: 333100 Loss: 0.028324875980615616  PSNR: 18.541122436523438
[TRAIN] Iter: 333200 Loss: 0.02782154083251953  PSNR: 18.750717163085938
[TRAIN] Iter: 333300 Loss: 0.02196878008544445  PSNR: 19.78432846069336
[TRAIN] Iter: 333400 Loss: 0.025561006739735603  PSNR: 19.27254295349121
[TRAIN] Iter: 333500 Loss: 0.024954533204436302  PSNR: 19.261770248413086
[TRAIN] Iter: 333600 Loss: 0.029191218316555023  PSNR: 18.69340705871582
[TRAIN] Iter: 333700 Loss: 0.031362924724817276  PSNR: 18.155546188354492
[TRAIN] Iter: 333800 Loss: 0.02754884958267212  PSNR: 18.7537841796875
[TRAIN] Iter: 333900 Loss: 0.023852284997701645  PSNR: 19.48659324645996
[TRAIN] Iter: 334000 Loss: 0.02459373138844967  PSNR: 19.204294204711914
[TRAIN] Iter: 334100 Loss: 0.027784151956439018  PSNR: 18.495502471923828
[TRAIN] Iter: 334200 Loss: 0.029901644214987755  PSNR: 18.409557342529297
[TRAIN] Iter: 334300 Loss: 0.028455298393964767  PSNR: 19.038925170898438
[TRAIN] Iter: 334400 Loss: 0.02187209576368332  PSNR: 19.83913803100586
[TRAIN] Iter: 334500 Loss: 0.023177964612841606  PSNR: 19.42502212524414
[TRAIN] Iter: 334600 Loss: 0.020343687385320663  PSNR: 20.18634605407715
[TRAIN] Iter: 334700 Loss: 0.021911481395363808  PSNR: 19.709035873413086
[TRAIN] Iter: 334800 Loss: 0.021682534366846085  PSNR: 19.962724685668945
[TRAIN] Iter: 334900 Loss: 0.02865901216864586  PSNR: 18.76886749267578
[TRAIN] Iter: 335000 Loss: 0.03535846620798111  PSNR: 17.675207138061523
[TRAIN] Iter: 335100 Loss: 0.027201823890209198  PSNR: 18.91091537475586
[TRAIN] Iter: 335200 Loss: 0.032536622136831284  PSNR: 17.9397029876709
[TRAIN] Iter: 335300 Loss: 0.03108731471002102  PSNR: 18.524436950683594
[TRAIN] Iter: 335400 Loss: 0.02298177033662796  PSNR: 19.425792694091797
[TRAIN] Iter: 335500 Loss: 0.030945448204874992  PSNR: 18.402212142944336
[TRAIN] Iter: 335600 Loss: 0.02213471010327339  PSNR: 19.502506256103516
[TRAIN] Iter: 335700 Loss: 0.02162519097328186  PSNR: 19.41254997253418
[TRAIN] Iter: 335800 Loss: 0.030891848728060722  PSNR: 18.30582046508789
[TRAIN] Iter: 335900 Loss: 0.02650677040219307  PSNR: 18.879838943481445
[TRAIN] Iter: 336000 Loss: 0.026547320187091827  PSNR: 18.84165382385254
[TRAIN] Iter: 336100 Loss: 0.028688011690974236  PSNR: 18.617258071899414
[TRAIN] Iter: 336200 Loss: 0.02817792072892189  PSNR: 18.759511947631836
[TRAIN] Iter: 336300 Loss: 0.02525818720459938  PSNR: 19.38150978088379
[TRAIN] Iter: 336400 Loss: 0.025450292974710464  PSNR: 19.331998825073242
[TRAIN] Iter: 336500 Loss: 0.030275117605924606  PSNR: 18.4827938079834
[TRAIN] Iter: 336600 Loss: 0.028474491089582443  PSNR: 18.695110321044922
[TRAIN] Iter: 336700 Loss: 0.027666479349136353  PSNR: 18.641965866088867
[TRAIN] Iter: 336800 Loss: 0.020676836371421814  PSNR: 20.01541519165039
[TRAIN] Iter: 336900 Loss: 0.02891979180276394  PSNR: 18.57612419128418
[TRAIN] Iter: 337000 Loss: 0.026645652949810028  PSNR: 19.058977127075195
[TRAIN] Iter: 337100 Loss: 0.026046639308333397  PSNR: 19.145387649536133
[TRAIN] Iter: 337200 Loss: 0.02774060145020485  PSNR: 18.75143814086914
[TRAIN] Iter: 337300 Loss: 0.02229585498571396  PSNR: 19.82892417907715
[TRAIN] Iter: 337400 Loss: 0.020094238221645355  PSNR: 20.25518798828125
[TRAIN] Iter: 337500 Loss: 0.022247470915317535  PSNR: 19.685380935668945
[TRAIN] Iter: 337600 Loss: 0.032180048525333405  PSNR: 18.213441848754883
[TRAIN] Iter: 337700 Loss: 0.029129380360245705  PSNR: 18.47665023803711
[TRAIN] Iter: 337800 Loss: 0.0341765433549881  PSNR: 17.8219051361084
[TRAIN] Iter: 337900 Loss: 0.02785416878759861  PSNR: 18.686107635498047
[TRAIN] Iter: 338000 Loss: 0.027695944532752037  PSNR: 18.975399017333984
[TRAIN] Iter: 338100 Loss: 0.027457071468234062  PSNR: 18.589588165283203
[TRAIN] Iter: 338200 Loss: 0.02929288148880005  PSNR: 18.42173957824707
[TRAIN] Iter: 338300 Loss: 0.027804432436823845  PSNR: 18.896907806396484
[TRAIN] Iter: 338400 Loss: 0.02862492762506008  PSNR: 18.600759506225586
[TRAIN] Iter: 338500 Loss: 0.026552967727184296  PSNR: 19.28525733947754
[TRAIN] Iter: 338600 Loss: 0.031149115413427353  PSNR: 18.196088790893555
[TRAIN] Iter: 338700 Loss: 0.028437316417694092  PSNR: 18.622852325439453
[TRAIN] Iter: 338800 Loss: 0.02495955117046833  PSNR: 19.41303253173828
[TRAIN] Iter: 338900 Loss: 0.025571802631020546  PSNR: 19.359619140625
[TRAIN] Iter: 339000 Loss: 0.0291215181350708  PSNR: 18.63713264465332
[TRAIN] Iter: 339100 Loss: 0.025392955169081688  PSNR: 19.25991439819336
[TRAIN] Iter: 339200 Loss: 0.025176692754030228  PSNR: 18.937467575073242
[TRAIN] Iter: 339300 Loss: 0.033484891057014465  PSNR: 17.7979679107666
[TRAIN] Iter: 339400 Loss: 0.027867790311574936  PSNR: 19.134010314941406
[TRAIN] Iter: 339500 Loss: 0.026538828387856483  PSNR: 19.110889434814453
[TRAIN] Iter: 339600 Loss: 0.02229977771639824  PSNR: 19.71076202392578
[TRAIN] Iter: 339700 Loss: 0.022701291367411613  PSNR: 19.494094848632812
[TRAIN] Iter: 339800 Loss: 0.02421727031469345  PSNR: 19.508092880249023
[TRAIN] Iter: 339900 Loss: 0.02906607836484909  PSNR: 18.581003189086914
Saved checkpoints at ./logs/TUT-LAB-nerf/340000.tar
[TRAIN] Iter: 340000 Loss: 0.03148039057850838  PSNR: 18.232635498046875
[TRAIN] Iter: 340100 Loss: 0.02554262802004814  PSNR: 19.182851791381836
[TRAIN] Iter: 340200 Loss: 0.023264730349183083  PSNR: 19.619518280029297
[TRAIN] Iter: 340300 Loss: 0.026565590873360634  PSNR: 19.206275939941406
[TRAIN] Iter: 340400 Loss: 0.03004644252359867  PSNR: 18.41403579711914
[TRAIN] Iter: 340500 Loss: 0.024884019047021866  PSNR: 19.294435501098633
[TRAIN] Iter: 340600 Loss: 0.026218954473733902  PSNR: 19.03414535522461
[TRAIN] Iter: 340700 Loss: 0.033802710473537445  PSNR: 17.77692222595215
[TRAIN] Iter: 340800 Loss: 0.03101430833339691  PSNR: 18.370906829833984
[TRAIN] Iter: 340900 Loss: 0.0268169566988945  PSNR: 18.712902069091797
[TRAIN] Iter: 341000 Loss: 0.02194126695394516  PSNR: 19.809667587280273
[TRAIN] Iter: 341100 Loss: 0.02228385955095291  PSNR: 19.736827850341797
[TRAIN] Iter: 341200 Loss: 0.032104380428791046  PSNR: 18.177841186523438
[TRAIN] Iter: 341300 Loss: 0.03405167534947395  PSNR: 17.91180992126465
[TRAIN] Iter: 341400 Loss: 0.029834505170583725  PSNR: 18.412952423095703
[TRAIN] Iter: 341500 Loss: 0.027609560638666153  PSNR: 18.84029769897461
[TRAIN] Iter: 341600 Loss: 0.02685312181711197  PSNR: 18.82260513305664
[TRAIN] Iter: 341700 Loss: 0.03388310223817825  PSNR: 17.83978271484375
[TRAIN] Iter: 341800 Loss: 0.02611294761300087  PSNR: 19.140592575073242
[TRAIN] Iter: 341900 Loss: 0.026590954512357712  PSNR: 18.800495147705078
[TRAIN] Iter: 342000 Loss: 0.029713224619627  PSNR: 18.512144088745117
[TRAIN] Iter: 342100 Loss: 0.03158385679125786  PSNR: 18.193429946899414
[TRAIN] Iter: 342200 Loss: 0.029062069952487946  PSNR: 18.576202392578125
[TRAIN] Iter: 342300 Loss: 0.02667245641350746  PSNR: 19.05229377746582
[TRAIN] Iter: 342400 Loss: 0.02809099480509758  PSNR: 18.57999610900879
[TRAIN] Iter: 342500 Loss: 0.030877456068992615  PSNR: 18.264341354370117
[TRAIN] Iter: 342600 Loss: 0.03350740671157837  PSNR: 17.828861236572266
[TRAIN] Iter: 342700 Loss: 0.027535894885659218  PSNR: 18.676246643066406
[TRAIN] Iter: 342800 Loss: 0.027206873521208763  PSNR: 18.97350311279297
[TRAIN] Iter: 342900 Loss: 0.03424696624279022  PSNR: 17.8145751953125
[TRAIN] Iter: 343000 Loss: 0.026414288207888603  PSNR: 18.995092391967773
[TRAIN] Iter: 343100 Loss: 0.02974247932434082  PSNR: 18.651416778564453
[TRAIN] Iter: 343200 Loss: 0.03374525532126427  PSNR: 18.11073112487793
[TRAIN] Iter: 343300 Loss: 0.025636611506342888  PSNR: 19.25753402709961
[TRAIN] Iter: 343400 Loss: 0.029276030138134956  PSNR: 18.666215896606445
[TRAIN] Iter: 343500 Loss: 0.025350909680128098  PSNR: 19.514774322509766
[TRAIN] Iter: 343600 Loss: 0.032728709280490875  PSNR: 17.922697067260742
[TRAIN] Iter: 343700 Loss: 0.026812108233571053  PSNR: 18.74576187133789
[TRAIN] Iter: 343800 Loss: 0.029434755444526672  PSNR: 18.58448600769043
[TRAIN] Iter: 343900 Loss: 0.023506779223680496  PSNR: 19.50303077697754
[TRAIN] Iter: 344000 Loss: 0.03061564266681671  PSNR: 18.339937210083008
[TRAIN] Iter: 344100 Loss: 0.02935829386115074  PSNR: 18.59750747680664
[TRAIN] Iter: 344200 Loss: 0.025486059486865997  PSNR: 18.709346771240234
[TRAIN] Iter: 344300 Loss: 0.02502499148249626  PSNR: 19.56418800354004
[TRAIN] Iter: 344400 Loss: 0.03137103468179703  PSNR: 18.237537384033203
[TRAIN] Iter: 344500 Loss: 0.026963606476783752  PSNR: 19.004274368286133
[TRAIN] Iter: 344600 Loss: 0.024045277386903763  PSNR: 19.361114501953125
[TRAIN] Iter: 344700 Loss: 0.028992027044296265  PSNR: 18.459619522094727
[TRAIN] Iter: 344800 Loss: 0.02787311002612114  PSNR: 18.5927677154541
[TRAIN] Iter: 344900 Loss: 0.025292085483670235  PSNR: 19.131967544555664
[TRAIN] Iter: 345000 Loss: 0.02788248471915722  PSNR: 18.624176025390625
[TRAIN] Iter: 345100 Loss: 0.029572978615760803  PSNR: 18.478057861328125
[TRAIN] Iter: 345200 Loss: 0.02935834228992462  PSNR: 18.72486114501953
[TRAIN] Iter: 345300 Loss: 0.02429313026368618  PSNR: 19.0999698638916
[TRAIN] Iter: 345400 Loss: 0.023455940186977386  PSNR: 19.080318450927734
[TRAIN] Iter: 345500 Loss: 0.025483524426817894  PSNR: 19.216724395751953
[TRAIN] Iter: 345600 Loss: 0.022271614521741867  PSNR: 19.48006820678711
[TRAIN] Iter: 345700 Loss: 0.03295360878109932  PSNR: 17.961074829101562
[TRAIN] Iter: 345800 Loss: 0.03149193525314331  PSNR: 18.181045532226562
[TRAIN] Iter: 345900 Loss: 0.02764076553285122  PSNR: 18.754968643188477
[TRAIN] Iter: 346000 Loss: 0.02063518390059471  PSNR: 20.009052276611328
[TRAIN] Iter: 346100 Loss: 0.03313338756561279  PSNR: 18.05220603942871
[TRAIN] Iter: 346200 Loss: 0.024955589324235916  PSNR: 19.16147804260254
[TRAIN] Iter: 346300 Loss: 0.026463350281119347  PSNR: 18.94688606262207
[TRAIN] Iter: 346400 Loss: 0.024997171014547348  PSNR: 19.002174377441406
[TRAIN] Iter: 346500 Loss: 0.028280086815357208  PSNR: 18.690988540649414
[TRAIN] Iter: 346600 Loss: 0.020183615386486053  PSNR: 20.11235809326172
[TRAIN] Iter: 346700 Loss: 0.029402203857898712  PSNR: 18.6412410736084
[TRAIN] Iter: 346800 Loss: 0.021237917244434357  PSNR: 19.88155746459961
[TRAIN] Iter: 346900 Loss: 0.02742682211101055  PSNR: 18.73783302307129
[TRAIN] Iter: 347000 Loss: 0.03122115507721901  PSNR: 18.250221252441406
[TRAIN] Iter: 347100 Loss: 0.019990064203739166  PSNR: 20.280298233032227
[TRAIN] Iter: 347200 Loss: 0.02930164337158203  PSNR: 18.560012817382812
[TRAIN] Iter: 347300 Loss: 0.024717822670936584  PSNR: 19.15981101989746
[TRAIN] Iter: 347400 Loss: 0.024365250021219254  PSNR: 19.428524017333984
[TRAIN] Iter: 347500 Loss: 0.026926254853606224  PSNR: 18.958330154418945
[TRAIN] Iter: 347600 Loss: 0.0312827005982399  PSNR: 18.059703826904297
[TRAIN] Iter: 347700 Loss: 0.023891441524028778  PSNR: 19.564088821411133
[TRAIN] Iter: 347800 Loss: 0.02792387828230858  PSNR: 18.671552658081055
[TRAIN] Iter: 347900 Loss: 0.028807364404201508  PSNR: 18.502422332763672
[TRAIN] Iter: 348000 Loss: 0.03119218908250332  PSNR: 18.175077438354492
[TRAIN] Iter: 348100 Loss: 0.02925458550453186  PSNR: 18.51034164428711
[TRAIN] Iter: 348200 Loss: 0.027961408719420433  PSNR: 18.519582748413086
[TRAIN] Iter: 348300 Loss: 0.0294463150203228  PSNR: 18.452909469604492
[TRAIN] Iter: 348400 Loss: 0.026693349704146385  PSNR: 19.12761688232422
[TRAIN] Iter: 348500 Loss: 0.021677203476428986  PSNR: 19.667287826538086
[TRAIN] Iter: 348600 Loss: 0.03307855874300003  PSNR: 17.941205978393555
[TRAIN] Iter: 348700 Loss: 0.025301232933998108  PSNR: 18.790647506713867
[TRAIN] Iter: 348800 Loss: 0.028438366949558258  PSNR: 18.74641990661621
[TRAIN] Iter: 348900 Loss: 0.023071035742759705  PSNR: 19.32433319091797
[TRAIN] Iter: 349000 Loss: 0.031088149175047874  PSNR: 18.20068359375
[TRAIN] Iter: 349100 Loss: 0.029802940785884857  PSNR: 18.346717834472656
[TRAIN] Iter: 349200 Loss: 0.023560939356684685  PSNR: 19.384618759155273
[TRAIN] Iter: 349300 Loss: 0.02856312319636345  PSNR: 18.539209365844727
[TRAIN] Iter: 349400 Loss: 0.023089252412319183  PSNR: 19.874797821044922
[TRAIN] Iter: 349500 Loss: 0.03208822011947632  PSNR: 18.046720504760742
[TRAIN] Iter: 349600 Loss: 0.022534381598234177  PSNR: 19.673912048339844
[TRAIN] Iter: 349700 Loss: 0.025509372353553772  PSNR: 19.19772720336914
[TRAIN] Iter: 349800 Loss: 0.021952688694000244  PSNR: 19.76067352294922
[TRAIN] Iter: 349900 Loss: 0.028509080410003662  PSNR: 18.632049560546875
Saved checkpoints at ./logs/TUT-LAB-nerf/350000.tar
0 0.00036525726318359375
torch.Size([320, 640, 3]) torch.Size([320, 640])
1 14.300186157226562
2 14.780377626419067
3 18.198988437652588
4 14.866974830627441
5 14.787487983703613
6 18.447798490524292
7 14.856034278869629
8 18.27776551246643
9 14.879668235778809
10 18.2158784866333
11 14.904568195343018
12 18.127716779708862
13 14.914170026779175
14 18.190304279327393
15 15.002878904342651
16 18.08484196662903
17 14.823843955993652
18 18.216337203979492
19 15.085972309112549
20 14.807370662689209
21 18.072089672088623
22 14.84304141998291
23 18.48043131828308
24 14.799447536468506
25 18.054624795913696
26 14.818016052246094
27 18.46386218070984
28 14.96908187866211
29 18.10351252555847
30 14.718718528747559
31 14.784870862960815
32 18.58000612258911
33 15.083679914474487
34 18.6417236328125
35 14.794755220413208
36 17.64549970626831
37 14.86083197593689
38 18.235515832901
39 14.832278966903687
40 18.32781147956848
41 14.819408416748047
42 18.279032230377197
43 14.830372095108032
44 18.03187394142151
45 15.257209777832031
46 17.69222855567932
47 15.275872230529785
48 15.121345281600952
49 17.92536187171936
50 15.053882598876953
51 18.43522834777832
52 14.957089185714722
53 18.18052649497986
54 14.899601936340332
55 18.091012716293335
56 14.902909278869629
57 18.095513343811035
58 14.971994400024414
59 18.08881640434265
60 14.995314121246338
61 17.967642784118652
62 15.026224613189697
63 14.90650463104248
64 18.016443729400635
65 14.864678382873535
66 18.259974479675293
67 14.792173862457275
68 18.298782348632812
69 14.781422138214111
70 18.414671659469604
71 14.82766079902649
72 18.32228684425354
73 14.735190629959106
74 18.274773120880127
75 14.77313780784607
76 14.801289081573486
77 18.339752435684204
78 14.800005674362183
79 18.38614821434021
80 14.693950891494751
81 19.15626072883606
82 16.226287364959717
83 15.766942501068115
84 14.80404806137085
85 18.962714195251465
86 16.27816104888916
87 15.824972152709961
88 14.75846815109253
89 18.746147632598877
90 14.995532751083374
91 17.860999822616577
92 15.05993938446045
93 17.806504487991333
94 15.19722843170166
95 14.759880304336548
96 18.148216247558594
97 14.782427072525024
98 18.406243801116943
99 14.782850742340088
100 18.173075675964355
101 14.774802207946777
102 18.381327152252197
103 14.83053970336914
104 18.191652536392212
105 14.834844827651978
106 18.115909814834595
107 15.017249345779419
108 17.99138617515564
109 15.203869819641113
110 14.918468475341797
111 18.5080668926239
112 15.057506322860718
113 18.49982762336731
114 14.77130126953125
115 18.330774784088135
116 14.810685634613037
117 18.261457204818726
118 14.722160816192627
119 17.39610481262207
Done, saving (120, 320, 640, 3) (120, 320, 640)
extras:{'raw': tensor([[[ 2.4321e-01, -1.0452e-01, -2.7888e-01, -2.1660e+01],
         [ 2.7094e-01, -1.6391e-01, -5.6070e-01, -2.9247e+01],
         [ 5.8917e-01,  5.6211e-01,  6.2191e-01, -1.9762e+01],
         ...,
         [ 3.8791e+00,  6.2170e+00,  1.5159e+01,  2.7715e+02],
         [ 2.5870e+00,  5.2083e+00,  1.4363e+01,  2.9201e+02],
         [ 4.3848e+00,  6.6286e+00,  1.6053e+01,  2.6659e+02]],

        [[-1.5979e+00, -1.7947e+00, -1.9828e+00, -2.4634e+01],
         [-6.7506e-01, -8.7877e-01, -1.1068e+00, -1.6531e+01],
         [-7.2954e-01, -9.2977e-01, -1.1617e+00, -1.6190e+01],
         ...,
         [-2.7554e+00, -4.2559e+00, -6.2837e+00,  1.2679e+02],
         [-2.5029e+00, -4.1252e+00, -5.9116e+00,  1.5959e+02],
         [-2.5445e+00, -3.9115e+00, -5.3779e+00,  1.6495e+02]],

        [[-4.7953e-01, -5.3248e-01, -4.6898e-01, -9.9809e+00],
         [-3.1127e-01, -4.7323e-01, -7.9623e-01, -2.2664e+01],
         [-9.1607e-01, -9.8954e-01, -1.1730e+00, -1.6151e+01],
         ...,
         [-1.6220e+00, -2.4968e+00, -5.4421e+00,  5.6375e+02],
         [-1.6366e+00, -2.5093e+00, -5.4535e+00,  5.6381e+02],
         [-1.1373e+00, -2.1262e+00, -5.2255e+00,  5.5544e+02]],

        ...,

        [[-3.3733e-01, -3.9304e-01, -9.0927e-02, -1.2960e+01],
         [-5.3110e-01, -5.8988e-01, -3.2615e-01, -2.2631e+01],
         [-4.8622e-01, -5.4805e-01, -2.7599e-01, -2.1790e+01],
         ...,
         [-3.4119e+00, -2.3995e+00,  8.2135e-01, -4.3020e+01],
         [-3.4848e+00, -2.4904e+00,  3.6446e-01, -4.5265e+01],
         [-3.0501e+00, -2.1106e+00,  5.3373e-01, -3.8260e+01]],

        [[-1.6470e+00, -1.3008e+00, -5.3559e-01, -2.2685e+01],
         [-1.0649e+00, -1.0782e+00, -5.5139e-01, -3.7865e+01],
         [ 4.2096e-01,  5.6975e-01,  9.8285e-01, -1.8361e+01],
         ...,
         [-8.4296e+00, -6.6160e+00, -4.1659e+00, -8.1050e+00],
         [-8.9135e+00, -7.0786e+00, -4.7529e+00,  3.1396e+00],
         [-8.0573e+00, -6.1965e+00, -3.5516e+00, -5.3028e-01]],

        [[-9.2308e-01, -1.1173e+00, -1.1368e+00, -2.8117e+01],
         [ 1.4889e-02, -8.8279e-02, -3.0523e-01, -6.3635e+00],
         [-7.6737e-03, -6.4400e-02, -9.3041e-02, -1.4044e+01],
         ...,
         [-6.3249e+00, -1.0363e+01, -1.6178e+01,  3.3150e+02],
         [-7.3266e+00, -1.0817e+01, -1.6096e+01,  3.3354e+02],
         [-6.1999e+00, -9.7623e+00, -1.5164e+01,  3.2127e+02]]],
       grad_fn=<ReshapeAliasBackward0>), 'rgb0': tensor([[0.7340, 0.6897, 0.6804],
        [0.1389, 0.1350, 0.1059],
        [0.4677, 0.4591, 0.4739],
        ...,
        [0.3705, 0.3710, 0.4205],
        [0.6216, 0.5945, 0.6054],
        [0.5612, 0.5342, 0.5148]], grad_fn=<ReshapeAliasBackward0>), 'disp0': tensor([ 41.5143, 106.5767, 500.3220,  ..., 102.7139,  11.0658,  39.9925],
       grad_fn=<ReshapeAliasBackward0>), 'acc0': tensor([1., 1., 1.,  ..., 1., 1., 1.], grad_fn=<ReshapeAliasBackward0>), 'z_std': tensor([0.0022, 0.0021, 0.2742,  ..., 0.0019, 0.0023, 0.0024])}
0 0.0004329681396484375
torch.Size([320, 640, 3]) torch.Size([320, 640])
1 17.62946581840515
2 15.0736083984375
3 17.723783493041992
4 13.410951614379883
5 13.201010704040527
6 16.25167202949524
7 13.233789205551147
8 16.22355341911316
9 13.359167098999023
10 13.264878749847412
11 16.288835763931274
12 13.174188137054443
13 16.22484517097473
14 13.31042218208313
15 13.210880041122437
16 16.318581581115723
17 13.21022081375122
18 16.270387172698975
19 13.191949129104614
20 16.167184352874756
21 13.412398099899292
22 13.201648950576782
23 16.308594942092896
24 13.220360517501831
25 16.197341442108154
26 13.325145244598389
27 13.2091805934906
28 16.357078075408936
29 13.143470764160156
30 16.19389510154724
31 13.231082677841187
32 13.207237958908081
33 16.30555009841919
34 13.046353340148926
35 16.261887073516846
36 13.031039953231812
37 13.03205394744873
38 16.857133388519287
39 13.030827522277832
40 16.422627210617065
41 13.247812032699585
42 16.326157569885254
43 13.185401201248169
44 13.059215545654297
45 16.490257501602173
46 13.16971755027771
47 16.26374840736389
48 13.058427572250366
49 13.17232871055603
50 16.359630584716797
51 13.087878465652466
52 16.470383882522583
53 13.174030065536499
54 13.015506505966187
55 16.521815538406372
56 13.071261644363403
57 16.48712158203125
58 13.113736867904663
59 13.013244867324829
60 16.57653498649597
61 13.172900915145874
62 16.342114210128784
63 13.027651071548462
64 13.000440835952759
65 16.717191457748413
66 13.206519842147827
67 16.348806381225586
68 13.244035959243774
69 13.254054307937622
70 16.3242244720459
71 13.16087818145752
72 16.257827520370483
73 13.26055097579956
74 16.30875325202942
75 13.24211573600769
76 13.205376386642456
77 16.247735023498535
78 13.221479892730713
79 16.258492469787598
80 13.2378990650177
81 13.197683811187744
82 16.26541543006897
83 13.21876311302185
84 16.2665855884552
85 13.248295307159424
86 13.20937204360962
87 16.306291580200195
88 13.228558778762817
89 16.28493309020996
90 13.216776847839355
91 16.24908423423767
92 13.194428443908691
93 13.218024969100952
94 16.26761031150818
95 13.181828737258911
96 16.292770385742188
97 13.019089221954346
98 13.02418303489685
99 16.716548681259155
100 13.1347177028656
101 16.442450046539307
102 13.128263473510742
103 13.099972486495972
104 16.42867398262024
105 13.124382495880127
106 16.43073558807373
107 13.106317043304443
108 13.135970830917358
109 16.493751764297485
110 13.069836616516113
111 16.564554452896118
112 13.07498550415039
113 13.027462720870972
114 16.560399532318115
115 13.020050287246704
116 16.48592257499695
117 13.130311965942383
118 13.062989234924316
119 16.72967290878296
test poses shape torch.Size([13, 3, 4])
0 0.0006420612335205078
torch.Size([320, 640, 3]) torch.Size([320, 640])
1 16.48269295692444
2 13.210156679153442
3 16.36333727836609
4 13.33474612236023
5 13.088050127029419
6 16.4672749042511
7 13.118834972381592
8 16.492611169815063
9 13.277798175811768
10 13.069599151611328
11 16.38451075553894
12 13.25698471069336
Saved test set
[TRAIN] Iter: 350000 Loss: 0.034938935190439224  PSNR: 17.731081008911133
[TRAIN] Iter: 350100 Loss: 0.030727043747901917  PSNR: 18.338016510009766
[TRAIN] Iter: 350200 Loss: 0.031736165285110474  PSNR: 18.261537551879883
[TRAIN] Iter: 350300 Loss: 0.029076144099235535  PSNR: 18.499107360839844
[TRAIN] Iter: 350400 Loss: 0.030784741044044495  PSNR: 18.610261917114258
[TRAIN] Iter: 350500 Loss: 0.03487256169319153  PSNR: 17.751575469970703
[TRAIN] Iter: 350600 Loss: 0.027763953432440758  PSNR: 18.579925537109375
[TRAIN] Iter: 350700 Loss: 0.029365455731749535  PSNR: 18.54530143737793
[TRAIN] Iter: 350800 Loss: 0.025042079389095306  PSNR: 19.08098030090332
[TRAIN] Iter: 350900 Loss: 0.02956426702439785  PSNR: 18.286420822143555
[TRAIN] Iter: 351000 Loss: 0.028122231364250183  PSNR: 18.836238861083984
[TRAIN] Iter: 351100 Loss: 0.03514423221349716  PSNR: 17.671903610229492
[TRAIN] Iter: 351200 Loss: 0.031830236315727234  PSNR: 18.068113327026367
[TRAIN] Iter: 351300 Loss: 0.024783695116639137  PSNR: 19.234588623046875
[TRAIN] Iter: 351400 Loss: 0.027155961841344833  PSNR: 18.64652442932129
[TRAIN] Iter: 351500 Loss: 0.03119291365146637  PSNR: 18.219758987426758
[TRAIN] Iter: 351600 Loss: 0.030634205788373947  PSNR: 18.2620906829834
[TRAIN] Iter: 351700 Loss: 0.027242332696914673  PSNR: 18.856304168701172
[TRAIN] Iter: 351800 Loss: 0.03093918040394783  PSNR: 18.17341423034668
[TRAIN] Iter: 351900 Loss: 0.025549709796905518  PSNR: 18.744903564453125
[TRAIN] Iter: 352000 Loss: 0.022141333669424057  PSNR: 19.527023315429688
[TRAIN] Iter: 352100 Loss: 0.026208940893411636  PSNR: 19.066890716552734
[TRAIN] Iter: 352200 Loss: 0.031676389276981354  PSNR: 18.156055450439453
[TRAIN] Iter: 352300 Loss: 0.02307683415710926  PSNR: 19.65727424621582
[TRAIN] Iter: 352400 Loss: 0.021862143650650978  PSNR: 19.70134925842285
[TRAIN] Iter: 352500 Loss: 0.02304243855178356  PSNR: 19.52075958251953
[TRAIN] Iter: 352600 Loss: 0.025082886219024658  PSNR: 19.30144691467285
[TRAIN] Iter: 352700 Loss: 0.024337362498044968  PSNR: 19.284727096557617
[TRAIN] Iter: 352800 Loss: 0.0322398915886879  PSNR: 18.175270080566406
[TRAIN] Iter: 352900 Loss: 0.02278168499469757  PSNR: 19.5443115234375
[TRAIN] Iter: 353000 Loss: 0.026703476905822754  PSNR: 18.872175216674805
[TRAIN] Iter: 353100 Loss: 0.025374460965394974  PSNR: 18.726238250732422
[TRAIN] Iter: 353200 Loss: 0.024095740169286728  PSNR: 19.35947036743164
[TRAIN] Iter: 353300 Loss: 0.026878925040364265  PSNR: 18.765106201171875
[TRAIN] Iter: 353400 Loss: 0.023192860186100006  PSNR: 19.424087524414062
[TRAIN] Iter: 353500 Loss: 0.025050923228263855  PSNR: 19.111133575439453
[TRAIN] Iter: 353600 Loss: 0.03098032809793949  PSNR: 18.47779083251953
[TRAIN] Iter: 353700 Loss: 0.026181207969784737  PSNR: 18.92652130126953
[TRAIN] Iter: 353800 Loss: 0.026531953364610672  PSNR: 19.07792091369629
[TRAIN] Iter: 353900 Loss: 0.0372801274061203  PSNR: 17.53990936279297
[TRAIN] Iter: 354000 Loss: 0.02123797871172428  PSNR: 19.775066375732422
[TRAIN] Iter: 354100 Loss: 0.030054137110710144  PSNR: 18.327659606933594
[TRAIN] Iter: 354200 Loss: 0.029612064361572266  PSNR: 18.61665916442871
[TRAIN] Iter: 354300 Loss: 0.025695500895380974  PSNR: 19.09267807006836
[TRAIN] Iter: 354400 Loss: 0.03105325624346733  PSNR: 18.19167137145996
[TRAIN] Iter: 354500 Loss: 0.021645747125148773  PSNR: 20.102510452270508
[TRAIN] Iter: 354600 Loss: 0.02598918229341507  PSNR: 19.338314056396484
[TRAIN] Iter: 354700 Loss: 0.027208738029003143  PSNR: 18.93592071533203
[TRAIN] Iter: 354800 Loss: 0.027789179235696793  PSNR: 18.86575698852539
[TRAIN] Iter: 354900 Loss: 0.027443626895546913  PSNR: 18.69812774658203
[TRAIN] Iter: 355000 Loss: 0.021513309329748154  PSNR: 19.737560272216797
[TRAIN] Iter: 355100 Loss: 0.023144429549574852  PSNR: 19.450031280517578
[TRAIN] Iter: 355200 Loss: 0.02698608487844467  PSNR: 18.824613571166992
[TRAIN] Iter: 355300 Loss: 0.02760908380150795  PSNR: 18.56599998474121
[TRAIN] Iter: 355400 Loss: 0.021008778363466263  PSNR: 19.816024780273438
[TRAIN] Iter: 355500 Loss: 0.027906809002161026  PSNR: 18.796546936035156
[TRAIN] Iter: 355600 Loss: 0.022793985903263092  PSNR: 19.739534378051758
[TRAIN] Iter: 355700 Loss: 0.0310002863407135  PSNR: 18.283023834228516
[TRAIN] Iter: 355800 Loss: 0.031667374074459076  PSNR: 18.08038330078125
[TRAIN] Iter: 355900 Loss: 0.020631805062294006  PSNR: 20.083322525024414
[TRAIN] Iter: 356000 Loss: 0.03510122001171112  PSNR: 17.692216873168945
[TRAIN] Iter: 356100 Loss: 0.03573475033044815  PSNR: 17.651100158691406
[TRAIN] Iter: 356200 Loss: 0.030093073844909668  PSNR: 18.446990966796875
[TRAIN] Iter: 356300 Loss: 0.027366377413272858  PSNR: 19.09676170349121
[TRAIN] Iter: 356400 Loss: 0.019780484959483147  PSNR: 20.281360626220703
[TRAIN] Iter: 356500 Loss: 0.025431405752897263  PSNR: 19.298357009887695
[TRAIN] Iter: 356600 Loss: 0.03128904104232788  PSNR: 18.134307861328125
[TRAIN] Iter: 356700 Loss: 0.03158843144774437  PSNR: 18.302398681640625
[TRAIN] Iter: 356800 Loss: 0.029041405767202377  PSNR: 18.72616958618164
[TRAIN] Iter: 356900 Loss: 0.022874057292938232  PSNR: 19.707714080810547
[TRAIN] Iter: 357000 Loss: 0.019370727241039276  PSNR: 20.248855590820312
[TRAIN] Iter: 357100 Loss: 0.02184709906578064  PSNR: 19.73314094543457
[TRAIN] Iter: 357200 Loss: 0.033572565764188766  PSNR: 17.913143157958984
[TRAIN] Iter: 357300 Loss: 0.024409623816609383  PSNR: 19.25274085998535
[TRAIN] Iter: 357400 Loss: 0.02974715456366539  PSNR: 18.479429244995117
[TRAIN] Iter: 357500 Loss: 0.019538547843694687  PSNR: 20.29832649230957
[TRAIN] Iter: 357600 Loss: 0.028476359322667122  PSNR: 18.98430633544922
[TRAIN] Iter: 357700 Loss: 0.02993938885629177  PSNR: 18.51009178161621
[TRAIN] Iter: 357800 Loss: 0.028484618291258812  PSNR: 18.681974411010742
[TRAIN] Iter: 357900 Loss: 0.027215780690312386  PSNR: 18.86482810974121
[TRAIN] Iter: 358000 Loss: 0.03158551827073097  PSNR: 18.18787956237793
[TRAIN] Iter: 358100 Loss: 0.021106496453285217  PSNR: 19.846511840820312
[TRAIN] Iter: 358200 Loss: 0.021192193031311035  PSNR: 19.95673179626465
[TRAIN] Iter: 358300 Loss: 0.029362330213189125  PSNR: 18.574068069458008
[TRAIN] Iter: 358400 Loss: 0.02347078174352646  PSNR: 19.203554153442383
[TRAIN] Iter: 358500 Loss: 0.026990965008735657  PSNR: 18.971879959106445
[TRAIN] Iter: 358600 Loss: 0.030415594577789307  PSNR: 18.36052894592285
[TRAIN] Iter: 358700 Loss: 0.02576405555009842  PSNR: 19.06435775756836
[TRAIN] Iter: 358800 Loss: 0.03186601772904396  PSNR: 18.248071670532227
[TRAIN] Iter: 358900 Loss: 0.03137504681944847  PSNR: 18.180660247802734
[TRAIN] Iter: 359000 Loss: 0.02832675538957119  PSNR: 18.775039672851562
[TRAIN] Iter: 359100 Loss: 0.0317666232585907  PSNR: 18.038236618041992
[TRAIN] Iter: 359200 Loss: 0.02081957459449768  PSNR: 20.05755615234375
[TRAIN] Iter: 359300 Loss: 0.020206738263368607  PSNR: 20.25023078918457
[TRAIN] Iter: 359400 Loss: 0.023408036679029465  PSNR: 19.615333557128906
[TRAIN] Iter: 359500 Loss: 0.023094579577445984  PSNR: 19.566246032714844
[TRAIN] Iter: 359600 Loss: 0.03056517243385315  PSNR: 18.307493209838867
[TRAIN] Iter: 359700 Loss: 0.026060985401272774  PSNR: 19.360769271850586
[TRAIN] Iter: 359800 Loss: 0.028609465807676315  PSNR: 18.587501525878906
[TRAIN] Iter: 359900 Loss: 0.03341391310095787  PSNR: 18.041353225708008
Saved checkpoints at ./logs/TUT-LAB-nerf/360000.tar
[TRAIN] Iter: 360000 Loss: 0.026423566043376923  PSNR: 18.95098114013672
[TRAIN] Iter: 360100 Loss: 0.02599130943417549  PSNR: 19.116403579711914
[TRAIN] Iter: 360200 Loss: 0.02838212251663208  PSNR: 18.601436614990234
[TRAIN] Iter: 360300 Loss: 0.025438614189624786  PSNR: 19.0747013092041
[TRAIN] Iter: 360400 Loss: 0.032207027077674866  PSNR: 18.067298889160156
[TRAIN] Iter: 360500 Loss: 0.028423337265849113  PSNR: 18.443632125854492
[TRAIN] Iter: 360600 Loss: 0.029730726033449173  PSNR: 18.37736701965332
[TRAIN] Iter: 360700 Loss: 0.024885889142751694  PSNR: 18.936874389648438
[TRAIN] Iter: 360800 Loss: 0.027540799230337143  PSNR: 19.213516235351562
[TRAIN] Iter: 360900 Loss: 0.02705480344593525  PSNR: 18.715328216552734
[TRAIN] Iter: 361000 Loss: 0.0322340726852417  PSNR: 18.22011947631836
[TRAIN] Iter: 361100 Loss: 0.03319314122200012  PSNR: 18.024972915649414
[TRAIN] Iter: 361200 Loss: 0.024858582764863968  PSNR: 19.45755386352539
[TRAIN] Iter: 361300 Loss: 0.020770002156496048  PSNR: 19.99766731262207
[TRAIN] Iter: 361400 Loss: 0.03050939179956913  PSNR: 18.470060348510742
[TRAIN] Iter: 361500 Loss: 0.030997566878795624  PSNR: 18.2684383392334
[TRAIN] Iter: 361600 Loss: 0.026033787056803703  PSNR: 19.08159065246582
[TRAIN] Iter: 361700 Loss: 0.027423139661550522  PSNR: 18.968576431274414
[TRAIN] Iter: 361800 Loss: 0.022435937076807022  PSNR: 19.7930850982666
[TRAIN] Iter: 361900 Loss: 0.022952068597078323  PSNR: 19.6248779296875
[TRAIN] Iter: 362000 Loss: 0.023448793217539787  PSNR: 19.501359939575195
[TRAIN] Iter: 362100 Loss: 0.028425663709640503  PSNR: 18.66062355041504
[TRAIN] Iter: 362200 Loss: 0.02955353632569313  PSNR: 18.49593162536621
[TRAIN] Iter: 362300 Loss: 0.026901856064796448  PSNR: 18.85548973083496
[TRAIN] Iter: 362400 Loss: 0.02939707599580288  PSNR: 18.55588722229004
[TRAIN] Iter: 362500 Loss: 0.02332288771867752  PSNR: 19.61275291442871
[TRAIN] Iter: 362600 Loss: 0.029421910643577576  PSNR: 18.700496673583984
[TRAIN] Iter: 362700 Loss: 0.025021731853485107  PSNR: 19.36412811279297
[TRAIN] Iter: 362800 Loss: 0.029485022649168968  PSNR: 18.548002243041992
[TRAIN] Iter: 362900 Loss: 0.022114388644695282  PSNR: 19.54047203063965
[TRAIN] Iter: 363000 Loss: 0.019262515008449554  PSNR: 20.335983276367188
[TRAIN] Iter: 363100 Loss: 0.030105452984571457  PSNR: 18.423124313354492
[TRAIN] Iter: 363200 Loss: 0.02402873896062374  PSNR: 19.29634666442871
[TRAIN] Iter: 363300 Loss: 0.02173803374171257  PSNR: 19.777658462524414
[TRAIN] Iter: 363400 Loss: 0.0334717258810997  PSNR: 17.918155670166016
[TRAIN] Iter: 363500 Loss: 0.0266991276293993  PSNR: 18.823196411132812
[TRAIN] Iter: 363600 Loss: 0.022303659468889236  PSNR: 19.578752517700195
[TRAIN] Iter: 363700 Loss: 0.02775522507727146  PSNR: 18.628158569335938
[TRAIN] Iter: 363800 Loss: 0.029402602463960648  PSNR: 18.386810302734375
[TRAIN] Iter: 363900 Loss: 0.03126576170325279  PSNR: 18.174169540405273
[TRAIN] Iter: 364000 Loss: 0.029880758374929428  PSNR: 18.574321746826172
[TRAIN] Iter: 364100 Loss: 0.0285708736628294  PSNR: 18.666656494140625
[TRAIN] Iter: 364200 Loss: 0.028678331524133682  PSNR: 18.818180084228516
[TRAIN] Iter: 364300 Loss: 0.032910238951444626  PSNR: 17.988439559936523
[TRAIN] Iter: 364400 Loss: 0.02558683231472969  PSNR: 18.94339370727539
[TRAIN] Iter: 364500 Loss: 0.02847728505730629  PSNR: 18.501066207885742
[TRAIN] Iter: 364600 Loss: 0.028620315715670586  PSNR: 18.62704849243164
[TRAIN] Iter: 364700 Loss: 0.024861115962266922  PSNR: 19.15321159362793
[TRAIN] Iter: 364800 Loss: 0.022654300555586815  PSNR: 19.67413330078125
[TRAIN] Iter: 364900 Loss: 0.031216487288475037  PSNR: 18.293912887573242
[TRAIN] Iter: 365000 Loss: 0.03240186348557472  PSNR: 18.04139518737793
[TRAIN] Iter: 365100 Loss: 0.03003321774303913  PSNR: 18.3746395111084
[TRAIN] Iter: 365200 Loss: 0.021859757602214813  PSNR: 19.791669845581055
[TRAIN] Iter: 365300 Loss: 0.033064186573028564  PSNR: 17.96868133544922
[TRAIN] Iter: 365400 Loss: 0.026841193437576294  PSNR: 18.875625610351562
[TRAIN] Iter: 365500 Loss: 0.028500564396381378  PSNR: 18.608606338500977
[TRAIN] Iter: 365600 Loss: 0.03483464568853378  PSNR: 17.832416534423828
[TRAIN] Iter: 365700 Loss: 0.031129026785492897  PSNR: 18.290678024291992
[TRAIN] Iter: 365800 Loss: 0.03250238299369812  PSNR: 18.040874481201172
[TRAIN] Iter: 365900 Loss: 0.028093261644244194  PSNR: 18.538389205932617
[TRAIN] Iter: 366000 Loss: 0.03025616705417633  PSNR: 18.370548248291016
[TRAIN] Iter: 366100 Loss: 0.027917612344026566  PSNR: 18.748109817504883
[TRAIN] Iter: 366200 Loss: 0.027363507077097893  PSNR: 18.619720458984375
[TRAIN] Iter: 366300 Loss: 0.030099160969257355  PSNR: 18.394304275512695
[TRAIN] Iter: 366400 Loss: 0.027239156886935234  PSNR: 18.80884552001953
[TRAIN] Iter: 366500 Loss: 0.02764173597097397  PSNR: 18.59668731689453
[TRAIN] Iter: 366600 Loss: 0.028987862169742584  PSNR: 18.543926239013672
[TRAIN] Iter: 366700 Loss: 0.030063757672905922  PSNR: 18.348554611206055
[TRAIN] Iter: 366800 Loss: 0.02518429234623909  PSNR: 19.23367691040039
[TRAIN] Iter: 366900 Loss: 0.031478237360715866  PSNR: 18.075197219848633
[TRAIN] Iter: 367000 Loss: 0.03201434761285782  PSNR: 18.11846923828125
[TRAIN] Iter: 367100 Loss: 0.02717427909374237  PSNR: 18.811649322509766
[TRAIN] Iter: 367200 Loss: 0.03190197795629501  PSNR: 18.07831573486328
[TRAIN] Iter: 367300 Loss: 0.029976021498441696  PSNR: 18.607614517211914
[TRAIN] Iter: 367400 Loss: 0.02540280856192112  PSNR: 19.508575439453125
[TRAIN] Iter: 367500 Loss: 0.03730582818388939  PSNR: 17.49147605895996
[TRAIN] Iter: 367600 Loss: 0.029690150171518326  PSNR: 18.455459594726562
[TRAIN] Iter: 367700 Loss: 0.02702016569674015  PSNR: 19.024581909179688
[TRAIN] Iter: 367800 Loss: 0.03067273087799549  PSNR: 18.3380069732666
[TRAIN] Iter: 367900 Loss: 0.02898695319890976  PSNR: 18.67765998840332
[TRAIN] Iter: 368000 Loss: 0.02386738359928131  PSNR: 19.534944534301758
[TRAIN] Iter: 368100 Loss: 0.02972627431154251  PSNR: 18.388530731201172
[TRAIN] Iter: 368200 Loss: 0.03156207129359245  PSNR: 18.242021560668945
[TRAIN] Iter: 368300 Loss: 0.029869114980101585  PSNR: 18.554912567138672
[TRAIN] Iter: 368400 Loss: 0.026623783633112907  PSNR: 18.76097297668457
[TRAIN] Iter: 368500 Loss: 0.02589428424835205  PSNR: 19.325443267822266
[TRAIN] Iter: 368600 Loss: 0.03003588691353798  PSNR: 18.358488082885742
[TRAIN] Iter: 368700 Loss: 0.024594515562057495  PSNR: 19.507564544677734
[TRAIN] Iter: 368800 Loss: 0.031345244497060776  PSNR: 18.327329635620117
[TRAIN] Iter: 368900 Loss: 0.028706898912787437  PSNR: 18.613649368286133
[TRAIN] Iter: 369000 Loss: 0.028777960687875748  PSNR: 18.57042694091797
[TRAIN] Iter: 369100 Loss: 0.025593087077140808  PSNR: 19.082889556884766
[TRAIN] Iter: 369200 Loss: 0.023141276091337204  PSNR: 19.441274642944336
[TRAIN] Iter: 369300 Loss: 0.028275100514292717  PSNR: 18.70060157775879
[TRAIN] Iter: 369400 Loss: 0.02682565152645111  PSNR: 18.90921401977539
[TRAIN] Iter: 369500 Loss: 0.024354426190257072  PSNR: 19.319995880126953
[TRAIN] Iter: 369600 Loss: 0.03387485817074776  PSNR: 17.942943572998047
[TRAIN] Iter: 369700 Loss: 0.034414853900671005  PSNR: 17.8948917388916
[TRAIN] Iter: 369800 Loss: 0.026187045499682426  PSNR: 18.889177322387695
[TRAIN] Iter: 369900 Loss: 0.02778792940080166  PSNR: 18.801130294799805
Saved checkpoints at ./logs/TUT-LAB-nerf/370000.tar
[TRAIN] Iter: 370000 Loss: 0.03101455420255661  PSNR: 18.320425033569336
[TRAIN] Iter: 370100 Loss: 0.032767780125141144  PSNR: 17.985687255859375
[TRAIN] Iter: 370200 Loss: 0.028624873608350754  PSNR: 18.446189880371094
[TRAIN] Iter: 370300 Loss: 0.03015344776213169  PSNR: 18.52159881591797
[TRAIN] Iter: 370400 Loss: 0.024277042597532272  PSNR: 19.095666885375977
[TRAIN] Iter: 370500 Loss: 0.023641180247068405  PSNR: 19.519683837890625
[TRAIN] Iter: 370600 Loss: 0.028784871101379395  PSNR: 18.561992645263672
[TRAIN] Iter: 370700 Loss: 0.026951374486088753  PSNR: 18.95900535583496
[TRAIN] Iter: 370800 Loss: 0.03348853066563606  PSNR: 18.265369415283203
[TRAIN] Iter: 370900 Loss: 0.02177637442946434  PSNR: 19.890995025634766
[TRAIN] Iter: 371000 Loss: 0.021442221477627754  PSNR: 19.81610107421875
[TRAIN] Iter: 371100 Loss: 0.023782875388860703  PSNR: 19.40523338317871
[TRAIN] Iter: 371200 Loss: 0.02795468084514141  PSNR: 18.756996154785156
[TRAIN] Iter: 371300 Loss: 0.022840753197669983  PSNR: 19.448579788208008
[TRAIN] Iter: 371400 Loss: 0.021066119894385338  PSNR: 20.00760269165039
[TRAIN] Iter: 371500 Loss: 0.031040038913488388  PSNR: 18.21978759765625
[TRAIN] Iter: 371600 Loss: 0.023741891607642174  PSNR: 19.53435707092285
[TRAIN] Iter: 371700 Loss: 0.03161348029971123  PSNR: 18.443222045898438
[TRAIN] Iter: 371800 Loss: 0.023809097707271576  PSNR: 19.33539390563965
[TRAIN] Iter: 371900 Loss: 0.02487245760858059  PSNR: 19.293354034423828
[TRAIN] Iter: 372000 Loss: 0.02301996573805809  PSNR: 19.630451202392578
[TRAIN] Iter: 372100 Loss: 0.02898470312356949  PSNR: 18.54398536682129
[TRAIN] Iter: 372200 Loss: 0.022599056363105774  PSNR: 19.420001983642578
[TRAIN] Iter: 372300 Loss: 0.022598912939429283  PSNR: 19.584957122802734
[TRAIN] Iter: 372400 Loss: 0.026821088045835495  PSNR: 18.727378845214844
[TRAIN] Iter: 372500 Loss: 0.032990023493766785  PSNR: 18.09670066833496
[TRAIN] Iter: 372600 Loss: 0.02822958119213581  PSNR: 18.65530014038086
[TRAIN] Iter: 372700 Loss: 0.02998775616288185  PSNR: 18.370492935180664
[TRAIN] Iter: 372800 Loss: 0.02989146299660206  PSNR: 18.58119010925293
[TRAIN] Iter: 372900 Loss: 0.030631227418780327  PSNR: 18.278385162353516
[TRAIN] Iter: 373000 Loss: 0.030867626890540123  PSNR: 18.14763832092285
[TRAIN] Iter: 373100 Loss: 0.027187440544366837  PSNR: 18.712560653686523
[TRAIN] Iter: 373200 Loss: 0.024801481515169144  PSNR: 19.280723571777344
[TRAIN] Iter: 373300 Loss: 0.02772081457078457  PSNR: 18.773958206176758
[TRAIN] Iter: 373400 Loss: 0.025849703699350357  PSNR: 19.034767150878906
[TRAIN] Iter: 373500 Loss: 0.02393028326332569  PSNR: 19.530405044555664
[TRAIN] Iter: 373600 Loss: 0.02929973602294922  PSNR: 18.42458152770996
[TRAIN] Iter: 373700 Loss: 0.02684009075164795  PSNR: 18.770950317382812
[TRAIN] Iter: 373800 Loss: 0.03241461142897606  PSNR: 17.97751808166504
[TRAIN] Iter: 373900 Loss: 0.024630798026919365  PSNR: 18.936172485351562
[TRAIN] Iter: 374000 Loss: 0.02571363002061844  PSNR: 18.705448150634766
[TRAIN] Iter: 374100 Loss: 0.025677654892206192  PSNR: 18.97815704345703
[TRAIN] Iter: 374200 Loss: 0.02572658658027649  PSNR: 18.997806549072266
[TRAIN] Iter: 374300 Loss: 0.030466977506875992  PSNR: 18.384965896606445
[TRAIN] Iter: 374400 Loss: 0.026650290936231613  PSNR: 18.99907684326172
[TRAIN] Iter: 374500 Loss: 0.029696166515350342  PSNR: 18.415908813476562
[TRAIN] Iter: 374600 Loss: 0.022518333047628403  PSNR: 19.390228271484375
[TRAIN] Iter: 374700 Loss: 0.03228306770324707  PSNR: 18.022905349731445
[TRAIN] Iter: 374800 Loss: 0.02489134483039379  PSNR: 19.20358657836914
[TRAIN] Iter: 374900 Loss: 0.03003741428256035  PSNR: 18.44217300415039
[TRAIN] Iter: 375000 Loss: 0.030457034707069397  PSNR: 18.285987854003906
[TRAIN] Iter: 375100 Loss: 0.026714246720075607  PSNR: 19.334300994873047
[TRAIN] Iter: 375200 Loss: 0.02718769945204258  PSNR: 19.03354835510254
[TRAIN] Iter: 375300 Loss: 0.03451494127511978  PSNR: 17.804567337036133
[TRAIN] Iter: 375400 Loss: 0.029684539884328842  PSNR: 18.453216552734375
[TRAIN] Iter: 375500 Loss: 0.026316611096262932  PSNR: 18.948144912719727
[TRAIN] Iter: 375600 Loss: 0.02486630529165268  PSNR: 19.174345016479492
[TRAIN] Iter: 375700 Loss: 0.03149355947971344  PSNR: 18.199243545532227
[TRAIN] Iter: 375800 Loss: 0.02272341400384903  PSNR: 19.32220458984375
[TRAIN] Iter: 375900 Loss: 0.0218648798763752  PSNR: 19.873611450195312
[TRAIN] Iter: 376000 Loss: 0.02535666525363922  PSNR: 18.894712448120117
[TRAIN] Iter: 376100 Loss: 0.029255645349621773  PSNR: 18.610586166381836
[TRAIN] Iter: 376200 Loss: 0.02889101952314377  PSNR: 18.522432327270508
[TRAIN] Iter: 376300 Loss: 0.02764897421002388  PSNR: 18.77600860595703
[TRAIN] Iter: 376400 Loss: 0.030735254287719727  PSNR: 18.505802154541016
[TRAIN] Iter: 376500 Loss: 0.02748878300189972  PSNR: 18.78628158569336
[TRAIN] Iter: 376600 Loss: 0.030010372400283813  PSNR: 18.39690399169922
[TRAIN] Iter: 376700 Loss: 0.025822842493653297  PSNR: 18.895835876464844
[TRAIN] Iter: 376800 Loss: 0.026984933763742447  PSNR: 18.864755630493164
[TRAIN] Iter: 376900 Loss: 0.0229025986045599  PSNR: 19.62192153930664
[TRAIN] Iter: 377000 Loss: 0.02527935802936554  PSNR: 19.227672576904297
[TRAIN] Iter: 377100 Loss: 0.033777594566345215  PSNR: 17.863224029541016
[TRAIN] Iter: 377200 Loss: 0.02718479000031948  PSNR: 18.776588439941406
[TRAIN] Iter: 377300 Loss: 0.028001924976706505  PSNR: 18.71308135986328
[TRAIN] Iter: 377400 Loss: 0.025912512093782425  PSNR: 19.19424819946289
[TRAIN] Iter: 377500 Loss: 0.025268366560339928  PSNR: 19.194639205932617
[TRAIN] Iter: 377600 Loss: 0.02661857381463051  PSNR: 18.757871627807617
[TRAIN] Iter: 377700 Loss: 0.024511458352208138  PSNR: 19.25449562072754
[TRAIN] Iter: 377800 Loss: 0.02668207883834839  PSNR: 18.879261016845703
[TRAIN] Iter: 377900 Loss: 0.032446980476379395  PSNR: 18.007532119750977
[TRAIN] Iter: 378000 Loss: 0.0282658152282238  PSNR: 18.684972763061523
[TRAIN] Iter: 378100 Loss: 0.02643616683781147  PSNR: 18.82980728149414
[TRAIN] Iter: 378200 Loss: 0.026519592851400375  PSNR: 18.499162673950195
[TRAIN] Iter: 378300 Loss: 0.027080124244093895  PSNR: 18.380891799926758
[TRAIN] Iter: 378400 Loss: 0.03266601264476776  PSNR: 17.968008041381836
[TRAIN] Iter: 378500 Loss: 0.027809888124465942  PSNR: 18.7602481842041
[TRAIN] Iter: 378600 Loss: 0.031144438311457634  PSNR: 18.267026901245117
[TRAIN] Iter: 378700 Loss: 0.024800291284918785  PSNR: 19.036357879638672
[TRAIN] Iter: 378800 Loss: 0.030444975942373276  PSNR: 18.309722900390625
[TRAIN] Iter: 378900 Loss: 0.026587674394249916  PSNR: 18.962553024291992
[TRAIN] Iter: 379000 Loss: 0.032256126403808594  PSNR: 18.094690322875977
[TRAIN] Iter: 379100 Loss: 0.031085669994354248  PSNR: 18.285377502441406
[TRAIN] Iter: 379200 Loss: 0.02431556209921837  PSNR: 19.41156768798828
[TRAIN] Iter: 379300 Loss: 0.025501150637865067  PSNR: 18.781658172607422
[TRAIN] Iter: 379400 Loss: 0.030533041805028915  PSNR: 18.253725051879883
[TRAIN] Iter: 379500 Loss: 0.023870429024100304  PSNR: 19.42544174194336
[TRAIN] Iter: 379600 Loss: 0.02060423605144024  PSNR: 20.172344207763672
[TRAIN] Iter: 379700 Loss: 0.02677847631275654  PSNR: 18.75457763671875
[TRAIN] Iter: 379800 Loss: 0.027930516749620438  PSNR: 18.72184944152832
[TRAIN] Iter: 379900 Loss: 0.026860535144805908  PSNR: 18.881025314331055
Saved checkpoints at ./logs/TUT-LAB-nerf/380000.tar
[TRAIN] Iter: 380000 Loss: 0.021655546501278877  PSNR: 19.999807357788086
[TRAIN] Iter: 380100 Loss: 0.030695484951138496  PSNR: 18.273221969604492
[TRAIN] Iter: 380200 Loss: 0.024536270648241043  PSNR: 18.927358627319336
[TRAIN] Iter: 380300 Loss: 0.025135856121778488  PSNR: 19.155630111694336
[TRAIN] Iter: 380400 Loss: 0.02743658609688282  PSNR: 18.862703323364258
[TRAIN] Iter: 380500 Loss: 0.028178062289953232  PSNR: 18.57242202758789
[TRAIN] Iter: 380600 Loss: 0.025850409641861916  PSNR: 19.251331329345703
[TRAIN] Iter: 380700 Loss: 0.027400711551308632  PSNR: 18.747230529785156
[TRAIN] Iter: 380800 Loss: 0.026524068787693977  PSNR: 18.895170211791992
[TRAIN] Iter: 380900 Loss: 0.02571231499314308  PSNR: 19.434711456298828
[TRAIN] Iter: 381000 Loss: 0.026345321908593178  PSNR: 18.971864700317383
[TRAIN] Iter: 381100 Loss: 0.033894382417201996  PSNR: 17.916400909423828
[TRAIN] Iter: 381200 Loss: 0.03236622363328934  PSNR: 17.91635513305664
[TRAIN] Iter: 381300 Loss: 0.026670271530747414  PSNR: 18.829496383666992
[TRAIN] Iter: 381400 Loss: 0.03370729088783264  PSNR: 17.848270416259766
[TRAIN] Iter: 381500 Loss: 0.021727656945586205  PSNR: 19.8066463470459
[TRAIN] Iter: 381600 Loss: 0.028947623446583748  PSNR: 18.572181701660156
[TRAIN] Iter: 381700 Loss: 0.029068004339933395  PSNR: 18.871814727783203
[TRAIN] Iter: 381800 Loss: 0.02614593878388405  PSNR: 18.965858459472656
[TRAIN] Iter: 381900 Loss: 0.02409183979034424  PSNR: 19.427326202392578
[TRAIN] Iter: 382000 Loss: 0.02319033071398735  PSNR: 19.338836669921875
[TRAIN] Iter: 382100 Loss: 0.02719821035861969  PSNR: 18.751033782958984
[TRAIN] Iter: 382200 Loss: 0.02114870399236679  PSNR: 19.91094207763672
[TRAIN] Iter: 382300 Loss: 0.03000595048069954  PSNR: 18.49323081970215
[TRAIN] Iter: 382400 Loss: 0.0280781053006649  PSNR: 18.95199966430664
[TRAIN] Iter: 382500 Loss: 0.025146793574094772  PSNR: 19.12210464477539
[TRAIN] Iter: 382600 Loss: 0.025245577096939087  PSNR: 18.901336669921875
[TRAIN] Iter: 382700 Loss: 0.032190192490816116  PSNR: 18.116683959960938
[TRAIN] Iter: 382800 Loss: 0.026697995141148567  PSNR: 19.0638427734375
[TRAIN] Iter: 382900 Loss: 0.02773643098771572  PSNR: 18.838350296020508
[TRAIN] Iter: 383000 Loss: 0.026902269572019577  PSNR: 18.692974090576172
[TRAIN] Iter: 383100 Loss: 0.021664828062057495  PSNR: 19.95688819885254
[TRAIN] Iter: 383200 Loss: 0.025402281433343887  PSNR: 19.024812698364258
[TRAIN] Iter: 383300 Loss: 0.024935346096754074  PSNR: 19.470529556274414
[TRAIN] Iter: 383400 Loss: 0.03206157311797142  PSNR: 18.13323402404785
[TRAIN] Iter: 383500 Loss: 0.029244069010019302  PSNR: 18.512964248657227
[TRAIN] Iter: 383600 Loss: 0.028693141415715218  PSNR: 18.625497817993164
[TRAIN] Iter: 383700 Loss: 0.03172197937965393  PSNR: 18.040939331054688
[TRAIN] Iter: 383800 Loss: 0.018174955621361732  PSNR: 20.677413940429688
[TRAIN] Iter: 383900 Loss: 0.030546819791197777  PSNR: 18.196531295776367
[TRAIN] Iter: 384000 Loss: 0.02845665253698826  PSNR: 18.644380569458008
[TRAIN] Iter: 384100 Loss: 0.02717500552535057  PSNR: 18.904376983642578
[TRAIN] Iter: 384200 Loss: 0.02604593150317669  PSNR: 18.965518951416016
[TRAIN] Iter: 384300 Loss: 0.03114274889230728  PSNR: 18.293621063232422
[TRAIN] Iter: 384400 Loss: 0.027538564056158066  PSNR: 18.750478744506836
[TRAIN] Iter: 384500 Loss: 0.03232137858867645  PSNR: 18.185121536254883
[TRAIN] Iter: 384600 Loss: 0.019249653443694115  PSNR: 20.24005889892578
[TRAIN] Iter: 384700 Loss: 0.025091825053095818  PSNR: 19.032556533813477
[TRAIN] Iter: 384800 Loss: 0.029022756963968277  PSNR: 18.509300231933594
[TRAIN] Iter: 384900 Loss: 0.021684885025024414  PSNR: 19.665321350097656
[TRAIN] Iter: 385000 Loss: 0.027232138440012932  PSNR: 18.933870315551758
[TRAIN] Iter: 385100 Loss: 0.020701967179775238  PSNR: 20.04425811767578
[TRAIN] Iter: 385200 Loss: 0.030147243291139603  PSNR: 18.42777442932129
[TRAIN] Iter: 385300 Loss: 0.03120211698114872  PSNR: 18.2288875579834
[TRAIN] Iter: 385400 Loss: 0.025065628811717033  PSNR: 19.156049728393555
[TRAIN] Iter: 385500 Loss: 0.023521333932876587  PSNR: 19.540857315063477
[TRAIN] Iter: 385600 Loss: 0.0266728512942791  PSNR: 18.93062973022461
[TRAIN] Iter: 385700 Loss: 0.03518451750278473  PSNR: 17.692689895629883
[TRAIN] Iter: 385800 Loss: 0.027822133153676987  PSNR: 18.599397659301758
[TRAIN] Iter: 385900 Loss: 0.020455293357372284  PSNR: 20.04222297668457
[TRAIN] Iter: 386000 Loss: 0.025133252143859863  PSNR: 18.785032272338867
[TRAIN] Iter: 386100 Loss: 0.02397860586643219  PSNR: 19.345157623291016
[TRAIN] Iter: 386200 Loss: 0.025565139949321747  PSNR: 18.91855239868164
[TRAIN] Iter: 386300 Loss: 0.02660883218050003  PSNR: 18.618547439575195
[TRAIN] Iter: 386400 Loss: 0.023587405681610107  PSNR: 19.449382781982422
[TRAIN] Iter: 386500 Loss: 0.02868391014635563  PSNR: 18.57887077331543
[TRAIN] Iter: 386600 Loss: 0.03167097643017769  PSNR: 18.20455551147461
[TRAIN] Iter: 386700 Loss: 0.03078549914062023  PSNR: 18.40650749206543
[TRAIN] Iter: 386800 Loss: 0.02625802904367447  PSNR: 18.825925827026367
[TRAIN] Iter: 386900 Loss: 0.021019354462623596  PSNR: 19.945446014404297
[TRAIN] Iter: 387000 Loss: 0.02550256811082363  PSNR: 19.248863220214844
[TRAIN] Iter: 387100 Loss: 0.02418903261423111  PSNR: 19.419979095458984
[TRAIN] Iter: 387200 Loss: 0.025626439601182938  PSNR: 18.938884735107422
[TRAIN] Iter: 387300 Loss: 0.03176964074373245  PSNR: 18.269075393676758
[TRAIN] Iter: 387400 Loss: 0.03049345687031746  PSNR: 18.275615692138672
[TRAIN] Iter: 387500 Loss: 0.034080397337675095  PSNR: 17.807193756103516
[TRAIN] Iter: 387600 Loss: 0.032926954329013824  PSNR: 17.91385841369629
[TRAIN] Iter: 387700 Loss: 0.03268999606370926  PSNR: 18.071374893188477
[TRAIN] Iter: 387800 Loss: 0.030865110456943512  PSNR: 18.52655601501465
[TRAIN] Iter: 387900 Loss: 0.025619205087423325  PSNR: 18.963916778564453
[TRAIN] Iter: 388000 Loss: 0.03334171324968338  PSNR: 17.94034194946289
[TRAIN] Iter: 388100 Loss: 0.026129167526960373  PSNR: 19.066865921020508
[TRAIN] Iter: 388200 Loss: 0.024534229189157486  PSNR: 19.518234252929688
[TRAIN] Iter: 388300 Loss: 0.02551371045410633  PSNR: 19.15708351135254
[TRAIN] Iter: 388400 Loss: 0.02868281491100788  PSNR: 18.63239288330078
[TRAIN] Iter: 388500 Loss: 0.02733762376010418  PSNR: 18.91965103149414
[TRAIN] Iter: 388600 Loss: 0.023021146655082703  PSNR: 19.517621994018555
[TRAIN] Iter: 388700 Loss: 0.03076409548521042  PSNR: 18.447975158691406
[TRAIN] Iter: 388800 Loss: 0.024448692798614502  PSNR: 18.95724868774414
[TRAIN] Iter: 388900 Loss: 0.021422378718852997  PSNR: 19.630443572998047
[TRAIN] Iter: 389000 Loss: 0.026760507375001907  PSNR: 18.497909545898438
[TRAIN] Iter: 389100 Loss: 0.02670741081237793  PSNR: 19.1059513092041
[TRAIN] Iter: 389200 Loss: 0.03184293210506439  PSNR: 18.20082664489746
[TRAIN] Iter: 389300 Loss: 0.030910903587937355  PSNR: 18.27532958984375
[TRAIN] Iter: 389400 Loss: 0.03331008180975914  PSNR: 17.879173278808594
[TRAIN] Iter: 389500 Loss: 0.026490718126296997  PSNR: 19.080419540405273
[TRAIN] Iter: 389600 Loss: 0.03746005520224571  PSNR: 17.481914520263672
[TRAIN] Iter: 389700 Loss: 0.029962021857500076  PSNR: 18.53034019470215
[TRAIN] Iter: 389800 Loss: 0.02878338098526001  PSNR: 18.539751052856445
[TRAIN] Iter: 389900 Loss: 0.02735798805952072  PSNR: 18.798166275024414
Saved checkpoints at ./logs/TUT-LAB-nerf/390000.tar
[TRAIN] Iter: 390000 Loss: 0.02985282614827156  PSNR: 18.715181350708008
[TRAIN] Iter: 390100 Loss: 0.036054641008377075  PSNR: 17.642059326171875
[TRAIN] Iter: 390200 Loss: 0.026660535484552383  PSNR: 18.897024154663086
[TRAIN] Iter: 390300 Loss: 0.03182141110301018  PSNR: 18.25066566467285
[TRAIN] Iter: 390400 Loss: 0.027211807668209076  PSNR: 18.815601348876953
[TRAIN] Iter: 390500 Loss: 0.031111713498830795  PSNR: 18.38175392150879
[TRAIN] Iter: 390600 Loss: 0.0239420123398304  PSNR: 19.175151824951172
[TRAIN] Iter: 390700 Loss: 0.027094192802906036  PSNR: 18.862834930419922
[TRAIN] Iter: 390800 Loss: 0.022107083350419998  PSNR: 20.02410316467285
[TRAIN] Iter: 390900 Loss: 0.026746436953544617  PSNR: 19.390878677368164
[TRAIN] Iter: 391000 Loss: 0.02533499337732792  PSNR: 18.938232421875
[TRAIN] Iter: 391100 Loss: 0.020144999027252197  PSNR: 20.222854614257812
[TRAIN] Iter: 391200 Loss: 0.02554662525653839  PSNR: 19.438688278198242
[TRAIN] Iter: 391300 Loss: 0.02194078266620636  PSNR: 19.664331436157227
[TRAIN] Iter: 391400 Loss: 0.022685764357447624  PSNR: 19.53119468688965
[TRAIN] Iter: 391500 Loss: 0.02524571493268013  PSNR: 19.292192459106445
[TRAIN] Iter: 391600 Loss: 0.032941561192274094  PSNR: 18.047626495361328
[TRAIN] Iter: 391700 Loss: 0.025676365941762924  PSNR: 19.11195182800293
[TRAIN] Iter: 391800 Loss: 0.030215032398700714  PSNR: 18.365930557250977
[TRAIN] Iter: 391900 Loss: 0.02369564026594162  PSNR: 19.34856605529785
[TRAIN] Iter: 392000 Loss: 0.025424545630812645  PSNR: 18.88933563232422
[TRAIN] Iter: 392100 Loss: 0.020956210792064667  PSNR: 19.99161720275879
[TRAIN] Iter: 392200 Loss: 0.02664276212453842  PSNR: 19.13505744934082
[TRAIN] Iter: 392300 Loss: 0.03135507553815842  PSNR: 18.22981071472168
[TRAIN] Iter: 392400 Loss: 0.031039949506521225  PSNR: 18.297359466552734
[TRAIN] Iter: 392500 Loss: 0.029218411073088646  PSNR: 18.577152252197266
[TRAIN] Iter: 392600 Loss: 0.02258477360010147  PSNR: 19.64777946472168
[TRAIN] Iter: 392700 Loss: 0.03558892011642456  PSNR: 17.585378646850586
[TRAIN] Iter: 392800 Loss: 0.031124789267778397  PSNR: 17.92424201965332
[TRAIN] Iter: 392900 Loss: 0.028606943786144257  PSNR: 18.730152130126953
[TRAIN] Iter: 393000 Loss: 0.023561831563711166  PSNR: 19.602397918701172
[TRAIN] Iter: 393100 Loss: 0.027742775157094002  PSNR: 18.78749656677246
[TRAIN] Iter: 393200 Loss: 0.026358529925346375  PSNR: 19.044261932373047
[TRAIN] Iter: 393300 Loss: 0.03267227113246918  PSNR: 18.010915756225586
[TRAIN] Iter: 393400 Loss: 0.0323948934674263  PSNR: 17.989721298217773
[TRAIN] Iter: 393500 Loss: 0.034080781042575836  PSNR: 17.85797882080078
[TRAIN] Iter: 393600 Loss: 0.031889546662569046  PSNR: 18.11681365966797
[TRAIN] Iter: 393700 Loss: 0.021342594176530838  PSNR: 19.882122039794922
[TRAIN] Iter: 393800 Loss: 0.028641656041145325  PSNR: 18.394641876220703
[TRAIN] Iter: 393900 Loss: 0.024884343147277832  PSNR: 19.42902946472168
[TRAIN] Iter: 394000 Loss: 0.027109041810035706  PSNR: 18.818490982055664
[TRAIN] Iter: 394100 Loss: 0.020750433206558228  PSNR: 19.854564666748047
[TRAIN] Iter: 394200 Loss: 0.0268208310008049  PSNR: 19.30020523071289
[TRAIN] Iter: 394300 Loss: 0.027904294431209564  PSNR: 18.53301429748535
[TRAIN] Iter: 394400 Loss: 0.0258684940636158  PSNR: 19.07838249206543
[TRAIN] Iter: 394500 Loss: 0.02768482081592083  PSNR: 18.74312973022461
[TRAIN] Iter: 394600 Loss: 0.027549445629119873  PSNR: 19.3660945892334
[TRAIN] Iter: 394700 Loss: 0.02713075838983059  PSNR: 18.93612289428711
[TRAIN] Iter: 394800 Loss: 0.027873709797859192  PSNR: 18.677810668945312
[TRAIN] Iter: 394900 Loss: 0.029415445402264595  PSNR: 18.70446014404297
[TRAIN] Iter: 395000 Loss: 0.027203258126974106  PSNR: 18.81707763671875
[TRAIN] Iter: 395100 Loss: 0.027879774570465088  PSNR: 18.824005126953125
[TRAIN] Iter: 395200 Loss: 0.030885659158229828  PSNR: 18.302133560180664
[TRAIN] Iter: 395300 Loss: 0.024810153990983963  PSNR: 19.298118591308594
[TRAIN] Iter: 395400 Loss: 0.027460677549242973  PSNR: 18.749841690063477
[TRAIN] Iter: 395500 Loss: 0.026662785559892654  PSNR: 18.921281814575195
[TRAIN] Iter: 395600 Loss: 0.025460202246904373  PSNR: 18.835725784301758
[TRAIN] Iter: 395700 Loss: 0.02368514984846115  PSNR: 19.603918075561523
[TRAIN] Iter: 395800 Loss: 0.0338020995259285  PSNR: 17.92458724975586
[TRAIN] Iter: 395900 Loss: 0.02988387644290924  PSNR: 18.453256607055664
[TRAIN] Iter: 396000 Loss: 0.023297147825360298  PSNR: 19.691972732543945
[TRAIN] Iter: 396100 Loss: 0.02643601968884468  PSNR: 18.77313995361328
[TRAIN] Iter: 396200 Loss: 0.028477907180786133  PSNR: 18.806278228759766
[TRAIN] Iter: 396300 Loss: 0.03298155963420868  PSNR: 17.999818801879883
[TRAIN] Iter: 396400 Loss: 0.027241069823503494  PSNR: 18.67580795288086
[TRAIN] Iter: 396500 Loss: 0.02761189080774784  PSNR: 18.54667091369629
[TRAIN] Iter: 396600 Loss: 0.026817258447408676  PSNR: 19.14413833618164
[TRAIN] Iter: 396700 Loss: 0.025859065353870392  PSNR: 19.27570915222168
[TRAIN] Iter: 396800 Loss: 0.0255262590944767  PSNR: 19.22090721130371
[TRAIN] Iter: 396900 Loss: 0.02424372360110283  PSNR: 19.59642219543457
[TRAIN] Iter: 397000 Loss: 0.03378702700138092  PSNR: 17.878826141357422
[TRAIN] Iter: 397100 Loss: 0.02405843883752823  PSNR: 19.330581665039062
[TRAIN] Iter: 397200 Loss: 0.02381742373108864  PSNR: 19.298126220703125
[TRAIN] Iter: 397300 Loss: 0.026060676202178  PSNR: 19.249279022216797
[TRAIN] Iter: 397400 Loss: 0.02630283683538437  PSNR: 18.870121002197266
[TRAIN] Iter: 397500 Loss: 0.028668176382780075  PSNR: 18.655588150024414
[TRAIN] Iter: 397600 Loss: 0.024126404896378517  PSNR: 19.127714157104492
[TRAIN] Iter: 397700 Loss: 0.028441015630960464  PSNR: 18.67254066467285
[TRAIN] Iter: 397800 Loss: 0.03077189065515995  PSNR: 18.254899978637695
[TRAIN] Iter: 397900 Loss: 0.025206245481967926  PSNR: 19.330821990966797
[TRAIN] Iter: 398000 Loss: 0.03162164241075516  PSNR: 18.23964500427246
[TRAIN] Iter: 398100 Loss: 0.02563895471394062  PSNR: 18.995281219482422
[TRAIN] Iter: 398200 Loss: 0.02691148966550827  PSNR: 19.043832778930664
[TRAIN] Iter: 398300 Loss: 0.02306078001856804  PSNR: 19.493942260742188
[TRAIN] Iter: 398400 Loss: 0.0233038067817688  PSNR: 19.48690414428711
[TRAIN] Iter: 398500 Loss: 0.02437610551714897  PSNR: 19.134931564331055
[TRAIN] Iter: 398600 Loss: 0.023315872997045517  PSNR: 19.467479705810547
[TRAIN] Iter: 398700 Loss: 0.02226954512298107  PSNR: 19.42245101928711
[TRAIN] Iter: 398800 Loss: 0.027961600571870804  PSNR: 18.62636947631836
[TRAIN] Iter: 398900 Loss: 0.027698813006281853  PSNR: 18.81148910522461
[TRAIN] Iter: 399000 Loss: 0.026722144335508347  PSNR: 18.849815368652344
[TRAIN] Iter: 399100 Loss: 0.02418113872408867  PSNR: 19.38295555114746
[TRAIN] Iter: 399200 Loss: 0.02872428670525551  PSNR: 18.821125030517578
[TRAIN] Iter: 399300 Loss: 0.030666587874293327  PSNR: 18.343664169311523
[TRAIN] Iter: 399400 Loss: 0.02147645875811577  PSNR: 20.023029327392578
[TRAIN] Iter: 399500 Loss: 0.029168354347348213  PSNR: 18.57253074645996
[TRAIN] Iter: 399600 Loss: 0.03175351396203041  PSNR: 18.313772201538086
[TRAIN] Iter: 399700 Loss: 0.0315963551402092  PSNR: 18.213037490844727
[TRAIN] Iter: 399800 Loss: 0.023072591051459312  PSNR: 19.468557357788086
[TRAIN] Iter: 399900 Loss: 0.025379281491041183  PSNR: 19.011442184448242
Saved checkpoints at ./logs/TUT-LAB-nerf/400000.tar
0 0.0004153251647949219
torch.Size([320, 640, 3]) torch.Size([320, 640])
1 18.38233995437622
2 14.782038450241089
3 18.416990995407104
4 14.8109450340271
5 18.474722385406494
6 15.004229545593262
7 18.095439195632935
8 15.655862092971802
9 17.197396278381348
10 15.596392631530762
11 17.300984621047974
12 15.798255205154419
13 15.801530122756958
14 16.970351696014404
15 15.978996992111206
16 17.071651220321655
17 16.150010585784912
18 17.031766414642334
19 16.167179107666016
20 17.047284364700317
21 15.910776376724243
22 16.996973514556885
23 15.93699836730957
24 16.93568706512451
25 15.95202112197876
26 16.924413442611694
27 15.978412628173828
28 16.927590131759644
29 16.01052975654602
30 16.82794213294983
31 15.939054250717163
32 16.934481620788574
33 15.993308782577515
34 16.924468517303467
35 15.989966630935669
36 16.924511671066284
37 15.900038480758667
38 16.90260934829712
39 15.950472593307495
40 16.90848469734192
41 15.917038202285767
42 17.05489754676819
43 16.031105041503906
44 16.68988561630249
45 15.99625277519226
46 16.818447828292847
47 16.02966570854187
48 16.85348391532898
49 16.011876106262207
50 16.72983407974243
51 16.02352738380432
52 16.87700915336609
53 15.983515977859497
54 16.84548807144165
55 15.970757961273193
56 16.8476243019104
57 15.980644941329956
58 16.89638328552246
59 16.004446268081665
60 17.11651110649109
61 16.126595973968506
62 16.498632669448853
63 16.14288902282715
64 16.710187196731567
65 16.618096590042114
66 16.365073680877686
67 16.38204288482666
68 16.514249801635742
69 16.314457654953003
70 16.48294758796692
71 16.358405113220215
72 16.58822011947632
73 16.356457471847534
74 16.51417326927185
75 16.438981533050537
76 16.50952959060669
77 16.637747764587402
78 16.720168828964233
79 16.62328314781189
80 16.445211172103882
81 16.346248388290405
82 16.395249128341675
83 16.461249351501465
84 16.39219093322754
85 16.42790460586548
86 16.40109658241272
87 16.38552474975586
88 16.406939029693604
89 16.440511465072632
90 16.452245950698853
91 16.42359495162964
92 16.472434997558594
93 16.365914821624756
94 16.414376258850098
95 16.416199922561646
96 16.44955587387085
97 16.43518376350403
98 16.461848974227905
99 16.448242664337158
100 16.366745948791504
101 16.429394960403442
102 16.44488787651062
103 16.44531011581421
104 16.427890062332153
105 16.428178310394287
106 14.181495904922485
107 16.25045609474182
108 16.347804069519043
109 16.298154830932617
110 16.06649351119995
111 14.451817989349365
112 14.497419834136963
113 14.469001770019531
114 14.450904607772827
115 14.506790161132812
116 14.497270345687866
117 14.492510080337524
118 14.467281579971313
119 14.471914529800415
Done, saving (120, 320, 640, 3) (120, 320, 640)
extras:{'raw': tensor([[[-3.5283e-01, -5.3484e-01, -5.4060e-01, -2.5247e+01],
         [-2.7347e-01, -4.6194e-01, -5.0029e-01, -2.6585e+01],
         [ 3.5997e-01,  8.8791e-02, -7.2352e-02, -7.5075e+00],
         ...,
         [-4.5012e+00, -4.1116e+00, -2.1274e+00, -4.0679e+01],
         [-4.4754e+00, -4.1777e+00, -2.7792e+00, -2.8649e+01],
         [-4.6774e+00, -4.3993e+00, -3.1773e+00, -2.3492e+01]],

        [[ 8.3101e-02,  6.1772e-03, -3.6216e-02, -2.2198e+01],
         [ 7.3337e-02,  4.1005e-02, -2.3771e-01, -1.7035e+01],
         [-6.5042e-01, -7.0251e-01, -6.9540e-01,  6.3807e+00],
         ...,
         [ 1.2992e+01,  4.1786e+00, -8.5234e+00,  3.7752e+02],
         [ 1.1568e+01,  3.4868e+00, -8.2142e+00,  3.6605e+02],
         [ 1.3108e+01,  4.1172e+00, -8.4865e+00,  3.6165e+02]],

        [[-2.8549e+00, -2.8031e+00, -3.6065e+00,  3.0486e+00],
         [-2.7557e+00, -2.5674e+00, -3.4775e+00,  4.1765e+00],
         [-2.7676e+00, -2.5733e+00, -3.4847e+00,  4.2111e+00],
         ...,
         [-1.7105e-01, -1.9460e+00, -4.3713e+00,  1.6365e+02],
         [ 8.3248e-01, -8.3858e-01, -2.7922e+00,  1.8027e+02],
         [-1.1141e+00, -2.3448e+00, -4.4996e+00,  1.6406e+02]],

        ...,

        [[-9.3443e-01, -9.0096e-01, -4.2307e-01, -1.8897e+01],
         [ 7.7826e-01,  6.0445e-01,  5.3684e-01,  4.4994e+00],
         [ 1.0135e-01,  1.5207e-01,  4.6452e-01, -1.3305e+01],
         ...,
         [-5.6443e+00, -4.7125e+00, -3.6604e+00,  8.3795e+00],
         [-5.4981e+00, -4.3716e+00, -2.8257e+00,  1.3710e+01],
         [-7.7773e+00, -5.6704e+00, -2.5911e+00,  1.6540e+01]],

        [[-1.9332e+00, -2.0176e+00, -2.8538e+00,  4.5870e+00],
         [-2.6721e+00, -2.5697e+00, -3.3998e+00,  5.9563e+00],
         [-2.8218e+00, -2.6557e+00, -3.6635e+00,  5.9276e+00],
         ...,
         [-9.3506e-01, -1.7752e+00, -4.5700e+00,  2.3232e+02],
         [-1.3072e+00, -2.1485e+00, -4.8526e+00,  2.2912e+02],
         [-8.7606e-01, -1.5110e+00, -3.7781e+00,  2.2374e+02]],

        [[ 2.3077e-01,  1.6679e-02, -8.1561e-02,  1.0581e+01],
         [ 1.9641e-01, -4.7260e-03, -9.6043e-02,  8.6268e+00],
         [-3.9564e-03, -1.9109e-01, -3.3172e-01,  7.6008e+00],
         ...,
         [ 4.0850e+00,  2.4218e+00, -5.3740e-01,  2.7186e+02],
         [ 4.0914e+00,  2.4393e+00, -4.9719e-01,  2.7396e+02],
         [ 4.1616e+00,  2.4837e+00, -5.0384e-01,  2.7476e+02]]],
       grad_fn=<ReshapeAliasBackward0>), 'rgb0': tensor([[0.4261, 0.4048, 0.4678],
        [0.3268, 0.3121, 0.3290],
        [0.0436, 0.0493, 0.0292],
        ...,
        [0.6630, 0.6312, 0.6272],
        [0.0844, 0.1009, 0.0565],
        [0.5512, 0.4985, 0.4776]], grad_fn=<ReshapeAliasBackward0>), 'disp0': tensor([ 77.6457,  61.3916, 197.9150,  ...,  15.0700, 122.1573, 862.6360],
       grad_fn=<ReshapeAliasBackward0>), 'acc0': tensor([1., 1., 1.,  ..., 1., 1., 1.], grad_fn=<ReshapeAliasBackward0>), 'z_std': tensor([0.0030, 0.0024, 0.0165,  ..., 0.0023, 0.0019, 0.2860])}
0 0.0005040168762207031
torch.Size([320, 640, 3]) torch.Size([320, 640])
1 14.483141660690308
2 14.498521566390991
3 14.49198865890503
4 14.43761682510376
5 14.49028491973877
6 14.502079486846924
7 14.43714714050293
8 14.525797843933105
9 14.476787090301514
10 14.461092233657837
11 14.490299224853516
12 14.4740629196167
13 14.49703049659729
14 14.398293018341064
15 14.521364450454712
16 14.485608100891113
17 14.478748798370361
18 14.472047090530396
19 14.477460861206055
20 14.465973377227783
21 14.452878952026367
22 14.5001540184021
23 14.455150365829468
24 14.47553539276123
25 14.420795917510986
26 14.476628541946411
27 14.47012186050415
28 14.527652263641357
29 14.501784324645996
30 14.510251522064209
31 14.482877731323242
32 14.491906642913818
33 14.478559494018555
34 14.491734981536865
35 14.46443772315979
36 14.489955186843872
37 14.492292642593384
38 14.470358610153198
39 14.48571491241455
40 14.52202320098877
41 14.505154132843018
42 14.489977598190308
43 14.503514528274536
44 14.48305606842041
45 14.456146478652954
46 14.490855693817139
47 14.551712036132812
48 14.491628408432007
49 14.494212865829468
50 14.504870653152466
51 14.510822772979736
52 14.486019134521484
53 14.471208333969116
54 14.54469084739685
55 14.447554111480713
56 14.446598529815674
57 14.472935438156128
58 14.496560096740723
59 14.473693370819092
60 14.528504133224487
61 14.539098501205444
62 14.536495208740234
63 14.510302782058716
64 14.494571685791016
65 14.507691621780396
66 14.442146062850952
67 14.512658596038818
68 14.51080322265625
69 14.47818374633789
70 14.502757549285889
71 14.475126028060913
72 14.536052942276001
73 14.466972589492798
74 14.493674516677856
75 14.49421238899231
76 14.517756223678589
77 14.452800989151001
78 14.467512369155884
79 14.518687725067139
80 14.5140061378479
81 14.485966444015503
82 14.50314211845398
83 14.475772857666016
84 14.48902702331543
85 14.471335887908936
86 14.526234149932861
87 14.412357330322266
88 14.516016960144043
89 14.5122389793396
90 14.467806339263916
91 14.504602670669556
92 14.518258094787598
93 14.489157676696777
94 14.536405086517334
95 14.51258134841919
96 14.504332065582275
97 14.449970245361328
98 14.49437165260315
99 14.516429901123047
100 14.508092641830444
101 14.471527099609375
102 14.488204717636108
103 14.496304273605347
104 14.539933443069458
105 14.469288349151611
106 14.505068302154541
107 14.498708248138428
108 14.404231071472168
109 14.47936224937439
110 14.459619998931885
111 14.532273054122925
112 14.482819318771362
113 14.48386549949646
114 14.47142481803894
115 14.481133699417114
116 14.48792839050293
117 14.505510091781616
118 14.471403360366821
119 14.484342336654663
test poses shape torch.Size([13, 3, 4])
0 0.0006930828094482422
torch.Size([320, 640, 3]) torch.Size([320, 640])
1 14.529936790466309
2 14.536882877349854
3 14.494157314300537
4 14.569868087768555
5 14.535839319229126
6 14.528536558151245
7 14.555547714233398
8 14.49533748626709
9 14.549315214157104
10 14.543519496917725
11 14.516388893127441
12 14.488956451416016
Saved test set
[TRAIN] Iter: 400000 Loss: 0.027857061475515366  PSNR: 18.812585830688477
[TRAIN] Iter: 400100 Loss: 0.022671766579151154  PSNR: 20.00394630432129
[TRAIN] Iter: 400200 Loss: 0.03523293137550354  PSNR: 17.778579711914062
[TRAIN] Iter: 400300 Loss: 0.020311623811721802  PSNR: 20.07787322998047
[TRAIN] Iter: 400400 Loss: 0.030519582331180573  PSNR: 18.303754806518555
[TRAIN] Iter: 400500 Loss: 0.028419507667422295  PSNR: 18.624412536621094
[TRAIN] Iter: 400600 Loss: 0.025304915383458138  PSNR: 19.23447608947754
[TRAIN] Iter: 400700 Loss: 0.022198833525180817  PSNR: 19.90526580810547
[TRAIN] Iter: 400800 Loss: 0.02610662579536438  PSNR: 19.277027130126953
[TRAIN] Iter: 400900 Loss: 0.02673964574933052  PSNR: 19.276975631713867
[TRAIN] Iter: 401000 Loss: 0.027312785387039185  PSNR: 18.75109100341797
[TRAIN] Iter: 401100 Loss: 0.030228164047002792  PSNR: 18.332231521606445
[TRAIN] Iter: 401200 Loss: 0.026003459468483925  PSNR: 18.997665405273438
[TRAIN] Iter: 401300 Loss: 0.03200732171535492  PSNR: 18.11385154724121
[TRAIN] Iter: 401400 Loss: 0.026436667889356613  PSNR: 19.085460662841797
[TRAIN] Iter: 401500 Loss: 0.026058010756969452  PSNR: 19.08597183227539
[TRAIN] Iter: 401600 Loss: 0.02466660365462303  PSNR: 19.140745162963867
[TRAIN] Iter: 401700 Loss: 0.02769109606742859  PSNR: 18.92246437072754
[TRAIN] Iter: 401800 Loss: 0.023002784699201584  PSNR: 19.373655319213867
[TRAIN] Iter: 401900 Loss: 0.03415212780237198  PSNR: 17.878944396972656
[TRAIN] Iter: 402000 Loss: 0.030453864485025406  PSNR: 18.26620864868164
[TRAIN] Iter: 402100 Loss: 0.027375353500247  PSNR: 18.724748611450195
[TRAIN] Iter: 402200 Loss: 0.02542594075202942  PSNR: 19.04353141784668
[TRAIN] Iter: 402300 Loss: 0.01943807676434517  PSNR: 20.499732971191406
[TRAIN] Iter: 402400 Loss: 0.025499975308775902  PSNR: 19.21701431274414
[TRAIN] Iter: 402500 Loss: 0.0235020462423563  PSNR: 19.510644912719727
[TRAIN] Iter: 402600 Loss: 0.025555551052093506  PSNR: 19.24527359008789
[TRAIN] Iter: 402700 Loss: 0.03131342679262161  PSNR: 18.19004249572754
[TRAIN] Iter: 402800 Loss: 0.03187466040253639  PSNR: 18.210613250732422
[TRAIN] Iter: 402900 Loss: 0.01785164140164852  PSNR: 20.65813636779785
[TRAIN] Iter: 403000 Loss: 0.034371256828308105  PSNR: 17.819835662841797
[TRAIN] Iter: 403100 Loss: 0.0301176980137825  PSNR: 18.322202682495117
[TRAIN] Iter: 403200 Loss: 0.029218759387731552  PSNR: 18.432376861572266
[TRAIN] Iter: 403300 Loss: 0.02131592109799385  PSNR: 19.83121681213379
[TRAIN] Iter: 403400 Loss: 0.02866216003894806  PSNR: 18.592660903930664
[TRAIN] Iter: 403500 Loss: 0.026453092694282532  PSNR: 18.89969825744629
[TRAIN] Iter: 403600 Loss: 0.029007945209741592  PSNR: 18.45623016357422
[TRAIN] Iter: 403700 Loss: 0.026196464896202087  PSNR: 19.151779174804688
[TRAIN] Iter: 403800 Loss: 0.021835757419466972  PSNR: 19.733600616455078
[TRAIN] Iter: 403900 Loss: 0.02497546374797821  PSNR: 19.354660034179688
[TRAIN] Iter: 404000 Loss: 0.024449389427900314  PSNR: 19.569623947143555
[TRAIN] Iter: 404100 Loss: 0.022720862179994583  PSNR: 19.89591407775879
[TRAIN] Iter: 404200 Loss: 0.024234017357230186  PSNR: 19.45647430419922
[TRAIN] Iter: 404300 Loss: 0.02578415907919407  PSNR: 19.072004318237305
[TRAIN] Iter: 404400 Loss: 0.03184186667203903  PSNR: 18.110774993896484
[TRAIN] Iter: 404500 Loss: 0.03396028280258179  PSNR: 17.827972412109375
[TRAIN] Iter: 404600 Loss: 0.02750929817557335  PSNR: 18.460433959960938
[TRAIN] Iter: 404700 Loss: 0.024479687213897705  PSNR: 19.317516326904297
[TRAIN] Iter: 404800 Loss: 0.030995329841971397  PSNR: 18.20401954650879
[TRAIN] Iter: 404900 Loss: 0.029080836102366447  PSNR: 18.288393020629883
[TRAIN] Iter: 405000 Loss: 0.03148464486002922  PSNR: 18.258222579956055
[TRAIN] Iter: 405100 Loss: 0.02359768934547901  PSNR: 19.419017791748047
[TRAIN] Iter: 405200 Loss: 0.027427777647972107  PSNR: 18.972333908081055
[TRAIN] Iter: 405300 Loss: 0.03284519910812378  PSNR: 17.895944595336914
[TRAIN] Iter: 405400 Loss: 0.025699427351355553  PSNR: 19.307958602905273
[TRAIN] Iter: 405500 Loss: 0.02614101953804493  PSNR: 18.83414077758789
[TRAIN] Iter: 405600 Loss: 0.024421103298664093  PSNR: 19.57231330871582
[TRAIN] Iter: 405700 Loss: 0.022240083664655685  PSNR: 19.83559226989746
[TRAIN] Iter: 405800 Loss: 0.029209066182374954  PSNR: 18.53927230834961
[TRAIN] Iter: 405900 Loss: 0.026151027530431747  PSNR: 19.06561851501465
[TRAIN] Iter: 406000 Loss: 0.02322680689394474  PSNR: 19.373870849609375
[TRAIN] Iter: 406100 Loss: 0.027383632957935333  PSNR: 18.750625610351562
[TRAIN] Iter: 406200 Loss: 0.03408317267894745  PSNR: 17.739118576049805
[TRAIN] Iter: 406300 Loss: 0.027982989326119423  PSNR: 18.72127914428711
[TRAIN] Iter: 406400 Loss: 0.025561261922121048  PSNR: 18.947465896606445
[TRAIN] Iter: 406500 Loss: 0.0286303348839283  PSNR: 18.643892288208008
[TRAIN] Iter: 406600 Loss: 0.029296033084392548  PSNR: 18.542388916015625
[TRAIN] Iter: 406700 Loss: 0.023350924253463745  PSNR: 19.43501091003418
[TRAIN] Iter: 406800 Loss: 0.03018118068575859  PSNR: 18.528413772583008
[TRAIN] Iter: 406900 Loss: 0.024909526109695435  PSNR: 19.457956314086914
[TRAIN] Iter: 407000 Loss: 0.0254225917160511  PSNR: 19.009624481201172
[TRAIN] Iter: 407100 Loss: 0.03245839476585388  PSNR: 18.00524139404297
[TRAIN] Iter: 407200 Loss: 0.03262792155146599  PSNR: 18.047067642211914
[TRAIN] Iter: 407300 Loss: 0.026566827669739723  PSNR: 18.91280174255371
[TRAIN] Iter: 407400 Loss: 0.028361491858959198  PSNR: 18.774686813354492
[TRAIN] Iter: 407500 Loss: 0.026095502078533173  PSNR: 19.094318389892578
[TRAIN] Iter: 407600 Loss: 0.028575092554092407  PSNR: 18.76702308654785
[TRAIN] Iter: 407700 Loss: 0.026838265359401703  PSNR: 18.803930282592773
[TRAIN] Iter: 407800 Loss: 0.02474544569849968  PSNR: 19.511457443237305
[TRAIN] Iter: 407900 Loss: 0.027751542627811432  PSNR: 18.632307052612305
[TRAIN] Iter: 408000 Loss: 0.02836153842508793  PSNR: 18.610370635986328
[TRAIN] Iter: 408100 Loss: 0.025850810110569  PSNR: 19.087068557739258
[TRAIN] Iter: 408200 Loss: 0.023766323924064636  PSNR: 19.521373748779297
[TRAIN] Iter: 408300 Loss: 0.02113664522767067  PSNR: 19.890947341918945
[TRAIN] Iter: 408400 Loss: 0.027250593528151512  PSNR: 18.91950035095215
[TRAIN] Iter: 408500 Loss: 0.02099721133708954  PSNR: 19.91048240661621
[TRAIN] Iter: 408600 Loss: 0.02529614418745041  PSNR: 19.07347869873047
[TRAIN] Iter: 408700 Loss: 0.023499296978116035  PSNR: 19.501834869384766
[TRAIN] Iter: 408800 Loss: 0.022647395730018616  PSNR: 19.65004539489746
[TRAIN] Iter: 408900 Loss: 0.02658633142709732  PSNR: 19.082096099853516
[TRAIN] Iter: 409000 Loss: 0.027079394087195396  PSNR: 18.916173934936523
[TRAIN] Iter: 409100 Loss: 0.027470193803310394  PSNR: 18.895307540893555
[TRAIN] Iter: 409200 Loss: 0.026208054274320602  PSNR: 19.184810638427734
[TRAIN] Iter: 409300 Loss: 0.03240741789340973  PSNR: 18.0877685546875
[TRAIN] Iter: 409400 Loss: 0.02464604377746582  PSNR: 19.450674057006836
[TRAIN] Iter: 409500 Loss: 0.02636120840907097  PSNR: 18.95868682861328
[TRAIN] Iter: 409600 Loss: 0.024846509099006653  PSNR: 19.1716365814209
[TRAIN] Iter: 409700 Loss: 0.024509448558092117  PSNR: 19.216337203979492
[TRAIN] Iter: 409800 Loss: 0.022836606949567795  PSNR: 19.641551971435547
[TRAIN] Iter: 409900 Loss: 0.028900858014822006  PSNR: 18.53631019592285
Saved checkpoints at ./logs/TUT-LAB-nerf/410000.tar
[TRAIN] Iter: 410000 Loss: 0.02535567432641983  PSNR: 19.259225845336914
[TRAIN] Iter: 410100 Loss: 0.028706571087241173  PSNR: 18.879491806030273
[TRAIN] Iter: 410200 Loss: 0.027109824120998383  PSNR: 18.828227996826172
[TRAIN] Iter: 410300 Loss: 0.028878632932901382  PSNR: 18.634140014648438
[TRAIN] Iter: 410400 Loss: 0.02813452109694481  PSNR: 18.697391510009766
[TRAIN] Iter: 410500 Loss: 0.030717140063643456  PSNR: 18.247392654418945
[TRAIN] Iter: 410600 Loss: 0.02958507463335991  PSNR: 18.323572158813477
[TRAIN] Iter: 410700 Loss: 0.028388135135173798  PSNR: 18.693647384643555
[TRAIN] Iter: 410800 Loss: 0.027935191988945007  PSNR: 18.77045249938965
[TRAIN] Iter: 410900 Loss: 0.026575163006782532  PSNR: 18.77566909790039
[TRAIN] Iter: 411000 Loss: 0.031054357066750526  PSNR: 18.295032501220703
[TRAIN] Iter: 411100 Loss: 0.029970332980155945  PSNR: 18.441761016845703
[TRAIN] Iter: 411200 Loss: 0.024194851517677307  PSNR: 19.178577423095703
[TRAIN] Iter: 411300 Loss: 0.023455210030078888  PSNR: 19.846012115478516
[TRAIN] Iter: 411400 Loss: 0.024081379175186157  PSNR: 19.26894187927246
[TRAIN] Iter: 411500 Loss: 0.02400573156774044  PSNR: 19.219242095947266
[TRAIN] Iter: 411600 Loss: 0.0225997231900692  PSNR: 19.74285125732422
[TRAIN] Iter: 411700 Loss: 0.030355503782629967  PSNR: 18.392383575439453
[TRAIN] Iter: 411800 Loss: 0.027595099061727524  PSNR: 19.080154418945312
[TRAIN] Iter: 411900 Loss: 0.025261960923671722  PSNR: 19.134082794189453
[TRAIN] Iter: 412000 Loss: 0.02838953211903572  PSNR: 18.806955337524414
[TRAIN] Iter: 412100 Loss: 0.025903042405843735  PSNR: 19.08436393737793
[TRAIN] Iter: 412200 Loss: 0.025817211717367172  PSNR: 18.956918716430664
[TRAIN] Iter: 412300 Loss: 0.026225009933114052  PSNR: 19.369510650634766
[TRAIN] Iter: 412400 Loss: 0.025496279820799828  PSNR: 19.16712188720703
[TRAIN] Iter: 412500 Loss: 0.02510068006813526  PSNR: 19.00157928466797
[TRAIN] Iter: 412600 Loss: 0.02871621772646904  PSNR: 18.53566551208496
[TRAIN] Iter: 412700 Loss: 0.024499623104929924  PSNR: 19.404598236083984
[TRAIN] Iter: 412800 Loss: 0.025574402883648872  PSNR: 19.103137969970703
[TRAIN] Iter: 412900 Loss: 0.03174024820327759  PSNR: 18.214956283569336
[TRAIN] Iter: 413000 Loss: 0.03328487649559975  PSNR: 17.894901275634766
[TRAIN] Iter: 413100 Loss: 0.03074256330728531  PSNR: 18.30760955810547
[TRAIN] Iter: 413200 Loss: 0.029622970148921013  PSNR: 18.427406311035156
[TRAIN] Iter: 413300 Loss: 0.02661837823688984  PSNR: 18.98106575012207
[TRAIN] Iter: 413400 Loss: 0.028547950088977814  PSNR: 18.54800796508789
[TRAIN] Iter: 413500 Loss: 0.030986491590738297  PSNR: 18.64350700378418
[TRAIN] Iter: 413600 Loss: 0.02277049794793129  PSNR: 19.72820281982422
[TRAIN] Iter: 413700 Loss: 0.025547176599502563  PSNR: 19.2036190032959
[TRAIN] Iter: 413800 Loss: 0.031159671023488045  PSNR: 18.168827056884766
[TRAIN] Iter: 413900 Loss: 0.02484547719359398  PSNR: 19.150501251220703
[TRAIN] Iter: 414000 Loss: 0.023326193913817406  PSNR: 19.543540954589844
[TRAIN] Iter: 414100 Loss: 0.030817542225122452  PSNR: 18.264949798583984
[TRAIN] Iter: 414200 Loss: 0.021593144163489342  PSNR: 19.801471710205078
[TRAIN] Iter: 414300 Loss: 0.0217999629676342  PSNR: 19.81310272216797
[TRAIN] Iter: 414400 Loss: 0.03154012933373451  PSNR: 18.19704246520996
[TRAIN] Iter: 414500 Loss: 0.02555903047323227  PSNR: 18.943431854248047
[TRAIN] Iter: 414600 Loss: 0.02593209780752659  PSNR: 19.181968688964844
[TRAIN] Iter: 414700 Loss: 0.024945326149463654  PSNR: 18.963916778564453
[TRAIN] Iter: 414800 Loss: 0.023754987865686417  PSNR: 19.441530227661133
[TRAIN] Iter: 414900 Loss: 0.025590084493160248  PSNR: 19.14569854736328
[TRAIN] Iter: 415000 Loss: 0.02568116784095764  PSNR: 19.027267456054688
[TRAIN] Iter: 415100 Loss: 0.02528882399201393  PSNR: 19.046504974365234
[TRAIN] Iter: 415200 Loss: 0.029174499213695526  PSNR: 18.51498794555664
[TRAIN] Iter: 415300 Loss: 0.026402190327644348  PSNR: 19.056442260742188
[TRAIN] Iter: 415400 Loss: 0.02503795176744461  PSNR: 18.92005157470703
[TRAIN] Iter: 415500 Loss: 0.0290607288479805  PSNR: 18.4446964263916
[TRAIN] Iter: 415600 Loss: 0.024923402816057205  PSNR: 19.22038459777832
[TRAIN] Iter: 415700 Loss: 0.03179033845663071  PSNR: 18.388479232788086
[TRAIN] Iter: 415800 Loss: 0.02948593534529209  PSNR: 18.477867126464844
[TRAIN] Iter: 415900 Loss: 0.02030579373240471  PSNR: 20.050861358642578
[TRAIN] Iter: 416000 Loss: 0.028939560055732727  PSNR: 18.605405807495117
[TRAIN] Iter: 416100 Loss: 0.02517854794859886  PSNR: 19.33536720275879
[TRAIN] Iter: 416200 Loss: 0.031094102188944817  PSNR: 18.427433013916016
[TRAIN] Iter: 416300 Loss: 0.019975552335381508  PSNR: 20.23540687561035
[TRAIN] Iter: 416400 Loss: 0.026106584817171097  PSNR: 19.12322425842285
[TRAIN] Iter: 416500 Loss: 0.01933915540575981  PSNR: 20.19080924987793
[TRAIN] Iter: 416600 Loss: 0.027306562289595604  PSNR: 18.738012313842773
[TRAIN] Iter: 416700 Loss: 0.027987228706479073  PSNR: 18.588214874267578
[TRAIN] Iter: 416800 Loss: 0.026243899017572403  PSNR: 19.433637619018555
[TRAIN] Iter: 416900 Loss: 0.023609250783920288  PSNR: 19.139692306518555
[TRAIN] Iter: 417000 Loss: 0.029209274798631668  PSNR: 18.526456832885742
[TRAIN] Iter: 417100 Loss: 0.027013026177883148  PSNR: 18.899049758911133
[TRAIN] Iter: 417200 Loss: 0.027376381680369377  PSNR: 18.900726318359375
[TRAIN] Iter: 417300 Loss: 0.02935720980167389  PSNR: 18.456357955932617
[TRAIN] Iter: 417400 Loss: 0.026111287996172905  PSNR: 19.131948471069336
[TRAIN] Iter: 417500 Loss: 0.02902538701891899  PSNR: 18.57962989807129
[TRAIN] Iter: 417600 Loss: 0.024269241839647293  PSNR: 19.332674026489258
[TRAIN] Iter: 417700 Loss: 0.026989605277776718  PSNR: 18.706783294677734
[TRAIN] Iter: 417800 Loss: 0.02427978254854679  PSNR: 19.432085037231445
[TRAIN] Iter: 417900 Loss: 0.02523662894964218  PSNR: 19.2159366607666
[TRAIN] Iter: 418000 Loss: 0.029571933671832085  PSNR: 18.37621307373047
[TRAIN] Iter: 418100 Loss: 0.026641376316547394  PSNR: 18.809728622436523
[TRAIN] Iter: 418200 Loss: 0.02529427781701088  PSNR: 19.208139419555664
[TRAIN] Iter: 418300 Loss: 0.02721102349460125  PSNR: 18.810935974121094
[TRAIN] Iter: 418400 Loss: 0.027164209634065628  PSNR: 18.882457733154297
[TRAIN] Iter: 418500 Loss: 0.019657107070088387  PSNR: 20.6729736328125
[TRAIN] Iter: 418600 Loss: 0.028241567313671112  PSNR: 18.84259605407715
[TRAIN] Iter: 418700 Loss: 0.02551944926381111  PSNR: 19.0759220123291
[TRAIN] Iter: 418800 Loss: 0.019691888242959976  PSNR: 20.085723876953125
[TRAIN] Iter: 418900 Loss: 0.03057527169585228  PSNR: 18.341026306152344
[TRAIN] Iter: 419000 Loss: 0.026205897331237793  PSNR: 18.86713409423828
[TRAIN] Iter: 419100 Loss: 0.023998042568564415  PSNR: 19.105873107910156
[TRAIN] Iter: 419200 Loss: 0.031385742127895355  PSNR: 18.289804458618164
[TRAIN] Iter: 419300 Loss: 0.028481967747211456  PSNR: 18.625946044921875
[TRAIN] Iter: 419400 Loss: 0.028609758242964745  PSNR: 18.568275451660156
[TRAIN] Iter: 419500 Loss: 0.028834231197834015  PSNR: 18.606992721557617
[TRAIN] Iter: 419600 Loss: 0.023254334926605225  PSNR: 19.393266677856445
[TRAIN] Iter: 419700 Loss: 0.02677020989358425  PSNR: 18.878524780273438
[TRAIN] Iter: 419800 Loss: 0.025344710797071457  PSNR: 19.431562423706055
[TRAIN] Iter: 419900 Loss: 0.03148764371871948  PSNR: 18.087308883666992
Saved checkpoints at ./logs/TUT-LAB-nerf/420000.tar
[TRAIN] Iter: 420000 Loss: 0.02623900957405567  PSNR: 18.968921661376953
[TRAIN] Iter: 420100 Loss: 0.032939422875642776  PSNR: 17.9480037689209
[TRAIN] Iter: 420200 Loss: 0.02995188720524311  PSNR: 18.49582862854004
[TRAIN] Iter: 420300 Loss: 0.034008920192718506  PSNR: 17.798858642578125
[TRAIN] Iter: 420400 Loss: 0.03404199331998825  PSNR: 17.91131591796875
[TRAIN] Iter: 420500 Loss: 0.03214127942919731  PSNR: 18.12122917175293
[TRAIN] Iter: 420600 Loss: 0.024958312511444092  PSNR: 19.439529418945312
[TRAIN] Iter: 420700 Loss: 0.022817978635430336  PSNR: 19.788644790649414
[TRAIN] Iter: 420800 Loss: 0.024753928184509277  PSNR: 19.346786499023438
[TRAIN] Iter: 420900 Loss: 0.027518434450030327  PSNR: 19.137020111083984
[TRAIN] Iter: 421000 Loss: 0.020366467535495758  PSNR: 20.066377639770508
[TRAIN] Iter: 421100 Loss: 0.025140129029750824  PSNR: 18.750322341918945
[TRAIN] Iter: 421200 Loss: 0.024538377299904823  PSNR: 19.330490112304688
[TRAIN] Iter: 421300 Loss: 0.026794716715812683  PSNR: 18.86829376220703
[TRAIN] Iter: 421400 Loss: 0.023642703890800476  PSNR: 19.20763397216797
[TRAIN] Iter: 421500 Loss: 0.030035262927412987  PSNR: 18.319433212280273
[TRAIN] Iter: 421600 Loss: 0.020878706127405167  PSNR: 19.923124313354492
[TRAIN] Iter: 421700 Loss: 0.031213466078042984  PSNR: 18.135868072509766
[TRAIN] Iter: 421800 Loss: 0.027865026146173477  PSNR: 18.655101776123047
[TRAIN] Iter: 421900 Loss: 0.021207984536886215  PSNR: 19.880077362060547
[TRAIN] Iter: 422000 Loss: 0.018667802214622498  PSNR: 20.435333251953125
[TRAIN] Iter: 422100 Loss: 0.02582709677517414  PSNR: 19.503490447998047
[TRAIN] Iter: 422200 Loss: 0.023885171860456467  PSNR: 18.979543685913086
[TRAIN] Iter: 422300 Loss: 0.02363591268658638  PSNR: 19.824724197387695
[TRAIN] Iter: 422400 Loss: 0.030863799154758453  PSNR: 18.33313751220703
[TRAIN] Iter: 422500 Loss: 0.022978924214839935  PSNR: 19.405698776245117
[TRAIN] Iter: 422600 Loss: 0.02463669702410698  PSNR: 19.38083839416504
[TRAIN] Iter: 422700 Loss: 0.026089362800121307  PSNR: 18.985862731933594
[TRAIN] Iter: 422800 Loss: 0.02181089296936989  PSNR: 20.058021545410156
[TRAIN] Iter: 422900 Loss: 0.02939501777291298  PSNR: 18.442440032958984
[TRAIN] Iter: 423000 Loss: 0.02456982061266899  PSNR: 18.815515518188477
[TRAIN] Iter: 423100 Loss: 0.028564680367708206  PSNR: 18.487810134887695
[TRAIN] Iter: 423200 Loss: 0.029533162713050842  PSNR: 18.35929298400879
[TRAIN] Iter: 423300 Loss: 0.023632358759641647  PSNR: 19.339370727539062
[TRAIN] Iter: 423400 Loss: 0.02538030408322811  PSNR: 19.251829147338867
[TRAIN] Iter: 423500 Loss: 0.025052689015865326  PSNR: 19.211929321289062
[TRAIN] Iter: 423600 Loss: 0.022398438304662704  PSNR: 19.56712532043457
[TRAIN] Iter: 423700 Loss: 0.032258398830890656  PSNR: 18.05477523803711
[TRAIN] Iter: 423800 Loss: 0.026702260598540306  PSNR: 19.25179672241211
[TRAIN] Iter: 423900 Loss: 0.0255038570612669  PSNR: 19.114421844482422
[TRAIN] Iter: 424000 Loss: 0.02941829152405262  PSNR: 18.402402877807617
[TRAIN] Iter: 424100 Loss: 0.02731749787926674  PSNR: 18.852294921875
[TRAIN] Iter: 424200 Loss: 0.021806485950946808  PSNR: 19.970603942871094
[TRAIN] Iter: 424300 Loss: 0.02732931636273861  PSNR: 18.890331268310547
[TRAIN] Iter: 424400 Loss: 0.027857715263962746  PSNR: 19.04587745666504
[TRAIN] Iter: 424500 Loss: 0.03114466741681099  PSNR: 18.36029624938965
[TRAIN] Iter: 424600 Loss: 0.025677457451820374  PSNR: 19.042266845703125
[TRAIN] Iter: 424700 Loss: 0.025353169068694115  PSNR: 18.993423461914062
[TRAIN] Iter: 424800 Loss: 0.02802347019314766  PSNR: 18.690608978271484
[TRAIN] Iter: 424900 Loss: 0.02528814971446991  PSNR: 19.062345504760742
[TRAIN] Iter: 425000 Loss: 0.024773839861154556  PSNR: 19.3555965423584
[TRAIN] Iter: 425100 Loss: 0.029221031814813614  PSNR: 18.310840606689453
[TRAIN] Iter: 425200 Loss: 0.027252690866589546  PSNR: 18.974275588989258
[TRAIN] Iter: 425300 Loss: 0.029957126826047897  PSNR: 18.405696868896484
[TRAIN] Iter: 425400 Loss: 0.030191507190465927  PSNR: 18.3895206451416
[TRAIN] Iter: 425500 Loss: 0.029603447765111923  PSNR: 18.484384536743164
[TRAIN] Iter: 425600 Loss: 0.02256764844059944  PSNR: 19.778018951416016
[TRAIN] Iter: 425700 Loss: 0.02740856260061264  PSNR: 18.92030906677246
[TRAIN] Iter: 425800 Loss: 0.02839018404483795  PSNR: 18.65461540222168
[TRAIN] Iter: 425900 Loss: 0.02830270491540432  PSNR: 18.508201599121094
[TRAIN] Iter: 426000 Loss: 0.025267604738473892  PSNR: 19.03497314453125
[TRAIN] Iter: 426100 Loss: 0.03045835718512535  PSNR: 18.39828109741211
[TRAIN] Iter: 426200 Loss: 0.026339668780565262  PSNR: 18.954622268676758
[TRAIN] Iter: 426300 Loss: 0.027954725548624992  PSNR: 18.723793029785156
[TRAIN] Iter: 426400 Loss: 0.029437273740768433  PSNR: 18.881614685058594
[TRAIN] Iter: 426500 Loss: 0.030030900612473488  PSNR: 18.627300262451172
[TRAIN] Iter: 426600 Loss: 0.03234943747520447  PSNR: 18.018871307373047
[TRAIN] Iter: 426700 Loss: 0.02584903873503208  PSNR: 19.22351837158203
[TRAIN] Iter: 426800 Loss: 0.03381873667240143  PSNR: 17.938058853149414
[TRAIN] Iter: 426900 Loss: 0.028489287942647934  PSNR: 18.693933486938477
[TRAIN] Iter: 427000 Loss: 0.025792071595788002  PSNR: 18.976119995117188
[TRAIN] Iter: 427100 Loss: 0.019791189581155777  PSNR: 20.212717056274414
[TRAIN] Iter: 427200 Loss: 0.02766082063317299  PSNR: 18.891279220581055
[TRAIN] Iter: 427300 Loss: 0.026406530290842056  PSNR: 18.758602142333984
[TRAIN] Iter: 427400 Loss: 0.025947535410523415  PSNR: 19.082918167114258
[TRAIN] Iter: 427500 Loss: 0.027451232075691223  PSNR: 18.85870361328125
[TRAIN] Iter: 427600 Loss: 0.020063208416104317  PSNR: 20.029563903808594
[TRAIN] Iter: 427700 Loss: 0.030925404280424118  PSNR: 18.10862922668457
[TRAIN] Iter: 427800 Loss: 0.030613470822572708  PSNR: 18.352046966552734
[TRAIN] Iter: 427900 Loss: 0.02402397058904171  PSNR: 19.22088623046875
[TRAIN] Iter: 428000 Loss: 0.028352204710245132  PSNR: 18.660659790039062
[TRAIN] Iter: 428100 Loss: 0.02939164638519287  PSNR: 18.410497665405273
[TRAIN] Iter: 428200 Loss: 0.022913049906492233  PSNR: 19.612815856933594
[TRAIN] Iter: 428300 Loss: 0.022668976336717606  PSNR: 19.61249351501465
[TRAIN] Iter: 428400 Loss: 0.020896002650260925  PSNR: 19.745708465576172
[TRAIN] Iter: 428500 Loss: 0.026589669287204742  PSNR: 19.11186981201172
[TRAIN] Iter: 428600 Loss: 0.027233753353357315  PSNR: 19.06283950805664
[TRAIN] Iter: 428700 Loss: 0.02997046709060669  PSNR: 18.418310165405273
[TRAIN] Iter: 428800 Loss: 0.024204149842262268  PSNR: 19.18972396850586
[TRAIN] Iter: 428900 Loss: 0.029477344825863838  PSNR: 18.608150482177734
[TRAIN] Iter: 429000 Loss: 0.02207808941602707  PSNR: 19.693885803222656
[TRAIN] Iter: 429100 Loss: 0.026423582807183266  PSNR: 19.015092849731445
[TRAIN] Iter: 429200 Loss: 0.02768416330218315  PSNR: 18.80666732788086
[TRAIN] Iter: 429300 Loss: 0.025023315101861954  PSNR: 19.177162170410156
[TRAIN] Iter: 429400 Loss: 0.019521024078130722  PSNR: 20.203454971313477
[TRAIN] Iter: 429500 Loss: 0.026907838881015778  PSNR: 19.073932647705078
[TRAIN] Iter: 429600 Loss: 0.026682063937187195  PSNR: 18.94119644165039
[TRAIN] Iter: 429700 Loss: 0.02633422054350376  PSNR: 18.876150131225586
[TRAIN] Iter: 429800 Loss: 0.028616156429052353  PSNR: 18.684598922729492
[TRAIN] Iter: 429900 Loss: 0.024242132902145386  PSNR: 19.58176612854004
Saved checkpoints at ./logs/TUT-LAB-nerf/430000.tar
[TRAIN] Iter: 430000 Loss: 0.032242074608802795  PSNR: 18.281862258911133
[TRAIN] Iter: 430100 Loss: 0.029107321053743362  PSNR: 18.84836196899414
[TRAIN] Iter: 430200 Loss: 0.027376994490623474  PSNR: 18.79936981201172
[TRAIN] Iter: 430300 Loss: 0.021212752908468246  PSNR: 19.988929748535156
[TRAIN] Iter: 430400 Loss: 0.02613525092601776  PSNR: 18.941925048828125
[TRAIN] Iter: 430500 Loss: 0.030167514458298683  PSNR: 18.544713973999023
[TRAIN] Iter: 430600 Loss: 0.0276847705245018  PSNR: 18.828205108642578
[TRAIN] Iter: 430700 Loss: 0.021876905113458633  PSNR: 19.75789451599121
[TRAIN] Iter: 430800 Loss: 0.031124338507652283  PSNR: 18.172021865844727
[TRAIN] Iter: 430900 Loss: 0.02571825683116913  PSNR: 19.49789810180664
[TRAIN] Iter: 431000 Loss: 0.026062561199069023  PSNR: 18.884746551513672
[TRAIN] Iter: 431100 Loss: 0.02243577316403389  PSNR: 19.719280242919922
[TRAIN] Iter: 431200 Loss: 0.02458895370364189  PSNR: 19.180734634399414
[TRAIN] Iter: 431300 Loss: 0.030734064057469368  PSNR: 18.363727569580078
[TRAIN] Iter: 431400 Loss: 0.0275932215154171  PSNR: 18.68634033203125
[TRAIN] Iter: 431500 Loss: 0.026584981009364128  PSNR: 19.13560676574707
[TRAIN] Iter: 431600 Loss: 0.029028482735157013  PSNR: 18.54656410217285
[TRAIN] Iter: 431700 Loss: 0.022659482434391975  PSNR: 19.309917449951172
[TRAIN] Iter: 431800 Loss: 0.024949226528406143  PSNR: 19.39264678955078
[TRAIN] Iter: 431900 Loss: 0.026194779202342033  PSNR: 19.389301300048828
[TRAIN] Iter: 432000 Loss: 0.022870682179927826  PSNR: 19.779569625854492
[TRAIN] Iter: 432100 Loss: 0.024425234645605087  PSNR: 19.10235595703125
[TRAIN] Iter: 432200 Loss: 0.030346553772687912  PSNR: 18.200918197631836
[TRAIN] Iter: 432300 Loss: 0.028003843501210213  PSNR: 18.67633056640625
[TRAIN] Iter: 432400 Loss: 0.0276334285736084  PSNR: 19.10335350036621
[TRAIN] Iter: 432500 Loss: 0.02355443499982357  PSNR: 19.45632553100586
[TRAIN] Iter: 432600 Loss: 0.02823651023209095  PSNR: 18.646183013916016
[TRAIN] Iter: 432700 Loss: 0.028783196583390236  PSNR: 18.579547882080078
[TRAIN] Iter: 432800 Loss: 0.025865985080599785  PSNR: 19.039382934570312
[TRAIN] Iter: 432900 Loss: 0.03192298486828804  PSNR: 18.122692108154297
[TRAIN] Iter: 433000 Loss: 0.020886380225419998  PSNR: 19.953632354736328
[TRAIN] Iter: 433100 Loss: 0.03404010087251663  PSNR: 17.855501174926758
[TRAIN] Iter: 433200 Loss: 0.02366136573255062  PSNR: 19.495756149291992
[TRAIN] Iter: 433300 Loss: 0.023465726524591446  PSNR: 19.72625732421875
[TRAIN] Iter: 433400 Loss: 0.02645106054842472  PSNR: 18.776884078979492
[TRAIN] Iter: 433500 Loss: 0.022581912577152252  PSNR: 19.458967208862305
[TRAIN] Iter: 433600 Loss: 0.027551721781492233  PSNR: 18.682498931884766
[TRAIN] Iter: 433700 Loss: 0.032846830785274506  PSNR: 17.94864273071289
[TRAIN] Iter: 433800 Loss: 0.02565751224756241  PSNR: 18.91098403930664
[TRAIN] Iter: 433900 Loss: 0.025751236826181412  PSNR: 19.3687744140625
[TRAIN] Iter: 434000 Loss: 0.03544578701257706  PSNR: 17.604721069335938
[TRAIN] Iter: 434100 Loss: 0.02596592903137207  PSNR: 19.14961814880371
[TRAIN] Iter: 434200 Loss: 0.025515027344226837  PSNR: 18.91754722595215
[TRAIN] Iter: 434300 Loss: 0.02608170174062252  PSNR: 19.35586929321289
[TRAIN] Iter: 434400 Loss: 0.03126048296689987  PSNR: 18.195449829101562
[TRAIN] Iter: 434500 Loss: 0.02829526551067829  PSNR: 18.57247543334961
[TRAIN] Iter: 434600 Loss: 0.024773959070444107  PSNR: 19.338899612426758
[TRAIN] Iter: 434700 Loss: 0.023858629167079926  PSNR: 19.769092559814453
[TRAIN] Iter: 434800 Loss: 0.033580467104911804  PSNR: 18.055553436279297
[TRAIN] Iter: 434900 Loss: 0.027427781373262405  PSNR: 18.956283569335938
[TRAIN] Iter: 435000 Loss: 0.029503190889954567  PSNR: 18.42816734313965
[TRAIN] Iter: 435100 Loss: 0.03263499587774277  PSNR: 17.84726333618164
[TRAIN] Iter: 435200 Loss: 0.03194544464349747  PSNR: 18.143661499023438
[TRAIN] Iter: 435300 Loss: 0.03185495734214783  PSNR: 18.06769561767578
[TRAIN] Iter: 435400 Loss: 0.028868764638900757  PSNR: 18.524147033691406
[TRAIN] Iter: 435500 Loss: 0.029180143028497696  PSNR: 18.525142669677734
[TRAIN] Iter: 435600 Loss: 0.01952085830271244  PSNR: 20.095497131347656
[TRAIN] Iter: 435700 Loss: 0.03358200192451477  PSNR: 17.88076400756836
[TRAIN] Iter: 435800 Loss: 0.030003350228071213  PSNR: 18.3577823638916
[TRAIN] Iter: 435900 Loss: 0.025568615645170212  PSNR: 19.023082733154297
[TRAIN] Iter: 436000 Loss: 0.0227351151406765  PSNR: 19.499584197998047
[TRAIN] Iter: 436100 Loss: 0.03348299115896225  PSNR: 17.937702178955078
[TRAIN] Iter: 436200 Loss: 0.020353473722934723  PSNR: 20.090404510498047
[TRAIN] Iter: 436300 Loss: 0.020727403461933136  PSNR: 19.862699508666992
[TRAIN] Iter: 436400 Loss: 0.029709257185459137  PSNR: 18.393108367919922
[TRAIN] Iter: 436500 Loss: 0.028776533901691437  PSNR: 18.63433265686035
[TRAIN] Iter: 436600 Loss: 0.02146448940038681  PSNR: 19.704975128173828
[TRAIN] Iter: 436700 Loss: 0.030366595834493637  PSNR: 18.596460342407227
[TRAIN] Iter: 436800 Loss: 0.025437459349632263  PSNR: 18.973976135253906
[TRAIN] Iter: 436900 Loss: 0.03240043297410011  PSNR: 18.091121673583984
[TRAIN] Iter: 437000 Loss: 0.02992020919919014  PSNR: 18.416839599609375
[TRAIN] Iter: 437100 Loss: 0.023467544466257095  PSNR: 19.629878997802734
[TRAIN] Iter: 437200 Loss: 0.027777714654803276  PSNR: 18.63469886779785
[TRAIN] Iter: 437300 Loss: 0.02958846092224121  PSNR: 18.439550399780273
[TRAIN] Iter: 437400 Loss: 0.02062922529876232  PSNR: 20.169666290283203
[TRAIN] Iter: 437500 Loss: 0.02030591480433941  PSNR: 20.026103973388672
[TRAIN] Iter: 437600 Loss: 0.025968065485358238  PSNR: 19.05930519104004
[TRAIN] Iter: 437700 Loss: 0.025039268657565117  PSNR: 19.245763778686523
[TRAIN] Iter: 437800 Loss: 0.02432575635612011  PSNR: 19.060733795166016
[TRAIN] Iter: 437900 Loss: 0.03137867897748947  PSNR: 18.37668228149414
[TRAIN] Iter: 438000 Loss: 0.029314188286662102  PSNR: 18.577259063720703
[TRAIN] Iter: 438100 Loss: 0.02231314219534397  PSNR: 19.561325073242188
[TRAIN] Iter: 438200 Loss: 0.021854713559150696  PSNR: 19.929122924804688
[TRAIN] Iter: 438300 Loss: 0.024053961038589478  PSNR: 19.3046932220459
[TRAIN] Iter: 438400 Loss: 0.02372119575738907  PSNR: 19.110891342163086
[TRAIN] Iter: 438500 Loss: 0.02411392331123352  PSNR: 19.278751373291016
[TRAIN] Iter: 438600 Loss: 0.02255478873848915  PSNR: 19.274045944213867
[TRAIN] Iter: 438700 Loss: 0.027723398059606552  PSNR: 18.65279197692871
[TRAIN] Iter: 438800 Loss: 0.028085626661777496  PSNR: 18.771427154541016
[TRAIN] Iter: 438900 Loss: 0.029177729040384293  PSNR: 18.557722091674805
[TRAIN] Iter: 439000 Loss: 0.025540031492710114  PSNR: 19.18110466003418
[TRAIN] Iter: 439100 Loss: 0.029457349330186844  PSNR: 18.2197322845459
[TRAIN] Iter: 439200 Loss: 0.03261040151119232  PSNR: 18.068429946899414
[TRAIN] Iter: 439300 Loss: 0.03367788717150688  PSNR: 17.794652938842773
[TRAIN] Iter: 439400 Loss: 0.02315174788236618  PSNR: 19.345834732055664
[TRAIN] Iter: 439500 Loss: 0.02672801911830902  PSNR: 18.886831283569336
[TRAIN] Iter: 439600 Loss: 0.02944077178835869  PSNR: 18.389076232910156
[TRAIN] Iter: 439700 Loss: 0.02076614275574684  PSNR: 20.01474952697754
[TRAIN] Iter: 439800 Loss: 0.028236273676156998  PSNR: 18.793432235717773
[TRAIN] Iter: 439900 Loss: 0.027802200987935066  PSNR: 18.718521118164062
Saved checkpoints at ./logs/TUT-LAB-nerf/440000.tar
[TRAIN] Iter: 440000 Loss: 0.02232542261481285  PSNR: 19.551328659057617
[TRAIN] Iter: 440100 Loss: 0.030928999185562134  PSNR: 18.43909454345703
[TRAIN] Iter: 440200 Loss: 0.029651764780282974  PSNR: 18.386892318725586
[TRAIN] Iter: 440300 Loss: 0.023199204355478287  PSNR: 19.62849235534668
[TRAIN] Iter: 440400 Loss: 0.03044573776423931  PSNR: 18.165130615234375
[TRAIN] Iter: 440500 Loss: 0.026042846962809563  PSNR: 19.1845645904541
[TRAIN] Iter: 440600 Loss: 0.02954379841685295  PSNR: 18.411283493041992
[TRAIN] Iter: 440700 Loss: 0.026441365480422974  PSNR: 18.97001838684082
[TRAIN] Iter: 440800 Loss: 0.027208300307393074  PSNR: 18.994022369384766
[TRAIN] Iter: 440900 Loss: 0.02790694311261177  PSNR: 18.752687454223633
[TRAIN] Iter: 441000 Loss: 0.024787060916423798  PSNR: 19.316038131713867
[TRAIN] Iter: 441100 Loss: 0.022642843425273895  PSNR: 19.331283569335938
[TRAIN] Iter: 441200 Loss: 0.02804391458630562  PSNR: 18.58292007446289
[TRAIN] Iter: 441300 Loss: 0.024090005084872246  PSNR: 19.61564064025879
[TRAIN] Iter: 441400 Loss: 0.035312265157699585  PSNR: 17.682462692260742
[TRAIN] Iter: 441500 Loss: 0.02242134138941765  PSNR: 19.894027709960938
[TRAIN] Iter: 441600 Loss: 0.024843644350767136  PSNR: 18.781885147094727
[TRAIN] Iter: 441700 Loss: 0.02451280690729618  PSNR: 19.322416305541992
[TRAIN] Iter: 441800 Loss: 0.03045528009533882  PSNR: 18.16170310974121
[TRAIN] Iter: 441900 Loss: 0.024867791682481766  PSNR: 19.38467025756836
[TRAIN] Iter: 442000 Loss: 0.027160421013832092  PSNR: 19.023216247558594
[TRAIN] Iter: 442100 Loss: 0.023064130917191505  PSNR: 19.747705459594727
[TRAIN] Iter: 442200 Loss: 0.03338988125324249  PSNR: 17.985904693603516
[TRAIN] Iter: 442300 Loss: 0.02490498125553131  PSNR: 18.951740264892578
[TRAIN] Iter: 442400 Loss: 0.028272215276956558  PSNR: 18.63504409790039
[TRAIN] Iter: 442500 Loss: 0.02087528631091118  PSNR: 20.089651107788086
[TRAIN] Iter: 442600 Loss: 0.025659866631031036  PSNR: 19.1263427734375
[TRAIN] Iter: 442700 Loss: 0.03055877424776554  PSNR: 18.566572189331055
[TRAIN] Iter: 442800 Loss: 0.026782240718603134  PSNR: 19.008806228637695
[TRAIN] Iter: 442900 Loss: 0.031426846981048584  PSNR: 18.162891387939453
[TRAIN] Iter: 443000 Loss: 0.026093201711773872  PSNR: 19.127296447753906
[TRAIN] Iter: 443100 Loss: 0.022054653614759445  PSNR: 19.667539596557617
[TRAIN] Iter: 443200 Loss: 0.02940395660698414  PSNR: 18.531282424926758
[TRAIN] Iter: 443300 Loss: 0.02441702038049698  PSNR: 18.94269371032715
[TRAIN] Iter: 443400 Loss: 0.027164410799741745  PSNR: 19.011661529541016
[TRAIN] Iter: 443500 Loss: 0.02389693632721901  PSNR: 19.29429817199707
[TRAIN] Iter: 443600 Loss: 0.026656977832317352  PSNR: 18.754108428955078
[TRAIN] Iter: 443700 Loss: 0.030754797160625458  PSNR: 18.348533630371094
[TRAIN] Iter: 443800 Loss: 0.0245172418653965  PSNR: 19.351829528808594
[TRAIN] Iter: 443900 Loss: 0.029607586562633514  PSNR: 18.50380516052246
[TRAIN] Iter: 444000 Loss: 0.027086321264505386  PSNR: 18.956697463989258
[TRAIN] Iter: 444100 Loss: 0.02637529745697975  PSNR: 18.568876266479492
[TRAIN] Iter: 444200 Loss: 0.02945665642619133  PSNR: 18.51569938659668
[TRAIN] Iter: 444300 Loss: 0.02482570707798004  PSNR: 19.44805145263672
[TRAIN] Iter: 444400 Loss: 0.030386557802557945  PSNR: 18.14617347717285
[TRAIN] Iter: 444500 Loss: 0.029467670246958733  PSNR: 18.485301971435547
[TRAIN] Iter: 444600 Loss: 0.032011762261390686  PSNR: 18.114784240722656
[TRAIN] Iter: 444700 Loss: 0.029217196628451347  PSNR: 18.492170333862305
[TRAIN] Iter: 444800 Loss: 0.0323086753487587  PSNR: 18.04302406311035
[TRAIN] Iter: 444900 Loss: 0.025887086987495422  PSNR: 19.03919792175293
[TRAIN] Iter: 445000 Loss: 0.0261140838265419  PSNR: 18.985836029052734
[TRAIN] Iter: 445100 Loss: 0.022548619657754898  PSNR: 19.766063690185547
[TRAIN] Iter: 445200 Loss: 0.032197415828704834  PSNR: 18.106958389282227
[TRAIN] Iter: 445300 Loss: 0.028208453208208084  PSNR: 18.695756912231445
[TRAIN] Iter: 445400 Loss: 0.024676663801074028  PSNR: 19.280319213867188
[TRAIN] Iter: 445500 Loss: 0.02813909389078617  PSNR: 18.65033531188965
[TRAIN] Iter: 445600 Loss: 0.02888965606689453  PSNR: 18.848888397216797
[TRAIN] Iter: 445700 Loss: 0.021636800840497017  PSNR: 19.66732406616211
[TRAIN] Iter: 445800 Loss: 0.03145739063620567  PSNR: 18.228042602539062
[TRAIN] Iter: 445900 Loss: 0.03204737603664398  PSNR: 18.015642166137695
[TRAIN] Iter: 446000 Loss: 0.0220995731651783  PSNR: 19.735013961791992
[TRAIN] Iter: 446100 Loss: 0.0255124494433403  PSNR: 19.17111587524414
[TRAIN] Iter: 446200 Loss: 0.025565117597579956  PSNR: 18.982454299926758
[TRAIN] Iter: 446300 Loss: 0.02688681334257126  PSNR: 19.013303756713867
[TRAIN] Iter: 446400 Loss: 0.028079627081751823  PSNR: 18.631410598754883
[TRAIN] Iter: 446500 Loss: 0.024617964401841164  PSNR: 19.459083557128906
[TRAIN] Iter: 446600 Loss: 0.02971944399178028  PSNR: 18.492612838745117
[TRAIN] Iter: 446700 Loss: 0.024814274162054062  PSNR: 19.46565055847168
[TRAIN] Iter: 446800 Loss: 0.03151671215891838  PSNR: 18.190505981445312
[TRAIN] Iter: 446900 Loss: 0.028830673545598984  PSNR: 18.654281616210938
[TRAIN] Iter: 447000 Loss: 0.031208228319883347  PSNR: 18.147672653198242
[TRAIN] Iter: 447100 Loss: 0.02560218796133995  PSNR: 19.16574478149414
[TRAIN] Iter: 447200 Loss: 0.028390001505613327  PSNR: 18.592451095581055
[TRAIN] Iter: 447300 Loss: 0.02194676175713539  PSNR: 19.704395294189453
[TRAIN] Iter: 447400 Loss: 0.022706208750605583  PSNR: 20.1793270111084
[TRAIN] Iter: 447500 Loss: 0.02047952450811863  PSNR: 20.14822006225586
[TRAIN] Iter: 447600 Loss: 0.029268264770507812  PSNR: 18.54663848876953
[TRAIN] Iter: 447700 Loss: 0.020653171464800835  PSNR: 19.978551864624023
[TRAIN] Iter: 447800 Loss: 0.025510869920253754  PSNR: 19.16397476196289
[TRAIN] Iter: 447900 Loss: 0.02754242904484272  PSNR: 18.82560157775879
[TRAIN] Iter: 448000 Loss: 0.027174334973096848  PSNR: 19.001785278320312
[TRAIN] Iter: 448100 Loss: 0.029361983761191368  PSNR: 18.578495025634766
[TRAIN] Iter: 448200 Loss: 0.03018520213663578  PSNR: 18.328298568725586
[TRAIN] Iter: 448300 Loss: 0.023213982582092285  PSNR: 19.560091018676758
[TRAIN] Iter: 448400 Loss: 0.03092825412750244  PSNR: 18.06325340270996
[TRAIN] Iter: 448500 Loss: 0.028630757704377174  PSNR: 18.912918090820312
[TRAIN] Iter: 448600 Loss: 0.024378130212426186  PSNR: 19.173959732055664
[TRAIN] Iter: 448700 Loss: 0.022107023745775223  PSNR: 19.507225036621094
[TRAIN] Iter: 448800 Loss: 0.0200373325496912  PSNR: 20.243206024169922
[TRAIN] Iter: 448900 Loss: 0.026943428441882133  PSNR: 18.993045806884766
[TRAIN] Iter: 449000 Loss: 0.028968151658773422  PSNR: 18.581457138061523
[TRAIN] Iter: 449100 Loss: 0.02739381603896618  PSNR: 18.935895919799805
[TRAIN] Iter: 449200 Loss: 0.030121510848402977  PSNR: 18.381093978881836
[TRAIN] Iter: 449300 Loss: 0.029349692165851593  PSNR: 18.531761169433594
[TRAIN] Iter: 449400 Loss: 0.03198870271444321  PSNR: 18.15343475341797
[TRAIN] Iter: 449500 Loss: 0.022935854271054268  PSNR: 19.19932746887207
[TRAIN] Iter: 449600 Loss: 0.030849263072013855  PSNR: 18.394617080688477
[TRAIN] Iter: 449700 Loss: 0.026673194020986557  PSNR: 18.921100616455078
[TRAIN] Iter: 449800 Loss: 0.0329338014125824  PSNR: 17.89324951171875
[TRAIN] Iter: 449900 Loss: 0.031139696016907692  PSNR: 18.233945846557617
Saved checkpoints at ./logs/TUT-LAB-nerf/450000.tar
0 0.00045490264892578125
torch.Size([320, 640, 3]) torch.Size([320, 640])
1 16.381782293319702
2 16.44949960708618
3 16.444981575012207
4 16.44429898262024
5 16.394155740737915
6 16.461819171905518
7 16.387827396392822
8 16.47667384147644
9 16.38938593864441
10 16.522880792617798
11 16.401823043823242
12 16.540825605392456
13 16.3103609085083
14 16.46044611930847
15 16.35004734992981
16 16.505964756011963
17 16.360607862472534
18 16.545552015304565
19 16.39338207244873
20 16.457355260849
21 16.359763622283936
22 16.532121658325195
23 16.33174729347229
24 16.53692936897278
25 16.367413997650146
26 16.580906629562378
27 16.499220609664917
28 16.794018983840942
29 16.587900638580322
30 16.562511444091797
31 16.322141408920288
32 16.47457456588745
33 16.293140411376953
34 16.50952434539795
35 16.290767908096313
36 16.512703895568848
37 16.337613582611084
38 16.529967546463013
39 16.34389901161194
40 16.53149104118347
41 16.315841913223267
42 16.56097412109375
43 16.34441328048706
44 16.460137844085693
45 16.374578952789307
46 16.482048988342285
47 16.330402135849
48 16.55294704437256
49 16.39980673789978
50 16.475322484970093
51 16.3293719291687
52 16.436524629592896
53 16.429118394851685
54 16.43946623802185
55 16.39175796508789
56 16.491520404815674
57 16.49579930305481
58 16.342983961105347
59 16.452934503555298
60 16.38860535621643
61 16.492340087890625
62 16.405318021774292
63 16.49867296218872
64 16.27358078956604
65 16.457098484039307
66 16.359721183776855
67 16.515512704849243
68 16.32827353477478
69 16.490167379379272
70 16.362205743789673
71 16.516706466674805
72 16.328237056732178
73 16.516736030578613
74 16.320005893707275
75 16.60739541053772
76 16.24576997756958
77 16.519730806350708
78 16.334798097610474
79 16.570247888565063
80 16.327396869659424
81 16.589097499847412
82 16.307464599609375
83 16.466629028320312
84 16.290063858032227
85 16.547061443328857
86 16.544294357299805
87 16.847500324249268
88 16.423187017440796
89 16.495362997055054
90 16.259040355682373
91 16.56178116798401
92 15.710368394851685
93 15.856921672821045
94 16.24876379966736
95 16.510657787322998
96 16.11307191848755
97 14.564955711364746
98 14.386046886444092
99 14.623776912689209
100 14.398411273956299
101 14.543620109558105
102 14.400217771530151
103 14.478396892547607
104 14.477564573287964
105 14.4094717502594
106 14.59378719329834
107 14.375272035598755
108 14.60878038406372
109 14.348506212234497
110 14.566080331802368
111 14.421149253845215
112 14.503011226654053
113 14.474447965621948
114 14.454711675643921
115 14.558606624603271
116 14.359395265579224
117 14.61905574798584
118 14.372753381729126
119 14.576618671417236
Done, saving (120, 320, 640, 3) (120, 320, 640)
extras:{'raw': tensor([[[ 1.2167e-01, -6.5190e-02, -2.4115e-01,  2.3800e+01],
         [ 4.4858e-01,  3.1294e-01,  1.4747e-01, -1.0083e+01],
         [ 1.1951e-01, -2.4792e-02, -1.7832e-01, -5.6511e-01],
         ...,
         [ 1.1231e+00,  4.0621e-01, -3.1401e+00,  3.9527e+02],
         [ 1.7542e+00,  1.0650e+00, -2.2017e+00,  4.1089e+02],
         [ 1.6728e+00,  9.3329e-01, -2.2324e+00,  4.1958e+02]],

        [[-1.2523e+00, -8.2570e-01,  3.8985e-01, -5.3823e+00],
         [ 5.9082e-01,  8.5567e-02, -2.7456e-02, -5.3867e+00],
         [ 2.8874e-01,  7.1904e-02,  3.8973e-01, -1.3599e+01],
         ...,
         [-3.7060e-01,  6.0518e-02,  1.8046e+00, -6.7176e-01],
         [ 6.9260e-01,  1.1933e+00,  2.9948e+00, -1.4603e+01],
         [-4.1541e-01,  6.9566e-02,  1.8918e+00, -6.3561e+00]],

        [[-4.2922e-01, -5.8297e-01, -3.5807e-01, -3.4822e+00],
         [-3.8797e-01, -6.6877e-01, -6.9100e-01,  4.8346e+00],
         [-4.7029e-01, -7.2598e-01, -7.9451e-01,  7.9599e+00],
         ...,
         [ 1.1338e+00,  7.9142e-01,  1.6616e+00,  3.6668e+00],
         [ 6.9321e-01,  8.9655e-01,  2.2705e+00, -6.9206e+00],
         [ 4.6551e-01,  7.2437e-01,  2.2509e+00, -7.1768e+00]],

        ...,

        [[-8.0749e-01, -8.2053e-01, -9.1338e-01, -4.6877e+00],
         [-2.6194e-01, -4.9272e-01, -6.9555e-01, -1.6035e+01],
         [-4.5465e-01, -4.2129e-01, -4.4322e-01, -1.6625e+01],
         ...,
         [-7.5803e+00, -1.3450e+01, -2.2378e+01,  1.9932e+02],
         [-7.2381e+00, -1.3257e+01, -2.2427e+01,  1.8063e+02],
         [-9.6303e+00, -1.4869e+01, -2.4236e+01,  2.0669e+02]],

        [[ 2.8156e-02, -2.7811e-01, -2.6665e-01, -1.4754e+01],
         [ 1.4071e-01, -2.7060e-02,  3.3041e-01, -1.6780e+01],
         [-1.9760e-01, -2.7921e-01,  1.0414e-01, -1.9715e+01],
         ...,
         [-1.8716e+01, -1.5813e+01, -1.3605e+01,  6.8130e+01],
         [-1.9481e+01, -1.6143e+01, -1.2835e+01,  4.9332e+01],
         [-1.8481e+01, -1.5190e+01, -1.1825e+01,  5.3112e+01]],

        [[-2.4266e+00, -2.2970e+00, -2.2977e+00,  3.8910e+00],
         [-2.7227e+00, -2.6468e+00, -2.6539e+00,  1.4544e+01],
         [-2.7265e+00, -2.6518e+00, -2.6659e+00,  1.4645e+01],
         ...,
         [ 2.8181e+00,  4.7335e+00,  1.5601e+01,  3.6206e+02],
         [ 3.5737e+00,  5.2417e+00,  1.5312e+01,  3.7980e+02],
         [ 3.3328e+00,  4.9503e+00,  1.4955e+01,  3.8122e+02]]],
       grad_fn=<ReshapeAliasBackward0>), 'rgb0': tensor([[0.5307, 0.4874, 0.4361],
        [0.6067, 0.5490, 0.5868],
        [0.4652, 0.3961, 0.3492],
        ...,
        [0.4275, 0.4152, 0.3988],
        [0.7326, 0.5970, 0.5335],
        [0.1480, 0.1428, 0.1322]], grad_fn=<ReshapeAliasBackward0>), 'disp0': tensor([ 18.6403,  19.5685, 108.2355,  ...,  20.7638,  20.1268,  87.6557],
       grad_fn=<ReshapeAliasBackward0>), 'acc0': tensor([1., 1., 1.,  ..., 1., 1., 1.], grad_fn=<ReshapeAliasBackward0>), 'z_std': tensor([0.0023, 0.0034, 0.0018,  ..., 0.0012, 0.0022, 0.0022])}
0 0.0005366802215576172
torch.Size([320, 640, 3]) torch.Size([320, 640])
1 14.571158409118652
2 14.441908597946167
3 14.507989168167114
4 14.465448141098022
5 14.348766803741455
6 14.5670166015625
7 14.33656644821167
8 14.549936771392822
9 14.402767896652222
10 14.555957794189453
11 14.45773720741272
12 14.533674001693726
13 14.441176176071167
14 14.424000263214111
15 14.53442907333374
16 14.33631181716919
17 14.558167457580566
18 14.294080257415771
19 14.636373519897461
20 14.398335695266724
21 14.572247505187988
22 14.437314510345459
23 14.4902663230896
24 14.514289379119873
25 14.365333318710327
26 14.592091798782349
27 14.352449893951416
28 14.548314809799194
29 14.345043897628784
30 14.58508586883545
31 14.399874925613403
32 14.48603630065918
33 14.487311601638794
34 14.452370166778564
35 14.57671332359314
36 14.35508918762207
37 14.568912506103516
38 14.38616681098938
39 14.52152419090271
40 14.403508424758911
41 14.557557344436646
42 14.480082035064697
43 14.450345993041992
44 14.544157981872559
45 14.342579126358032
46 14.621357679367065
47 14.356600284576416
48 14.595176935195923
49 14.359858751296997
50 14.538308143615723
51 14.44034481048584
52 14.5044527053833
53 14.482176780700684
54 14.395451068878174
55 14.507122039794922
56 14.34105634689331
57 14.623367309570312
58 14.362012386322021
59 14.596194982528687
60 14.386588335037231
61 14.52969217300415
62 14.456348896026611
63 14.38153338432312
64 14.550159931182861
65 14.304203987121582
66 14.637766361236572
67 14.333887338638306
68 14.622007846832275
69 14.36887788772583
70 14.530131101608276
71 14.467468738555908
72 14.476355075836182
73 14.501482486724854
74 14.365448236465454
75 14.630588054656982
76 14.398990631103516
77 14.566138982772827
78 14.353919982910156
79 14.561132907867432
80 14.393999338150024
81 14.531729936599731
82 14.483282089233398
83 14.71832537651062
84 14.296078443527222
85 14.297165393829346
86 14.608535289764404
87 14.389928340911865
88 14.584579229354858
89 14.401821613311768
90 14.53260850906372
91 14.44172716140747
92 14.411430835723877
93 14.539273500442505
94 14.335714340209961
95 14.691542863845825
96 14.371610641479492
97 14.758042097091675
98 14.292158603668213
99 14.5120267868042
100 14.422704696655273
101 14.647329568862915
102 14.272641658782959
103 14.266174077987671
104 14.58692455291748
105 14.278633117675781
106 14.89447546005249
107 14.198167085647583
108 14.75292444229126
109 14.130359649658203
110 14.88802194595337
111 14.320619583129883
112 14.151328802108765
113 14.563704013824463
114 14.345430135726929
115 14.933974742889404
116 14.149705648422241
117 14.837090015411377
118 14.165986776351929
119 14.832583904266357
test poses shape torch.Size([13, 3, 4])
0 0.0006513595581054688
torch.Size([320, 640, 3]) torch.Size([320, 640])
1 14.28900694847107
2 14.665528535842896
3 14.233712196350098
4 14.91304898262024
5 14.218221664428711
6 14.890094995498657
7 14.176151514053345
8 14.885618209838867
9 14.236563205718994
10 14.294999837875366
11 14.815212965011597
12 14.21126103401184
Saved test set
[TRAIN] Iter: 450000 Loss: 0.02554827556014061  PSNR: 18.87053108215332
[TRAIN] Iter: 450100 Loss: 0.03276195004582405  PSNR: 17.95289421081543
[TRAIN] Iter: 450200 Loss: 0.023806987330317497  PSNR: 19.575624465942383
[TRAIN] Iter: 450300 Loss: 0.03188709914684296  PSNR: 17.98177719116211
[TRAIN] Iter: 450400 Loss: 0.023375388234853745  PSNR: 19.42514419555664
[TRAIN] Iter: 450500 Loss: 0.03158186376094818  PSNR: 18.19918441772461
[TRAIN] Iter: 450600 Loss: 0.028806785121560097  PSNR: 18.549015045166016
[TRAIN] Iter: 450700 Loss: 0.027358509600162506  PSNR: 18.993391036987305
[TRAIN] Iter: 450800 Loss: 0.02978682890534401  PSNR: 18.279701232910156
[TRAIN] Iter: 450900 Loss: 0.028847746551036835  PSNR: 18.585983276367188
[TRAIN] Iter: 451000 Loss: 0.02856936864554882  PSNR: 18.571025848388672
[TRAIN] Iter: 451100 Loss: 0.023262513801455498  PSNR: 19.51833152770996
[TRAIN] Iter: 451200 Loss: 0.026008961722254753  PSNR: 19.102813720703125
[TRAIN] Iter: 451300 Loss: 0.026250764727592468  PSNR: 18.94999885559082
[TRAIN] Iter: 451400 Loss: 0.0329008549451828  PSNR: 18.012895584106445
[TRAIN] Iter: 451500 Loss: 0.028681214898824692  PSNR: 18.505084991455078
[TRAIN] Iter: 451600 Loss: 0.023914417251944542  PSNR: 19.80950927734375
[TRAIN] Iter: 451700 Loss: 0.029087157920002937  PSNR: 18.571290969848633
[TRAIN] Iter: 451800 Loss: 0.0283631831407547  PSNR: 18.749353408813477
[TRAIN] Iter: 451900 Loss: 0.03214948624372482  PSNR: 18.061504364013672
[TRAIN] Iter: 452000 Loss: 0.03027193993330002  PSNR: 18.361207962036133
[TRAIN] Iter: 452100 Loss: 0.03246307373046875  PSNR: 18.140033721923828
[TRAIN] Iter: 452200 Loss: 0.024762902408838272  PSNR: 19.208349227905273
[TRAIN] Iter: 452300 Loss: 0.031674399971961975  PSNR: 18.26650619506836
[TRAIN] Iter: 452400 Loss: 0.02321883663535118  PSNR: 19.021039962768555
[TRAIN] Iter: 452500 Loss: 0.02732023410499096  PSNR: 18.925880432128906
[TRAIN] Iter: 452600 Loss: 0.029789716005325317  PSNR: 18.481502532958984
[TRAIN] Iter: 452700 Loss: 0.02977789379656315  PSNR: 18.456846237182617
[TRAIN] Iter: 452800 Loss: 0.023632854223251343  PSNR: 19.403030395507812
[TRAIN] Iter: 452900 Loss: 0.02506003901362419  PSNR: 19.044160842895508
[TRAIN] Iter: 453000 Loss: 0.029982974752783775  PSNR: 18.610414505004883
[TRAIN] Iter: 453100 Loss: 0.02680104970932007  PSNR: 18.770084381103516
[TRAIN] Iter: 453200 Loss: 0.02911263331770897  PSNR: 18.645349502563477
[TRAIN] Iter: 453300 Loss: 0.02294129878282547  PSNR: 19.340831756591797
[TRAIN] Iter: 453400 Loss: 0.026824086904525757  PSNR: 18.809682846069336
[TRAIN] Iter: 453500 Loss: 0.02346469648182392  PSNR: 19.507287979125977
[TRAIN] Iter: 453600 Loss: 0.032967351377010345  PSNR: 17.925996780395508
[TRAIN] Iter: 453700 Loss: 0.021281080320477486  PSNR: 19.984153747558594
[TRAIN] Iter: 453800 Loss: 0.03271672874689102  PSNR: 18.058012008666992
[TRAIN] Iter: 453900 Loss: 0.027249041944742203  PSNR: 18.72568130493164
[TRAIN] Iter: 454000 Loss: 0.02889813669025898  PSNR: 18.38796615600586
[TRAIN] Iter: 454100 Loss: 0.02468498796224594  PSNR: 19.378952026367188
[TRAIN] Iter: 454200 Loss: 0.024067414924502373  PSNR: 19.4099063873291
[TRAIN] Iter: 454300 Loss: 0.02000688761472702  PSNR: 20.133167266845703
[TRAIN] Iter: 454400 Loss: 0.01871037855744362  PSNR: 20.23954963684082
[TRAIN] Iter: 454500 Loss: 0.027172137051820755  PSNR: 19.14059066772461
[TRAIN] Iter: 454600 Loss: 0.022777356207370758  PSNR: 19.616710662841797
[TRAIN] Iter: 454700 Loss: 0.025308193638920784  PSNR: 19.030534744262695
[TRAIN] Iter: 454800 Loss: 0.029894523322582245  PSNR: 18.40648651123047
[TRAIN] Iter: 454900 Loss: 0.025183964520692825  PSNR: 19.24535369873047
[TRAIN] Iter: 455000 Loss: 0.03442848473787308  PSNR: 17.70014762878418
[TRAIN] Iter: 455100 Loss: 0.02515316754579544  PSNR: 19.5133056640625
[TRAIN] Iter: 455200 Loss: 0.026918120682239532  PSNR: 18.866716384887695
[TRAIN] Iter: 455300 Loss: 0.025234758853912354  PSNR: 19.121883392333984
[TRAIN] Iter: 455400 Loss: 0.025688093155622482  PSNR: 19.01081085205078
[TRAIN] Iter: 455500 Loss: 0.02103615552186966  PSNR: 20.09561538696289
[TRAIN] Iter: 455600 Loss: 0.02993304468691349  PSNR: 18.23626136779785
[TRAIN] Iter: 455700 Loss: 0.02931026928126812  PSNR: 18.594053268432617
[TRAIN] Iter: 455800 Loss: 0.02348429523408413  PSNR: 19.600751876831055
[TRAIN] Iter: 455900 Loss: 0.02915438637137413  PSNR: 18.581254959106445
[TRAIN] Iter: 456000 Loss: 0.025441061705350876  PSNR: 19.20899772644043
[TRAIN] Iter: 456100 Loss: 0.0270528681576252  PSNR: 18.932777404785156
[TRAIN] Iter: 456200 Loss: 0.024806898087263107  PSNR: 19.444194793701172
[TRAIN] Iter: 456300 Loss: 0.030932806432247162  PSNR: 18.2204647064209
[TRAIN] Iter: 456400 Loss: 0.025667373090982437  PSNR: 18.68146514892578
[TRAIN] Iter: 456500 Loss: 0.027551565319299698  PSNR: 19.160377502441406
[TRAIN] Iter: 456600 Loss: 0.035520974546670914  PSNR: 17.536184310913086
[TRAIN] Iter: 456700 Loss: 0.028391845524311066  PSNR: 18.705337524414062
[TRAIN] Iter: 456800 Loss: 0.029465923085808754  PSNR: 18.734210968017578
[TRAIN] Iter: 456900 Loss: 0.026230541989207268  PSNR: 18.982240676879883
[TRAIN] Iter: 457000 Loss: 0.02406039461493492  PSNR: 19.311677932739258
[TRAIN] Iter: 457100 Loss: 0.026726538315415382  PSNR: 19.03329849243164
[TRAIN] Iter: 457200 Loss: 0.03136191517114639  PSNR: 18.224716186523438
[TRAIN] Iter: 457300 Loss: 0.03285631537437439  PSNR: 17.904525756835938
[TRAIN] Iter: 457400 Loss: 0.029314912855625153  PSNR: 18.49817657470703
[TRAIN] Iter: 457500 Loss: 0.031732358038425446  PSNR: 18.13660430908203
[TRAIN] Iter: 457600 Loss: 0.024212084710597992  PSNR: 19.530376434326172
[TRAIN] Iter: 457700 Loss: 0.027243642136454582  PSNR: 18.892528533935547
[TRAIN] Iter: 457800 Loss: 0.03072594851255417  PSNR: 18.156444549560547
[TRAIN] Iter: 457900 Loss: 0.028241995722055435  PSNR: 18.900815963745117
[TRAIN] Iter: 458000 Loss: 0.020817551761865616  PSNR: 20.172435760498047
[TRAIN] Iter: 458100 Loss: 0.026998449116945267  PSNR: 18.967723846435547
[TRAIN] Iter: 458200 Loss: 0.024116292595863342  PSNR: 19.311601638793945
[TRAIN] Iter: 458300 Loss: 0.023155495524406433  PSNR: 19.57278823852539
[TRAIN] Iter: 458400 Loss: 0.02922920137643814  PSNR: 18.519039154052734
[TRAIN] Iter: 458500 Loss: 0.025839226320385933  PSNR: 19.04686164855957
[TRAIN] Iter: 458600 Loss: 0.03177791088819504  PSNR: 18.174556732177734
[TRAIN] Iter: 458700 Loss: 0.031959615647792816  PSNR: 18.181976318359375
[TRAIN] Iter: 458800 Loss: 0.031272947788238525  PSNR: 18.23268699645996
[TRAIN] Iter: 458900 Loss: 0.032234758138656616  PSNR: 18.02083396911621
[TRAIN] Iter: 459000 Loss: 0.025508087128400803  PSNR: 19.169450759887695
[TRAIN] Iter: 459100 Loss: 0.027794402092695236  PSNR: 18.760238647460938
[TRAIN] Iter: 459200 Loss: 0.023984331637620926  PSNR: 19.44868278503418
[TRAIN] Iter: 459300 Loss: 0.029597576707601547  PSNR: 18.989774703979492
[TRAIN] Iter: 459400 Loss: 0.02117261476814747  PSNR: 19.658029556274414
[TRAIN] Iter: 459500 Loss: 0.029057249426841736  PSNR: 18.503877639770508
[TRAIN] Iter: 459600 Loss: 0.023233281448483467  PSNR: 19.430139541625977
[TRAIN] Iter: 459700 Loss: 0.02665800042450428  PSNR: 18.832792282104492
[TRAIN] Iter: 459800 Loss: 0.028004996478557587  PSNR: 18.523019790649414
[TRAIN] Iter: 459900 Loss: 0.029343705624341965  PSNR: 18.489919662475586
Saved checkpoints at ./logs/TUT-LAB-nerf/460000.tar
[TRAIN] Iter: 460000 Loss: 0.021405581384897232  PSNR: 19.965280532836914
[TRAIN] Iter: 460100 Loss: 0.02167189121246338  PSNR: 19.971757888793945
[TRAIN] Iter: 460200 Loss: 0.024696167558431625  PSNR: 19.153024673461914
[TRAIN] Iter: 460300 Loss: 0.022985339164733887  PSNR: 19.55837631225586
[TRAIN] Iter: 460400 Loss: 0.03219466656446457  PSNR: 17.992660522460938
[TRAIN] Iter: 460500 Loss: 0.03179224953055382  PSNR: 18.143524169921875
[TRAIN] Iter: 460600 Loss: 0.029976099729537964  PSNR: 18.38052749633789
[TRAIN] Iter: 460700 Loss: 0.024864932522177696  PSNR: 19.127479553222656
[TRAIN] Iter: 460800 Loss: 0.02938973531126976  PSNR: 18.57383155822754
[TRAIN] Iter: 460900 Loss: 0.025850288569927216  PSNR: 19.01668930053711
[TRAIN] Iter: 461000 Loss: 0.027282249182462692  PSNR: 18.722537994384766
[TRAIN] Iter: 461100 Loss: 0.022026129066944122  PSNR: 19.544462203979492
[TRAIN] Iter: 461200 Loss: 0.03249514102935791  PSNR: 18.05738639831543
[TRAIN] Iter: 461300 Loss: 0.026259122416377068  PSNR: 18.866060256958008
[TRAIN] Iter: 461400 Loss: 0.027164163067936897  PSNR: 18.780061721801758
[TRAIN] Iter: 461500 Loss: 0.023009002208709717  PSNR: 19.628381729125977
[TRAIN] Iter: 461600 Loss: 0.02907649800181389  PSNR: 18.55654525756836
[TRAIN] Iter: 461700 Loss: 0.02912539802491665  PSNR: 18.680063247680664
[TRAIN] Iter: 461800 Loss: 0.02562466822564602  PSNR: 19.0685977935791
[TRAIN] Iter: 461900 Loss: 0.029632557183504105  PSNR: 18.32744598388672
[TRAIN] Iter: 462000 Loss: 0.025914400815963745  PSNR: 19.10059928894043
[TRAIN] Iter: 462100 Loss: 0.027020901441574097  PSNR: 18.93978500366211
[TRAIN] Iter: 462200 Loss: 0.025041893124580383  PSNR: 19.076507568359375
[TRAIN] Iter: 462300 Loss: 0.029052112251520157  PSNR: 18.57455825805664
[TRAIN] Iter: 462400 Loss: 0.033641040325164795  PSNR: 17.826385498046875
[TRAIN] Iter: 462500 Loss: 0.02489331364631653  PSNR: 19.275962829589844
[TRAIN] Iter: 462600 Loss: 0.025004515424370766  PSNR: 19.67937469482422
[TRAIN] Iter: 462700 Loss: 0.026776442304253578  PSNR: 18.9636287689209
[TRAIN] Iter: 462800 Loss: 0.022575143724679947  PSNR: 19.64389419555664
[TRAIN] Iter: 462900 Loss: 0.030484089627861977  PSNR: 18.346969604492188
[TRAIN] Iter: 463000 Loss: 0.023999502882361412  PSNR: 19.42487907409668
[TRAIN] Iter: 463100 Loss: 0.02247818373143673  PSNR: 19.64694595336914
[TRAIN] Iter: 463200 Loss: 0.02657300792634487  PSNR: 18.996929168701172
[TRAIN] Iter: 463300 Loss: 0.019221387803554535  PSNR: 20.105472564697266
[TRAIN] Iter: 463400 Loss: 0.03324681147933006  PSNR: 17.86025047302246
[TRAIN] Iter: 463500 Loss: 0.02393840253353119  PSNR: 19.848041534423828
[TRAIN] Iter: 463600 Loss: 0.02848881296813488  PSNR: 18.708473205566406
[TRAIN] Iter: 463700 Loss: 0.02791868895292282  PSNR: 18.817502975463867
[TRAIN] Iter: 463800 Loss: 0.023899685591459274  PSNR: 18.951740264892578
[TRAIN] Iter: 463900 Loss: 0.02588217705488205  PSNR: 19.08161163330078
[TRAIN] Iter: 464000 Loss: 0.02081250585615635  PSNR: 19.876075744628906
[TRAIN] Iter: 464100 Loss: 0.026223571971058846  PSNR: 19.100723266601562
[TRAIN] Iter: 464200 Loss: 0.02759777568280697  PSNR: 18.67838478088379
[TRAIN] Iter: 464300 Loss: 0.03340441733598709  PSNR: 17.890012741088867
[TRAIN] Iter: 464400 Loss: 0.024367239326238632  PSNR: 19.380199432373047
[TRAIN] Iter: 464500 Loss: 0.03127027675509453  PSNR: 18.066726684570312
[TRAIN] Iter: 464600 Loss: 0.026009391993284225  PSNR: 18.85576057434082
[TRAIN] Iter: 464700 Loss: 0.02757597714662552  PSNR: 18.775310516357422
[TRAIN] Iter: 464800 Loss: 0.029333902522921562  PSNR: 18.474048614501953
[TRAIN] Iter: 464900 Loss: 0.02986537292599678  PSNR: 18.384990692138672
[TRAIN] Iter: 465000 Loss: 0.030109349638223648  PSNR: 18.396656036376953
[TRAIN] Iter: 465100 Loss: 0.03192968666553497  PSNR: 18.143381118774414
[TRAIN] Iter: 465200 Loss: 0.02430647239089012  PSNR: 19.36050796508789
[TRAIN] Iter: 465300 Loss: 0.03210794925689697  PSNR: 18.288806915283203
[TRAIN] Iter: 465400 Loss: 0.027279675006866455  PSNR: 18.604936599731445
[TRAIN] Iter: 465500 Loss: 0.021224135532975197  PSNR: 19.965694427490234
[TRAIN] Iter: 465600 Loss: 0.026402821764349937  PSNR: 18.633255004882812
[TRAIN] Iter: 465700 Loss: 0.031428590416908264  PSNR: 18.201313018798828
[TRAIN] Iter: 465800 Loss: 0.022700205445289612  PSNR: 19.590513229370117
[TRAIN] Iter: 465900 Loss: 0.030363580211997032  PSNR: 18.40584945678711
[TRAIN] Iter: 466000 Loss: 0.020162567496299744  PSNR: 20.118837356567383
[TRAIN] Iter: 466100 Loss: 0.027701806277036667  PSNR: 18.719482421875
[TRAIN] Iter: 466200 Loss: 0.028620727360248566  PSNR: 18.638784408569336
[TRAIN] Iter: 466300 Loss: 0.0259445421397686  PSNR: 19.10735321044922
[TRAIN] Iter: 466400 Loss: 0.03376290947198868  PSNR: 17.953323364257812
[TRAIN] Iter: 466500 Loss: 0.022798649966716766  PSNR: 19.695058822631836
[TRAIN] Iter: 466600 Loss: 0.026419200003147125  PSNR: 18.85163688659668
[TRAIN] Iter: 466700 Loss: 0.029971810057759285  PSNR: 18.59903907775879
[TRAIN] Iter: 466800 Loss: 0.02794312685728073  PSNR: 18.694869995117188
[TRAIN] Iter: 466900 Loss: 0.023954074829816818  PSNR: 19.54465675354004
[TRAIN] Iter: 467000 Loss: 0.026766758412122726  PSNR: 19.027385711669922
[TRAIN] Iter: 467100 Loss: 0.02114824764430523  PSNR: 19.8464412689209
[TRAIN] Iter: 467200 Loss: 0.03182332217693329  PSNR: 18.145559310913086
[TRAIN] Iter: 467300 Loss: 0.027885179966688156  PSNR: 18.79402732849121
[TRAIN] Iter: 467400 Loss: 0.020490339025855064  PSNR: 20.28267478942871
[TRAIN] Iter: 467500 Loss: 0.021192684769630432  PSNR: 20.030595779418945
[TRAIN] Iter: 467600 Loss: 0.02509850636124611  PSNR: 18.92243766784668
[TRAIN] Iter: 467700 Loss: 0.028119459748268127  PSNR: 18.845373153686523
[TRAIN] Iter: 467800 Loss: 0.024480624124407768  PSNR: 19.45965003967285
[TRAIN] Iter: 467900 Loss: 0.035903409123420715  PSNR: 17.53216552734375
[TRAIN] Iter: 468000 Loss: 0.02868100255727768  PSNR: 18.58645248413086
[TRAIN] Iter: 468100 Loss: 0.025317611172795296  PSNR: 19.200197219848633
[TRAIN] Iter: 468200 Loss: 0.02120659500360489  PSNR: 19.967100143432617
[TRAIN] Iter: 468300 Loss: 0.030455762520432472  PSNR: 18.32117462158203
[TRAIN] Iter: 468400 Loss: 0.022740881890058517  PSNR: 19.905231475830078
[TRAIN] Iter: 468500 Loss: 0.0242879968136549  PSNR: 19.308774948120117
[TRAIN] Iter: 468600 Loss: 0.027881795540452003  PSNR: 18.73371124267578
[TRAIN] Iter: 468700 Loss: 0.02992931753396988  PSNR: 18.561275482177734
[TRAIN] Iter: 468800 Loss: 0.02460804022848606  PSNR: 19.48521614074707
[TRAIN] Iter: 468900 Loss: 0.026534443721175194  PSNR: 18.903514862060547
[TRAIN] Iter: 469000 Loss: 0.022740911692380905  PSNR: 19.515483856201172
[TRAIN] Iter: 469100 Loss: 0.02859509363770485  PSNR: 18.609079360961914
[TRAIN] Iter: 469200 Loss: 0.024271104484796524  PSNR: 19.396211624145508
[TRAIN] Iter: 469300 Loss: 0.03201618418097496  PSNR: 18.230112075805664
[TRAIN] Iter: 469400 Loss: 0.03135751187801361  PSNR: 18.178258895874023
[TRAIN] Iter: 469500 Loss: 0.021872615441679955  PSNR: 19.646995544433594
[TRAIN] Iter: 469600 Loss: 0.028341472148895264  PSNR: 18.72049331665039
[TRAIN] Iter: 469700 Loss: 0.021552549675107002  PSNR: 19.72084617614746
[TRAIN] Iter: 469800 Loss: 0.02168957144021988  PSNR: 19.76219367980957
[TRAIN] Iter: 469900 Loss: 0.02991071529686451  PSNR: 18.547401428222656
Saved checkpoints at ./logs/TUT-LAB-nerf/470000.tar
[TRAIN] Iter: 470000 Loss: 0.023994894698262215  PSNR: 19.545494079589844
[TRAIN] Iter: 470100 Loss: 0.03129984438419342  PSNR: 18.291305541992188
[TRAIN] Iter: 470200 Loss: 0.03380843251943588  PSNR: 17.89872932434082
[TRAIN] Iter: 470300 Loss: 0.03461504727602005  PSNR: 17.82267951965332
[TRAIN] Iter: 470400 Loss: 0.02430884540081024  PSNR: 19.156476974487305
[TRAIN] Iter: 470500 Loss: 0.01987999863922596  PSNR: 20.458967208862305
[TRAIN] Iter: 470600 Loss: 0.026167357340455055  PSNR: 19.11310386657715
[TRAIN] Iter: 470700 Loss: 0.021054599434137344  PSNR: 19.92214012145996
[TRAIN] Iter: 470800 Loss: 0.02928857132792473  PSNR: 18.44567108154297
[TRAIN] Iter: 470900 Loss: 0.024343959987163544  PSNR: 19.667695999145508
[TRAIN] Iter: 471000 Loss: 0.031022820621728897  PSNR: 18.173864364624023
[TRAIN] Iter: 471100 Loss: 0.027955934405326843  PSNR: 18.827058792114258
[TRAIN] Iter: 471200 Loss: 0.029092490673065186  PSNR: 18.50544548034668
[TRAIN] Iter: 471300 Loss: 0.020709866657853127  PSNR: 20.01112937927246
[TRAIN] Iter: 471400 Loss: 0.027245081961154938  PSNR: 18.832653045654297
[TRAIN] Iter: 471500 Loss: 0.023320801556110382  PSNR: 19.423437118530273
[TRAIN] Iter: 471600 Loss: 0.022422101348638535  PSNR: 19.651277542114258
[TRAIN] Iter: 471700 Loss: 0.028747396543622017  PSNR: 18.625186920166016
[TRAIN] Iter: 471800 Loss: 0.022117439657449722  PSNR: 19.986276626586914
[TRAIN] Iter: 471900 Loss: 0.024966057389974594  PSNR: 19.31275177001953
[TRAIN] Iter: 472000 Loss: 0.0242624469101429  PSNR: 19.515697479248047
[TRAIN] Iter: 472100 Loss: 0.019915614277124405  PSNR: 20.214357376098633
[TRAIN] Iter: 472200 Loss: 0.029300998896360397  PSNR: 18.45737648010254
[TRAIN] Iter: 472300 Loss: 0.023812390863895416  PSNR: 19.50601577758789
[TRAIN] Iter: 472400 Loss: 0.029993996024131775  PSNR: 18.615455627441406
[TRAIN] Iter: 472500 Loss: 0.02503325790166855  PSNR: 19.416820526123047
[TRAIN] Iter: 472600 Loss: 0.03351067751646042  PSNR: 17.817873001098633
[TRAIN] Iter: 472700 Loss: 0.029825441539287567  PSNR: 18.444116592407227
[TRAIN] Iter: 472800 Loss: 0.02796916291117668  PSNR: 18.714387893676758
[TRAIN] Iter: 472900 Loss: 0.02181595377624035  PSNR: 19.808076858520508
[TRAIN] Iter: 473000 Loss: 0.03107638657093048  PSNR: 18.34840202331543
[TRAIN] Iter: 473100 Loss: 0.028804702684283257  PSNR: 18.438688278198242
[TRAIN] Iter: 473200 Loss: 0.03184330463409424  PSNR: 18.18863868713379
[TRAIN] Iter: 473300 Loss: 0.030022596940398216  PSNR: 18.341459274291992
[TRAIN] Iter: 473400 Loss: 0.02426084689795971  PSNR: 19.30476188659668
[TRAIN] Iter: 473500 Loss: 0.032321032136678696  PSNR: 18.134859085083008
[TRAIN] Iter: 473600 Loss: 0.03421143442392349  PSNR: 17.81617546081543
[TRAIN] Iter: 473700 Loss: 0.029636602848768234  PSNR: 18.56328582763672
[TRAIN] Iter: 473800 Loss: 0.025159213691949844  PSNR: 19.684097290039062
[TRAIN] Iter: 473900 Loss: 0.028259050101041794  PSNR: 18.97138786315918
[TRAIN] Iter: 474000 Loss: 0.02735053189098835  PSNR: 18.997940063476562
[TRAIN] Iter: 474100 Loss: 0.02998245321214199  PSNR: 18.585206985473633
[TRAIN] Iter: 474200 Loss: 0.0245143324136734  PSNR: 19.400087356567383
[TRAIN] Iter: 474300 Loss: 0.027371369302272797  PSNR: 18.869415283203125
[TRAIN] Iter: 474400 Loss: 0.02580614574253559  PSNR: 19.139511108398438
[TRAIN] Iter: 474500 Loss: 0.025836491957306862  PSNR: 19.056880950927734
[TRAIN] Iter: 474600 Loss: 0.022764751687645912  PSNR: 19.8359317779541
[TRAIN] Iter: 474700 Loss: 0.031054452061653137  PSNR: 18.22401237487793
[TRAIN] Iter: 474800 Loss: 0.02552410587668419  PSNR: 18.832901000976562
[TRAIN] Iter: 474900 Loss: 0.02925190143287182  PSNR: 18.50680923461914
[TRAIN] Iter: 475000 Loss: 0.029617413878440857  PSNR: 18.83749771118164
[TRAIN] Iter: 475100 Loss: 0.03289829194545746  PSNR: 17.925880432128906
[TRAIN] Iter: 475200 Loss: 0.023337356746196747  PSNR: 19.904827117919922
[TRAIN] Iter: 475300 Loss: 0.025663865730166435  PSNR: 19.310218811035156
[TRAIN] Iter: 475400 Loss: 0.02487868070602417  PSNR: 19.211015701293945
[TRAIN] Iter: 475500 Loss: 0.029929032549262047  PSNR: 18.4132137298584
[TRAIN] Iter: 475600 Loss: 0.0270406324416399  PSNR: 19.435293197631836
[TRAIN] Iter: 475700 Loss: 0.02640828862786293  PSNR: 18.771974563598633
[TRAIN] Iter: 475800 Loss: 0.028941685333848  PSNR: 18.84333610534668
[TRAIN] Iter: 475900 Loss: 0.025977525860071182  PSNR: 19.50724220275879
[TRAIN] Iter: 476000 Loss: 0.024683289229869843  PSNR: 18.77522850036621
[TRAIN] Iter: 476100 Loss: 0.029629051685333252  PSNR: 18.43712043762207
[TRAIN] Iter: 476200 Loss: 0.026753881946206093  PSNR: 19.002239227294922
[TRAIN] Iter: 476300 Loss: 0.027437176555395126  PSNR: 18.821311950683594
[TRAIN] Iter: 476400 Loss: 0.024743828922510147  PSNR: 19.22486686706543
[TRAIN] Iter: 476500 Loss: 0.024146299809217453  PSNR: 19.41144561767578
[TRAIN] Iter: 476600 Loss: 0.03154369443655014  PSNR: 18.318777084350586
[TRAIN] Iter: 476700 Loss: 0.02467603236436844  PSNR: 19.077362060546875
[TRAIN] Iter: 476800 Loss: 0.027427775785326958  PSNR: 19.004798889160156
[TRAIN] Iter: 476900 Loss: 0.0260683074593544  PSNR: 19.030029296875
[TRAIN] Iter: 477000 Loss: 0.02947782725095749  PSNR: 18.455875396728516
[TRAIN] Iter: 477100 Loss: 0.02251497283577919  PSNR: 19.473291397094727
[TRAIN] Iter: 477200 Loss: 0.02337503805756569  PSNR: 19.510114669799805
[TRAIN] Iter: 477300 Loss: 0.023211192339658737  PSNR: 19.463960647583008
[TRAIN] Iter: 477400 Loss: 0.02529732696712017  PSNR: 19.126018524169922
[TRAIN] Iter: 477500 Loss: 0.02497827634215355  PSNR: 19.227794647216797
[TRAIN] Iter: 477600 Loss: 0.023342914879322052  PSNR: 19.52556037902832
[TRAIN] Iter: 477700 Loss: 0.02300547994673252  PSNR: 19.70303726196289
[TRAIN] Iter: 477800 Loss: 0.032030873000621796  PSNR: 18.20337677001953
[TRAIN] Iter: 477900 Loss: 0.026721369475126266  PSNR: 18.931215286254883
[TRAIN] Iter: 478000 Loss: 0.028089389204978943  PSNR: 18.645849227905273
[TRAIN] Iter: 478100 Loss: 0.03117765486240387  PSNR: 18.31637954711914
[TRAIN] Iter: 478200 Loss: 0.028669461607933044  PSNR: 18.585708618164062
[TRAIN] Iter: 478300 Loss: 0.02370966598391533  PSNR: 19.25008201599121
[TRAIN] Iter: 478400 Loss: 0.02930629253387451  PSNR: 18.355518341064453
[TRAIN] Iter: 478500 Loss: 0.02603285387158394  PSNR: 19.13596534729004
[TRAIN] Iter: 478600 Loss: 0.028368165716528893  PSNR: 18.7863712310791
[TRAIN] Iter: 478700 Loss: 0.023500844836235046  PSNR: 19.363950729370117
[TRAIN] Iter: 478800 Loss: 0.03451547771692276  PSNR: 17.84677505493164
[TRAIN] Iter: 478900 Loss: 0.01996389962732792  PSNR: 20.162540435791016
[TRAIN] Iter: 479000 Loss: 0.02266058884561062  PSNR: 19.592601776123047
[TRAIN] Iter: 479100 Loss: 0.02960098162293434  PSNR: 18.431922912597656
[TRAIN] Iter: 479200 Loss: 0.024655012413859367  PSNR: 19.202695846557617
[TRAIN] Iter: 479300 Loss: 0.020133260637521744  PSNR: 20.022974014282227
[TRAIN] Iter: 479400 Loss: 0.03164996579289436  PSNR: 18.2396183013916
[TRAIN] Iter: 479500 Loss: 0.031076576560735703  PSNR: 18.341033935546875
[TRAIN] Iter: 479600 Loss: 0.025187984108924866  PSNR: 19.5035400390625
[TRAIN] Iter: 479700 Loss: 0.026271626353263855  PSNR: 18.965633392333984
[TRAIN] Iter: 479800 Loss: 0.02646203711628914  PSNR: 18.959224700927734
[TRAIN] Iter: 479900 Loss: 0.02321489527821541  PSNR: 19.670007705688477
Saved checkpoints at ./logs/TUT-LAB-nerf/480000.tar
[TRAIN] Iter: 480000 Loss: 0.031164605170488358  PSNR: 18.257949829101562
[TRAIN] Iter: 480100 Loss: 0.032369621098041534  PSNR: 18.091218948364258
[TRAIN] Iter: 480200 Loss: 0.031550392508506775  PSNR: 18.20917320251465
[TRAIN] Iter: 480300 Loss: 0.025029273703694344  PSNR: 19.01692008972168
[TRAIN] Iter: 480400 Loss: 0.024879571050405502  PSNR: 19.220502853393555
[TRAIN] Iter: 480500 Loss: 0.0228082537651062  PSNR: 19.63663101196289
[TRAIN] Iter: 480600 Loss: 0.032398756593465805  PSNR: 18.094053268432617
[TRAIN] Iter: 480700 Loss: 0.02822493016719818  PSNR: 18.732812881469727
[TRAIN] Iter: 480800 Loss: 0.02497454732656479  PSNR: 19.42908477783203
[TRAIN] Iter: 480900 Loss: 0.024510370567440987  PSNR: 19.472686767578125
[TRAIN] Iter: 481000 Loss: 0.02526683360338211  PSNR: 18.534992218017578
[TRAIN] Iter: 481100 Loss: 0.03351201117038727  PSNR: 18.039337158203125
[TRAIN] Iter: 481200 Loss: 0.028485339134931564  PSNR: 18.657339096069336
[TRAIN] Iter: 481300 Loss: 0.022177206352353096  PSNR: 19.596355438232422
[TRAIN] Iter: 481400 Loss: 0.02572179213166237  PSNR: 19.699695587158203
[TRAIN] Iter: 481500 Loss: 0.02509569190442562  PSNR: 18.98575782775879
[TRAIN] Iter: 481600 Loss: 0.025541137903928757  PSNR: 18.961971282958984
[TRAIN] Iter: 481700 Loss: 0.03212020546197891  PSNR: 18.091360092163086
[TRAIN] Iter: 481800 Loss: 0.023595960810780525  PSNR: 19.356239318847656
[TRAIN] Iter: 481900 Loss: 0.026056503877043724  PSNR: 18.9766902923584
[TRAIN] Iter: 482000 Loss: 0.025891289114952087  PSNR: 19.49932289123535
[TRAIN] Iter: 482100 Loss: 0.028973665088415146  PSNR: 18.497268676757812
[TRAIN] Iter: 482200 Loss: 0.026615768671035767  PSNR: 18.498268127441406
[TRAIN] Iter: 482300 Loss: 0.025485839694738388  PSNR: 19.11737060546875
[TRAIN] Iter: 482400 Loss: 0.024717722088098526  PSNR: 19.34238624572754
[TRAIN] Iter: 482500 Loss: 0.022349070757627487  PSNR: 19.771642684936523
[TRAIN] Iter: 482600 Loss: 0.0328001007437706  PSNR: 18.092872619628906
[TRAIN] Iter: 482700 Loss: 0.024437155574560165  PSNR: 19.490875244140625
[TRAIN] Iter: 482800 Loss: 0.029810113832354546  PSNR: 18.454315185546875
[TRAIN] Iter: 482900 Loss: 0.022557374089956284  PSNR: 20.178449630737305
[TRAIN] Iter: 483000 Loss: 0.023517508059740067  PSNR: 19.7254638671875
[TRAIN] Iter: 483100 Loss: 0.023272765800356865  PSNR: 19.43181800842285
[TRAIN] Iter: 483200 Loss: 0.02124151773750782  PSNR: 19.351608276367188
[TRAIN] Iter: 483300 Loss: 0.030645694583654404  PSNR: 18.399198532104492
[TRAIN] Iter: 483400 Loss: 0.02866946905851364  PSNR: 18.587236404418945
[TRAIN] Iter: 483500 Loss: 0.024127312004566193  PSNR: 19.53602409362793
[TRAIN] Iter: 483600 Loss: 0.0333852656185627  PSNR: 17.81952667236328
[TRAIN] Iter: 483700 Loss: 0.024494264274835587  PSNR: 19.45375633239746
[TRAIN] Iter: 483800 Loss: 0.021983465179800987  PSNR: 19.872892379760742
[TRAIN] Iter: 483900 Loss: 0.02282116934657097  PSNR: 19.78459930419922
[TRAIN] Iter: 484000 Loss: 0.026683542877435684  PSNR: 19.04119110107422
[TRAIN] Iter: 484100 Loss: 0.02396097406744957  PSNR: 19.27118492126465
[TRAIN] Iter: 484200 Loss: 0.02939939685165882  PSNR: 18.672149658203125
[TRAIN] Iter: 484300 Loss: 0.024857159703969955  PSNR: 19.4144229888916
[TRAIN] Iter: 484400 Loss: 0.03160109370946884  PSNR: 18.202810287475586
[TRAIN] Iter: 484500 Loss: 0.026471393182873726  PSNR: 18.99336051940918
[TRAIN] Iter: 484600 Loss: 0.03383370861411095  PSNR: 17.89261245727539
[TRAIN] Iter: 484700 Loss: 0.02934061363339424  PSNR: 18.643022537231445
[TRAIN] Iter: 484800 Loss: 0.024142345413565636  PSNR: 19.220251083374023
[TRAIN] Iter: 484900 Loss: 0.02572900801897049  PSNR: 18.921010971069336
[TRAIN] Iter: 485000 Loss: 0.02354506216943264  PSNR: 19.518335342407227
[TRAIN] Iter: 485100 Loss: 0.024862296879291534  PSNR: 19.101858139038086
[TRAIN] Iter: 485200 Loss: 0.02728845551609993  PSNR: 18.520654678344727
[TRAIN] Iter: 485300 Loss: 0.027987223118543625  PSNR: 18.749225616455078
[TRAIN] Iter: 485400 Loss: 0.027296405285596848  PSNR: 18.678091049194336
[TRAIN] Iter: 485500 Loss: 0.02114623226225376  PSNR: 19.89478302001953
[TRAIN] Iter: 485600 Loss: 0.025501102209091187  PSNR: 19.1381778717041
[TRAIN] Iter: 485700 Loss: 0.025985268875956535  PSNR: 19.06734848022461
[TRAIN] Iter: 485800 Loss: 0.02814457379281521  PSNR: 18.72676658630371
[TRAIN] Iter: 485900 Loss: 0.02811611071228981  PSNR: 18.645366668701172
[TRAIN] Iter: 486000 Loss: 0.023603251203894615  PSNR: 19.484342575073242
[TRAIN] Iter: 486100 Loss: 0.02950838953256607  PSNR: 18.243328094482422
[TRAIN] Iter: 486200 Loss: 0.024673238396644592  PSNR: 19.34686851501465
[TRAIN] Iter: 486300 Loss: 0.022895969450473785  PSNR: 19.298809051513672
[TRAIN] Iter: 486400 Loss: 0.02532782219350338  PSNR: 19.182626724243164
[TRAIN] Iter: 486500 Loss: 0.02037169225513935  PSNR: 19.97372055053711
[TRAIN] Iter: 486600 Loss: 0.023843243718147278  PSNR: 19.500715255737305
[TRAIN] Iter: 486700 Loss: 0.02670302242040634  PSNR: 18.892431259155273
[TRAIN] Iter: 486800 Loss: 0.024674000218510628  PSNR: 19.011268615722656
[TRAIN] Iter: 486900 Loss: 0.023089298978447914  PSNR: 19.33501434326172
[TRAIN] Iter: 487000 Loss: 0.019324705004692078  PSNR: 20.28933334350586
[TRAIN] Iter: 487100 Loss: 0.02534990757703781  PSNR: 19.281688690185547
[TRAIN] Iter: 487200 Loss: 0.0248082485049963  PSNR: 19.1224308013916
[TRAIN] Iter: 487300 Loss: 0.029781516641378403  PSNR: 18.46010971069336
[TRAIN] Iter: 487400 Loss: 0.02317863516509533  PSNR: 19.669532775878906
[TRAIN] Iter: 487500 Loss: 0.027869760990142822  PSNR: 18.190223693847656
[TRAIN] Iter: 487600 Loss: 0.03093189187347889  PSNR: 18.231828689575195
[TRAIN] Iter: 487700 Loss: 0.027928952127695084  PSNR: 18.69491958618164
[TRAIN] Iter: 487800 Loss: 0.025479910895228386  PSNR: 19.345317840576172
[TRAIN] Iter: 487900 Loss: 0.0313793420791626  PSNR: 18.279735565185547
[TRAIN] Iter: 488000 Loss: 0.03190275654196739  PSNR: 18.167821884155273
[TRAIN] Iter: 488100 Loss: 0.025513533502817154  PSNR: 19.085933685302734
[TRAIN] Iter: 488200 Loss: 0.0244305357336998  PSNR: 19.35605239868164
[TRAIN] Iter: 488300 Loss: 0.02167646400630474  PSNR: 20.009098052978516
[TRAIN] Iter: 488400 Loss: 0.02631267160177231  PSNR: 19.108264923095703
[TRAIN] Iter: 488500 Loss: 0.025742044672369957  PSNR: 18.98554039001465
[TRAIN] Iter: 488600 Loss: 0.025214657187461853  PSNR: 19.023576736450195
[TRAIN] Iter: 488700 Loss: 0.03200043365359306  PSNR: 18.09762191772461
[TRAIN] Iter: 488800 Loss: 0.02955281361937523  PSNR: 18.606069564819336
[TRAIN] Iter: 488900 Loss: 0.02243543043732643  PSNR: 19.554035186767578
[TRAIN] Iter: 489000 Loss: 0.030594846233725548  PSNR: 18.153587341308594
[TRAIN] Iter: 489100 Loss: 0.029492203146219254  PSNR: 18.4375057220459
[TRAIN] Iter: 489200 Loss: 0.027734799310564995  PSNR: 18.817407608032227
[TRAIN] Iter: 489300 Loss: 0.02163633517920971  PSNR: 19.726144790649414
[TRAIN] Iter: 489400 Loss: 0.023430727422237396  PSNR: 19.598995208740234
[TRAIN] Iter: 489500 Loss: 0.022081248462200165  PSNR: 19.694480895996094
[TRAIN] Iter: 489600 Loss: 0.0299026221036911  PSNR: 18.342649459838867
[TRAIN] Iter: 489700 Loss: 0.021604221314191818  PSNR: 19.93010711669922
[TRAIN] Iter: 489800 Loss: 0.02860938012599945  PSNR: 18.521974563598633
[TRAIN] Iter: 489900 Loss: 0.025257106870412827  PSNR: 19.22429084777832
Saved checkpoints at ./logs/TUT-LAB-nerf/490000.tar
[TRAIN] Iter: 490000 Loss: 0.029336243867874146  PSNR: 18.519628524780273
[TRAIN] Iter: 490100 Loss: 0.032314036041498184  PSNR: 18.05828285217285
[TRAIN] Iter: 490200 Loss: 0.020748360082507133  PSNR: 19.858421325683594
[TRAIN] Iter: 490300 Loss: 0.022183433175086975  PSNR: 19.509479522705078
[TRAIN] Iter: 490400 Loss: 0.023421233519911766  PSNR: 19.530454635620117
[TRAIN] Iter: 490500 Loss: 0.026337282732129097  PSNR: 18.989484786987305
[TRAIN] Iter: 490600 Loss: 0.03296805918216705  PSNR: 18.06357192993164
[TRAIN] Iter: 490700 Loss: 0.028336212038993835  PSNR: 18.786996841430664
[TRAIN] Iter: 490800 Loss: 0.027119364589452744  PSNR: 18.862083435058594
[TRAIN] Iter: 490900 Loss: 0.026143964380025864  PSNR: 19.101518630981445
[TRAIN] Iter: 491000 Loss: 0.022476881742477417  PSNR: 19.632333755493164
[TRAIN] Iter: 491100 Loss: 0.02410600148141384  PSNR: 19.276521682739258
[TRAIN] Iter: 491200 Loss: 0.029469892382621765  PSNR: 18.483108520507812
[TRAIN] Iter: 491300 Loss: 0.03166166692972183  PSNR: 18.32799530029297
[TRAIN] Iter: 491400 Loss: 0.025901807472109795  PSNR: 19.06964874267578
[TRAIN] Iter: 491500 Loss: 0.026491349563002586  PSNR: 19.42046546936035
[TRAIN] Iter: 491600 Loss: 0.03065277263522148  PSNR: 18.267696380615234
[TRAIN] Iter: 491700 Loss: 0.035287946462631226  PSNR: 17.62309455871582
[TRAIN] Iter: 491800 Loss: 0.03184244781732559  PSNR: 18.172658920288086
[TRAIN] Iter: 491900 Loss: 0.026791367679834366  PSNR: 18.980253219604492
[TRAIN] Iter: 492000 Loss: 0.03033592738211155  PSNR: 18.323291778564453
[TRAIN] Iter: 492100 Loss: 0.02755781076848507  PSNR: 18.675935745239258
[TRAIN] Iter: 492200 Loss: 0.02486976981163025  PSNR: 18.696453094482422
[TRAIN] Iter: 492300 Loss: 0.028771575540304184  PSNR: 18.776124954223633
[TRAIN] Iter: 492400 Loss: 0.027591153979301453  PSNR: 18.74360466003418
[TRAIN] Iter: 492500 Loss: 0.020652037113904953  PSNR: 19.938013076782227
[TRAIN] Iter: 492600 Loss: 0.02768988534808159  PSNR: 18.857690811157227
[TRAIN] Iter: 492700 Loss: 0.02228766307234764  PSNR: 19.409297943115234
[TRAIN] Iter: 492800 Loss: 0.02331780083477497  PSNR: 19.585472106933594
[TRAIN] Iter: 492900 Loss: 0.02904640883207321  PSNR: 18.596851348876953
[TRAIN] Iter: 493000 Loss: 0.02140616625547409  PSNR: 19.83012580871582
[TRAIN] Iter: 493100 Loss: 0.025280460715293884  PSNR: 19.063091278076172
[TRAIN] Iter: 493200 Loss: 0.02797757275402546  PSNR: 18.813108444213867
[TRAIN] Iter: 493300 Loss: 0.01976992003619671  PSNR: 20.38808250427246
[TRAIN] Iter: 493400 Loss: 0.02003249153494835  PSNR: 20.005666732788086
[TRAIN] Iter: 493500 Loss: 0.025582104921340942  PSNR: 19.09543228149414
[TRAIN] Iter: 493600 Loss: 0.027965659275650978  PSNR: 18.733173370361328
[TRAIN] Iter: 493700 Loss: 0.03291885182261467  PSNR: 17.97280502319336
[TRAIN] Iter: 493800 Loss: 0.027125177904963493  PSNR: 19.1434268951416
[TRAIN] Iter: 493900 Loss: 0.029239878058433533  PSNR: 18.79025650024414
[TRAIN] Iter: 494000 Loss: 0.024693958461284637  PSNR: 19.115995407104492
[TRAIN] Iter: 494100 Loss: 0.02842610888183117  PSNR: 18.701183319091797
[TRAIN] Iter: 494200 Loss: 0.02059296704828739  PSNR: 19.94388198852539
[TRAIN] Iter: 494300 Loss: 0.025844672694802284  PSNR: 19.19824981689453
[TRAIN] Iter: 494400 Loss: 0.02615535445511341  PSNR: 18.985549926757812
[TRAIN] Iter: 494500 Loss: 0.022956177592277527  PSNR: 19.543235778808594
[TRAIN] Iter: 494600 Loss: 0.024571716785430908  PSNR: 19.134611129760742
[TRAIN] Iter: 494700 Loss: 0.025809396058321  PSNR: 19.27590560913086
[TRAIN] Iter: 494800 Loss: 0.019701534882187843  PSNR: 20.37665557861328
[TRAIN] Iter: 494900 Loss: 0.02546028047800064  PSNR: 19.16250228881836
[TRAIN] Iter: 495000 Loss: 0.02461131289601326  PSNR: 19.33445930480957
[TRAIN] Iter: 495100 Loss: 0.0255131758749485  PSNR: 19.160133361816406
[TRAIN] Iter: 495200 Loss: 0.028811505064368248  PSNR: 18.630895614624023
[TRAIN] Iter: 495300 Loss: 0.028324563056230545  PSNR: 18.78520393371582
[TRAIN] Iter: 495400 Loss: 0.026733405888080597  PSNR: 18.98979377746582
[TRAIN] Iter: 495500 Loss: 0.02387489564716816  PSNR: 19.231060028076172
[TRAIN] Iter: 495600 Loss: 0.0230628103017807  PSNR: 19.472187042236328
[TRAIN] Iter: 495700 Loss: 0.022233955562114716  PSNR: 19.94647216796875
[TRAIN] Iter: 495800 Loss: 0.029304973781108856  PSNR: 18.51686668395996
[TRAIN] Iter: 495900 Loss: 0.023984942585229874  PSNR: 19.234661102294922
[TRAIN] Iter: 496000 Loss: 0.02353576198220253  PSNR: 19.455537796020508
[TRAIN] Iter: 496100 Loss: 0.03030744567513466  PSNR: 18.461034774780273
[TRAIN] Iter: 496200 Loss: 0.025662235915660858  PSNR: 18.98214340209961
[TRAIN] Iter: 496300 Loss: 0.026430096477270126  PSNR: 19.017370223999023
[TRAIN] Iter: 496400 Loss: 0.027406979352235794  PSNR: 18.877635955810547
[TRAIN] Iter: 496500 Loss: 0.029017385095357895  PSNR: 18.58115577697754
[TRAIN] Iter: 496600 Loss: 0.027098044753074646  PSNR: 18.928932189941406
[TRAIN] Iter: 496700 Loss: 0.027740545570850372  PSNR: 18.9298152923584
[TRAIN] Iter: 496800 Loss: 0.024738062173128128  PSNR: 19.31186294555664
[TRAIN] Iter: 496900 Loss: 0.027026591822504997  PSNR: 18.929508209228516
[TRAIN] Iter: 497000 Loss: 0.03317565098404884  PSNR: 18.02501678466797
[TRAIN] Iter: 497100 Loss: 0.023082496598362923  PSNR: 19.585241317749023
[TRAIN] Iter: 497200 Loss: 0.019614387303590775  PSNR: 20.270484924316406
[TRAIN] Iter: 497300 Loss: 0.030403384938836098  PSNR: 18.32655906677246
[TRAIN] Iter: 497400 Loss: 0.025612108409404755  PSNR: 19.226709365844727
[TRAIN] Iter: 497500 Loss: 0.02122587338089943  PSNR: 19.920452117919922
[TRAIN] Iter: 497600 Loss: 0.01985207200050354  PSNR: 20.22928810119629
[TRAIN] Iter: 497700 Loss: 0.029185764491558075  PSNR: 18.640737533569336
[TRAIN] Iter: 497800 Loss: 0.026419414207339287  PSNR: 19.12906265258789
[TRAIN] Iter: 497900 Loss: 0.030786057934165  PSNR: 18.320575714111328
[TRAIN] Iter: 498000 Loss: 0.024829821661114693  PSNR: 19.356842041015625
[TRAIN] Iter: 498100 Loss: 0.02449316717684269  PSNR: 19.39823341369629
[TRAIN] Iter: 498200 Loss: 0.02811388671398163  PSNR: 18.681365966796875
[TRAIN] Iter: 498300 Loss: 0.023853160440921783  PSNR: 19.47724723815918
[TRAIN] Iter: 498400 Loss: 0.02134002558887005  PSNR: 19.96441078186035
[TRAIN] Iter: 498500 Loss: 0.03133483976125717  PSNR: 18.16023826599121
[TRAIN] Iter: 498600 Loss: 0.02665192075073719  PSNR: 18.775245666503906
[TRAIN] Iter: 498700 Loss: 0.020777251571416855  PSNR: 19.977903366088867
[TRAIN] Iter: 498800 Loss: 0.025973306968808174  PSNR: 18.894521713256836
[TRAIN] Iter: 498900 Loss: 0.02219337597489357  PSNR: 19.315916061401367
[TRAIN] Iter: 499000 Loss: 0.026043353602290154  PSNR: 19.344995498657227
[TRAIN] Iter: 499100 Loss: 0.0294185783714056  PSNR: 18.561250686645508
[TRAIN] Iter: 499200 Loss: 0.028527647256851196  PSNR: 18.694509506225586
[TRAIN] Iter: 499300 Loss: 0.0214688740670681  PSNR: 19.774856567382812
[TRAIN] Iter: 499400 Loss: 0.026018615812063217  PSNR: 19.078792572021484
[TRAIN] Iter: 499500 Loss: 0.02888023480772972  PSNR: 18.560983657836914
[TRAIN] Iter: 499600 Loss: 0.03155343979597092  PSNR: 18.318941116333008
[TRAIN] Iter: 499700 Loss: 0.029921891167759895  PSNR: 18.477779388427734
[TRAIN] Iter: 499800 Loss: 0.02959977090358734  PSNR: 18.44154167175293
[TRAIN] Iter: 499900 Loss: 0.025688737630844116  PSNR: 19.10424041748047
Saved checkpoints at ./logs/TUT-LAB-nerf/500000.tar
0 0.0004305839538574219
torch.Size([320, 640, 3]) torch.Size([320, 640])
1 16.809875965118408
2 17.928669929504395
3 16.59607458114624
4 17.557244300842285
5 16.44109344482422
6 17.549338817596436
7 16.65646743774414
8 17.06565237045288
9 16.58977484703064
10 16.897715091705322
11 16.736226797103882
12 16.802496910095215
13 16.860878467559814
14 16.80751395225525
15 16.852195739746094
16 16.716832637786865
17 16.851147174835205
18 16.770575523376465
19 16.84598469734192
20 16.786548137664795
21 16.94408869743347
22 16.656490087509155
23 16.906120538711548
24 16.73917865753174
25 16.888849020004272
26 16.72268271446228
27 16.854583501815796
28 16.764058589935303
29 17.360305547714233
30 16.283004999160767
31 17.324366807937622
32 16.33332586288452
33 17.360093355178833
34 16.2066752910614
35 17.313318252563477
36 16.232383728027344
37 17.376670598983765
38 16.280428171157837
39 17.354376792907715
40 16.34062647819519
41 17.2642560005188
42 16.273812532424927
43 17.263132572174072
44 16.302989959716797
45 17.302565813064575
46 16.3959801197052
47 17.24863576889038
48 16.396378993988037
49 17.07605767250061
50 16.514511108398438
51 17.053784608840942
52 16.587957620620728
53 16.983933925628662
54 16.596415996551514
55 16.85580325126648
56 16.723830223083496
57 16.84657120704651
58 16.75762963294983
59 16.79637885093689
60 16.8013174533844
61 16.748488664627075
62 16.885393857955933
63 16.786163330078125
64 16.892460107803345
65 16.7741756439209
66 16.797723531723022
67 16.76426362991333
68 16.9209885597229
69 16.704302549362183
70 17.012351512908936
71 16.784125328063965
72 16.923906326293945
73 16.715072870254517
74 17.411218404769897
75 16.22761058807373
76 17.376567125320435
77 16.24218440055847
78 17.405576944351196
79 16.305420875549316
80 17.341163396835327
81 12.903181314468384
82 17.382477283477783
83 16.11245632171631
84 17.326158046722412
85 15.475036859512329
86 14.884760856628418
87 14.72541880607605
88 14.943206310272217
89 14.974550008773804
90 14.829617261886597
91 15.134620904922485
92 14.463428497314453
93 15.424772500991821
94 14.344833374023438
95 15.418507099151611
96 14.225337266921997
97 14.879130363464355
98 14.798970460891724
99 14.986287832260132
100 15.010112524032593
101 14.777515411376953
102 15.229683637619019
103 14.426994323730469
104 15.4769446849823
105 14.323943138122559
106 15.421250343322754
107 14.207923889160156
108 14.89437985420227
109 14.830305099487305
110 14.921038150787354
111 15.005066633224487
112 14.782131433486938
113 15.260742664337158
114 14.380639791488647
115 15.422957181930542
116 14.375293731689453
117 15.442470073699951
118 14.225437879562378
119 14.811800479888916
Done, saving (120, 320, 640, 3) (120, 320, 640)
extras:{'raw': tensor([[[-5.2068e-01, -7.9366e-01, -1.0817e+00, -4.4894e+01],
         [-9.5287e-01, -8.4090e-01, -4.7980e-01,  1.0329e+01],
         [-9.5824e-01, -8.7587e-01, -5.1903e-01,  1.1272e+01],
         ...,
         [-7.8546e+00, -5.3337e+00,  4.0975e+00, -8.5492e+01],
         [-8.7321e+00, -6.2441e+00,  2.8743e+00, -7.3614e+01],
         [-8.5332e+00, -6.0917e+00,  3.2623e+00, -8.4698e+01]],

        [[ 1.1695e-01,  1.7006e-01,  5.7064e-01, -1.1091e+01],
         [ 3.7463e-01,  3.4733e-01,  4.3626e-01,  6.6439e+00],
         [-1.2042e+00, -1.0065e+00, -5.9716e-01, -1.2502e+01],
         ...,
         [-2.7797e-02,  7.3371e-01,  3.0013e+00,  1.1197e+01],
         [-4.5838e-01,  4.2715e-01,  3.2070e+00,  1.0803e+01],
         [-4.5786e+00, -2.9012e+00,  5.6360e-01,  1.2334e+01]],

        [[-2.1079e-01, -4.9069e-01, -4.0220e-01, -1.2798e+01],
         [-7.7852e-02, -2.7759e-01, -2.0015e-01,  5.4409e+00],
         [-8.2112e-02, -2.8036e-01, -2.0067e-01,  5.4413e+00],
         ...,
         [-1.2416e+01, -1.1569e+01, -1.2492e+01,  5.5958e+01],
         [-1.4143e+01, -1.2234e+01, -1.1727e+01,  5.9986e+01],
         [-1.4902e+01, -1.3740e+01, -1.4215e+01,  3.5649e+01]],

        ...,

        [[-4.7534e-01, -8.0921e-01, -8.2785e-01, -2.6947e+01],
         [-8.6923e-01, -9.5373e-01, -8.5383e-01, -2.3408e+01],
         [-5.2080e-01, -9.8034e-01, -1.5270e+00, -2.9327e+01],
         ...,
         [-1.7568e+01, -1.2696e+01, -7.7094e+00,  1.3973e+02],
         [-1.4396e+01, -8.8941e+00, -1.3816e+00,  1.4685e+02],
         [-1.8896e+01, -1.3947e+01, -8.6560e+00,  1.4083e+02]],

        [[-2.9454e-01, -5.0740e-01, -7.4349e-01,  1.1959e+00],
         [-1.4463e-01, -2.8483e-01, -3.3257e-01,  1.4250e+01],
         [-1.7180e-01, -3.4074e-01, -5.0810e-01,  1.3586e+01],
         ...,
         [ 1.9428e+00,  1.3532e+00,  3.5349e-01,  1.1359e+02],
         [ 1.9021e+00,  1.2211e+00,  3.7183e-02,  1.0262e+02],
         [ 2.8593e+00,  2.1366e+00,  1.0071e+00,  1.0002e+02]],

        [[ 2.2875e-01,  5.5249e-02, -5.2758e-02, -1.1513e+01],
         [ 1.4596e-01,  4.2909e-02, -5.5433e-03, -4.8263e+00],
         [ 2.1991e-01,  5.8111e-02, -1.8635e-02, -3.1581e+00],
         ...,
         [-3.7475e+00, -4.2204e+00, -8.6236e+00,  7.4855e+02],
         [-4.4460e+00, -4.9940e+00, -9.9688e+00,  7.5928e+02],
         [-4.4927e+00, -4.7374e+00, -9.1778e+00,  7.5281e+02]]],
       grad_fn=<ReshapeAliasBackward0>), 'rgb0': tensor([[0.2375, 0.2667, 0.3654],
        [0.6230, 0.6087, 0.6253],
        [0.4527, 0.4162, 0.4343],
        ...,
        [0.2241, 0.2404, 0.2865],
        [0.4241, 0.4073, 0.4005],
        [0.5442, 0.5097, 0.4903]], grad_fn=<ReshapeAliasBackward0>), 'disp0': tensor([ 42.4242,  12.8687, 122.0732,  ...,  32.5518,  15.3099,  76.1218],
       grad_fn=<ReshapeAliasBackward0>), 'acc0': tensor([1., 1., 1.,  ..., 1., 1., 1.], grad_fn=<ReshapeAliasBackward0>), 'z_std': tensor([0.1080, 0.0059, 0.0015,  ..., 0.0032, 0.0044, 0.0031])}
0 0.0006844997406005859
torch.Size([320, 640, 3]) torch.Size([320, 640])
1 14.850193500518799
2 14.825478792190552
3 14.884853601455688
4 15.103224754333496
5 14.59596562385559
6 15.44658899307251
7 14.312246561050415
8 15.50969409942627
9 14.303435564041138
10 14.899293422698975
11 14.728412866592407
12 14.840610980987549
13 14.972779989242554
14 14.870621919631958
15 15.211066961288452
16 14.517712354660034
17 15.423434257507324
18 14.28788447380066
19 15.481014251708984
20 14.959399223327637
21 14.90324592590332
22 14.090596914291382
23 14.814323425292969
24 15.212242603302002
25 14.494380950927734
26 15.463167667388916
27 14.773115873336792
28 14.673941135406494
29 14.808143138885498
30 15.256221771240234
31 14.645294189453125
32 14.5102858543396
33 15.177862405776978
34 14.587341547012329
35 15.053305625915527
36 14.607776641845703
37 15.188658237457275
38 14.583353519439697
39 15.303625106811523
40 14.45569396018982
41 15.297794818878174
42 14.3419930934906
43 14.710028171539307
44 15.315347671508789
45 14.574997663497925
46 15.142640113830566
47 14.524026870727539
48 15.252006769180298
49 14.526226282119751
50 15.258350133895874
51 14.543659210205078
52 14.60665774345398
53 15.106117248535156
54 14.631852626800537
55 15.190016984939575
56 14.541747331619263
57 15.198753118515015
58 14.508907079696655
59 15.243616342544556
60 14.560330629348755
61 15.291736841201782
62 14.5561203956604
63 14.441025733947754
64 15.290650367736816
65 14.507108926773071
66 15.307622909545898
67 14.428818702697754
68 15.381052494049072
69 14.46717381477356
70 15.365132331848145
71 14.276923179626465
72 14.554111003875732
73 15.481619596481323
74 14.308785438537598
75 15.27181363105774
76 14.483067989349365
77 15.583566188812256
78 14.297754764556885
79 15.456780910491943
80 14.262216806411743
81 15.472465991973877
82 14.3255455493927
83 14.546614170074463
84 15.28244948387146
85 14.483898639678955
86 15.414989233016968
87 14.399033308029175
88 15.538203716278076
89 14.531798839569092
90 15.147495746612549
91 14.526410102844238
92 14.479946374893188
93 15.328245401382446
94 14.52137279510498
95 15.254917621612549
96 14.47218942642212
97 15.25359582901001
98 14.479948282241821
99 15.293043613433838
100 14.525092601776123
101 15.291426181793213
102 14.514019012451172
103 14.477964639663696
104 15.32895040512085
105 14.520228862762451
106 15.347535610198975
107 14.539862394332886
108 15.334045171737671
109 14.535396575927734
110 15.356102228164673
111 14.516283988952637
112 14.404897689819336
113 15.280903816223145
114 14.488516092300415
115 15.346421718597412
116 14.47108244895935
117 15.223073244094849
118 14.51126766204834
119 15.321655511856079
test poses shape torch.Size([13, 3, 4])
0 0.0008573532104492188
torch.Size([320, 640, 3]) torch.Size([320, 640])
1 15.341357946395874
2 14.540379762649536
3 14.544090747833252
4 15.342702388763428
5 14.573269605636597
6 15.327461004257202
7 14.514949798583984
8 15.389563798904419
9 14.536025762557983
10 15.376210451126099
11 14.536750078201294
12 15.430045366287231
Saved test set
[TRAIN] Iter: 500000 Loss: 0.028034565970301628  PSNR: 18.691404342651367
[TRAIN] Iter: 500100 Loss: 0.025227632373571396  PSNR: 19.339750289916992
[TRAIN] Iter: 500200 Loss: 0.024912703782320023  PSNR: 19.26628303527832
[TRAIN] Iter: 500300 Loss: 0.026757385581731796  PSNR: 18.829832077026367
[TRAIN] Iter: 500400 Loss: 0.022796127945184708  PSNR: 19.588605880737305
[TRAIN] Iter: 500500 Loss: 0.02760254591703415  PSNR: 18.76352882385254
[TRAIN] Iter: 500600 Loss: 0.03503566235303879  PSNR: 17.728851318359375
[TRAIN] Iter: 500700 Loss: 0.025911346077919006  PSNR: 19.1534481048584
[TRAIN] Iter: 500800 Loss: 0.02126140519976616  PSNR: 20.137887954711914
[TRAIN] Iter: 500900 Loss: 0.029395703226327896  PSNR: 18.46092987060547
[TRAIN] Iter: 501000 Loss: 0.025882963091135025  PSNR: 19.26072120666504
[TRAIN] Iter: 501100 Loss: 0.028367485851049423  PSNR: 18.712879180908203
[TRAIN] Iter: 501200 Loss: 0.03073049895465374  PSNR: 18.32266616821289
[TRAIN] Iter: 501300 Loss: 0.029052086174488068  PSNR: 18.674907684326172
[TRAIN] Iter: 501400 Loss: 0.03205597400665283  PSNR: 18.076248168945312
[TRAIN] Iter: 501500 Loss: 0.02301902323961258  PSNR: 20.005704879760742
[TRAIN] Iter: 501600 Loss: 0.028659122064709663  PSNR: 18.58547019958496
[TRAIN] Iter: 501700 Loss: 0.028361648321151733  PSNR: 18.943567276000977
[TRAIN] Iter: 501800 Loss: 0.028963100165128708  PSNR: 18.556428909301758
[TRAIN] Iter: 501900 Loss: 0.03060610592365265  PSNR: 18.31559181213379
[TRAIN] Iter: 502000 Loss: 0.03057488054037094  PSNR: 18.33201026916504
[TRAIN] Iter: 502100 Loss: 0.022579513490200043  PSNR: 19.818519592285156
[TRAIN] Iter: 502200 Loss: 0.02765549346804619  PSNR: 18.7817325592041
[TRAIN] Iter: 502300 Loss: 0.03006577119231224  PSNR: 18.38595962524414
[TRAIN] Iter: 502400 Loss: 0.02420005574822426  PSNR: 19.893409729003906
[TRAIN] Iter: 502500 Loss: 0.025334157049655914  PSNR: 19.190340042114258
[TRAIN] Iter: 502600 Loss: 0.028728129342198372  PSNR: 18.422096252441406
[TRAIN] Iter: 502700 Loss: 0.030712107196450233  PSNR: 18.320016860961914
[TRAIN] Iter: 502800 Loss: 0.03125734627246857  PSNR: 18.32487678527832
[TRAIN] Iter: 502900 Loss: 0.025269141420722008  PSNR: 19.260860443115234
[TRAIN] Iter: 503000 Loss: 0.0301662664860487  PSNR: 18.431604385375977
[TRAIN] Iter: 503100 Loss: 0.026374436914920807  PSNR: 19.00297737121582
[TRAIN] Iter: 503200 Loss: 0.027012940496206284  PSNR: 18.93486785888672
[TRAIN] Iter: 503300 Loss: 0.029187096282839775  PSNR: 18.54669952392578
[TRAIN] Iter: 503400 Loss: 0.025858309119939804  PSNR: 19.112516403198242
[TRAIN] Iter: 503500 Loss: 0.030928408727049828  PSNR: 18.15989112854004
[TRAIN] Iter: 503600 Loss: 0.02946378104388714  PSNR: 18.490238189697266
[TRAIN] Iter: 503700 Loss: 0.024392040446400642  PSNR: 19.537996292114258
[TRAIN] Iter: 503800 Loss: 0.024105297401547432  PSNR: 19.52631378173828
[TRAIN] Iter: 503900 Loss: 0.025358116254210472  PSNR: 19.531431198120117
[TRAIN] Iter: 504000 Loss: 0.024804288521409035  PSNR: 18.910415649414062
[TRAIN] Iter: 504100 Loss: 0.029525652527809143  PSNR: 18.501327514648438
[TRAIN] Iter: 504200 Loss: 0.02740630879998207  PSNR: 18.962419509887695
[TRAIN] Iter: 504300 Loss: 0.028079260140657425  PSNR: 18.857311248779297
[TRAIN] Iter: 504400 Loss: 0.02449352666735649  PSNR: 19.21759033203125
[TRAIN] Iter: 504500 Loss: 0.025604378432035446  PSNR: 19.100744247436523
[TRAIN] Iter: 504600 Loss: 0.021382635459303856  PSNR: 20.001338958740234
[TRAIN] Iter: 504700 Loss: 0.027569610625505447  PSNR: 19.094011306762695
[TRAIN] Iter: 504800 Loss: 0.025688517838716507  PSNR: 19.32843589782715
[TRAIN] Iter: 504900 Loss: 0.028794653713703156  PSNR: 18.675100326538086
[TRAIN] Iter: 505000 Loss: 0.02717742696404457  PSNR: 18.948923110961914
[TRAIN] Iter: 505100 Loss: 0.02820313721895218  PSNR: 18.656597137451172
[TRAIN] Iter: 505200 Loss: 0.024772068485617638  PSNR: 19.335851669311523
[TRAIN] Iter: 505300 Loss: 0.02915593981742859  PSNR: 18.423410415649414
[TRAIN] Iter: 505400 Loss: 0.028158951550722122  PSNR: 18.512163162231445
[TRAIN] Iter: 505500 Loss: 0.02465343289077282  PSNR: 19.023435592651367
[TRAIN] Iter: 505600 Loss: 0.03151118382811546  PSNR: 18.311208724975586
[TRAIN] Iter: 505700 Loss: 0.024246813729405403  PSNR: 19.603740692138672
[TRAIN] Iter: 505800 Loss: 0.030237551778554916  PSNR: 18.461448669433594
[TRAIN] Iter: 505900 Loss: 0.030981237068772316  PSNR: 18.24224090576172
[TRAIN] Iter: 506000 Loss: 0.027520684525370598  PSNR: 18.855195999145508
[TRAIN] Iter: 506100 Loss: 0.027558311820030212  PSNR: 18.716541290283203
[TRAIN] Iter: 506200 Loss: 0.029871314764022827  PSNR: 18.46733856201172
[TRAIN] Iter: 506300 Loss: 0.021709559485316277  PSNR: 19.853443145751953
[TRAIN] Iter: 506400 Loss: 0.02047180011868477  PSNR: 20.192150115966797
[TRAIN] Iter: 506500 Loss: 0.030455248430371284  PSNR: 18.2966251373291
[TRAIN] Iter: 506600 Loss: 0.029418282210826874  PSNR: 18.377056121826172
[TRAIN] Iter: 506700 Loss: 0.023281363770365715  PSNR: 19.697160720825195
[TRAIN] Iter: 506800 Loss: 0.03222454711794853  PSNR: 18.099979400634766
[TRAIN] Iter: 506900 Loss: 0.02863459289073944  PSNR: 18.50322151184082
[TRAIN] Iter: 507000 Loss: 0.02575499191880226  PSNR: 18.936504364013672
[TRAIN] Iter: 507100 Loss: 0.03064851649105549  PSNR: 18.372451782226562
[TRAIN] Iter: 507200 Loss: 0.033677488565444946  PSNR: 17.92195701599121
[TRAIN] Iter: 507300 Loss: 0.02280838042497635  PSNR: 19.267953872680664
[TRAIN] Iter: 507400 Loss: 0.02502709999680519  PSNR: 19.260725021362305
[TRAIN] Iter: 507500 Loss: 0.030121255666017532  PSNR: 18.34604263305664
[TRAIN] Iter: 507600 Loss: 0.028271865099668503  PSNR: 18.644439697265625
[TRAIN] Iter: 507700 Loss: 0.02536025270819664  PSNR: 19.17056655883789
[TRAIN] Iter: 507800 Loss: 0.028261469677090645  PSNR: 18.48330307006836
[TRAIN] Iter: 507900 Loss: 0.023513466119766235  PSNR: 19.55698585510254
[TRAIN] Iter: 508000 Loss: 0.0229328703135252  PSNR: 19.558950424194336
[TRAIN] Iter: 508100 Loss: 0.02622126042842865  PSNR: 19.02439308166504
[TRAIN] Iter: 508200 Loss: 0.02103639394044876  PSNR: 20.09342384338379
[TRAIN] Iter: 508300 Loss: 0.026515807956457138  PSNR: 18.867473602294922
[TRAIN] Iter: 508400 Loss: 0.030404625460505486  PSNR: 18.24604034423828
[TRAIN] Iter: 508500 Loss: 0.02369489148259163  PSNR: 19.803647994995117
[TRAIN] Iter: 508600 Loss: 0.02318839356303215  PSNR: 19.5964412689209
[TRAIN] Iter: 508700 Loss: 0.024314459413290024  PSNR: 19.44994354248047
[TRAIN] Iter: 508800 Loss: 0.03137282282114029  PSNR: 18.120195388793945
[TRAIN] Iter: 508900 Loss: 0.025256192311644554  PSNR: 19.235105514526367
[TRAIN] Iter: 509000 Loss: 0.03605182468891144  PSNR: 17.517059326171875
[TRAIN] Iter: 509100 Loss: 0.03285062685608864  PSNR: 17.892736434936523
[TRAIN] Iter: 509200 Loss: 0.023612499237060547  PSNR: 19.685808181762695
[TRAIN] Iter: 509300 Loss: 0.02440940961241722  PSNR: 19.518878936767578
[TRAIN] Iter: 509400 Loss: 0.028455505147576332  PSNR: 18.724618911743164
[TRAIN] Iter: 509500 Loss: 0.03133128955960274  PSNR: 18.26569366455078
[TRAIN] Iter: 509600 Loss: 0.028177982196211815  PSNR: 18.766300201416016
[TRAIN] Iter: 509700 Loss: 0.023086000233888626  PSNR: 19.2680606842041
[TRAIN] Iter: 509800 Loss: 0.02889784798026085  PSNR: 18.636938095092773
[TRAIN] Iter: 509900 Loss: 0.020739786326885223  PSNR: 20.108030319213867
Saved checkpoints at ./logs/TUT-LAB-nerf/510000.tar
[TRAIN] Iter: 510000 Loss: 0.0271310955286026  PSNR: 18.877756118774414
[TRAIN] Iter: 510100 Loss: 0.027221720665693283  PSNR: 18.7424373626709
[TRAIN] Iter: 510200 Loss: 0.026242448017001152  PSNR: 19.03643035888672
[TRAIN] Iter: 510300 Loss: 0.024052338674664497  PSNR: 19.363977432250977
[TRAIN] Iter: 510400 Loss: 0.022952601313591003  PSNR: 19.4311580657959
[TRAIN] Iter: 510500 Loss: 0.02352112904191017  PSNR: 19.121204376220703
[TRAIN] Iter: 510600 Loss: 0.029344996437430382  PSNR: 18.48396873474121
[TRAIN] Iter: 510700 Loss: 0.02372993528842926  PSNR: 19.32508659362793
[TRAIN] Iter: 510800 Loss: 0.02780921198427677  PSNR: 18.57245635986328
[TRAIN] Iter: 510900 Loss: 0.02764797955751419  PSNR: 18.8392391204834
[TRAIN] Iter: 511000 Loss: 0.028829602524638176  PSNR: 18.54375648498535
[TRAIN] Iter: 511100 Loss: 0.02857738547027111  PSNR: 18.646678924560547
[TRAIN] Iter: 511200 Loss: 0.024985654279589653  PSNR: 19.163206100463867
[TRAIN] Iter: 511300 Loss: 0.03132355213165283  PSNR: 18.294776916503906
[TRAIN] Iter: 511400 Loss: 0.0213931817561388  PSNR: 19.92364501953125
[TRAIN] Iter: 511500 Loss: 0.030565716326236725  PSNR: 18.341638565063477
[TRAIN] Iter: 511600 Loss: 0.026734620332717896  PSNR: 18.72090721130371
[TRAIN] Iter: 511700 Loss: 0.026897333562374115  PSNR: 18.943525314331055
[TRAIN] Iter: 511800 Loss: 0.02413121797144413  PSNR: 19.42971420288086
[TRAIN] Iter: 511900 Loss: 0.022091638296842575  PSNR: 19.634687423706055
[TRAIN] Iter: 512000 Loss: 0.02624371647834778  PSNR: 19.013080596923828
[TRAIN] Iter: 512100 Loss: 0.022900190204381943  PSNR: 19.72688102722168
[TRAIN] Iter: 512200 Loss: 0.025952378287911415  PSNR: 19.072673797607422
[TRAIN] Iter: 512300 Loss: 0.029076557606458664  PSNR: 18.52583122253418
[TRAIN] Iter: 512400 Loss: 0.027757681906223297  PSNR: 18.919361114501953
[TRAIN] Iter: 512500 Loss: 0.024562716484069824  PSNR: 19.36599349975586
[TRAIN] Iter: 512600 Loss: 0.027710409834980965  PSNR: 18.857088088989258
[TRAIN] Iter: 512700 Loss: 0.029975131154060364  PSNR: 18.37757682800293
[TRAIN] Iter: 512800 Loss: 0.025741316378116608  PSNR: 19.18724250793457
[TRAIN] Iter: 512900 Loss: 0.02375459112226963  PSNR: 19.142261505126953
[TRAIN] Iter: 513000 Loss: 0.03135028854012489  PSNR: 18.25178337097168
[TRAIN] Iter: 513100 Loss: 0.02187814563512802  PSNR: 19.807594299316406
[TRAIN] Iter: 513200 Loss: 0.028573699295520782  PSNR: 18.613826751708984
[TRAIN] Iter: 513300 Loss: 0.032755110412836075  PSNR: 18.089414596557617
[TRAIN] Iter: 513400 Loss: 0.019011596217751503  PSNR: 20.310659408569336
[TRAIN] Iter: 513500 Loss: 0.026480086147785187  PSNR: 19.09833335876465
[TRAIN] Iter: 513600 Loss: 0.03158947080373764  PSNR: 18.156383514404297
[TRAIN] Iter: 513700 Loss: 0.025448931381106377  PSNR: 18.844097137451172
[TRAIN] Iter: 513800 Loss: 0.025681404396891594  PSNR: 18.85965919494629
[TRAIN] Iter: 513900 Loss: 0.02867741324007511  PSNR: 18.584823608398438
[TRAIN] Iter: 514000 Loss: 0.023918313905596733  PSNR: 19.425579071044922
[TRAIN] Iter: 514100 Loss: 0.02280544489622116  PSNR: 19.6385555267334
[TRAIN] Iter: 514200 Loss: 0.024980220943689346  PSNR: 19.22842788696289
[TRAIN] Iter: 514300 Loss: 0.029255125671625137  PSNR: 18.529672622680664
[TRAIN] Iter: 514400 Loss: 0.023566363379359245  PSNR: 19.729015350341797
[TRAIN] Iter: 514500 Loss: 0.025556674227118492  PSNR: 19.27016258239746
[TRAIN] Iter: 514600 Loss: 0.026753012090921402  PSNR: 18.785306930541992
[TRAIN] Iter: 514700 Loss: 0.028348170220851898  PSNR: 18.58856773376465
[TRAIN] Iter: 514800 Loss: 0.028555748984217644  PSNR: 18.68609619140625
[TRAIN] Iter: 514900 Loss: 0.0314875990152359  PSNR: 18.247631072998047
[TRAIN] Iter: 515000 Loss: 0.029622986912727356  PSNR: 18.438312530517578
[TRAIN] Iter: 515100 Loss: 0.022666269913315773  PSNR: 19.58748435974121
[TRAIN] Iter: 515200 Loss: 0.026005499064922333  PSNR: 19.10474967956543
[TRAIN] Iter: 515300 Loss: 0.019786283373832703  PSNR: 20.4024658203125
[TRAIN] Iter: 515400 Loss: 0.024831213057041168  PSNR: 19.2281436920166
[TRAIN] Iter: 515500 Loss: 0.024999361485242844  PSNR: 18.8544979095459
[TRAIN] Iter: 515600 Loss: 0.023919370025396347  PSNR: 19.418590545654297
[TRAIN] Iter: 515700 Loss: 0.023772340267896652  PSNR: 19.16598129272461
[TRAIN] Iter: 515800 Loss: 0.02676638960838318  PSNR: 18.813146591186523
[TRAIN] Iter: 515900 Loss: 0.0267777256667614  PSNR: 18.94998550415039
[TRAIN] Iter: 516000 Loss: 0.02859288826584816  PSNR: 18.52481460571289
[TRAIN] Iter: 516100 Loss: 0.031452953815460205  PSNR: 18.41712760925293
[TRAIN] Iter: 516200 Loss: 0.022809483110904694  PSNR: 19.473220825195312
[TRAIN] Iter: 516300 Loss: 0.023651745170354843  PSNR: 19.250741958618164
[TRAIN] Iter: 516400 Loss: 0.021573392674326897  PSNR: 19.695514678955078
[TRAIN] Iter: 516500 Loss: 0.025539763271808624  PSNR: 19.152254104614258
[TRAIN] Iter: 516600 Loss: 0.0295417420566082  PSNR: 18.554317474365234
[TRAIN] Iter: 516700 Loss: 0.021868877112865448  PSNR: 19.74927520751953
[TRAIN] Iter: 516800 Loss: 0.0227004736661911  PSNR: 19.523984909057617
[TRAIN] Iter: 516900 Loss: 0.027109475806355476  PSNR: 18.869661331176758
[TRAIN] Iter: 517000 Loss: 0.02536800131201744  PSNR: 18.684789657592773
[TRAIN] Iter: 517100 Loss: 0.027638187631964684  PSNR: 18.793926239013672
[TRAIN] Iter: 517200 Loss: 0.028546510264277458  PSNR: 18.816570281982422
[TRAIN] Iter: 517300 Loss: 0.024645037949085236  PSNR: 19.515155792236328
[TRAIN] Iter: 517400 Loss: 0.025765785947442055  PSNR: 19.05442237854004
[TRAIN] Iter: 517500 Loss: 0.026549605652689934  PSNR: 18.675203323364258
[TRAIN] Iter: 517600 Loss: 0.029036786407232285  PSNR: 18.507583618164062
[TRAIN] Iter: 517700 Loss: 0.026690931990742683  PSNR: 18.978910446166992
[TRAIN] Iter: 517800 Loss: 0.02409997023642063  PSNR: 18.884870529174805
[TRAIN] Iter: 517900 Loss: 0.027742154896259308  PSNR: 18.800783157348633
[TRAIN] Iter: 518000 Loss: 0.033702120184898376  PSNR: 17.95475959777832
[TRAIN] Iter: 518100 Loss: 0.02436312660574913  PSNR: 19.408823013305664
[TRAIN] Iter: 518200 Loss: 0.03356792777776718  PSNR: 17.989635467529297
[TRAIN] Iter: 518300 Loss: 0.02742617577314377  PSNR: 18.75027084350586
[TRAIN] Iter: 518400 Loss: 0.029647819697856903  PSNR: 18.476335525512695
[TRAIN] Iter: 518500 Loss: 0.025435490533709526  PSNR: 19.273305892944336
[TRAIN] Iter: 518600 Loss: 0.026974894106388092  PSNR: 19.078855514526367
[TRAIN] Iter: 518700 Loss: 0.02747824601829052  PSNR: 19.12188720703125
[TRAIN] Iter: 518800 Loss: 0.02734776958823204  PSNR: 18.941232681274414
[TRAIN] Iter: 518900 Loss: 0.026891406625509262  PSNR: 18.950393676757812
[TRAIN] Iter: 519000 Loss: 0.030150314792990685  PSNR: 18.26991844177246
[TRAIN] Iter: 519100 Loss: 0.02407580427825451  PSNR: 19.333742141723633
[TRAIN] Iter: 519200 Loss: 0.028008166700601578  PSNR: 18.806718826293945
[TRAIN] Iter: 519300 Loss: 0.030035821720957756  PSNR: 18.405364990234375
[TRAIN] Iter: 519400 Loss: 0.022976405918598175  PSNR: 19.551769256591797
[TRAIN] Iter: 519500 Loss: 0.022771786898374557  PSNR: 19.661239624023438
[TRAIN] Iter: 519600 Loss: 0.02434505894780159  PSNR: 19.133882522583008
[TRAIN] Iter: 519700 Loss: 0.027643274515867233  PSNR: 18.7890567779541
[TRAIN] Iter: 519800 Loss: 0.0324132964015007  PSNR: 18.001766204833984
[TRAIN] Iter: 519900 Loss: 0.02399994432926178  PSNR: 19.30035400390625
Saved checkpoints at ./logs/TUT-LAB-nerf/520000.tar
[TRAIN] Iter: 520000 Loss: 0.027968410402536392  PSNR: 18.681013107299805
[TRAIN] Iter: 520100 Loss: 0.030344275757670403  PSNR: 18.34827423095703
[TRAIN] Iter: 520200 Loss: 0.0249149389564991  PSNR: 19.450044631958008
[TRAIN] Iter: 520300 Loss: 0.030503971502184868  PSNR: 18.45457649230957
[TRAIN] Iter: 520400 Loss: 0.02824060618877411  PSNR: 18.67405891418457
[TRAIN] Iter: 520500 Loss: 0.022871151566505432  PSNR: 19.62220001220703
[TRAIN] Iter: 520600 Loss: 0.03180709481239319  PSNR: 18.248159408569336
[TRAIN] Iter: 520700 Loss: 0.026048529893159866  PSNR: 19.07178497314453
[TRAIN] Iter: 520800 Loss: 0.02901913970708847  PSNR: 18.622072219848633
[TRAIN] Iter: 520900 Loss: 0.030355731025338173  PSNR: 18.30999183654785
[TRAIN] Iter: 521000 Loss: 0.029067708179354668  PSNR: 18.625511169433594
[TRAIN] Iter: 521100 Loss: 0.028004072606563568  PSNR: 19.247438430786133
[TRAIN] Iter: 521200 Loss: 0.02499336376786232  PSNR: 19.183500289916992
[TRAIN] Iter: 521300 Loss: 0.023915741592645645  PSNR: 18.970041275024414
[TRAIN] Iter: 521400 Loss: 0.024425243958830833  PSNR: 19.444862365722656
[TRAIN] Iter: 521500 Loss: 0.02625298872590065  PSNR: 19.268709182739258
[TRAIN] Iter: 521600 Loss: 0.023101860657334328  PSNR: 19.401775360107422
[TRAIN] Iter: 521700 Loss: 0.02381943166255951  PSNR: 19.32489776611328
[TRAIN] Iter: 521800 Loss: 0.029118746519088745  PSNR: 18.50994873046875
[TRAIN] Iter: 521900 Loss: 0.025215107947587967  PSNR: 18.767166137695312
[TRAIN] Iter: 522000 Loss: 0.024817906320095062  PSNR: 19.12226104736328
[TRAIN] Iter: 522100 Loss: 0.027746059000492096  PSNR: 18.838293075561523
[TRAIN] Iter: 522200 Loss: 0.026130154728889465  PSNR: 19.029020309448242
[TRAIN] Iter: 522300 Loss: 0.028413252905011177  PSNR: 18.551252365112305
[TRAIN] Iter: 522400 Loss: 0.025375669822096825  PSNR: 19.120784759521484
[TRAIN] Iter: 522500 Loss: 0.027631958946585655  PSNR: 19.27790641784668
[TRAIN] Iter: 522600 Loss: 0.02523932233452797  PSNR: 19.271581649780273
[TRAIN] Iter: 522700 Loss: 0.030067896470427513  PSNR: 18.415983200073242
[TRAIN] Iter: 522800 Loss: 0.02891673892736435  PSNR: 18.674516677856445
[TRAIN] Iter: 522900 Loss: 0.03189810365438461  PSNR: 18.098962783813477
[TRAIN] Iter: 523000 Loss: 0.02868930995464325  PSNR: 18.58168601989746
[TRAIN] Iter: 523100 Loss: 0.024409079924225807  PSNR: 19.045421600341797
[TRAIN] Iter: 523200 Loss: 0.026619765907526016  PSNR: 18.934432983398438
[TRAIN] Iter: 523300 Loss: 0.027338411659002304  PSNR: 18.770322799682617
[TRAIN] Iter: 523400 Loss: 0.03138601407408714  PSNR: 18.24050521850586
[TRAIN] Iter: 523500 Loss: 0.02597452886402607  PSNR: 19.14007568359375
[TRAIN] Iter: 523600 Loss: 0.02842790260910988  PSNR: 18.79458999633789
[TRAIN] Iter: 523700 Loss: 0.025444038212299347  PSNR: 19.11304473876953
[TRAIN] Iter: 523800 Loss: 0.028745044022798538  PSNR: 18.587417602539062
[TRAIN] Iter: 523900 Loss: 0.026590099558234215  PSNR: 19.151782989501953
[TRAIN] Iter: 524000 Loss: 0.02071451209485531  PSNR: 19.828800201416016
[TRAIN] Iter: 524100 Loss: 0.024084921926259995  PSNR: 19.205463409423828
[TRAIN] Iter: 524200 Loss: 0.02742411568760872  PSNR: 18.736494064331055
[TRAIN] Iter: 524300 Loss: 0.02107851207256317  PSNR: 20.02325439453125
[TRAIN] Iter: 524400 Loss: 0.02049986831843853  PSNR: 20.0408935546875
[TRAIN] Iter: 524500 Loss: 0.029976680874824524  PSNR: 18.310243606567383
[TRAIN] Iter: 524600 Loss: 0.022774145007133484  PSNR: 19.647075653076172
[TRAIN] Iter: 524700 Loss: 0.02958507277071476  PSNR: 18.456247329711914
[TRAIN] Iter: 524800 Loss: 0.029228705912828445  PSNR: 18.373132705688477
[TRAIN] Iter: 524900 Loss: 0.023775724694132805  PSNR: 19.23571014404297
[TRAIN] Iter: 525000 Loss: 0.024163171648979187  PSNR: 19.407724380493164
[TRAIN] Iter: 525100 Loss: 0.022435829043388367  PSNR: 19.335712432861328
[TRAIN] Iter: 525200 Loss: 0.02586749568581581  PSNR: 19.021554946899414
[TRAIN] Iter: 525300 Loss: 0.024458765983581543  PSNR: 19.37814712524414
[TRAIN] Iter: 525400 Loss: 0.025116214528679848  PSNR: 18.9313907623291
[TRAIN] Iter: 525500 Loss: 0.02254914678633213  PSNR: 19.06753158569336
[TRAIN] Iter: 525600 Loss: 0.03069836087524891  PSNR: 18.500654220581055
[TRAIN] Iter: 525700 Loss: 0.024370715022087097  PSNR: 19.393190383911133
[TRAIN] Iter: 525800 Loss: 0.02489110641181469  PSNR: 19.51023292541504
[TRAIN] Iter: 525900 Loss: 0.023578254505991936  PSNR: 19.468809127807617
[TRAIN] Iter: 526000 Loss: 0.019424885511398315  PSNR: 20.13683319091797
[TRAIN] Iter: 526100 Loss: 0.02345704846084118  PSNR: 19.531604766845703
[TRAIN] Iter: 526200 Loss: 0.019180895760655403  PSNR: 20.504898071289062
[TRAIN] Iter: 526300 Loss: 0.031595371663570404  PSNR: 18.22431182861328
[TRAIN] Iter: 526400 Loss: 0.024400196969509125  PSNR: 19.25126838684082
[TRAIN] Iter: 526500 Loss: 0.02885371446609497  PSNR: 18.575082778930664
[TRAIN] Iter: 526600 Loss: 0.02462034486234188  PSNR: 19.26211166381836
[TRAIN] Iter: 526700 Loss: 0.0240172129124403  PSNR: 19.2515869140625
[TRAIN] Iter: 526800 Loss: 0.024998292326927185  PSNR: 19.376733779907227
[TRAIN] Iter: 526900 Loss: 0.0265521090477705  PSNR: 18.78363609313965
[TRAIN] Iter: 527000 Loss: 0.025545500218868256  PSNR: 18.875783920288086
[TRAIN] Iter: 527100 Loss: 0.02183205634355545  PSNR: 19.943984985351562
[TRAIN] Iter: 527200 Loss: 0.024790145456790924  PSNR: 19.220027923583984
[TRAIN] Iter: 527300 Loss: 0.02441168949007988  PSNR: 19.431591033935547
[TRAIN] Iter: 527400 Loss: 0.029089350253343582  PSNR: 18.595455169677734
[TRAIN] Iter: 527500 Loss: 0.026433289051055908  PSNR: 18.93670654296875
[TRAIN] Iter: 527600 Loss: 0.02434522844851017  PSNR: 19.25467300415039
[TRAIN] Iter: 527700 Loss: 0.027085423469543457  PSNR: 18.53602409362793
[TRAIN] Iter: 527800 Loss: 0.03059721738100052  PSNR: 18.367536544799805
[TRAIN] Iter: 527900 Loss: 0.03609168529510498  PSNR: 17.57522964477539
[TRAIN] Iter: 528000 Loss: 0.029757581651210785  PSNR: 18.43197250366211
[TRAIN] Iter: 528100 Loss: 0.028978727757930756  PSNR: 18.72995376586914
[TRAIN] Iter: 528200 Loss: 0.027596592903137207  PSNR: 18.7273006439209
[TRAIN] Iter: 528300 Loss: 0.025009583681821823  PSNR: 19.390968322753906
[TRAIN] Iter: 528400 Loss: 0.025219662114977837  PSNR: 19.135385513305664
[TRAIN] Iter: 528500 Loss: 0.028761770576238632  PSNR: 18.712303161621094
[TRAIN] Iter: 528600 Loss: 0.01968519762158394  PSNR: 20.19792938232422
[TRAIN] Iter: 528700 Loss: 0.02703433856368065  PSNR: 18.832229614257812
[TRAIN] Iter: 528800 Loss: 0.01963043585419655  PSNR: 20.337228775024414
[TRAIN] Iter: 528900 Loss: 0.029985832050442696  PSNR: 18.26825523376465
[TRAIN] Iter: 529000 Loss: 0.032040879130363464  PSNR: 18.247695922851562
[TRAIN] Iter: 529100 Loss: 0.0270480178296566  PSNR: 18.717361450195312
[TRAIN] Iter: 529200 Loss: 0.025927089154720306  PSNR: 19.136499404907227
[TRAIN] Iter: 529300 Loss: 0.027894672006368637  PSNR: 18.724924087524414
[TRAIN] Iter: 529400 Loss: 0.028212236240506172  PSNR: 19.03512191772461
[TRAIN] Iter: 529500 Loss: 0.028918474912643433  PSNR: 18.758338928222656
[TRAIN] Iter: 529600 Loss: 0.02939032018184662  PSNR: 18.453067779541016
[TRAIN] Iter: 529700 Loss: 0.029263492673635483  PSNR: 18.47045135498047
[TRAIN] Iter: 529800 Loss: 0.02865201234817505  PSNR: 18.663301467895508
[TRAIN] Iter: 529900 Loss: 0.024825923144817352  PSNR: 19.10299301147461
Saved checkpoints at ./logs/TUT-LAB-nerf/530000.tar
[TRAIN] Iter: 530000 Loss: 0.0308628361672163  PSNR: 18.241308212280273
[TRAIN] Iter: 530100 Loss: 0.028675071895122528  PSNR: 18.559602737426758
[TRAIN] Iter: 530200 Loss: 0.028467722237110138  PSNR: 18.528640747070312
[TRAIN] Iter: 530300 Loss: 0.025494689121842384  PSNR: 19.216758728027344
[TRAIN] Iter: 530400 Loss: 0.028507238253951073  PSNR: 18.635520935058594
[TRAIN] Iter: 530500 Loss: 0.028435848653316498  PSNR: 18.523962020874023
[TRAIN] Iter: 530600 Loss: 0.030752675607800484  PSNR: 18.73105239868164
[TRAIN] Iter: 530700 Loss: 0.0257326178252697  PSNR: 19.12133026123047
[TRAIN] Iter: 530800 Loss: 0.024680733680725098  PSNR: 19.359512329101562
[TRAIN] Iter: 530900 Loss: 0.02511589229106903  PSNR: 19.259845733642578
[TRAIN] Iter: 531000 Loss: 0.02385188639163971  PSNR: 19.287506103515625
[TRAIN] Iter: 531100 Loss: 0.029886655509471893  PSNR: 18.276397705078125
[TRAIN] Iter: 531200 Loss: 0.027376137673854828  PSNR: 18.91925048828125
[TRAIN] Iter: 531300 Loss: 0.03358791768550873  PSNR: 17.945032119750977
[TRAIN] Iter: 531400 Loss: 0.02602890506386757  PSNR: 19.00251007080078
[TRAIN] Iter: 531500 Loss: 0.03239836543798447  PSNR: 18.06401824951172
[TRAIN] Iter: 531600 Loss: 0.02699386328458786  PSNR: 18.7876033782959
[TRAIN] Iter: 531700 Loss: 0.02767215669155121  PSNR: 18.748626708984375
[TRAIN] Iter: 531800 Loss: 0.025376904755830765  PSNR: 19.08696174621582
[TRAIN] Iter: 531900 Loss: 0.03222762048244476  PSNR: 18.04080581665039
[TRAIN] Iter: 532000 Loss: 0.02839355729520321  PSNR: 18.582447052001953
[TRAIN] Iter: 532100 Loss: 0.02673393115401268  PSNR: 18.8084716796875
[TRAIN] Iter: 532200 Loss: 0.029283523559570312  PSNR: 18.364667892456055
[TRAIN] Iter: 532300 Loss: 0.02454017661511898  PSNR: 19.26612091064453
[TRAIN] Iter: 532400 Loss: 0.031916409730911255  PSNR: 18.11941146850586
[TRAIN] Iter: 532500 Loss: 0.025678804144263268  PSNR: 19.271406173706055
[TRAIN] Iter: 532600 Loss: 0.02437068521976471  PSNR: 19.428747177124023
[TRAIN] Iter: 532700 Loss: 0.029140017926692963  PSNR: 18.572792053222656
[TRAIN] Iter: 532800 Loss: 0.027016330510377884  PSNR: 18.892559051513672
[TRAIN] Iter: 532900 Loss: 0.019171854481101036  PSNR: 20.33345603942871
[TRAIN] Iter: 533000 Loss: 0.031375594437122345  PSNR: 18.131147384643555
[TRAIN] Iter: 533100 Loss: 0.032538726925849915  PSNR: 18.092741012573242
[TRAIN] Iter: 533200 Loss: 0.027060573920607567  PSNR: 18.845975875854492
[TRAIN] Iter: 533300 Loss: 0.026753123849630356  PSNR: 19.074199676513672
[TRAIN] Iter: 533400 Loss: 0.025087304413318634  PSNR: 19.063562393188477
[TRAIN] Iter: 533500 Loss: 0.02992173470556736  PSNR: 18.409687042236328
[TRAIN] Iter: 533600 Loss: 0.02752479910850525  PSNR: 18.82276153564453
[TRAIN] Iter: 533700 Loss: 0.020050834864377975  PSNR: 20.22358512878418
[TRAIN] Iter: 533800 Loss: 0.0223005972802639  PSNR: 19.6885929107666
[TRAIN] Iter: 533900 Loss: 0.026427701115608215  PSNR: 18.950881958007812
[TRAIN] Iter: 534000 Loss: 0.020642753690481186  PSNR: 20.30435562133789
[TRAIN] Iter: 534100 Loss: 0.02396492287516594  PSNR: 19.361248016357422
[TRAIN] Iter: 534200 Loss: 0.026302333921194077  PSNR: 19.19254493713379
[TRAIN] Iter: 534300 Loss: 0.02321186661720276  PSNR: 19.56300926208496
[TRAIN] Iter: 534400 Loss: 0.03230578452348709  PSNR: 18.10506248474121
[TRAIN] Iter: 534500 Loss: 0.023866062983870506  PSNR: 19.676137924194336
[TRAIN] Iter: 534600 Loss: 0.028519444167613983  PSNR: 18.590036392211914
[TRAIN] Iter: 534700 Loss: 0.023472081869840622  PSNR: 19.45644760131836
[TRAIN] Iter: 534800 Loss: 0.032822802662849426  PSNR: 18.05957794189453
[TRAIN] Iter: 534900 Loss: 0.026398882269859314  PSNR: 18.992843627929688
[TRAIN] Iter: 535000 Loss: 0.02728816121816635  PSNR: 18.764436721801758
[TRAIN] Iter: 535100 Loss: 0.02734614908695221  PSNR: 18.821718215942383
[TRAIN] Iter: 535200 Loss: 0.02007085084915161  PSNR: 20.12108612060547
[TRAIN] Iter: 535300 Loss: 0.03021230921149254  PSNR: 18.29792594909668
[TRAIN] Iter: 535400 Loss: 0.020853472873568535  PSNR: 20.017684936523438
[TRAIN] Iter: 535500 Loss: 0.02398539148271084  PSNR: 19.29065704345703
[TRAIN] Iter: 535600 Loss: 0.022253703325986862  PSNR: 19.44382667541504
[TRAIN] Iter: 535700 Loss: 0.030376208946108818  PSNR: 18.56570053100586
[TRAIN] Iter: 535800 Loss: 0.025925910100340843  PSNR: 19.22402572631836
[TRAIN] Iter: 535900 Loss: 0.03322586417198181  PSNR: 18.11847496032715
[TRAIN] Iter: 536000 Loss: 0.030672702938318253  PSNR: 18.444231033325195
[TRAIN] Iter: 536100 Loss: 0.02620256133377552  PSNR: 19.322866439819336
[TRAIN] Iter: 536200 Loss: 0.0344853475689888  PSNR: 17.83536720275879
[TRAIN] Iter: 536300 Loss: 0.021077463403344154  PSNR: 19.923812866210938
[TRAIN] Iter: 536400 Loss: 0.02760634385049343  PSNR: 18.805166244506836
[TRAIN] Iter: 536500 Loss: 0.022666314616799355  PSNR: 19.631507873535156
[TRAIN] Iter: 536600 Loss: 0.021735118702054024  PSNR: 19.80691146850586
[TRAIN] Iter: 536700 Loss: 0.027508312836289406  PSNR: 18.741086959838867
[TRAIN] Iter: 536800 Loss: 0.026921618729829788  PSNR: 18.799575805664062
[TRAIN] Iter: 536900 Loss: 0.018287986516952515  PSNR: 20.57859992980957
[TRAIN] Iter: 537000 Loss: 0.03237150236964226  PSNR: 18.164270401000977
[TRAIN] Iter: 537100 Loss: 0.022295290604233742  PSNR: 19.714365005493164
[TRAIN] Iter: 537200 Loss: 0.025273621082305908  PSNR: 18.867847442626953
[TRAIN] Iter: 537300 Loss: 0.026832982897758484  PSNR: 18.900516510009766
[TRAIN] Iter: 537400 Loss: 0.02314244769513607  PSNR: 19.47477149963379
[TRAIN] Iter: 537500 Loss: 0.03437238931655884  PSNR: 17.683368682861328
[TRAIN] Iter: 537600 Loss: 0.03208744525909424  PSNR: 18.125354766845703
[TRAIN] Iter: 537700 Loss: 0.02283606305718422  PSNR: 19.472064971923828
[TRAIN] Iter: 537800 Loss: 0.020636599510908127  PSNR: 20.015077590942383
[TRAIN] Iter: 537900 Loss: 0.027051901444792747  PSNR: 18.98689842224121
[TRAIN] Iter: 538000 Loss: 0.02887400984764099  PSNR: 18.595827102661133
[TRAIN] Iter: 538100 Loss: 0.027036456391215324  PSNR: 18.97658920288086
[TRAIN] Iter: 538200 Loss: 0.035679928958415985  PSNR: 17.790409088134766
[TRAIN] Iter: 538300 Loss: 0.02511521801352501  PSNR: 19.356477737426758
[TRAIN] Iter: 538400 Loss: 0.022723114117980003  PSNR: 19.39682388305664
[TRAIN] Iter: 538500 Loss: 0.028505533933639526  PSNR: 18.578750610351562
[TRAIN] Iter: 538600 Loss: 0.033513978123664856  PSNR: 17.982574462890625
[TRAIN] Iter: 538700 Loss: 0.0315229706466198  PSNR: 18.290878295898438
[TRAIN] Iter: 538800 Loss: 0.03013678267598152  PSNR: 18.1575870513916
[TRAIN] Iter: 538900 Loss: 0.028034720569849014  PSNR: 18.66455078125
[TRAIN] Iter: 539000 Loss: 0.029442131519317627  PSNR: 18.516616821289062
[TRAIN] Iter: 539100 Loss: 0.026204325258731842  PSNR: 19.052778244018555
[TRAIN] Iter: 539200 Loss: 0.03164494410157204  PSNR: 18.455448150634766
[TRAIN] Iter: 539300 Loss: 0.023694880306720734  PSNR: 19.501001358032227
[TRAIN] Iter: 539400 Loss: 0.023924874141812325  PSNR: 19.316740036010742
[TRAIN] Iter: 539500 Loss: 0.026422947645187378  PSNR: 19.083269119262695
[TRAIN] Iter: 539600 Loss: 0.028707977384328842  PSNR: 18.47911834716797
[TRAIN] Iter: 539700 Loss: 0.026714392006397247  PSNR: 19.103961944580078
[TRAIN] Iter: 539800 Loss: 0.022730985656380653  PSNR: 19.381690979003906
[TRAIN] Iter: 539900 Loss: 0.029159337282180786  PSNR: 18.58795738220215
Saved checkpoints at ./logs/TUT-LAB-nerf/540000.tar
[TRAIN] Iter: 540000 Loss: 0.028435437008738518  PSNR: 18.68120002746582
[TRAIN] Iter: 540100 Loss: 0.01939612254500389  PSNR: 20.339387893676758
[TRAIN] Iter: 540200 Loss: 0.026662776246666908  PSNR: 18.90675926208496
[TRAIN] Iter: 540300 Loss: 0.026025913655757904  PSNR: 18.958433151245117
[TRAIN] Iter: 540400 Loss: 0.022432276979088783  PSNR: 19.6373291015625
[TRAIN] Iter: 540500 Loss: 0.021460741758346558  PSNR: 19.849687576293945
[TRAIN] Iter: 540600 Loss: 0.02854599617421627  PSNR: 18.669466018676758
[TRAIN] Iter: 540700 Loss: 0.026587054133415222  PSNR: 18.890626907348633
[TRAIN] Iter: 540800 Loss: 0.02717585861682892  PSNR: 18.954008102416992
[TRAIN] Iter: 540900 Loss: 0.030986238270998  PSNR: 18.348541259765625
[TRAIN] Iter: 541000 Loss: 0.025865957140922546  PSNR: 19.15549659729004
[TRAIN] Iter: 541100 Loss: 0.023144401609897614  PSNR: 19.52864646911621
[TRAIN] Iter: 541200 Loss: 0.028940754011273384  PSNR: 18.37420654296875
[TRAIN] Iter: 541300 Loss: 0.02686152793467045  PSNR: 19.198070526123047
[TRAIN] Iter: 541400 Loss: 0.02491799369454384  PSNR: 19.01738166809082
[TRAIN] Iter: 541500 Loss: 0.027911817654967308  PSNR: 18.69585609436035
[TRAIN] Iter: 541600 Loss: 0.031880274415016174  PSNR: 18.1446533203125
[TRAIN] Iter: 541700 Loss: 0.023877263069152832  PSNR: 19.541240692138672
[TRAIN] Iter: 541800 Loss: 0.02746841311454773  PSNR: 18.915868759155273
[TRAIN] Iter: 541900 Loss: 0.023943746462464333  PSNR: 19.54641342163086
[TRAIN] Iter: 542000 Loss: 0.02382601797580719  PSNR: 19.637609481811523
[TRAIN] Iter: 542100 Loss: 0.026553520932793617  PSNR: 18.815250396728516
[TRAIN] Iter: 542200 Loss: 0.026167025789618492  PSNR: 18.837413787841797
[TRAIN] Iter: 542300 Loss: 0.016723254695534706  PSNR: 20.783672332763672
[TRAIN] Iter: 542400 Loss: 0.028508208692073822  PSNR: 18.49326515197754
[TRAIN] Iter: 542500 Loss: 0.02604971081018448  PSNR: 19.339860916137695
[TRAIN] Iter: 542600 Loss: 0.03192968666553497  PSNR: 18.23552703857422
[TRAIN] Iter: 542700 Loss: 0.025609612464904785  PSNR: 19.240259170532227
[TRAIN] Iter: 542800 Loss: 0.027536310255527496  PSNR: 18.922975540161133
[TRAIN] Iter: 542900 Loss: 0.025853727012872696  PSNR: 19.230741500854492
[TRAIN] Iter: 543000 Loss: 0.02190416492521763  PSNR: 19.87119483947754
[TRAIN] Iter: 543100 Loss: 0.029231220483779907  PSNR: 18.407899856567383
[TRAIN] Iter: 543200 Loss: 0.020597685128450394  PSNR: 20.023700714111328
[TRAIN] Iter: 543300 Loss: 0.028540179133415222  PSNR: 18.467741012573242
[TRAIN] Iter: 543400 Loss: 0.02288326993584633  PSNR: 19.36261749267578
[TRAIN] Iter: 543500 Loss: 0.02877960354089737  PSNR: 18.572078704833984
[TRAIN] Iter: 543600 Loss: 0.02495446614921093  PSNR: 19.18201446533203
[TRAIN] Iter: 543700 Loss: 0.028529806062579155  PSNR: 18.67931365966797
[TRAIN] Iter: 543800 Loss: 0.02943909913301468  PSNR: 18.508535385131836
[TRAIN] Iter: 543900 Loss: 0.0268450565636158  PSNR: 18.91441535949707
[TRAIN] Iter: 544000 Loss: 0.025775132700800896  PSNR: 19.161901473999023
[TRAIN] Iter: 544100 Loss: 0.0303004402667284  PSNR: 18.529647827148438
[TRAIN] Iter: 544200 Loss: 0.024698682129383087  PSNR: 19.135189056396484
[TRAIN] Iter: 544300 Loss: 0.025364907458424568  PSNR: 19.334012985229492
[TRAIN] Iter: 544400 Loss: 0.0249404925853014  PSNR: 18.89177894592285
[TRAIN] Iter: 544500 Loss: 0.03319589048624039  PSNR: 17.884944915771484
[TRAIN] Iter: 544600 Loss: 0.023317847400903702  PSNR: 19.50635528564453
[TRAIN] Iter: 544700 Loss: 0.026058722287416458  PSNR: 19.038589477539062
[TRAIN] Iter: 544800 Loss: 0.019572664052248  PSNR: 20.24660301208496
[TRAIN] Iter: 544900 Loss: 0.02383112534880638  PSNR: 19.515949249267578
[TRAIN] Iter: 545000 Loss: 0.029011046513915062  PSNR: 18.722639083862305
[TRAIN] Iter: 545100 Loss: 0.027895612642169  PSNR: 18.69164276123047
[TRAIN] Iter: 545200 Loss: 0.030740706250071526  PSNR: 18.396812438964844
[TRAIN] Iter: 545300 Loss: 0.024866744875907898  PSNR: 19.25063133239746
[TRAIN] Iter: 545400 Loss: 0.027369260787963867  PSNR: 18.612741470336914
[TRAIN] Iter: 545500 Loss: 0.02979910746216774  PSNR: 18.480852127075195
[TRAIN] Iter: 545600 Loss: 0.029644109308719635  PSNR: 18.563678741455078
[TRAIN] Iter: 545700 Loss: 0.030690345913171768  PSNR: 18.34351348876953
[TRAIN] Iter: 545800 Loss: 0.03417551517486572  PSNR: 17.787723541259766
[TRAIN] Iter: 545900 Loss: 0.021621407940983772  PSNR: 19.617319107055664
[TRAIN] Iter: 546000 Loss: 0.026199709624052048  PSNR: 18.841629028320312
[TRAIN] Iter: 546100 Loss: 0.03141467273235321  PSNR: 18.458715438842773
[TRAIN] Iter: 546200 Loss: 0.02803799882531166  PSNR: 18.737747192382812
[TRAIN] Iter: 546300 Loss: 0.02989504300057888  PSNR: 18.377473831176758
[TRAIN] Iter: 546400 Loss: 0.02943967655301094  PSNR: 18.463977813720703
[TRAIN] Iter: 546500 Loss: 0.02796287089586258  PSNR: 18.699792861938477
[TRAIN] Iter: 546600 Loss: 0.025982949882745743  PSNR: 18.915433883666992
[TRAIN] Iter: 546700 Loss: 0.03348749130964279  PSNR: 17.964494705200195
[TRAIN] Iter: 546800 Loss: 0.02782100811600685  PSNR: 18.680891036987305
[TRAIN] Iter: 546900 Loss: 0.02740197628736496  PSNR: 18.849180221557617
[TRAIN] Iter: 547000 Loss: 0.02754260040819645  PSNR: 18.608583450317383
[TRAIN] Iter: 547100 Loss: 0.028098121285438538  PSNR: 18.548782348632812
[TRAIN] Iter: 547200 Loss: 0.0256970077753067  PSNR: 19.532297134399414
[TRAIN] Iter: 547300 Loss: 0.025577913969755173  PSNR: 19.068124771118164
[TRAIN] Iter: 547400 Loss: 0.02369203418493271  PSNR: 19.306888580322266
[TRAIN] Iter: 547500 Loss: 0.023660782724618912  PSNR: 19.4765682220459
[TRAIN] Iter: 547600 Loss: 0.023109853267669678  PSNR: 19.413555145263672
[TRAIN] Iter: 547700 Loss: 0.027233369648456573  PSNR: 18.73574447631836
[TRAIN] Iter: 547800 Loss: 0.02782929502427578  PSNR: 19.03306007385254
[TRAIN] Iter: 547900 Loss: 0.02839066833257675  PSNR: 18.595657348632812
[TRAIN] Iter: 548000 Loss: 0.02716229110956192  PSNR: 18.82682991027832
[TRAIN] Iter: 548100 Loss: 0.019956814125180244  PSNR: 20.42234992980957
[TRAIN] Iter: 548200 Loss: 0.020093873143196106  PSNR: 20.075532913208008
[TRAIN] Iter: 548300 Loss: 0.029000429436564445  PSNR: 18.609769821166992
[TRAIN] Iter: 548400 Loss: 0.024921007454395294  PSNR: 19.16054916381836
[TRAIN] Iter: 548500 Loss: 0.03190496191382408  PSNR: 18.17041015625
[TRAIN] Iter: 548600 Loss: 0.026547057554125786  PSNR: 19.120210647583008
[TRAIN] Iter: 548700 Loss: 0.022152498364448547  PSNR: 19.84384536743164
[TRAIN] Iter: 548800 Loss: 0.02735622227191925  PSNR: 18.82454490661621
[TRAIN] Iter: 548900 Loss: 0.029404960572719574  PSNR: 18.471132278442383
[TRAIN] Iter: 549000 Loss: 0.02936743199825287  PSNR: 18.511924743652344
[TRAIN] Iter: 549100 Loss: 0.026275385171175003  PSNR: 18.822540283203125
[TRAIN] Iter: 549200 Loss: 0.031038835644721985  PSNR: 18.383838653564453
[TRAIN] Iter: 549300 Loss: 0.02419072762131691  PSNR: 19.323284149169922
[TRAIN] Iter: 549400 Loss: 0.02339787967503071  PSNR: 19.779556274414062
[TRAIN] Iter: 549500 Loss: 0.022408805787563324  PSNR: 19.543033599853516
[TRAIN] Iter: 549600 Loss: 0.02328093908727169  PSNR: 19.39850616455078
[TRAIN] Iter: 549700 Loss: 0.031119128689169884  PSNR: 18.286989212036133
[TRAIN] Iter: 549800 Loss: 0.02287544310092926  PSNR: 19.487024307250977
[TRAIN] Iter: 549900 Loss: 0.022432953119277954  PSNR: 19.795488357543945
Saved checkpoints at ./logs/TUT-LAB-nerf/550000.tar
0 0.00042510032653808594
torch.Size([320, 640, 3]) torch.Size([320, 640])
1 15.726902484893799
2 17.014081478118896
3 15.706177711486816
4 17.04720449447632
5 15.725448608398438
6 17.035120725631714
7 15.676030397415161
8 17.03715968132019
9 15.730830430984497
10 17.027435064315796
11 15.704705953598022
12 17.060835599899292
13 15.55890965461731
14 17.128403186798096
15 15.541390657424927
16 17.15687322616577
17 15.55066967010498
18 17.108095169067383
19 16.05189061164856
20 17.763665199279785
21 16.22093367576599
22 17.626346349716187
23 16.131203413009644
24 17.771385192871094
25 26.302600622177124
26 16.30920386314392
27 18.009909868240356
28 16.451464414596558
29 23.31415319442749
30 21.666189670562744
31 18.032886505126953
32 16.36448097229004
33 22.23192048072815
34 17.726359605789185
35 16.367372512817383
36 18.290122270584106
37 16.107724905014038
38 27.391160011291504
39 16.2662672996521
40 18.04964542388916
41 38.6199951171875
42 33.7010235786438
43 33.830344915390015
44 29.921597242355347
45 33.60074520111084
46 33.330414056777954
47 33.41711497306824
48 34.08849120140076
49 33.824873208999634
50 33.84638428688049
51 27.96766757965088
52 33.112730264663696
53 33.53671932220459
54 33.225177526474
55 31.298368453979492
56 34.32934904098511
57 27.960257291793823
58 35.1893572807312
59 34.24214196205139
60 42.80816674232483
61 20.075136423110962
62 17.32657551765442
63 15.97453498840332
64 17.273360013961792
65 16.06940507888794
66 17.291364669799805
67 16.098694801330566
68 17.33000135421753
69 16.0696918964386
70 17.311511516571045
71 16.115333080291748
72 17.37031841278076
73 16.061380624771118
74 17.413304567337036
75 15.887634754180908
76 17.404030323028564
77 16.054845333099365
78 17.31045389175415
79 16.026625633239746
80 17.41960859298706
81 16.07355237007141
82 17.282468557357788
83 16.02582573890686
84 17.358917474746704
85 16.05928635597229
86 17.378347158432007
87 15.940555334091187
88 11.54779839515686
89 17.380009651184082
90 15.734011173248291
91 17.568642377853394
92 15.547085285186768
93 15.437451839447021
94 14.015366792678833
95 15.779948949813843
96 13.89048171043396
97 15.741960763931274
98 13.978271484375
99 14.17973279953003
100 15.620627641677856
101 13.871976613998413
102 15.741511821746826
103 14.014626026153564
104 15.577606678009033
105 14.141297340393066
106 15.509670495986938
107 14.18178415298462
108 14.183032512664795
109 15.496121644973755
110 14.154005289077759
111 15.485955953598022
112 14.148892402648926
113 15.512616872787476
114 14.148467540740967
115 14.219737768173218
116 15.4902822971344
117 14.066847324371338
118 15.520848512649536
119 14.107269287109375
Done, saving (120, 320, 640, 3) (120, 320, 640)
extras:{'raw': tensor([[[-9.2692e-01, -1.1419e+00, -1.1768e+00, -1.8365e+01],
         [ 4.1156e-01,  1.4187e-01, -2.9494e-01, -2.1174e+01],
         [ 1.9264e-01,  1.3110e-01, -7.4446e-02,  5.4862e-01],
         ...,
         [ 1.1450e+01,  1.2363e+01,  1.7575e+01,  3.6334e+02],
         [ 7.5467e+00,  8.9620e+00,  1.4304e+01,  3.4295e+02],
         [ 1.1208e+01,  1.2445e+01,  1.8566e+01,  3.5762e+02]],

        [[ 3.8000e-01,  6.8948e-02, -9.9131e-02, -2.0805e+01],
         [-7.0010e-01, -8.0680e-01, -1.1029e+00, -2.3428e+01],
         [ 2.4033e-01,  1.8761e-02, -1.0988e-01,  1.0275e+01],
         ...,
         [ 1.0134e+00,  4.7152e-01, -6.5770e+00,  6.8703e+02],
         [ 8.2660e-01,  2.9988e-01, -6.7606e+00,  6.8472e+02],
         [ 7.6515e-01,  3.8879e-01, -6.9004e+00,  6.8478e+02]],

        [[-1.0211e+00, -1.1781e+00, -1.3651e+00, -1.2051e+01],
         [-2.0076e+00, -2.2506e+00, -2.7543e+00, -3.9908e+01],
         [-1.1293e+00, -1.2644e+00, -1.6876e+00, -7.7325e+00],
         ...,
         [-3.4280e+00, -5.8191e+00, -7.7685e+00,  2.5126e+02],
         [-4.4440e+00, -6.7512e+00, -8.8961e+00,  2.5817e+02],
         [-1.7008e+00, -3.9474e+00, -4.9464e+00,  2.6242e+02]],

        ...,

        [[-1.1411e-01, -2.5268e-01, -2.5774e-01, -1.3778e+01],
         [-7.1340e-01, -1.1628e+00, -1.8509e+00, -3.0111e+01],
         [ 1.7310e-01, -4.1788e-02, -3.7322e-02, -2.6304e+00],
         ...,
         [-3.9425e+00, -3.5056e+00, -1.9089e+00, -4.3292e+01],
         [-3.7081e+00, -3.3633e+00, -1.8091e+00, -4.2021e+01],
         [-4.3992e+00, -4.1055e+00, -2.8193e+00, -2.9909e+01]],

        [[ 3.5356e-01,  2.3198e-01,  2.0787e-01,  4.0731e+00],
         [ 2.1669e-01,  1.3293e-01,  1.6449e-01,  4.8443e+00],
         [ 1.9049e-01,  1.0999e-01,  1.4231e-01,  5.1005e+00],
         ...,
         [ 7.7284e+00,  3.1830e+00, -2.6829e+00,  1.8171e+02],
         [ 7.3658e+00,  2.9290e+00, -3.0273e+00,  1.8226e+02],
         [ 7.1296e+00,  2.8145e+00, -3.0528e+00,  1.7965e+02]],

        [[ 2.0143e-01,  4.3177e-02, -3.0620e-02,  1.1916e+01],
         [ 1.6560e-01,  9.3503e-02,  1.6481e-01, -1.1394e+01],
         [ 7.2602e-01,  5.1448e-01,  4.2960e-01, -1.2833e+01],
         ...,
         [ 2.4763e+01,  2.0991e+01,  1.9139e+01,  2.5365e+02],
         [ 2.5643e+01,  2.1957e+01,  2.0539e+01,  2.7346e+02],
         [ 2.5437e+01,  2.1847e+01,  2.0763e+01,  2.7320e+02]]],
       grad_fn=<ReshapeAliasBackward0>), 'rgb0': tensor([[0.5736, 0.5511, 0.5214],
        [0.5443, 0.4887, 0.4645],
        [0.2368, 0.2056, 0.1302],
        ...,
        [0.5036, 0.4528, 0.4542],
        [0.5669, 0.5391, 0.5367],
        [0.5542, 0.5063, 0.4678]], grad_fn=<ReshapeAliasBackward0>), 'disp0': tensor([  16.9818,   35.5566,   50.1040,  ...,   36.2801, 1819.0450,
         680.4127], grad_fn=<ReshapeAliasBackward0>), 'acc0': tensor([1., 1., 1.,  ..., 1., 1., 1.], grad_fn=<ReshapeAliasBackward0>), 'z_std': tensor([0.1078, 0.0022, 0.0018,  ..., 0.0034, 0.2235, 0.0970])}
0 0.0004603862762451172
torch.Size([320, 640, 3]) torch.Size([320, 640])
1 13.872358560562134
2 15.249499559402466
3 13.887961387634277
4 13.860483169555664
5 15.248114824295044
6 13.86407995223999
7 15.164039373397827
8 13.901094198226929
9 15.195705652236938
10 13.814934015274048
11 13.84408974647522
12 15.28557825088501
13 13.746521234512329
14 15.33808708190918
15 13.889746904373169
16 15.203874111175537
17 13.809794425964355
18 13.861370325088501
19 15.239388942718506
20 13.810324907302856
21 15.273407697677612
22 13.873172998428345
23 15.204957008361816
24 13.85347604751587
25 13.80755615234375
26 15.248655557632446
27 13.784160614013672
28 15.262105703353882
29 13.862999439239502
30 15.221708297729492
31 13.864055633544922
32 13.834343910217285
33 15.219881772994995
34 13.785367488861084
35 15.421350717544556
36 13.699320077896118
37 15.313713312149048
38 13.838682413101196
39 13.743696928024292
40 15.318495273590088
41 13.73798394203186
42 15.415385007858276
43 13.769837379455566
44 15.265441417694092
45 13.817402601242065
46 13.747282266616821
47 15.4547119140625
48 13.553220987319946
49 15.453503131866455
50 13.52183222770691
51 15.572590589523315
52 13.618149995803833
53 13.713836669921875
54 15.580674886703491
55 13.58026933670044
56 15.507762670516968
57 13.533928155899048
58 15.40137791633606
59 13.6387038230896
60 13.845056772232056
61 15.529661893844604
62 13.562724113464355
63 15.486965656280518
64 13.53057599067688
65 15.5198814868927
66 13.597615480422974
67 13.847389698028564
68 15.275667667388916
69 13.805952787399292
70 15.4511137008667
71 13.545629978179932
72 15.479635000228882
73 13.884501695632935
74 13.83495831489563
75 15.280831813812256
76 13.785987615585327
77 15.312070369720459
78 13.814108610153198
79 15.269126892089844
80 13.82802152633667
81 13.799734115600586
82 15.273396015167236
83 13.795784711837769
84 15.273203611373901
85 13.838740348815918
86 15.271963834762573
87 13.757498979568481
88 13.765464782714844
89 15.284578800201416
90 13.76366400718689
91 15.221880912780762
92 13.86440134048462
93 15.294090747833252
94 13.81828236579895
95 13.822922229766846
96 15.251980781555176
97 13.809682130813599
98 15.26969051361084
99 13.803313732147217
100 15.325130462646484
101 13.813029050827026
102 13.802933692932129
103 15.35792064666748
104 13.810227155685425
105 15.286437749862671
106 13.804162979125977
107 15.296754360198975
108 13.786497831344604
109 13.783262968063354
110 15.339317083358765
111 13.742699384689331
112 15.278611660003662
113 13.767850875854492
114 15.274247407913208
115 13.8032808303833
116 13.773769617080688
117 15.366252422332764
118 13.761171340942383
119 15.419053077697754
test poses shape torch.Size([13, 3, 4])
0 0.0007798671722412109
torch.Size([320, 640, 3]) torch.Size([320, 640])
1 15.451088666915894
2 13.895700216293335
3 13.725177526473999
4 15.527015924453735
5 13.736530780792236
6 15.486125946044922
7 13.759811162948608
8 15.344781875610352
9 13.77474045753479
10 13.819900274276733
11 15.440794467926025
12 13.62434434890747
Saved test set
[TRAIN] Iter: 550000 Loss: 0.02666984125971794  PSNR: 18.846155166625977
[TRAIN] Iter: 550100 Loss: 0.024639267474412918  PSNR: 19.337923049926758
[TRAIN] Iter: 550200 Loss: 0.028863629326224327  PSNR: 18.552244186401367
[TRAIN] Iter: 550300 Loss: 0.0242706798017025  PSNR: 19.121522903442383
[TRAIN] Iter: 550400 Loss: 0.02833649143576622  PSNR: 18.513530731201172
[TRAIN] Iter: 550500 Loss: 0.029520560055971146  PSNR: 18.446348190307617
[TRAIN] Iter: 550600 Loss: 0.02592489868402481  PSNR: 19.344396591186523
[TRAIN] Iter: 550700 Loss: 0.021650992333889008  PSNR: 19.727577209472656
[TRAIN] Iter: 550800 Loss: 0.020963456481695175  PSNR: 19.954160690307617
[TRAIN] Iter: 550900 Loss: 0.03095320053398609  PSNR: 18.249629974365234
[TRAIN] Iter: 551000 Loss: 0.02314290590584278  PSNR: 19.573883056640625
[TRAIN] Iter: 551100 Loss: 0.032955657690763474  PSNR: 18.01458740234375
[TRAIN] Iter: 551200 Loss: 0.03033066913485527  PSNR: 18.352216720581055
[TRAIN] Iter: 551300 Loss: 0.03042392060160637  PSNR: 18.261938095092773
[TRAIN] Iter: 551400 Loss: 0.02088790014386177  PSNR: 20.030298233032227
[TRAIN] Iter: 551500 Loss: 0.026872001588344574  PSNR: 18.988950729370117
[TRAIN] Iter: 551600 Loss: 0.02701806090772152  PSNR: 19.12374496459961
[TRAIN] Iter: 551700 Loss: 0.02166469395160675  PSNR: 19.65222930908203
[TRAIN] Iter: 551800 Loss: 0.02798609808087349  PSNR: 18.79070472717285
[TRAIN] Iter: 551900 Loss: 0.019952796399593353  PSNR: 20.32177734375
[TRAIN] Iter: 552000 Loss: 0.023296942934393883  PSNR: 19.2493896484375
[TRAIN] Iter: 552100 Loss: 0.022105123847723007  PSNR: 19.60953712463379
[TRAIN] Iter: 552200 Loss: 0.027405882254242897  PSNR: 18.957530975341797
[TRAIN] Iter: 552300 Loss: 0.03191995620727539  PSNR: 18.12358283996582
[TRAIN] Iter: 552400 Loss: 0.025760667398571968  PSNR: 19.41717529296875
[TRAIN] Iter: 552500 Loss: 0.02856508269906044  PSNR: 18.65057373046875
[TRAIN] Iter: 552600 Loss: 0.024976983666419983  PSNR: 19.1081485748291
[TRAIN] Iter: 552700 Loss: 0.026724258437752724  PSNR: 18.90793228149414
[TRAIN] Iter: 552800 Loss: 0.022551653906702995  PSNR: 19.848873138427734
[TRAIN] Iter: 552900 Loss: 0.02490047551691532  PSNR: 19.165637969970703
[TRAIN] Iter: 553000 Loss: 0.03186461329460144  PSNR: 18.12900161743164
[TRAIN] Iter: 553100 Loss: 0.02742007002234459  PSNR: 18.889902114868164
[TRAIN] Iter: 553200 Loss: 0.03028126060962677  PSNR: 18.39604377746582
[TRAIN] Iter: 553300 Loss: 0.02194751426577568  PSNR: 19.987594604492188
[TRAIN] Iter: 553400 Loss: 0.029431302100419998  PSNR: 18.48821258544922
[TRAIN] Iter: 553500 Loss: 0.029964234679937363  PSNR: 18.425588607788086
[TRAIN] Iter: 553600 Loss: 0.03193199634552002  PSNR: 18.14151382446289
[TRAIN] Iter: 553700 Loss: 0.027658816426992416  PSNR: 18.80699920654297
[TRAIN] Iter: 553800 Loss: 0.026000119745731354  PSNR: 18.936403274536133
[TRAIN] Iter: 553900 Loss: 0.02095794677734375  PSNR: 19.742307662963867
[TRAIN] Iter: 554000 Loss: 0.022208020091056824  PSNR: 19.716732025146484
[TRAIN] Iter: 554100 Loss: 0.031085258349776268  PSNR: 18.306560516357422
[TRAIN] Iter: 554200 Loss: 0.029302161186933517  PSNR: 18.420461654663086
[TRAIN] Iter: 554300 Loss: 0.02511928789317608  PSNR: 19.341684341430664
[TRAIN] Iter: 554400 Loss: 0.026553597301244736  PSNR: 18.915878295898438
[TRAIN] Iter: 554500 Loss: 0.021957309916615486  PSNR: 19.729108810424805
[TRAIN] Iter: 554600 Loss: 0.02696014568209648  PSNR: 18.589027404785156
[TRAIN] Iter: 554700 Loss: 0.02765832096338272  PSNR: 18.98014259338379
[TRAIN] Iter: 554800 Loss: 0.02303389273583889  PSNR: 19.616357803344727
[TRAIN] Iter: 554900 Loss: 0.033248282968997955  PSNR: 18.006193161010742
[TRAIN] Iter: 555000 Loss: 0.02184811420738697  PSNR: 20.039134979248047
[TRAIN] Iter: 555100 Loss: 0.02347027324140072  PSNR: 19.598278045654297
[TRAIN] Iter: 555200 Loss: 0.02607257105410099  PSNR: 18.999771118164062
[TRAIN] Iter: 555300 Loss: 0.02888535149395466  PSNR: 18.472673416137695
[TRAIN] Iter: 555400 Loss: 0.027222909033298492  PSNR: 19.04718589782715
[TRAIN] Iter: 555500 Loss: 0.023377154022455215  PSNR: 19.604015350341797
[TRAIN] Iter: 555600 Loss: 0.031757619231939316  PSNR: 18.132854461669922
[TRAIN] Iter: 555700 Loss: 0.029960501939058304  PSNR: 18.289819717407227
[TRAIN] Iter: 555800 Loss: 0.02585924230515957  PSNR: 19.341306686401367
[TRAIN] Iter: 555900 Loss: 0.03490765765309334  PSNR: 17.693382263183594
[TRAIN] Iter: 556000 Loss: 0.026516828685998917  PSNR: 19.024658203125
[TRAIN] Iter: 556100 Loss: 0.02553810551762581  PSNR: 19.268831253051758
[TRAIN] Iter: 556200 Loss: 0.026692476123571396  PSNR: 18.8924503326416
[TRAIN] Iter: 556300 Loss: 0.026228126138448715  PSNR: 18.974607467651367
[TRAIN] Iter: 556400 Loss: 0.02510448917746544  PSNR: 18.993885040283203
[TRAIN] Iter: 556500 Loss: 0.028098413720726967  PSNR: 18.718379974365234
[TRAIN] Iter: 556600 Loss: 0.031208444386720657  PSNR: 18.299484252929688
[TRAIN] Iter: 556700 Loss: 0.029212238267064095  PSNR: 18.525772094726562
[TRAIN] Iter: 556800 Loss: 0.02453862875699997  PSNR: 19.492271423339844
[TRAIN] Iter: 556900 Loss: 0.023395411670207977  PSNR: 19.435176849365234
[TRAIN] Iter: 557000 Loss: 0.031411707401275635  PSNR: 18.302852630615234
[TRAIN] Iter: 557100 Loss: 0.029751727357506752  PSNR: 18.572368621826172
[TRAIN] Iter: 557200 Loss: 0.02532009966671467  PSNR: 19.179597854614258
[TRAIN] Iter: 557300 Loss: 0.031898513436317444  PSNR: 18.10260009765625
[TRAIN] Iter: 557400 Loss: 0.021855782717466354  PSNR: 20.108070373535156
[TRAIN] Iter: 557500 Loss: 0.025432486087083817  PSNR: 19.138572692871094
[TRAIN] Iter: 557600 Loss: 0.02684812806546688  PSNR: 18.682090759277344
[TRAIN] Iter: 557700 Loss: 0.025346484035253525  PSNR: 19.353914260864258
[TRAIN] Iter: 557800 Loss: 0.026302125304937363  PSNR: 18.97745704650879
[TRAIN] Iter: 557900 Loss: 0.03180135041475296  PSNR: 18.199649810791016
[TRAIN] Iter: 558000 Loss: 0.023498423397541046  PSNR: 19.226709365844727
[TRAIN] Iter: 558100 Loss: 0.02504248172044754  PSNR: 18.83252716064453
[TRAIN] Iter: 558200 Loss: 0.02622057870030403  PSNR: 19.025815963745117
[TRAIN] Iter: 558300 Loss: 0.02421662211418152  PSNR: 18.997175216674805
[TRAIN] Iter: 558400 Loss: 0.028735952451825142  PSNR: 18.553220748901367
[TRAIN] Iter: 558500 Loss: 0.025566954165697098  PSNR: 19.205337524414062
[TRAIN] Iter: 558600 Loss: 0.022644372656941414  PSNR: 19.36172866821289
[TRAIN] Iter: 558700 Loss: 0.019810806959867477  PSNR: 20.318889617919922
[TRAIN] Iter: 558800 Loss: 0.026811368763446808  PSNR: 18.97840118408203
[TRAIN] Iter: 558900 Loss: 0.027302231639623642  PSNR: 18.799068450927734
[TRAIN] Iter: 559000 Loss: 0.024237051606178284  PSNR: 19.206451416015625
[TRAIN] Iter: 559100 Loss: 0.030767064541578293  PSNR: 18.135475158691406
[TRAIN] Iter: 559200 Loss: 0.025877399370074272  PSNR: 18.987564086914062
[TRAIN] Iter: 559300 Loss: 0.020293571054935455  PSNR: 20.190143585205078
[TRAIN] Iter: 559400 Loss: 0.03469924256205559  PSNR: 17.802982330322266
[TRAIN] Iter: 559500 Loss: 0.03154005855321884  PSNR: 18.209062576293945
[TRAIN] Iter: 559600 Loss: 0.022535227239131927  PSNR: 19.6723575592041
[TRAIN] Iter: 559700 Loss: 0.02865022048354149  PSNR: 18.695083618164062
[TRAIN] Iter: 559800 Loss: 0.021614495664834976  PSNR: 19.82228660583496
[TRAIN] Iter: 559900 Loss: 0.028499454259872437  PSNR: 18.657960891723633
Saved checkpoints at ./logs/TUT-LAB-nerf/560000.tar
[TRAIN] Iter: 560000 Loss: 0.023533061146736145  PSNR: 19.933883666992188
[TRAIN] Iter: 560100 Loss: 0.025731846690177917  PSNR: 19.134103775024414
[TRAIN] Iter: 560200 Loss: 0.02886044606566429  PSNR: 18.76272201538086
[TRAIN] Iter: 560300 Loss: 0.030455119907855988  PSNR: 18.250301361083984
[TRAIN] Iter: 560400 Loss: 0.021436527371406555  PSNR: 19.83744239807129
[TRAIN] Iter: 560500 Loss: 0.025261959061026573  PSNR: 19.162704467773438
[TRAIN] Iter: 560600 Loss: 0.02892831340432167  PSNR: 18.668201446533203
[TRAIN] Iter: 560700 Loss: 0.01959698647260666  PSNR: 20.091047286987305
[TRAIN] Iter: 560800 Loss: 0.02228240668773651  PSNR: 19.94226837158203
[TRAIN] Iter: 560900 Loss: 0.023273076862096786  PSNR: 19.694103240966797
[TRAIN] Iter: 561000 Loss: 0.03286145627498627  PSNR: 17.944259643554688
[TRAIN] Iter: 561100 Loss: 0.02854352816939354  PSNR: 18.62472915649414
[TRAIN] Iter: 561200 Loss: 0.02544865384697914  PSNR: 19.066137313842773
[TRAIN] Iter: 561300 Loss: 0.030888140201568604  PSNR: 18.210355758666992
[TRAIN] Iter: 561400 Loss: 0.029408207163214684  PSNR: 18.457902908325195
[TRAIN] Iter: 561500 Loss: 0.024583086371421814  PSNR: 18.879812240600586
[TRAIN] Iter: 561600 Loss: 0.029483092948794365  PSNR: 18.452741622924805
[TRAIN] Iter: 561700 Loss: 0.027911454439163208  PSNR: 18.813465118408203
[TRAIN] Iter: 561800 Loss: 0.02563565969467163  PSNR: 18.987272262573242
[TRAIN] Iter: 561900 Loss: 0.030610736459493637  PSNR: 18.365917205810547
[TRAIN] Iter: 562000 Loss: 0.024649035185575485  PSNR: 19.427371978759766
[TRAIN] Iter: 562100 Loss: 0.025327617302536964  PSNR: 19.16635513305664
[TRAIN] Iter: 562200 Loss: 0.02240895666182041  PSNR: 19.81098175048828
[TRAIN] Iter: 562300 Loss: 0.02871542237699032  PSNR: 18.896621704101562
[TRAIN] Iter: 562400 Loss: 0.021375589072704315  PSNR: 20.07445526123047
[TRAIN] Iter: 562500 Loss: 0.019814977422356606  PSNR: 20.243932723999023
[TRAIN] Iter: 562600 Loss: 0.025318412110209465  PSNR: 19.200489044189453
[TRAIN] Iter: 562700 Loss: 0.02457030862569809  PSNR: 19.274078369140625
[TRAIN] Iter: 562800 Loss: 0.029662538319826126  PSNR: 18.54566764831543
[TRAIN] Iter: 562900 Loss: 0.03324532136321068  PSNR: 17.912601470947266
[TRAIN] Iter: 563000 Loss: 0.024771861732006073  PSNR: 19.23698616027832
[TRAIN] Iter: 563100 Loss: 0.03189932554960251  PSNR: 18.12908363342285
[TRAIN] Iter: 563200 Loss: 0.027174072340130806  PSNR: 18.84079933166504
[TRAIN] Iter: 563300 Loss: 0.029536008834838867  PSNR: 18.600141525268555
[TRAIN] Iter: 563400 Loss: 0.028930164873600006  PSNR: 18.490385055541992
[TRAIN] Iter: 563500 Loss: 0.03045271709561348  PSNR: 18.36896514892578
[TRAIN] Iter: 563600 Loss: 0.03326132148504257  PSNR: 17.912368774414062
[TRAIN] Iter: 563700 Loss: 0.02627110853791237  PSNR: 18.9841251373291
[TRAIN] Iter: 563800 Loss: 0.019855961203575134  PSNR: 20.394189834594727
[TRAIN] Iter: 563900 Loss: 0.019154291599988937  PSNR: 20.301450729370117
[TRAIN] Iter: 564000 Loss: 0.02869461290538311  PSNR: 18.849742889404297
[TRAIN] Iter: 564100 Loss: 0.02942650392651558  PSNR: 18.64451789855957
[TRAIN] Iter: 564200 Loss: 0.02380342036485672  PSNR: 19.496814727783203
[TRAIN] Iter: 564300 Loss: 0.026827946305274963  PSNR: 19.280805587768555
[TRAIN] Iter: 564400 Loss: 0.032870858907699585  PSNR: 17.87862205505371
[TRAIN] Iter: 564500 Loss: 0.027616310864686966  PSNR: 18.88653564453125
[TRAIN] Iter: 564600 Loss: 0.024329785257577896  PSNR: 19.468891143798828
[TRAIN] Iter: 564700 Loss: 0.02886199951171875  PSNR: 18.588823318481445
[TRAIN] Iter: 564800 Loss: 0.02916114404797554  PSNR: 18.53965187072754
[TRAIN] Iter: 564900 Loss: 0.03443503379821777  PSNR: 17.760812759399414
[TRAIN] Iter: 565000 Loss: 0.030841711908578873  PSNR: 18.289209365844727
[TRAIN] Iter: 565100 Loss: 0.02583620697259903  PSNR: 18.975149154663086
[TRAIN] Iter: 565200 Loss: 0.021094132214784622  PSNR: 20.01338005065918
[TRAIN] Iter: 565300 Loss: 0.026230551302433014  PSNR: 19.243501663208008
[TRAIN] Iter: 565400 Loss: 0.025849392637610435  PSNR: 18.98871612548828
[TRAIN] Iter: 565500 Loss: 0.024858392775058746  PSNR: 19.117141723632812
[TRAIN] Iter: 565600 Loss: 0.02084042690694332  PSNR: 19.936460494995117
[TRAIN] Iter: 565700 Loss: 0.02562987431883812  PSNR: 18.998199462890625
[TRAIN] Iter: 565800 Loss: 0.025595668703317642  PSNR: 19.19269561767578
[TRAIN] Iter: 565900 Loss: 0.026759251952171326  PSNR: 18.95556640625
[TRAIN] Iter: 566000 Loss: 0.023403581231832504  PSNR: 19.364179611206055
[TRAIN] Iter: 566100 Loss: 0.019663121551275253  PSNR: 20.329648971557617
[TRAIN] Iter: 566200 Loss: 0.02999645471572876  PSNR: 18.3067684173584
[TRAIN] Iter: 566300 Loss: 0.026828475296497345  PSNR: 18.79446792602539
[TRAIN] Iter: 566400 Loss: 0.02543787844479084  PSNR: 19.122695922851562
[TRAIN] Iter: 566500 Loss: 0.029047871008515358  PSNR: 18.545352935791016
[TRAIN] Iter: 566600 Loss: 0.026911377906799316  PSNR: 19.02369499206543
[TRAIN] Iter: 566700 Loss: 0.023160478100180626  PSNR: 19.528648376464844
[TRAIN] Iter: 566800 Loss: 0.021972112357616425  PSNR: 19.809972763061523
[TRAIN] Iter: 566900 Loss: 0.02825448289513588  PSNR: 18.606769561767578
[TRAIN] Iter: 567000 Loss: 0.03501644358038902  PSNR: 17.685081481933594
[TRAIN] Iter: 567100 Loss: 0.025245200842618942  PSNR: 18.98771858215332
[TRAIN] Iter: 567200 Loss: 0.024728653952479362  PSNR: 19.34722328186035
[TRAIN] Iter: 567300 Loss: 0.026402084156870842  PSNR: 18.903959274291992
[TRAIN] Iter: 567400 Loss: 0.028338126838207245  PSNR: 18.862241744995117
[TRAIN] Iter: 567500 Loss: 0.02033476158976555  PSNR: 20.19359588623047
[TRAIN] Iter: 567600 Loss: 0.026034560054540634  PSNR: 18.98435401916504
[TRAIN] Iter: 567700 Loss: 0.02701316401362419  PSNR: 18.398927688598633
[TRAIN] Iter: 567800 Loss: 0.03138427063822746  PSNR: 18.191757202148438
[TRAIN] Iter: 567900 Loss: 0.034745730459690094  PSNR: 17.86974334716797
[TRAIN] Iter: 568000 Loss: 0.03087856061756611  PSNR: 18.296409606933594
[TRAIN] Iter: 568100 Loss: 0.023159636184573174  PSNR: 19.50424575805664
[TRAIN] Iter: 568200 Loss: 0.03075648844242096  PSNR: 18.390949249267578
[TRAIN] Iter: 568300 Loss: 0.023758847266435623  PSNR: 19.80068016052246
[TRAIN] Iter: 568400 Loss: 0.02948581427335739  PSNR: 18.44140625
[TRAIN] Iter: 568500 Loss: 0.023862749338150024  PSNR: 19.122802734375
[TRAIN] Iter: 568600 Loss: 0.0277429036796093  PSNR: 18.681129455566406
[TRAIN] Iter: 568700 Loss: 0.027000397443771362  PSNR: 18.895263671875
[TRAIN] Iter: 568800 Loss: 0.02609153464436531  PSNR: 19.001867294311523
[TRAIN] Iter: 568900 Loss: 0.028001753613352776  PSNR: 18.926469802856445
[TRAIN] Iter: 569000 Loss: 0.03216032311320305  PSNR: 18.150890350341797
[TRAIN] Iter: 569100 Loss: 0.02028481476008892  PSNR: 19.981252670288086
[TRAIN] Iter: 569200 Loss: 0.028124649077653885  PSNR: 18.737407684326172
[TRAIN] Iter: 569300 Loss: 0.021350454539060593  PSNR: 19.89200210571289
[TRAIN] Iter: 569400 Loss: 0.021045373752713203  PSNR: 19.758541107177734
[TRAIN] Iter: 569500 Loss: 0.024449422955513  PSNR: 19.43890380859375
[TRAIN] Iter: 569600 Loss: 0.029147956520318985  PSNR: 18.66740608215332
[TRAIN] Iter: 569700 Loss: 0.026921194046735764  PSNR: 18.94322967529297
[TRAIN] Iter: 569800 Loss: 0.031265489757061005  PSNR: 18.168701171875
[TRAIN] Iter: 569900 Loss: 0.02375086396932602  PSNR: 19.12484359741211
Saved checkpoints at ./logs/TUT-LAB-nerf/570000.tar
[TRAIN] Iter: 570000 Loss: 0.020100675523281097  PSNR: 20.09137725830078
[TRAIN] Iter: 570100 Loss: 0.026785679161548615  PSNR: 18.77393913269043
[TRAIN] Iter: 570200 Loss: 0.02892936021089554  PSNR: 18.54816246032715
[TRAIN] Iter: 570300 Loss: 0.030251819640398026  PSNR: 18.220046997070312
[TRAIN] Iter: 570400 Loss: 0.024470005184412003  PSNR: 19.211441040039062
[TRAIN] Iter: 570500 Loss: 0.023134581744670868  PSNR: 19.548078536987305
[TRAIN] Iter: 570600 Loss: 0.02547888457775116  PSNR: 18.95893096923828
[TRAIN] Iter: 570700 Loss: 0.02845369279384613  PSNR: 18.628131866455078
[TRAIN] Iter: 570800 Loss: 0.025654852390289307  PSNR: 19.061201095581055
[TRAIN] Iter: 570900 Loss: 0.021275848150253296  PSNR: 19.959482192993164
[TRAIN] Iter: 571000 Loss: 0.019678840413689613  PSNR: 20.307056427001953
[TRAIN] Iter: 571100 Loss: 0.03236459940671921  PSNR: 18.126018524169922
[TRAIN] Iter: 571200 Loss: 0.023790154606103897  PSNR: 19.549394607543945
[TRAIN] Iter: 571300 Loss: 0.024596966803073883  PSNR: 19.22992706298828
[TRAIN] Iter: 571400 Loss: 0.028249751776456833  PSNR: 18.855993270874023
[TRAIN] Iter: 571500 Loss: 0.0237069483846426  PSNR: 19.503042221069336
[TRAIN] Iter: 571600 Loss: 0.026351340115070343  PSNR: 18.717121124267578
[TRAIN] Iter: 571700 Loss: 0.02455533854663372  PSNR: 19.229061126708984
[TRAIN] Iter: 571800 Loss: 0.02655724808573723  PSNR: 18.91095542907715
[TRAIN] Iter: 571900 Loss: 0.023353394120931625  PSNR: 19.607290267944336
[TRAIN] Iter: 572000 Loss: 0.02824252098798752  PSNR: 18.748027801513672
[TRAIN] Iter: 572100 Loss: 0.023358669131994247  PSNR: 19.35263442993164
[TRAIN] Iter: 572200 Loss: 0.025931863114237785  PSNR: 19.146501541137695
[TRAIN] Iter: 572300 Loss: 0.030683184042572975  PSNR: 18.353410720825195
[TRAIN] Iter: 572400 Loss: 0.02528967708349228  PSNR: 19.196693420410156
[TRAIN] Iter: 572500 Loss: 0.02233148366212845  PSNR: 19.778831481933594
[TRAIN] Iter: 572600 Loss: 0.027576260268688202  PSNR: 19.000133514404297
[TRAIN] Iter: 572700 Loss: 0.021394014358520508  PSNR: 19.88262939453125
[TRAIN] Iter: 572800 Loss: 0.027584142982959747  PSNR: 18.37151527404785
[TRAIN] Iter: 572900 Loss: 0.03062993288040161  PSNR: 18.22012710571289
[TRAIN] Iter: 573000 Loss: 0.021245529875159264  PSNR: 19.895605087280273
[TRAIN] Iter: 573100 Loss: 0.030812233686447144  PSNR: 18.18791961669922
[TRAIN] Iter: 573200 Loss: 0.0201975479722023  PSNR: 20.036401748657227
[TRAIN] Iter: 573300 Loss: 0.027336562052369118  PSNR: 18.575986862182617
[TRAIN] Iter: 573400 Loss: 0.02497517690062523  PSNR: 19.30484390258789
[TRAIN] Iter: 573500 Loss: 0.029869496822357178  PSNR: 18.34441566467285
[TRAIN] Iter: 573600 Loss: 0.028920985758304596  PSNR: 18.528169631958008
[TRAIN] Iter: 573700 Loss: 0.03183818235993385  PSNR: 18.104631423950195
[TRAIN] Iter: 573800 Loss: 0.03437444940209389  PSNR: 17.763906478881836
[TRAIN] Iter: 573900 Loss: 0.0238934513181448  PSNR: 19.756988525390625
[TRAIN] Iter: 574000 Loss: 0.02497544139623642  PSNR: 19.265928268432617
[TRAIN] Iter: 574100 Loss: 0.018855445086956024  PSNR: 20.574743270874023
[TRAIN] Iter: 574200 Loss: 0.033148106187582016  PSNR: 18.289196014404297
[TRAIN] Iter: 574300 Loss: 0.024696849286556244  PSNR: 19.319049835205078
[TRAIN] Iter: 574400 Loss: 0.027005653828382492  PSNR: 18.800230026245117
[TRAIN] Iter: 574500 Loss: 0.02437163144350052  PSNR: 19.303516387939453
[TRAIN] Iter: 574600 Loss: 0.028218891471624374  PSNR: 18.842758178710938
[TRAIN] Iter: 574700 Loss: 0.02313738502562046  PSNR: 19.60325050354004
[TRAIN] Iter: 574800 Loss: 0.025699321180582047  PSNR: 19.10469627380371
[TRAIN] Iter: 574900 Loss: 0.025995630770921707  PSNR: 19.018321990966797
[TRAIN] Iter: 575000 Loss: 0.03139959275722504  PSNR: 18.276782989501953
[TRAIN] Iter: 575100 Loss: 0.024403413757681847  PSNR: 19.385217666625977
[TRAIN] Iter: 575200 Loss: 0.023066893219947815  PSNR: 19.970535278320312
[TRAIN] Iter: 575300 Loss: 0.027284175157546997  PSNR: 18.37347412109375
[TRAIN] Iter: 575400 Loss: 0.025311443954706192  PSNR: 19.445661544799805
[TRAIN] Iter: 575500 Loss: 0.021664129570126534  PSNR: 19.797876358032227
[TRAIN] Iter: 575600 Loss: 0.03179457038640976  PSNR: 18.159648895263672
[TRAIN] Iter: 575700 Loss: 0.03267423063516617  PSNR: 18.308706283569336
[TRAIN] Iter: 575800 Loss: 0.026749126613140106  PSNR: 19.03693199157715
[TRAIN] Iter: 575900 Loss: 0.027444519102573395  PSNR: 18.758623123168945
[TRAIN] Iter: 576000 Loss: 0.02541985549032688  PSNR: 18.933609008789062
[TRAIN] Iter: 576100 Loss: 0.02854928746819496  PSNR: 18.423208236694336
[TRAIN] Iter: 576200 Loss: 0.02688579633831978  PSNR: 18.954668045043945
[TRAIN] Iter: 576300 Loss: 0.02376820705831051  PSNR: 19.111257553100586
[TRAIN] Iter: 576400 Loss: 0.027805393561720848  PSNR: 18.680156707763672
[TRAIN] Iter: 576500 Loss: 0.022922933101654053  PSNR: 19.392465591430664
[TRAIN] Iter: 576600 Loss: 0.021935943514108658  PSNR: 19.80912971496582
[TRAIN] Iter: 576700 Loss: 0.02466188371181488  PSNR: 19.28292465209961
[TRAIN] Iter: 576800 Loss: 0.029347069561481476  PSNR: 18.3896427154541
[TRAIN] Iter: 576900 Loss: 0.026710519567131996  PSNR: 19.161909103393555
[TRAIN] Iter: 577000 Loss: 0.027305787429213524  PSNR: 18.680849075317383
[TRAIN] Iter: 577100 Loss: 0.032975923269987106  PSNR: 17.875717163085938
[TRAIN] Iter: 577200 Loss: 0.030087720602750778  PSNR: 18.403812408447266
[TRAIN] Iter: 577300 Loss: 0.03188290446996689  PSNR: 18.237266540527344
[TRAIN] Iter: 577400 Loss: 0.02747730165719986  PSNR: 18.830944061279297
[TRAIN] Iter: 577500 Loss: 0.027198772877454758  PSNR: 18.800935745239258
[TRAIN] Iter: 577600 Loss: 0.026882842183113098  PSNR: 18.942996978759766
[TRAIN] Iter: 577700 Loss: 0.03248056769371033  PSNR: 17.944664001464844
[TRAIN] Iter: 577800 Loss: 0.025888774544000626  PSNR: 19.361364364624023
[TRAIN] Iter: 577900 Loss: 0.020634055137634277  PSNR: 19.99970245361328
[TRAIN] Iter: 578000 Loss: 0.023802772164344788  PSNR: 19.421602249145508
[TRAIN] Iter: 578100 Loss: 0.020849110558629036  PSNR: 20.43895149230957
[TRAIN] Iter: 578200 Loss: 0.028543123975396156  PSNR: 18.597185134887695
[TRAIN] Iter: 578300 Loss: 0.028295619413256645  PSNR: 18.619792938232422
[TRAIN] Iter: 578400 Loss: 0.02319459617137909  PSNR: 19.48284149169922
[TRAIN] Iter: 578500 Loss: 0.02017929032444954  PSNR: 20.18450927734375
[TRAIN] Iter: 578600 Loss: 0.02994811162352562  PSNR: 18.509538650512695
[TRAIN] Iter: 578700 Loss: 0.03133325278759003  PSNR: 18.17110252380371
[TRAIN] Iter: 578800 Loss: 0.026531793177127838  PSNR: 18.82152557373047
[TRAIN] Iter: 578900 Loss: 0.022139661014080048  PSNR: 19.757238388061523
[TRAIN] Iter: 579000 Loss: 0.024369189515709877  PSNR: 19.45207977294922
[TRAIN] Iter: 579100 Loss: 0.027231302112340927  PSNR: 18.85244369506836
[TRAIN] Iter: 579200 Loss: 0.02961638569831848  PSNR: 18.366281509399414
[TRAIN] Iter: 579300 Loss: 0.02776363492012024  PSNR: 18.58091926574707
[TRAIN] Iter: 579400 Loss: 0.020772919058799744  PSNR: 20.05622673034668
[TRAIN] Iter: 579500 Loss: 0.03207018971443176  PSNR: 17.975971221923828
[TRAIN] Iter: 579600 Loss: 0.01867986097931862  PSNR: 20.446746826171875
[TRAIN] Iter: 579700 Loss: 0.03082001954317093  PSNR: 18.306650161743164
[TRAIN] Iter: 579800 Loss: 0.028702475130558014  PSNR: 18.507274627685547
[TRAIN] Iter: 579900 Loss: 0.029225420206785202  PSNR: 18.696247100830078
Saved checkpoints at ./logs/TUT-LAB-nerf/580000.tar
[TRAIN] Iter: 580000 Loss: 0.025896962732076645  PSNR: 19.020553588867188
[TRAIN] Iter: 580100 Loss: 0.02309541590511799  PSNR: 19.646413803100586
[TRAIN] Iter: 580200 Loss: 0.026284508407115936  PSNR: 18.759634017944336
[TRAIN] Iter: 580300 Loss: 0.029812421649694443  PSNR: 18.465322494506836
[TRAIN] Iter: 580400 Loss: 0.02829727903008461  PSNR: 18.781299591064453
[TRAIN] Iter: 580500 Loss: 0.029895901679992676  PSNR: 18.576921463012695
[TRAIN] Iter: 580600 Loss: 0.025361135601997375  PSNR: 19.013479232788086
[TRAIN] Iter: 580700 Loss: 0.023598212748765945  PSNR: 19.035375595092773
[TRAIN] Iter: 580800 Loss: 0.0242592953145504  PSNR: 19.26953125
[TRAIN] Iter: 580900 Loss: 0.02914581447839737  PSNR: 18.550445556640625
[TRAIN] Iter: 581000 Loss: 0.03471575677394867  PSNR: 17.766372680664062
[TRAIN] Iter: 581100 Loss: 0.02673921547830105  PSNR: 19.167070388793945
[TRAIN] Iter: 581200 Loss: 0.02349098026752472  PSNR: 19.378833770751953
[TRAIN] Iter: 581300 Loss: 0.024093717336654663  PSNR: 19.693979263305664
[TRAIN] Iter: 581400 Loss: 0.0263555645942688  PSNR: 19.032058715820312
[TRAIN] Iter: 581500 Loss: 0.02075124904513359  PSNR: 20.014625549316406
[TRAIN] Iter: 581600 Loss: 0.02253381535410881  PSNR: 19.658113479614258
[TRAIN] Iter: 581700 Loss: 0.029790472239255905  PSNR: 18.308414459228516
[TRAIN] Iter: 581800 Loss: 0.02530818246304989  PSNR: 19.082483291625977
[TRAIN] Iter: 581900 Loss: 0.028786983340978622  PSNR: 18.697050094604492
[TRAIN] Iter: 582000 Loss: 0.02640375867486  PSNR: 19.07310676574707
[TRAIN] Iter: 582100 Loss: 0.02826128900051117  PSNR: 18.555980682373047
[TRAIN] Iter: 582200 Loss: 0.029726233333349228  PSNR: 18.412885665893555
[TRAIN] Iter: 582300 Loss: 0.02729257196187973  PSNR: 18.814088821411133
[TRAIN] Iter: 582400 Loss: 0.02820129692554474  PSNR: 18.734548568725586
[TRAIN] Iter: 582500 Loss: 0.029990054666996002  PSNR: 18.389659881591797
[TRAIN] Iter: 582600 Loss: 0.029821662232279778  PSNR: 18.734861373901367
[TRAIN] Iter: 582700 Loss: 0.023208625614643097  PSNR: 19.518112182617188
[TRAIN] Iter: 582800 Loss: 0.02776312455534935  PSNR: 18.678579330444336
[TRAIN] Iter: 582900 Loss: 0.02764079160988331  PSNR: 18.906131744384766
[TRAIN] Iter: 583000 Loss: 0.02538266032934189  PSNR: 19.170543670654297
[TRAIN] Iter: 583100 Loss: 0.026605822145938873  PSNR: 18.853904724121094
[TRAIN] Iter: 583200 Loss: 0.026063719764351845  PSNR: 19.020156860351562
[TRAIN] Iter: 583300 Loss: 0.027599569410085678  PSNR: 18.937578201293945
[TRAIN] Iter: 583400 Loss: 0.02222214825451374  PSNR: 19.69167709350586
[TRAIN] Iter: 583500 Loss: 0.027412917464971542  PSNR: 19.116840362548828
[TRAIN] Iter: 583600 Loss: 0.02460026927292347  PSNR: 19.4527587890625
[TRAIN] Iter: 583700 Loss: 0.03169522061944008  PSNR: 18.06545639038086
[TRAIN] Iter: 583800 Loss: 0.018986279144883156  PSNR: 20.32491683959961
[TRAIN] Iter: 583900 Loss: 0.026089590042829514  PSNR: 18.5156307220459
[TRAIN] Iter: 584000 Loss: 0.021258875727653503  PSNR: 19.86359214782715
[TRAIN] Iter: 584100 Loss: 0.025367368012666702  PSNR: 19.3267879486084
[TRAIN] Iter: 584200 Loss: 0.030988849699497223  PSNR: 18.467422485351562
[TRAIN] Iter: 584300 Loss: 0.01933615282177925  PSNR: 20.53311538696289
[TRAIN] Iter: 584400 Loss: 0.02315833419561386  PSNR: 19.59783363342285
[TRAIN] Iter: 584500 Loss: 0.027306541800498962  PSNR: 18.916086196899414
[TRAIN] Iter: 584600 Loss: 0.031881481409072876  PSNR: 18.261730194091797
[TRAIN] Iter: 584700 Loss: 0.028565164655447006  PSNR: 18.65635108947754
[TRAIN] Iter: 584800 Loss: 0.026193583384156227  PSNR: 18.985172271728516
[TRAIN] Iter: 584900 Loss: 0.025703752413392067  PSNR: 19.250701904296875
[TRAIN] Iter: 585000 Loss: 0.02335244230926037  PSNR: 19.449716567993164
[TRAIN] Iter: 585100 Loss: 0.027338247746229172  PSNR: 19.385112762451172
[TRAIN] Iter: 585200 Loss: 0.020210351794958115  PSNR: 20.126312255859375
[TRAIN] Iter: 585300 Loss: 0.02142620086669922  PSNR: 19.770221710205078
[TRAIN] Iter: 585400 Loss: 0.027426136657595634  PSNR: 18.926393508911133
[TRAIN] Iter: 585500 Loss: 0.02564404159784317  PSNR: 19.205474853515625
[TRAIN] Iter: 585600 Loss: 0.02074223756790161  PSNR: 20.069904327392578
[TRAIN] Iter: 585700 Loss: 0.023575307801365852  PSNR: 19.553823471069336
[TRAIN] Iter: 585800 Loss: 0.025858789682388306  PSNR: 19.47188949584961
[TRAIN] Iter: 585900 Loss: 0.021509133279323578  PSNR: 19.797555923461914
[TRAIN] Iter: 586000 Loss: 0.025869151577353477  PSNR: 19.2131290435791
[TRAIN] Iter: 586100 Loss: 0.03071882389485836  PSNR: 18.38677406311035
[TRAIN] Iter: 586200 Loss: 0.024700546637177467  PSNR: 19.015352249145508
[TRAIN] Iter: 586300 Loss: 0.02768828719854355  PSNR: 18.584808349609375
[TRAIN] Iter: 586400 Loss: 0.02783716842532158  PSNR: 18.86050796508789
[TRAIN] Iter: 586500 Loss: 0.02616237848997116  PSNR: 18.983097076416016
[TRAIN] Iter: 586600 Loss: 0.029782548546791077  PSNR: 18.340030670166016
[TRAIN] Iter: 586700 Loss: 0.02470370940864086  PSNR: 19.003633499145508
[TRAIN] Iter: 586800 Loss: 0.025168132036924362  PSNR: 19.07288932800293
[TRAIN] Iter: 586900 Loss: 0.029139531776309013  PSNR: 18.53758430480957
[TRAIN] Iter: 587000 Loss: 0.032293010503053665  PSNR: 18.15998077392578
[TRAIN] Iter: 587100 Loss: 0.022939540445804596  PSNR: 19.395051956176758
[TRAIN] Iter: 587200 Loss: 0.02303462103009224  PSNR: 19.593664169311523
[TRAIN] Iter: 587300 Loss: 0.02799203433096409  PSNR: 18.98091697692871
[TRAIN] Iter: 587400 Loss: 0.026465633884072304  PSNR: 18.985441207885742
[TRAIN] Iter: 587500 Loss: 0.0205067228525877  PSNR: 20.24129867553711
[TRAIN] Iter: 587600 Loss: 0.03011050634086132  PSNR: 18.353626251220703
[TRAIN] Iter: 587700 Loss: 0.023024870082736015  PSNR: 19.58156394958496
[TRAIN] Iter: 587800 Loss: 0.02449638769030571  PSNR: 19.523984909057617
[TRAIN] Iter: 587900 Loss: 0.025083448737859726  PSNR: 19.18975257873535
[TRAIN] Iter: 588000 Loss: 0.03047989122569561  PSNR: 18.36622428894043
[TRAIN] Iter: 588100 Loss: 0.02959497831761837  PSNR: 18.544815063476562
[TRAIN] Iter: 588200 Loss: 0.02254633791744709  PSNR: 19.78646469116211
[TRAIN] Iter: 588300 Loss: 0.027399465441703796  PSNR: 18.818248748779297
[TRAIN] Iter: 588400 Loss: 0.02560957707464695  PSNR: 19.130285263061523
[TRAIN] Iter: 588500 Loss: 0.028419192880392075  PSNR: 18.770801544189453
[TRAIN] Iter: 588600 Loss: 0.03251762315630913  PSNR: 18.116844177246094
[TRAIN] Iter: 588700 Loss: 0.02628183364868164  PSNR: 19.000370025634766
[TRAIN] Iter: 588800 Loss: 0.02607792615890503  PSNR: 18.978004455566406
[TRAIN] Iter: 588900 Loss: 0.027993477880954742  PSNR: 18.549205780029297
[TRAIN] Iter: 589000 Loss: 0.026451921090483665  PSNR: 18.812828063964844
[TRAIN] Iter: 589100 Loss: 0.028602859005331993  PSNR: 18.656383514404297
[TRAIN] Iter: 589200 Loss: 0.024619530886411667  PSNR: 19.62832260131836
[TRAIN] Iter: 589300 Loss: 0.028745945543050766  PSNR: 18.46722984313965
[TRAIN] Iter: 589400 Loss: 0.027993464842438698  PSNR: 18.661975860595703
[TRAIN] Iter: 589500 Loss: 0.02743203565478325  PSNR: 18.70442008972168
[TRAIN] Iter: 589600 Loss: 0.030539050698280334  PSNR: 18.428977966308594
[TRAIN] Iter: 589700 Loss: 0.024538002908229828  PSNR: 19.405216217041016
[TRAIN] Iter: 589800 Loss: 0.031149737536907196  PSNR: 18.19622230529785
[TRAIN] Iter: 589900 Loss: 0.031209133565425873  PSNR: 18.274263381958008
Saved checkpoints at ./logs/TUT-LAB-nerf/590000.tar
[TRAIN] Iter: 590000 Loss: 0.024824101477861404  PSNR: 19.2722225189209
[TRAIN] Iter: 590100 Loss: 0.022887833416461945  PSNR: 19.458906173706055
[TRAIN] Iter: 590200 Loss: 0.022150885313749313  PSNR: 19.764286041259766
[TRAIN] Iter: 590300 Loss: 0.02917667105793953  PSNR: 18.585229873657227
[TRAIN] Iter: 590400 Loss: 0.028587862849235535  PSNR: 18.601099014282227
[TRAIN] Iter: 590500 Loss: 0.027349211275577545  PSNR: 18.857236862182617
[TRAIN] Iter: 590600 Loss: 0.029048331081867218  PSNR: 18.611648559570312
[TRAIN] Iter: 590700 Loss: 0.02204766497015953  PSNR: 19.575937271118164
[TRAIN] Iter: 590800 Loss: 0.025615181773900986  PSNR: 19.219621658325195
[TRAIN] Iter: 590900 Loss: 0.03497820347547531  PSNR: 17.673229217529297
[TRAIN] Iter: 591000 Loss: 0.02584344707429409  PSNR: 19.021886825561523
[TRAIN] Iter: 591100 Loss: 0.022030998021364212  PSNR: 19.96710968017578
[TRAIN] Iter: 591200 Loss: 0.02609173208475113  PSNR: 18.99156951904297
[TRAIN] Iter: 591300 Loss: 0.028690043836832047  PSNR: 18.744762420654297
[TRAIN] Iter: 591400 Loss: 0.02695022150874138  PSNR: 18.9709529876709
[TRAIN] Iter: 591500 Loss: 0.029724204912781715  PSNR: 18.33465003967285
[TRAIN] Iter: 591600 Loss: 0.023371487855911255  PSNR: 19.481060028076172
[TRAIN] Iter: 591700 Loss: 0.03151866793632507  PSNR: 18.207094192504883
[TRAIN] Iter: 591800 Loss: 0.024161355569958687  PSNR: 19.045089721679688
[TRAIN] Iter: 591900 Loss: 0.028696049004793167  PSNR: 18.582746505737305
[TRAIN] Iter: 592000 Loss: 0.03137688338756561  PSNR: 18.407468795776367
[TRAIN] Iter: 592100 Loss: 0.02396278828382492  PSNR: 19.54404640197754
[TRAIN] Iter: 592200 Loss: 0.026269935071468353  PSNR: 18.963233947753906
[TRAIN] Iter: 592300 Loss: 0.025001291185617447  PSNR: 19.26543617248535
[TRAIN] Iter: 592400 Loss: 0.029583314433693886  PSNR: 18.44976806640625
[TRAIN] Iter: 592500 Loss: 0.024407759308815002  PSNR: 19.443775177001953
[TRAIN] Iter: 592600 Loss: 0.029977919533848763  PSNR: 18.661449432373047
[TRAIN] Iter: 592700 Loss: 0.027808424085378647  PSNR: 18.699617385864258
[TRAIN] Iter: 592800 Loss: 0.022248096764087677  PSNR: 19.77408218383789
[TRAIN] Iter: 592900 Loss: 0.03214990347623825  PSNR: 18.29926872253418
[TRAIN] Iter: 593000 Loss: 0.0234672874212265  PSNR: 19.480005264282227
[TRAIN] Iter: 593100 Loss: 0.020256541669368744  PSNR: 20.014333724975586
[TRAIN] Iter: 593200 Loss: 0.026666205376386642  PSNR: 18.981374740600586
[TRAIN] Iter: 593300 Loss: 0.0306510291993618  PSNR: 18.59469985961914
[TRAIN] Iter: 593400 Loss: 0.03438954800367355  PSNR: 17.75376319885254
[TRAIN] Iter: 593500 Loss: 0.02484578639268875  PSNR: 19.187824249267578
[TRAIN] Iter: 593600 Loss: 0.029156189411878586  PSNR: 18.54154396057129
[TRAIN] Iter: 593700 Loss: 0.019422048702836037  PSNR: 20.364768981933594
[TRAIN] Iter: 593800 Loss: 0.025409957394003868  PSNR: 19.13617515563965
[TRAIN] Iter: 593900 Loss: 0.031005535274744034  PSNR: 18.26321792602539
[TRAIN] Iter: 594000 Loss: 0.02334936521947384  PSNR: 19.58560562133789
[TRAIN] Iter: 594100 Loss: 0.027612730860710144  PSNR: 18.810699462890625
[TRAIN] Iter: 594200 Loss: 0.031331807374954224  PSNR: 18.104097366333008
[TRAIN] Iter: 594300 Loss: 0.02368263155221939  PSNR: 19.377763748168945
[TRAIN] Iter: 594400 Loss: 0.024488959461450577  PSNR: 19.53196907043457
[TRAIN] Iter: 594500 Loss: 0.029913023114204407  PSNR: 18.864660263061523
[TRAIN] Iter: 594600 Loss: 0.025831155478954315  PSNR: 19.188669204711914
[TRAIN] Iter: 594700 Loss: 0.02751849591732025  PSNR: 18.723649978637695
[TRAIN] Iter: 594800 Loss: 0.02690957672894001  PSNR: 18.777421951293945
[TRAIN] Iter: 594900 Loss: 0.027452955022454262  PSNR: 18.88164710998535
[TRAIN] Iter: 595000 Loss: 0.02439412847161293  PSNR: 19.290842056274414
[TRAIN] Iter: 595100 Loss: 0.02748674899339676  PSNR: 18.85605812072754
[TRAIN] Iter: 595200 Loss: 0.02550286240875721  PSNR: 18.84735679626465
[TRAIN] Iter: 595300 Loss: 0.02398860827088356  PSNR: 19.277225494384766
[TRAIN] Iter: 595400 Loss: 0.025425879284739494  PSNR: 19.168737411499023
[TRAIN] Iter: 595500 Loss: 0.023815786466002464  PSNR: 19.594192504882812
[TRAIN] Iter: 595600 Loss: 0.02936752885580063  PSNR: 18.65533447265625
[TRAIN] Iter: 595700 Loss: 0.026807069778442383  PSNR: 18.841489791870117
[TRAIN] Iter: 595800 Loss: 0.030381521210074425  PSNR: 18.224458694458008
[TRAIN] Iter: 595900 Loss: 0.024884145706892014  PSNR: 19.308828353881836
[TRAIN] Iter: 596000 Loss: 0.023997817188501358  PSNR: 19.354936599731445
[TRAIN] Iter: 596100 Loss: 0.02546664886176586  PSNR: 19.746925354003906
[TRAIN] Iter: 596200 Loss: 0.030029620975255966  PSNR: 18.42270851135254
[TRAIN] Iter: 596300 Loss: 0.02554677240550518  PSNR: 19.055591583251953
[TRAIN] Iter: 596400 Loss: 0.022556666284799576  PSNR: 19.685314178466797
[TRAIN] Iter: 596500 Loss: 0.02550610341131687  PSNR: 19.16783905029297
[TRAIN] Iter: 596600 Loss: 0.02862423099577427  PSNR: 18.704322814941406
[TRAIN] Iter: 596700 Loss: 0.02350006066262722  PSNR: 19.658367156982422
[TRAIN] Iter: 596800 Loss: 0.024822385981678963  PSNR: 19.20691680908203
[TRAIN] Iter: 596900 Loss: 0.0280911847949028  PSNR: 18.766023635864258
[TRAIN] Iter: 597000 Loss: 0.029129069298505783  PSNR: 18.55242347717285
[TRAIN] Iter: 597100 Loss: 0.02151096984744072  PSNR: 19.799043655395508
[TRAIN] Iter: 597200 Loss: 0.023332804441452026  PSNR: 19.543121337890625
[TRAIN] Iter: 597300 Loss: 0.02974260412156582  PSNR: 18.52105712890625
[TRAIN] Iter: 597400 Loss: 0.024633411318063736  PSNR: 19.352893829345703
[TRAIN] Iter: 597500 Loss: 0.024243196472525597  PSNR: 19.724590301513672
[TRAIN] Iter: 597600 Loss: 0.021329496055841446  PSNR: 19.953968048095703
[TRAIN] Iter: 597700 Loss: 0.024250168353319168  PSNR: 19.362625122070312
[TRAIN] Iter: 597800 Loss: 0.026328498497605324  PSNR: 19.11537742614746
[TRAIN] Iter: 597900 Loss: 0.020961854606866837  PSNR: 19.994815826416016
[TRAIN] Iter: 598000 Loss: 0.024273555725812912  PSNR: 19.590410232543945
[TRAIN] Iter: 598100 Loss: 0.026054663583636284  PSNR: 19.430383682250977
[TRAIN] Iter: 598200 Loss: 0.02564118057489395  PSNR: 19.030797958374023
[TRAIN] Iter: 598300 Loss: 0.028126925230026245  PSNR: 18.68256187438965
[TRAIN] Iter: 598400 Loss: 0.029068855568766594  PSNR: 18.56784439086914
[TRAIN] Iter: 598500 Loss: 0.030921395868062973  PSNR: 18.35663604736328
[TRAIN] Iter: 598600 Loss: 0.033461250364780426  PSNR: 17.98601722717285
[TRAIN] Iter: 598700 Loss: 0.028222618624567986  PSNR: 18.743867874145508
[TRAIN] Iter: 598800 Loss: 0.023070696741342545  PSNR: 19.405052185058594
[TRAIN] Iter: 598900 Loss: 0.0212428979575634  PSNR: 20.05082893371582
[TRAIN] Iter: 599000 Loss: 0.021925995126366615  PSNR: 19.89614486694336
[TRAIN] Iter: 599100 Loss: 0.025568123906850815  PSNR: 19.23044204711914
[TRAIN] Iter: 599200 Loss: 0.0237541813403368  PSNR: 19.565046310424805
[TRAIN] Iter: 599300 Loss: 0.02792576514184475  PSNR: 18.677419662475586
[TRAIN] Iter: 599400 Loss: 0.025850258767604828  PSNR: 19.162160873413086
[TRAIN] Iter: 599500 Loss: 0.024838099256157875  PSNR: 19.322336196899414
[TRAIN] Iter: 599600 Loss: 0.01929824985563755  PSNR: 20.437023162841797
[TRAIN] Iter: 599700 Loss: 0.03173832595348358  PSNR: 18.10472297668457
[TRAIN] Iter: 599800 Loss: 0.022647961974143982  PSNR: 19.660907745361328
[TRAIN] Iter: 599900 Loss: 0.027587413787841797  PSNR: 18.597455978393555
Saved checkpoints at ./logs/TUT-LAB-nerf/600000.tar
0 0.0004048347473144531
torch.Size([320, 640, 3]) torch.Size([320, 640])
1 17.25282335281372
2 15.510715246200562
3 17.192187547683716
4 15.538321018218994
5 17.225719451904297
6 15.513501405715942
7 17.256934881210327
8 15.518600225448608
9 17.273221254348755
10 15.437182426452637
11 15.49333906173706
12 17.295161724090576
13 15.510156631469727
14 17.23390793800354
15 15.509112119674683
16 17.172364711761475
17 15.49259328842163
18 17.200124502182007
19 15.557759761810303
20 17.23276972770691
21 15.385236978530884
22 17.198647260665894
23 15.44502592086792
24 17.49621343612671
25 15.579570055007935
26 17.089274406433105
27 15.515730857849121
28 17.216846227645874
29 15.486107349395752
30 17.217453479766846
31 15.461292266845703
32 17.29366135597229
33 15.505778551101685
34 17.36146569252014
35 15.394572973251343
36 15.540788888931274
37 17.44750428199768
38 15.340120077133179
39 17.38020133972168
40 15.382513046264648
41 17.425507307052612
42 15.281522512435913
43 17.395423650741577
44 15.47110104560852
45 17.324755668640137
46 15.43319320678711
47 17.334744691848755
48 15.130640745162964
49 17.568088054656982
50 15.130019664764404
51 17.7834529876709
52 15.007194995880127
53 17.6406991481781
54 15.08765721321106
55 17.581871509552002
56 15.192890882492065
57 17.544477224349976
58 15.215506315231323
59 17.58220863342285
60 15.111681461334229
61 15.658216953277588
62 17.143166542053223
63 15.472697257995605
64 17.236283540725708
65 15.705239057540894
66 17.32206678390503
67 15.723984479904175
68 17.895577669143677
69 15.866775751113892
70 17.448007822036743
71 15.595829010009766
72 17.27099609375
73 15.428062200546265
74 17.217190742492676
75 15.4857497215271
76 16.862523078918457
77 14.76847529411316
78 17.19186544418335
79 15.418508529663086
80 17.06684446334839
81 13.746824264526367
82 13.695954322814941
83 15.500870943069458
84 13.688719034194946
85 15.444398164749146
86 13.71782636642456
87 15.413931131362915
88 13.684289693832397
89 13.682863712310791
90 15.46760892868042
91 13.685452938079834
92 15.440770864486694
93 13.664173364639282
94 15.446993112564087
95 13.529491186141968
96 13.741465330123901
97 15.762145757675171
98 13.734771490097046
99 15.338548183441162
100 13.69028902053833
101 15.42587661743164
102 13.672101497650146
103 13.708463907241821
104 15.48299264907837
105 13.641751050949097
106 15.46442985534668
107 13.734811067581177
108 15.549123048782349
109 13.694199323654175
110 13.68098497390747
111 15.465628862380981
112 13.628507375717163
113 15.42496371269226
114 13.66780138015747
115 13.715274095535278
116 15.494139671325684
117 13.622167110443115
118 15.608673095703125
119 13.493698596954346
Done, saving (120, 320, 640, 3) (120, 320, 640)
extras:{'raw': tensor([[[-8.9414e-01, -8.9744e-01, -7.4252e-01, -1.8845e+01],
         [-1.2093e+00, -1.5434e+00, -2.2023e+00, -4.1749e+01],
         [ 2.5971e-01, -4.0871e-02, -2.4736e-01, -2.6490e+01],
         ...,
         [-1.9346e+00, -1.6877e+00, -6.4666e-01, -9.4175e+00],
         [-1.8811e+00, -1.5872e+00, -4.1511e-01, -9.1263e+00],
         [-1.1790e+00, -7.1746e-01,  1.1318e+00, -1.7992e+01]],

        [[-1.1396e+00, -1.2126e+00, -1.4301e+00, -1.4343e+01],
         [-1.3047e-01, -4.1616e-01, -6.8823e-01, -2.0996e+01],
         [-2.8926e-01, -2.5527e-01, -2.2309e-01, -1.6956e+01],
         ...,
         [-1.4046e+01, -2.1654e+01, -3.4393e+01,  3.0776e+02],
         [-1.2374e+01, -2.1995e+01, -3.9544e+01,  2.9371e+02],
         [-1.2716e+01, -2.1313e+01, -3.6055e+01,  3.1436e+02]],

        [[-1.2976e+00, -1.1947e+00, -9.3847e-01, -2.0745e+01],
         [ 1.0725e-01, -1.0837e-01, -3.1670e-01, -3.4785e+01],
         [-1.9493e+00, -1.8703e+00, -1.6987e+00, -2.4082e+01],
         ...,
         [-7.8577e+00, -6.0264e+00, -3.5646e+00,  5.4681e+00],
         [-7.7669e+00, -6.3694e+00, -4.7443e+00, -1.0566e+01],
         [-7.4464e+00, -6.1127e+00, -4.4994e+00, -9.9740e+00]],

        ...,

        [[-2.4226e+00, -2.2964e+00, -2.3632e+00,  1.8772e+01],
         [-1.0367e+00, -1.0478e+00, -1.2644e+00,  1.1218e+01],
         [-4.1064e-01, -5.2281e-01, -6.8775e-01,  1.0590e+01],
         ...,
         [ 1.6102e+01,  6.6994e+00, -5.3252e+00,  3.4476e+02],
         [ 1.3401e+01,  4.4551e+00, -7.1799e+00,  3.4640e+02],
         [ 1.2203e+01,  4.0778e+00, -6.0796e+00,  3.4114e+02]],

        [[-1.3914e+00, -1.2936e+00, -6.7047e-01, -1.6038e+01],
         [-4.0123e+00, -3.6738e+00, -2.1263e+00, -2.1271e+01],
         [-1.4698e+00, -1.6756e+00, -1.7506e+00, -3.1049e+01],
         ...,
         [-2.9567e-01,  3.4001e-01,  2.0962e+00, -5.2061e+00],
         [-1.6884e-01,  4.3091e-01,  1.9652e+00, -9.3696e+00],
         [-3.3099e-01,  2.1338e-01,  1.8692e+00, -2.2407e+00]],

        [[ 3.4418e-01,  1.4656e-01, -2.5202e-02,  1.5873e+00],
         [ 1.3271e+00,  1.0555e+00,  8.9603e-01, -3.3009e+01],
         [ 4.6269e-01,  4.3109e-01,  6.4968e-01, -3.6394e+01],
         ...,
         [ 8.0431e+00,  6.3113e+00,  4.4835e+00,  2.7389e+02],
         [ 8.1577e+00,  6.2998e+00,  4.2354e+00,  2.7571e+02],
         [ 7.4692e+00,  5.5917e+00,  3.2924e+00,  2.7721e+02]]],
       grad_fn=<ReshapeAliasBackward0>), 'rgb0': tensor([[0.2325, 0.2413, 0.2440],
        [0.6256, 0.6148, 0.6478],
        [0.7499, 0.6963, 0.6451],
        ...,
        [0.1388, 0.1481, 0.1295],
        [0.5286, 0.4370, 0.3717],
        [0.5668, 0.5245, 0.4831]], grad_fn=<ReshapeAliasBackward0>), 'disp0': tensor([ 20.7171,  30.4603,  18.8625,  ..., 294.1703,  21.0789,  74.1306],
       grad_fn=<ReshapeAliasBackward0>), 'acc0': tensor([1., 1., 1.,  ..., 1., 1., 1.], grad_fn=<ReshapeAliasBackward0>), 'z_std': tensor([0.0027, 0.0736, 0.0033,  ..., 0.2665, 0.0025, 0.0027])}
0 0.0005452632904052734
torch.Size([320, 640, 3]) torch.Size([320, 640])
1 13.70069432258606
2 15.352566480636597
3 13.839682579040527
4 13.512618064880371
5 15.662855625152588
6 13.467182159423828
7 15.62290072441101
8 13.606802940368652
9 13.7171311378479
10 15.485161542892456
11 13.45190715789795
12 15.82261323928833
13 13.320793390274048
14 15.714823961257935
15 13.456844806671143
16 13.709644317626953
17 15.544270277023315
18 13.572761535644531
19 15.720612049102783
20 13.413917064666748
21 15.69830060005188
22 13.463351011276245
23 13.676746606826782
24 15.60314655303955
25 13.405807733535767
26 15.83890676498413
27 13.347854375839233
28 15.766752481460571
29 13.476868152618408
30 13.630383491516113
31 15.678474187850952
32 13.428778409957886
33 15.836413860321045
34 13.292473793029785
35 13.667626142501831
36 15.728934049606323
37 13.643035173416138
38 15.451715230941772
39 13.657320022583008
40 15.51498556137085
41 13.67653203010559
42 13.663667440414429
43 15.474974393844604
44 13.624806642532349
45 15.497665882110596
46 13.663668632507324
47 15.510981321334839
48 13.679360628128052
49 13.650916576385498
50 15.44304895401001
51 13.648831367492676
52 15.468080043792725
53 13.624300241470337
54 15.479091167449951
55 13.688690185546875
56 13.624711990356445
57 15.503207445144653
58 13.638221740722656
59 15.464176416397095
60 13.733251333236694
61 13.677361488342285
62 15.524544715881348
63 13.667057514190674
64 15.470226049423218
65 13.653984308242798
66 15.532845735549927
67 13.573513507843018
68 13.64779019355774
69 15.739839792251587
70 13.65677785873413
71 15.383312463760376
72 13.666393280029297
73 15.561805248260498
74 13.682337522506714
75 13.616225719451904
76 15.537879228591919
77 13.636211156845093
78 15.540842294692993
79 13.718445777893066
80 15.529157400131226
81 13.640345573425293
82 13.672642946243286
83 15.572239875793457
84 13.586940288543701
85 15.590341567993164
86 13.567254304885864
87 15.575148582458496
88 13.850734233856201
89 13.514081478118896
90 15.618290424346924
91 13.484300374984741
92 15.64632534980774
93 13.60610842704773
94 13.637330532073975
95 15.615752458572388
96 13.650386810302734
97 15.606877565383911
98 13.2304527759552
99 16.110689640045166
100 13.383617877960205
101 13.518253326416016
102 15.700722932815552
103 13.407525777816772
104 15.931135892868042
105 13.308671712875366
106 15.802228927612305
107 13.26536750793457
108 13.754213809967041
109 15.708919286727905
110 13.310077428817749
111 15.812534093856812
112 13.599963426589966
113 15.577708721160889
114 13.557747840881348
115 13.530249118804932
116 15.871186971664429
117 13.299684524536133
118 15.837316513061523
119 13.434377431869507
test poses shape torch.Size([13, 3, 4])
0 0.0007224082946777344
torch.Size([320, 640, 3]) torch.Size([320, 640])
1 13.578894138336182
2 13.629607915878296
3 15.702335834503174
4 13.63628602027893
5 15.804567098617554
6 13.413213729858398
7 13.903484344482422
8 15.2820463180542
9 13.704861640930176
10 15.776915311813354
11 13.307158946990967
12 16.131174564361572
Saved test set
[TRAIN] Iter: 600000 Loss: 0.021246016025543213  PSNR: 19.98196029663086
[TRAIN] Iter: 600100 Loss: 0.02650747448205948  PSNR: 18.8659725189209
[TRAIN] Iter: 600200 Loss: 0.02735365927219391  PSNR: 18.766355514526367
[TRAIN] Iter: 600300 Loss: 0.027375448495149612  PSNR: 18.83411979675293
[TRAIN] Iter: 600400 Loss: 0.024420185014605522  PSNR: 19.356487274169922
[TRAIN] Iter: 600500 Loss: 0.0260503850877285  PSNR: 19.060157775878906
[TRAIN] Iter: 600600 Loss: 0.02232332155108452  PSNR: 19.804582595825195
[TRAIN] Iter: 600700 Loss: 0.02834673970937729  PSNR: 18.606557846069336
[TRAIN] Iter: 600800 Loss: 0.02554098144173622  PSNR: 19.34027862548828
[TRAIN] Iter: 600900 Loss: 0.03332465887069702  PSNR: 17.9172306060791
[TRAIN] Iter: 601000 Loss: 0.028281938284635544  PSNR: 18.479053497314453
[TRAIN] Iter: 601100 Loss: 0.026353761553764343  PSNR: 18.75448989868164
[TRAIN] Iter: 601200 Loss: 0.028641223907470703  PSNR: 18.66603660583496
[TRAIN] Iter: 601300 Loss: 0.029840191826224327  PSNR: 18.570404052734375
[TRAIN] Iter: 601400 Loss: 0.025485225021839142  PSNR: 19.206716537475586
[TRAIN] Iter: 601500 Loss: 0.02726680412888527  PSNR: 18.695852279663086
[TRAIN] Iter: 601600 Loss: 0.02225768193602562  PSNR: 19.68151092529297
[TRAIN] Iter: 601700 Loss: 0.028369180858135223  PSNR: 18.7463436126709
[TRAIN] Iter: 601800 Loss: 0.03271288424730301  PSNR: 18.05881118774414
[TRAIN] Iter: 601900 Loss: 0.02773953601717949  PSNR: 18.742902755737305
[TRAIN] Iter: 602000 Loss: 0.026126127690076828  PSNR: 19.016735076904297
[TRAIN] Iter: 602100 Loss: 0.02067960798740387  PSNR: 19.820043563842773
[TRAIN] Iter: 602200 Loss: 0.02066795900464058  PSNR: 20.04270362854004
[TRAIN] Iter: 602300 Loss: 0.026192693039774895  PSNR: 18.626466751098633
[TRAIN] Iter: 602400 Loss: 0.022885043174028397  PSNR: 19.486148834228516
[TRAIN] Iter: 602500 Loss: 0.029873458668589592  PSNR: 18.44317626953125
[TRAIN] Iter: 602600 Loss: 0.032548900693655014  PSNR: 17.85717010498047
[TRAIN] Iter: 602700 Loss: 0.03120620734989643  PSNR: 18.34349822998047
[TRAIN] Iter: 602800 Loss: 0.029855677857995033  PSNR: 18.436790466308594
[TRAIN] Iter: 602900 Loss: 0.021370280534029007  PSNR: 19.89764404296875
[TRAIN] Iter: 603000 Loss: 0.025371622294187546  PSNR: 19.08407974243164
[TRAIN] Iter: 603100 Loss: 0.026713866740465164  PSNR: 18.876022338867188
[TRAIN] Iter: 603200 Loss: 0.032422903925180435  PSNR: 18.070533752441406
[TRAIN] Iter: 603300 Loss: 0.02429051697254181  PSNR: 19.313684463500977
[TRAIN] Iter: 603400 Loss: 0.02320030704140663  PSNR: 19.751867294311523
[TRAIN] Iter: 603500 Loss: 0.03165075182914734  PSNR: 18.221647262573242
[TRAIN] Iter: 603600 Loss: 0.02627551183104515  PSNR: 18.843652725219727
[TRAIN] Iter: 603700 Loss: 0.02078785002231598  PSNR: 20.036758422851562
[TRAIN] Iter: 603800 Loss: 0.027260441333055496  PSNR: 18.684873580932617
[TRAIN] Iter: 603900 Loss: 0.032054267823696136  PSNR: 18.05080223083496
[TRAIN] Iter: 604000 Loss: 0.029116623103618622  PSNR: 18.50975227355957
[TRAIN] Iter: 604100 Loss: 0.029698628932237625  PSNR: 18.469688415527344
[TRAIN] Iter: 604200 Loss: 0.028569113463163376  PSNR: 18.695791244506836
[TRAIN] Iter: 604300 Loss: 0.02852199226617813  PSNR: 18.587745666503906
[TRAIN] Iter: 604400 Loss: 0.021302247419953346  PSNR: 19.788618087768555
[TRAIN] Iter: 604500 Loss: 0.0322984904050827  PSNR: 18.048721313476562
[TRAIN] Iter: 604600 Loss: 0.02318614348769188  PSNR: 19.382892608642578
[TRAIN] Iter: 604700 Loss: 0.025987733155488968  PSNR: 19.0196533203125
[TRAIN] Iter: 604800 Loss: 0.02467695251107216  PSNR: 19.340768814086914
[TRAIN] Iter: 604900 Loss: 0.020242823287844658  PSNR: 20.176284790039062
[TRAIN] Iter: 605000 Loss: 0.02441631816327572  PSNR: 19.41289520263672
[TRAIN] Iter: 605100 Loss: 0.027044033631682396  PSNR: 18.968725204467773
[TRAIN] Iter: 605200 Loss: 0.02272699773311615  PSNR: 19.376468658447266
[TRAIN] Iter: 605300 Loss: 0.022981122136116028  PSNR: 19.622882843017578
[TRAIN] Iter: 605400 Loss: 0.03005308471620083  PSNR: 17.934078216552734
[TRAIN] Iter: 605500 Loss: 0.029363684356212616  PSNR: 18.65803337097168
[TRAIN] Iter: 605600 Loss: 0.02484203316271305  PSNR: 19.000516891479492
[TRAIN] Iter: 605700 Loss: 0.026827655732631683  PSNR: 18.964447021484375
[TRAIN] Iter: 605800 Loss: 0.024148957803845406  PSNR: 19.543344497680664
[TRAIN] Iter: 605900 Loss: 0.025228017941117287  PSNR: 19.177051544189453
[TRAIN] Iter: 606000 Loss: 0.0310499370098114  PSNR: 18.083528518676758
[TRAIN] Iter: 606100 Loss: 0.02672553062438965  PSNR: 19.273508071899414
[TRAIN] Iter: 606200 Loss: 0.024352457374334335  PSNR: 19.599172592163086
[TRAIN] Iter: 606300 Loss: 0.02647022157907486  PSNR: 18.949512481689453
[TRAIN] Iter: 606400 Loss: 0.030094239860773087  PSNR: 18.35361099243164
[TRAIN] Iter: 606500 Loss: 0.028686873614788055  PSNR: 18.574310302734375
[TRAIN] Iter: 606600 Loss: 0.027204480022192  PSNR: 18.90999984741211
[TRAIN] Iter: 606700 Loss: 0.026189444586634636  PSNR: 19.124555587768555
[TRAIN] Iter: 606800 Loss: 0.02415611781179905  PSNR: 19.241371154785156
[TRAIN] Iter: 606900 Loss: 0.02264447696506977  PSNR: 19.814083099365234
[TRAIN] Iter: 607000 Loss: 0.03453214839100838  PSNR: 17.95064353942871
[TRAIN] Iter: 607100 Loss: 0.025684090331196785  PSNR: 18.96865463256836
[TRAIN] Iter: 607200 Loss: 0.02761327661573887  PSNR: 18.61717414855957
[TRAIN] Iter: 607300 Loss: 0.028491411358118057  PSNR: 18.640727996826172
[TRAIN] Iter: 607400 Loss: 0.023791667073965073  PSNR: 19.60210609436035
[TRAIN] Iter: 607500 Loss: 0.026986021548509598  PSNR: 18.432600021362305
[TRAIN] Iter: 607600 Loss: 0.02861318551003933  PSNR: 18.725500106811523
[TRAIN] Iter: 607700 Loss: 0.027295131236314774  PSNR: 18.822721481323242
[TRAIN] Iter: 607800 Loss: 0.025950133800506592  PSNR: 19.063800811767578
[TRAIN] Iter: 607900 Loss: 0.0241134874522686  PSNR: 19.198444366455078
[TRAIN] Iter: 608000 Loss: 0.021964961662888527  PSNR: 19.779163360595703
[TRAIN] Iter: 608100 Loss: 0.029041331261396408  PSNR: 18.604219436645508
[TRAIN] Iter: 608200 Loss: 0.031918127089738846  PSNR: 18.092723846435547
[TRAIN] Iter: 608300 Loss: 0.01953032612800598  PSNR: 20.267370223999023
[TRAIN] Iter: 608400 Loss: 0.021146807819604874  PSNR: 20.012317657470703
[TRAIN] Iter: 608500 Loss: 0.025815043598413467  PSNR: 19.052288055419922
[TRAIN] Iter: 608600 Loss: 0.02946968376636505  PSNR: 18.4790096282959
[TRAIN] Iter: 608700 Loss: 0.021662890911102295  PSNR: 19.840925216674805
[TRAIN] Iter: 608800 Loss: 0.025926504284143448  PSNR: 19.04570770263672
[TRAIN] Iter: 608900 Loss: 0.02664605900645256  PSNR: 19.014484405517578
[TRAIN] Iter: 609000 Loss: 0.02931903302669525  PSNR: 18.471830368041992
[TRAIN] Iter: 609100 Loss: 0.029521606862545013  PSNR: 18.523452758789062
[TRAIN] Iter: 609200 Loss: 0.02578539028763771  PSNR: 19.372800827026367
[TRAIN] Iter: 609300 Loss: 0.024042882025241852  PSNR: 19.341093063354492
[TRAIN] Iter: 609400 Loss: 0.026113100349903107  PSNR: 19.047269821166992
[TRAIN] Iter: 609500 Loss: 0.026666229590773582  PSNR: 19.089113235473633
[TRAIN] Iter: 609600 Loss: 0.0230257548391819  PSNR: 19.540557861328125
[TRAIN] Iter: 609700 Loss: 0.024677326902747154  PSNR: 19.382326126098633
[TRAIN] Iter: 609800 Loss: 0.025820616632699966  PSNR: 19.20932388305664
[TRAIN] Iter: 609900 Loss: 0.024168385192751884  PSNR: 19.427141189575195
Saved checkpoints at ./logs/TUT-LAB-nerf/610000.tar
[TRAIN] Iter: 610000 Loss: 0.026277316734194756  PSNR: 19.00286102294922
[TRAIN] Iter: 610100 Loss: 0.025195244699716568  PSNR: 19.462121963500977
[TRAIN] Iter: 610200 Loss: 0.026707706972956657  PSNR: 18.927785873413086
[TRAIN] Iter: 610300 Loss: 0.027063360437750816  PSNR: 18.88631248474121
[TRAIN] Iter: 610400 Loss: 0.023162564262747765  PSNR: 19.75115394592285
[TRAIN] Iter: 610500 Loss: 0.03001074306666851  PSNR: 18.443681716918945
[TRAIN] Iter: 610600 Loss: 0.026385314762592316  PSNR: 18.536243438720703
[TRAIN] Iter: 610700 Loss: 0.023249804973602295  PSNR: 19.82404327392578
[TRAIN] Iter: 610800 Loss: 0.028633171692490578  PSNR: 18.674510955810547
[TRAIN] Iter: 610900 Loss: 0.03105691820383072  PSNR: 18.245075225830078
[TRAIN] Iter: 611000 Loss: 0.029922202229499817  PSNR: 18.331968307495117
[TRAIN] Iter: 611100 Loss: 0.02435842901468277  PSNR: 19.46202278137207
[TRAIN] Iter: 611200 Loss: 0.023131417110562325  PSNR: 19.660030364990234
[TRAIN] Iter: 611300 Loss: 0.021906737238168716  PSNR: 19.521425247192383
[TRAIN] Iter: 611400 Loss: 0.027848470956087112  PSNR: 18.801427841186523
[TRAIN] Iter: 611500 Loss: 0.021998263895511627  PSNR: 19.78868293762207
[TRAIN] Iter: 611600 Loss: 0.021468646824359894  PSNR: 19.8160457611084
[TRAIN] Iter: 611700 Loss: 0.028863217681646347  PSNR: 18.70550537109375
[TRAIN] Iter: 611800 Loss: 0.024585332721471786  PSNR: 19.479652404785156
[TRAIN] Iter: 611900 Loss: 0.02311675250530243  PSNR: 19.58679962158203
[TRAIN] Iter: 612000 Loss: 0.032889820635318756  PSNR: 18.04503631591797
[TRAIN] Iter: 612100 Loss: 0.029727227985858917  PSNR: 18.367151260375977
[TRAIN] Iter: 612200 Loss: 0.02541278675198555  PSNR: 18.968530654907227
[TRAIN] Iter: 612300 Loss: 0.030945055186748505  PSNR: 18.316804885864258
[TRAIN] Iter: 612400 Loss: 0.026371270418167114  PSNR: 19.25760269165039
[TRAIN] Iter: 612500 Loss: 0.028791388496756554  PSNR: 18.50827407836914
[TRAIN] Iter: 612600 Loss: 0.020593374967575073  PSNR: 20.056560516357422
[TRAIN] Iter: 612700 Loss: 0.025762682780623436  PSNR: 19.101577758789062
[TRAIN] Iter: 612800 Loss: 0.03241248428821564  PSNR: 18.054277420043945
[TRAIN] Iter: 612900 Loss: 0.02823956124484539  PSNR: 18.54389190673828
[TRAIN] Iter: 613000 Loss: 0.024210231378674507  PSNR: 19.444299697875977
[TRAIN] Iter: 613100 Loss: 0.028102347627282143  PSNR: 18.98176383972168
[TRAIN] Iter: 613200 Loss: 0.023297203704714775  PSNR: 19.410308837890625
[TRAIN] Iter: 613300 Loss: 0.02300533466041088  PSNR: 19.689441680908203
[TRAIN] Iter: 613400 Loss: 0.03303790092468262  PSNR: 18.091960906982422
[TRAIN] Iter: 613500 Loss: 0.025107789784669876  PSNR: 19.591957092285156
[TRAIN] Iter: 613600 Loss: 0.026379166170954704  PSNR: 19.304420471191406
[TRAIN] Iter: 613700 Loss: 0.028664857149124146  PSNR: 18.63467025756836
[TRAIN] Iter: 613800 Loss: 0.03172430023550987  PSNR: 18.122190475463867
[TRAIN] Iter: 613900 Loss: 0.03049284778535366  PSNR: 18.261911392211914
[TRAIN] Iter: 614000 Loss: 0.02092306688427925  PSNR: 19.863542556762695
[TRAIN] Iter: 614100 Loss: 0.020301245152950287  PSNR: 19.938573837280273
[TRAIN] Iter: 614200 Loss: 0.03407009318470955  PSNR: 17.816308975219727
[TRAIN] Iter: 614300 Loss: 0.029698438942432404  PSNR: 18.460901260375977
[TRAIN] Iter: 614400 Loss: 0.02040148340165615  PSNR: 20.176179885864258
[TRAIN] Iter: 614500 Loss: 0.022690720856189728  PSNR: 19.34849739074707
[TRAIN] Iter: 614600 Loss: 0.02855059504508972  PSNR: 18.748159408569336
[TRAIN] Iter: 614700 Loss: 0.030469300225377083  PSNR: 18.29903221130371
[TRAIN] Iter: 614800 Loss: 0.024433430284261703  PSNR: 19.41733169555664
[TRAIN] Iter: 614900 Loss: 0.019638068974018097  PSNR: 20.24493980407715
[TRAIN] Iter: 615000 Loss: 0.03248618543148041  PSNR: 18.112886428833008
[TRAIN] Iter: 615100 Loss: 0.02351604774594307  PSNR: 19.254335403442383
[TRAIN] Iter: 615200 Loss: 0.023132162168622017  PSNR: 19.478591918945312
[TRAIN] Iter: 615300 Loss: 0.02661546692252159  PSNR: 18.99068260192871
[TRAIN] Iter: 615400 Loss: 0.030314980074763298  PSNR: 18.267959594726562
[TRAIN] Iter: 615500 Loss: 0.02844913676381111  PSNR: 18.655513763427734
[TRAIN] Iter: 615600 Loss: 0.027152035385370255  PSNR: 18.901958465576172
[TRAIN] Iter: 615700 Loss: 0.026737309992313385  PSNR: 18.802852630615234
[TRAIN] Iter: 615800 Loss: 0.028011616319417953  PSNR: 18.660131454467773
[TRAIN] Iter: 615900 Loss: 0.02976413071155548  PSNR: 18.38105583190918
[TRAIN] Iter: 616000 Loss: 0.021029386669397354  PSNR: 19.949357986450195
[TRAIN] Iter: 616100 Loss: 0.029801342636346817  PSNR: 18.344736099243164
[TRAIN] Iter: 616200 Loss: 0.02373947575688362  PSNR: 19.46563720703125
[TRAIN] Iter: 616300 Loss: 0.02436642535030842  PSNR: 19.137405395507812
[TRAIN] Iter: 616400 Loss: 0.02799948677420616  PSNR: 18.684879302978516
[TRAIN] Iter: 616500 Loss: 0.027981441468000412  PSNR: 19.0419921875
[TRAIN] Iter: 616600 Loss: 0.027194909751415253  PSNR: 18.85650634765625
[TRAIN] Iter: 616700 Loss: 0.03155867010354996  PSNR: 18.20987892150879
[TRAIN] Iter: 616800 Loss: 0.022625897079706192  PSNR: 19.766799926757812
[TRAIN] Iter: 616900 Loss: 0.02398553490638733  PSNR: 19.437772750854492
[TRAIN] Iter: 617000 Loss: 0.02387288212776184  PSNR: 19.3988094329834
[TRAIN] Iter: 617100 Loss: 0.02944403886795044  PSNR: 18.555208206176758
[TRAIN] Iter: 617200 Loss: 0.02704920619726181  PSNR: 18.93231964111328
[TRAIN] Iter: 617300 Loss: 0.030977053567767143  PSNR: 18.237834930419922
[TRAIN] Iter: 617400 Loss: 0.0205372404307127  PSNR: 20.092496871948242
[TRAIN] Iter: 617500 Loss: 0.02801181934773922  PSNR: 18.75286865234375
[TRAIN] Iter: 617600 Loss: 0.02521590143442154  PSNR: 19.240238189697266
[TRAIN] Iter: 617700 Loss: 0.027215439826250076  PSNR: 19.283533096313477
[TRAIN] Iter: 617800 Loss: 0.026488948613405228  PSNR: 18.8367919921875
[TRAIN] Iter: 617900 Loss: 0.023888491094112396  PSNR: 19.43407440185547
[TRAIN] Iter: 618000 Loss: 0.02460860088467598  PSNR: 19.22053337097168
[TRAIN] Iter: 618100 Loss: 0.0345534011721611  PSNR: 17.779308319091797
[TRAIN] Iter: 618200 Loss: 0.034532107412815094  PSNR: 17.836198806762695
[TRAIN] Iter: 618300 Loss: 0.025534696877002716  PSNR: 19.04918670654297
[TRAIN] Iter: 618400 Loss: 0.025143731385469437  PSNR: 19.562644958496094
[TRAIN] Iter: 618500 Loss: 0.030666377395391464  PSNR: 18.234106063842773
[TRAIN] Iter: 618600 Loss: 0.018536582589149475  PSNR: 20.59878921508789
[TRAIN] Iter: 618700 Loss: 0.028670301660895348  PSNR: 18.65435218811035
[TRAIN] Iter: 618800 Loss: 0.029859444126486778  PSNR: 18.487077713012695
[TRAIN] Iter: 618900 Loss: 0.03235752880573273  PSNR: 18.042261123657227
[TRAIN] Iter: 619000 Loss: 0.03293907642364502  PSNR: 18.09788703918457
[TRAIN] Iter: 619100 Loss: 0.02703121304512024  PSNR: 18.851526260375977
[TRAIN] Iter: 619200 Loss: 0.02910541743040085  PSNR: 18.62855339050293
[TRAIN] Iter: 619300 Loss: 0.028124770149588585  PSNR: 18.576730728149414
[TRAIN] Iter: 619400 Loss: 0.028851067647337914  PSNR: 18.543743133544922
[TRAIN] Iter: 619500 Loss: 0.031359121203422546  PSNR: 18.17074203491211
[TRAIN] Iter: 619600 Loss: 0.029995810240507126  PSNR: 18.450626373291016
[TRAIN] Iter: 619700 Loss: 0.02595783770084381  PSNR: 19.395328521728516
[TRAIN] Iter: 619800 Loss: 0.02822890318930149  PSNR: 18.6380615234375
[TRAIN] Iter: 619900 Loss: 0.026580994948744774  PSNR: 18.906219482421875
Saved checkpoints at ./logs/TUT-LAB-nerf/620000.tar
[TRAIN] Iter: 620000 Loss: 0.028912043198943138  PSNR: 18.372154235839844
[TRAIN] Iter: 620100 Loss: 0.019629430025815964  PSNR: 20.29638671875
[TRAIN] Iter: 620200 Loss: 0.02291101962327957  PSNR: 19.519412994384766
[TRAIN] Iter: 620300 Loss: 0.026980850845575333  PSNR: 18.922237396240234
[TRAIN] Iter: 620400 Loss: 0.031075026839971542  PSNR: 18.203712463378906
[TRAIN] Iter: 620500 Loss: 0.02228798158466816  PSNR: 19.612613677978516
[TRAIN] Iter: 620600 Loss: 0.019228525459766388  PSNR: 20.43708610534668
[TRAIN] Iter: 620700 Loss: 0.036439742892980576  PSNR: 17.50450325012207
[TRAIN] Iter: 620800 Loss: 0.03255287557840347  PSNR: 18.145278930664062
[TRAIN] Iter: 620900 Loss: 0.03358688950538635  PSNR: 17.904865264892578
[TRAIN] Iter: 621000 Loss: 0.02145184576511383  PSNR: 19.82380485534668
[TRAIN] Iter: 621100 Loss: 0.027544494718313217  PSNR: 18.734201431274414
[TRAIN] Iter: 621200 Loss: 0.02723526768386364  PSNR: 18.608339309692383
[TRAIN] Iter: 621300 Loss: 0.0232388935983181  PSNR: 19.57585906982422
[TRAIN] Iter: 621400 Loss: 0.030373213812708855  PSNR: 18.380752563476562
[TRAIN] Iter: 621500 Loss: 0.029630575329065323  PSNR: 18.481027603149414
[TRAIN] Iter: 621600 Loss: 0.02174420654773712  PSNR: 19.751434326171875
[TRAIN] Iter: 621700 Loss: 0.03119172528386116  PSNR: 18.197952270507812
[TRAIN] Iter: 621800 Loss: 0.02943110466003418  PSNR: 18.485719680786133
[TRAIN] Iter: 621900 Loss: 0.029719777405261993  PSNR: 18.3873291015625
[TRAIN] Iter: 622000 Loss: 0.022267665714025497  PSNR: 19.622392654418945
[TRAIN] Iter: 622100 Loss: 0.02397339418530464  PSNR: 19.54563331604004
[TRAIN] Iter: 622200 Loss: 0.027695447206497192  PSNR: 18.81157112121582
[TRAIN] Iter: 622300 Loss: 0.02685660868883133  PSNR: 18.879409790039062
[TRAIN] Iter: 622400 Loss: 0.024906758219003677  PSNR: 19.230682373046875
[TRAIN] Iter: 622500 Loss: 0.027159009128808975  PSNR: 18.821962356567383
[TRAIN] Iter: 622600 Loss: 0.025051027536392212  PSNR: 19.282522201538086
[TRAIN] Iter: 622700 Loss: 0.026475366204977036  PSNR: 19.248065948486328
[TRAIN] Iter: 622800 Loss: 0.024877961724996567  PSNR: 19.02838134765625
[TRAIN] Iter: 622900 Loss: 0.028999606147408485  PSNR: 18.68997573852539
[TRAIN] Iter: 623000 Loss: 0.02855595201253891  PSNR: 18.590309143066406
[TRAIN] Iter: 623100 Loss: 0.02951480820775032  PSNR: 18.534488677978516
[TRAIN] Iter: 623200 Loss: 0.025729816406965256  PSNR: 19.25960922241211
[TRAIN] Iter: 623300 Loss: 0.026904046535491943  PSNR: 18.835561752319336
[TRAIN] Iter: 623400 Loss: 0.027324285358190536  PSNR: 18.948287963867188
[TRAIN] Iter: 623500 Loss: 0.026860764250159264  PSNR: 19.000534057617188
[TRAIN] Iter: 623600 Loss: 0.02550181746482849  PSNR: 19.31926155090332
[TRAIN] Iter: 623700 Loss: 0.02415713481605053  PSNR: 19.58739471435547
[TRAIN] Iter: 623800 Loss: 0.024992287158966064  PSNR: 19.263381958007812
[TRAIN] Iter: 623900 Loss: 0.02269025146961212  PSNR: 19.453824996948242
[TRAIN] Iter: 624000 Loss: 0.02794419601559639  PSNR: 18.932626724243164
[TRAIN] Iter: 624100 Loss: 0.026006028056144714  PSNR: 19.3068904876709
[TRAIN] Iter: 624200 Loss: 0.026257937774062157  PSNR: 18.917388916015625
[TRAIN] Iter: 624300 Loss: 0.025927912443876266  PSNR: 19.20833969116211
[TRAIN] Iter: 624400 Loss: 0.024988742545247078  PSNR: 18.933685302734375
[TRAIN] Iter: 624500 Loss: 0.02618122100830078  PSNR: 19.15447235107422
[TRAIN] Iter: 624600 Loss: 0.023930851370096207  PSNR: 19.397308349609375
[TRAIN] Iter: 624700 Loss: 0.025412607938051224  PSNR: 18.880163192749023
[TRAIN] Iter: 624800 Loss: 0.022193679586052895  PSNR: 19.9352970123291
[TRAIN] Iter: 624900 Loss: 0.028081946074962616  PSNR: 18.753597259521484
[TRAIN] Iter: 625000 Loss: 0.02593146078288555  PSNR: 19.04562759399414
[TRAIN] Iter: 625100 Loss: 0.0315452441573143  PSNR: 18.172597885131836
[TRAIN] Iter: 625200 Loss: 0.02541927993297577  PSNR: 19.352720260620117
[TRAIN] Iter: 625300 Loss: 0.020103074610233307  PSNR: 20.0921630859375
[TRAIN] Iter: 625400 Loss: 0.028046444058418274  PSNR: 18.57169532775879
[TRAIN] Iter: 625500 Loss: 0.028795499354600906  PSNR: 18.696468353271484
[TRAIN] Iter: 625600 Loss: 0.024691326543688774  PSNR: 19.191448211669922
[TRAIN] Iter: 625700 Loss: 0.023265575990080833  PSNR: 19.452680587768555
[TRAIN] Iter: 625800 Loss: 0.02338973805308342  PSNR: 19.543657302856445
[TRAIN] Iter: 625900 Loss: 0.027576519176363945  PSNR: 18.563724517822266
[TRAIN] Iter: 626000 Loss: 0.030149517580866814  PSNR: 18.328969955444336
[TRAIN] Iter: 626100 Loss: 0.029108252376317978  PSNR: 18.592313766479492
[TRAIN] Iter: 626200 Loss: 0.019870128482580185  PSNR: 20.1123046875
[TRAIN] Iter: 626300 Loss: 0.027918824926018715  PSNR: 18.744760513305664
[TRAIN] Iter: 626400 Loss: 0.02101977914571762  PSNR: 19.891952514648438
[TRAIN] Iter: 626500 Loss: 0.025986716151237488  PSNR: 19.204404830932617
[TRAIN] Iter: 626600 Loss: 0.028311116620898247  PSNR: 18.888025283813477
[TRAIN] Iter: 626700 Loss: 0.024281449615955353  PSNR: 19.168869018554688
[TRAIN] Iter: 626800 Loss: 0.03149819374084473  PSNR: 18.070358276367188
[TRAIN] Iter: 626900 Loss: 0.02296251431107521  PSNR: 19.716943740844727
[TRAIN] Iter: 627000 Loss: 0.033962182700634  PSNR: 17.865768432617188
[TRAIN] Iter: 627100 Loss: 0.02502005361020565  PSNR: 19.38941192626953
[TRAIN] Iter: 627200 Loss: 0.023904455825686455  PSNR: 19.407766342163086
[TRAIN] Iter: 627300 Loss: 0.03063930571079254  PSNR: 18.35655975341797
[TRAIN] Iter: 627400 Loss: 0.03133559972047806  PSNR: 18.255001068115234
[TRAIN] Iter: 627500 Loss: 0.023760240525007248  PSNR: 19.21539878845215
[TRAIN] Iter: 627600 Loss: 0.03016991727054119  PSNR: 18.39488410949707
[TRAIN] Iter: 627700 Loss: 0.02451656386256218  PSNR: 18.825578689575195
[TRAIN] Iter: 627800 Loss: 0.028733575716614723  PSNR: 19.155603408813477
[TRAIN] Iter: 627900 Loss: 0.02919628843665123  PSNR: 18.50596046447754
[TRAIN] Iter: 628000 Loss: 0.018792930990457535  PSNR: 20.440818786621094
[TRAIN] Iter: 628100 Loss: 0.02599068358540535  PSNR: 18.837520599365234
[TRAIN] Iter: 628200 Loss: 0.02899749204516411  PSNR: 18.381412506103516
[TRAIN] Iter: 628300 Loss: 0.031569406390190125  PSNR: 18.232080459594727
[TRAIN] Iter: 628400 Loss: 0.028693195432424545  PSNR: 18.965856552124023
[TRAIN] Iter: 628500 Loss: 0.02725176140666008  PSNR: 18.848224639892578
[TRAIN] Iter: 628600 Loss: 0.0203351192176342  PSNR: 20.161773681640625
[TRAIN] Iter: 628700 Loss: 0.02441876009106636  PSNR: 19.298934936523438
[TRAIN] Iter: 628800 Loss: 0.02767024002969265  PSNR: 18.79257583618164
[TRAIN] Iter: 628900 Loss: 0.024507513269782066  PSNR: 19.14690589904785
[TRAIN] Iter: 629000 Loss: 0.027583930641412735  PSNR: 18.64444351196289
[TRAIN] Iter: 629100 Loss: 0.026868268847465515  PSNR: 18.83954429626465
[TRAIN] Iter: 629200 Loss: 0.02845769189298153  PSNR: 18.66979217529297
[TRAIN] Iter: 629300 Loss: 0.02241623029112816  PSNR: 19.52178382873535
[TRAIN] Iter: 629400 Loss: 0.02860998548567295  PSNR: 18.442846298217773
[TRAIN] Iter: 629500 Loss: 0.02903849631547928  PSNR: 18.459749221801758
[TRAIN] Iter: 629600 Loss: 0.027193289250135422  PSNR: 19.305112838745117
[TRAIN] Iter: 629700 Loss: 0.026133738458156586  PSNR: 19.064258575439453
[TRAIN] Iter: 629800 Loss: 0.02601260133087635  PSNR: 19.12602424621582
[TRAIN] Iter: 629900 Loss: 0.02912364900112152  PSNR: 18.456602096557617
Saved checkpoints at ./logs/TUT-LAB-nerf/630000.tar
[TRAIN] Iter: 630000 Loss: 0.031142069026827812  PSNR: 18.200637817382812
[TRAIN] Iter: 630100 Loss: 0.022710733115673065  PSNR: 19.18050765991211
[TRAIN] Iter: 630200 Loss: 0.02418818324804306  PSNR: 19.424724578857422
[TRAIN] Iter: 630300 Loss: 0.025936750695109367  PSNR: 18.982513427734375
[TRAIN] Iter: 630400 Loss: 0.02679758332669735  PSNR: 18.750030517578125
[TRAIN] Iter: 630500 Loss: 0.0245082825422287  PSNR: 19.217676162719727
[TRAIN] Iter: 630600 Loss: 0.024794112890958786  PSNR: 19.755474090576172
[TRAIN] Iter: 630700 Loss: 0.022499121725559235  PSNR: 19.798198699951172
[TRAIN] Iter: 630800 Loss: 0.032527074217796326  PSNR: 18.161205291748047
[TRAIN] Iter: 630900 Loss: 0.02350076287984848  PSNR: 19.267417907714844
[TRAIN] Iter: 631000 Loss: 0.023963619023561478  PSNR: 19.393959045410156
[TRAIN] Iter: 631100 Loss: 0.02566657029092312  PSNR: 19.06230926513672
[TRAIN] Iter: 631200 Loss: 0.0253264419734478  PSNR: 19.25594711303711
[TRAIN] Iter: 631300 Loss: 0.02382432296872139  PSNR: 19.47919464111328
[TRAIN] Iter: 631400 Loss: 0.02654980681836605  PSNR: 19.237390518188477
[TRAIN] Iter: 631500 Loss: 0.026757676154375076  PSNR: 18.714967727661133
[TRAIN] Iter: 631600 Loss: 0.026613105088472366  PSNR: 18.98961067199707
[TRAIN] Iter: 631700 Loss: 0.020121615380048752  PSNR: 20.15093231201172
[TRAIN] Iter: 631800 Loss: 0.02613726072013378  PSNR: 19.087093353271484
[TRAIN] Iter: 631900 Loss: 0.027374934405088425  PSNR: 18.777095794677734
[TRAIN] Iter: 632000 Loss: 0.01966235786676407  PSNR: 20.165292739868164
[TRAIN] Iter: 632100 Loss: 0.02997087687253952  PSNR: 18.42928695678711
[TRAIN] Iter: 632200 Loss: 0.023887209594249725  PSNR: 19.437211990356445
[TRAIN] Iter: 632300 Loss: 0.023042917251586914  PSNR: 19.55322265625
[TRAIN] Iter: 632400 Loss: 0.02635621838271618  PSNR: 19.020296096801758
[TRAIN] Iter: 632500 Loss: 0.027217786759138107  PSNR: 19.0011043548584
[TRAIN] Iter: 632600 Loss: 0.03426874801516533  PSNR: 17.834585189819336
[TRAIN] Iter: 632700 Loss: 0.031780876219272614  PSNR: 18.094074249267578
[TRAIN] Iter: 632800 Loss: 0.030400993302464485  PSNR: 18.446596145629883
[TRAIN] Iter: 632900 Loss: 0.02430490404367447  PSNR: 19.461456298828125
[TRAIN] Iter: 633000 Loss: 0.021942507475614548  PSNR: 19.947582244873047
[TRAIN] Iter: 633100 Loss: 0.031925268471241  PSNR: 18.112844467163086
[TRAIN] Iter: 633200 Loss: 0.030098633840680122  PSNR: 18.402759552001953
[TRAIN] Iter: 633300 Loss: 0.026726748794317245  PSNR: 18.89820098876953
[TRAIN] Iter: 633400 Loss: 0.029352393001317978  PSNR: 18.73652458190918
[TRAIN] Iter: 633500 Loss: 0.030104685574769974  PSNR: 18.316082000732422
[TRAIN] Iter: 633600 Loss: 0.02574615180492401  PSNR: 19.102519989013672
[TRAIN] Iter: 633700 Loss: 0.03083880990743637  PSNR: 18.269094467163086
[TRAIN] Iter: 633800 Loss: 0.02756553888320923  PSNR: 18.75227928161621
[TRAIN] Iter: 633900 Loss: 0.02342933975160122  PSNR: 19.37453269958496
[TRAIN] Iter: 634000 Loss: 0.023265795782208443  PSNR: 19.69925308227539
[TRAIN] Iter: 634100 Loss: 0.023539768531918526  PSNR: 19.301280975341797
[TRAIN] Iter: 634200 Loss: 0.03157419711351395  PSNR: 18.195140838623047
[TRAIN] Iter: 634300 Loss: 0.030534394085407257  PSNR: 18.351774215698242
[TRAIN] Iter: 634400 Loss: 0.02262018248438835  PSNR: 19.547969818115234
[TRAIN] Iter: 634500 Loss: 0.026927119120955467  PSNR: 18.95783233642578
[TRAIN] Iter: 634600 Loss: 0.030528873205184937  PSNR: 18.257375717163086
[TRAIN] Iter: 634700 Loss: 0.023649953305721283  PSNR: 19.419294357299805
[TRAIN] Iter: 634800 Loss: 0.027268480509519577  PSNR: 18.668437957763672
[TRAIN] Iter: 634900 Loss: 0.026319470256567  PSNR: 18.8148136138916
[TRAIN] Iter: 635000 Loss: 0.025221697986125946  PSNR: 19.277481079101562
[TRAIN] Iter: 635100 Loss: 0.03207371383905411  PSNR: 18.033973693847656
[TRAIN] Iter: 635200 Loss: 0.026899922639131546  PSNR: 18.971324920654297
[TRAIN] Iter: 635300 Loss: 0.032737817615270615  PSNR: 18.207218170166016
[TRAIN] Iter: 635400 Loss: 0.02839752286672592  PSNR: 18.764484405517578
[TRAIN] Iter: 635500 Loss: 0.026713687926530838  PSNR: 18.806827545166016
[TRAIN] Iter: 635600 Loss: 0.022447776049375534  PSNR: 19.662673950195312
[TRAIN] Iter: 635700 Loss: 0.027228713035583496  PSNR: 18.695526123046875
[TRAIN] Iter: 635800 Loss: 0.028596751391887665  PSNR: 18.66958999633789
[TRAIN] Iter: 635900 Loss: 0.026243150234222412  PSNR: 19.095958709716797
[TRAIN] Iter: 636000 Loss: 0.029091857373714447  PSNR: 18.565860748291016
[TRAIN] Iter: 636100 Loss: 0.020987600088119507  PSNR: 19.977497100830078
[TRAIN] Iter: 636200 Loss: 0.026139114052057266  PSNR: 18.899410247802734
[TRAIN] Iter: 636300 Loss: 0.0290524922311306  PSNR: 18.447946548461914
[TRAIN] Iter: 636400 Loss: 0.020147962495684624  PSNR: 20.278867721557617
[TRAIN] Iter: 636500 Loss: 0.03321187198162079  PSNR: 17.97096061706543
[TRAIN] Iter: 636600 Loss: 0.02585035003721714  PSNR: 19.03523063659668
[TRAIN] Iter: 636700 Loss: 0.03043489344418049  PSNR: 18.356155395507812
[TRAIN] Iter: 636800 Loss: 0.021568581461906433  PSNR: 19.70087242126465
[TRAIN] Iter: 636900 Loss: 0.026439059525728226  PSNR: 18.811002731323242
[TRAIN] Iter: 637000 Loss: 0.031019877642393112  PSNR: 18.3346004486084
[TRAIN] Iter: 637100 Loss: 0.029885845258831978  PSNR: 18.281620025634766
[TRAIN] Iter: 637200 Loss: 0.03010101616382599  PSNR: 18.396682739257812
[TRAIN] Iter: 637300 Loss: 0.029725810512900352  PSNR: 18.45429801940918
[TRAIN] Iter: 637400 Loss: 0.02743581496179104  PSNR: 18.840091705322266
[TRAIN] Iter: 637500 Loss: 0.020994525402784348  PSNR: 20.040342330932617
[TRAIN] Iter: 637600 Loss: 0.02399769239127636  PSNR: 19.1588134765625
[TRAIN] Iter: 637700 Loss: 0.030951421707868576  PSNR: 18.33659553527832
[TRAIN] Iter: 637800 Loss: 0.024201801046729088  PSNR: 19.36167335510254
[TRAIN] Iter: 637900 Loss: 0.028200585395097733  PSNR: 18.56612777709961
[TRAIN] Iter: 638000 Loss: 0.024505196139216423  PSNR: 19.199718475341797
[TRAIN] Iter: 638100 Loss: 0.024584989994764328  PSNR: 19.057954788208008
[TRAIN] Iter: 638200 Loss: 0.027350610122084618  PSNR: 19.186992645263672
[TRAIN] Iter: 638300 Loss: 0.025259528309106827  PSNR: 19.043045043945312
[TRAIN] Iter: 638400 Loss: 0.026921987533569336  PSNR: 18.852006912231445
[TRAIN] Iter: 638500 Loss: 0.03044193610548973  PSNR: 18.365575790405273
[TRAIN] Iter: 638600 Loss: 0.02821357361972332  PSNR: 18.789472579956055
[TRAIN] Iter: 638700 Loss: 0.026041101664304733  PSNR: 19.09562110900879
[TRAIN] Iter: 638800 Loss: 0.026979126036167145  PSNR: 18.68517303466797
[TRAIN] Iter: 638900 Loss: 0.027607128024101257  PSNR: 18.845687866210938
[TRAIN] Iter: 639000 Loss: 0.024435769766569138  PSNR: 19.046022415161133
[TRAIN] Iter: 639100 Loss: 0.02705221250653267  PSNR: 18.95463752746582
[TRAIN] Iter: 639200 Loss: 0.029670264571905136  PSNR: 18.488187789916992
[TRAIN] Iter: 639300 Loss: 0.02275456115603447  PSNR: 19.656356811523438
[TRAIN] Iter: 639400 Loss: 0.025883873924613  PSNR: 19.088407516479492
[TRAIN] Iter: 639500 Loss: 0.029139403253793716  PSNR: 18.670576095581055
[TRAIN] Iter: 639600 Loss: 0.029371190816164017  PSNR: 18.51681137084961
[TRAIN] Iter: 639700 Loss: 0.029111966490745544  PSNR: 18.504854202270508
[TRAIN] Iter: 639800 Loss: 0.023449361324310303  PSNR: 19.568025588989258
[TRAIN] Iter: 639900 Loss: 0.025135299190878868  PSNR: 19.119979858398438
Saved checkpoints at ./logs/TUT-LAB-nerf/640000.tar
[TRAIN] Iter: 640000 Loss: 0.02848716638982296  PSNR: 18.69272804260254
[TRAIN] Iter: 640100 Loss: 0.02107176184654236  PSNR: 19.89988136291504
[TRAIN] Iter: 640200 Loss: 0.028048470616340637  PSNR: 18.6666316986084
[TRAIN] Iter: 640300 Loss: 0.030760210007429123  PSNR: 18.294443130493164
[TRAIN] Iter: 640400 Loss: 0.030693743377923965  PSNR: 18.268653869628906
[TRAIN] Iter: 640500 Loss: 0.022924575954675674  PSNR: 19.62444496154785
[TRAIN] Iter: 640600 Loss: 0.028841285035014153  PSNR: 18.7648983001709
[TRAIN] Iter: 640700 Loss: 0.028267886489629745  PSNR: 18.760988235473633
[TRAIN] Iter: 640800 Loss: 0.030144333839416504  PSNR: 18.415800094604492
[TRAIN] Iter: 640900 Loss: 0.025758352130651474  PSNR: 19.226318359375
[TRAIN] Iter: 641000 Loss: 0.029148343950510025  PSNR: 18.51691436767578
[TRAIN] Iter: 641100 Loss: 0.024987826123833656  PSNR: 19.24365234375
[TRAIN] Iter: 641200 Loss: 0.02642711251974106  PSNR: 18.91015625
[TRAIN] Iter: 641300 Loss: 0.01774909533560276  PSNR: 20.679922103881836
[TRAIN] Iter: 641400 Loss: 0.025418773293495178  PSNR: 19.03628158569336
[TRAIN] Iter: 641500 Loss: 0.025907546281814575  PSNR: 19.17783546447754
[TRAIN] Iter: 641600 Loss: 0.02403607778251171  PSNR: 19.393840789794922
[TRAIN] Iter: 641700 Loss: 0.026798799633979797  PSNR: 18.93297576904297
[TRAIN] Iter: 641800 Loss: 0.023417484015226364  PSNR: 19.42243766784668
[TRAIN] Iter: 641900 Loss: 0.02437496930360794  PSNR: 19.183462142944336
[TRAIN] Iter: 642000 Loss: 0.0245484821498394  PSNR: 19.24945068359375
[TRAIN] Iter: 642100 Loss: 0.025468721985816956  PSNR: 19.181955337524414
[TRAIN] Iter: 642200 Loss: 0.025941424071788788  PSNR: 19.010387420654297
[TRAIN] Iter: 642300 Loss: 0.029297493398189545  PSNR: 18.53493881225586
[TRAIN] Iter: 642400 Loss: 0.027840018272399902  PSNR: 18.72509002685547
[TRAIN] Iter: 642500 Loss: 0.024535156786441803  PSNR: 19.33951759338379
[TRAIN] Iter: 642600 Loss: 0.02658446505665779  PSNR: 18.94772720336914
[TRAIN] Iter: 642700 Loss: 0.02894267812371254  PSNR: 18.54655647277832
[TRAIN] Iter: 642800 Loss: 0.02799747884273529  PSNR: 18.692733764648438
[TRAIN] Iter: 642900 Loss: 0.02169853076338768  PSNR: 19.839519500732422
[TRAIN] Iter: 643000 Loss: 0.023508738726377487  PSNR: 19.222637176513672
[TRAIN] Iter: 643100 Loss: 0.03083651140332222  PSNR: 18.27326011657715
[TRAIN] Iter: 643200 Loss: 0.020173054188489914  PSNR: 20.156780242919922
[TRAIN] Iter: 643300 Loss: 0.026465261355042458  PSNR: 19.01204490661621
[TRAIN] Iter: 643400 Loss: 0.030734263360500336  PSNR: 18.257469177246094
[TRAIN] Iter: 643500 Loss: 0.027515925467014313  PSNR: 18.950231552124023
[TRAIN] Iter: 643600 Loss: 0.028939522802829742  PSNR: 18.5828800201416
[TRAIN] Iter: 643700 Loss: 0.02866211347281933  PSNR: 18.698760986328125
[TRAIN] Iter: 643800 Loss: 0.02800714038312435  PSNR: 18.734128952026367
[TRAIN] Iter: 643900 Loss: 0.027203794568777084  PSNR: 18.76210594177246
[TRAIN] Iter: 644000 Loss: 0.025870705023407936  PSNR: 19.10332679748535
[TRAIN] Iter: 644100 Loss: 0.028958730399608612  PSNR: 18.509197235107422
[TRAIN] Iter: 644200 Loss: 0.027042461559176445  PSNR: 18.900922775268555
[TRAIN] Iter: 644300 Loss: 0.024669431149959564  PSNR: 18.779552459716797
[TRAIN] Iter: 644400 Loss: 0.025596389546990395  PSNR: 18.9902400970459
[TRAIN] Iter: 644500 Loss: 0.030247926712036133  PSNR: 18.1627197265625
[TRAIN] Iter: 644600 Loss: 0.020720619708299637  PSNR: 20.25861358642578
[TRAIN] Iter: 644700 Loss: 0.027795476838946342  PSNR: 18.589035034179688
[TRAIN] Iter: 644800 Loss: 0.027709852904081345  PSNR: 18.650331497192383
[TRAIN] Iter: 644900 Loss: 0.032290827482938766  PSNR: 18.128189086914062
[TRAIN] Iter: 645000 Loss: 0.029860327020287514  PSNR: 18.481264114379883
[TRAIN] Iter: 645100 Loss: 0.02361886575818062  PSNR: 19.299421310424805
[TRAIN] Iter: 645200 Loss: 0.03141147643327713  PSNR: 18.073007583618164
[TRAIN] Iter: 645300 Loss: 0.02723272517323494  PSNR: 19.0454158782959
[TRAIN] Iter: 645400 Loss: 0.028220385313034058  PSNR: 18.85231590270996
[TRAIN] Iter: 645500 Loss: 0.02620246261358261  PSNR: 19.295520782470703
[TRAIN] Iter: 645600 Loss: 0.025563908740878105  PSNR: 19.260848999023438
[TRAIN] Iter: 645700 Loss: 0.023619230836629868  PSNR: 19.318893432617188
[TRAIN] Iter: 645800 Loss: 0.02163579687476158  PSNR: 19.56192398071289
[TRAIN] Iter: 645900 Loss: 0.031048370525240898  PSNR: 18.233577728271484
[TRAIN] Iter: 646000 Loss: 0.02083141915500164  PSNR: 20.141433715820312
[TRAIN] Iter: 646100 Loss: 0.0267934687435627  PSNR: 18.937604904174805
[TRAIN] Iter: 646200 Loss: 0.025283731520175934  PSNR: 18.797643661499023
[TRAIN] Iter: 646300 Loss: 0.023041270673274994  PSNR: 20.221054077148438
[TRAIN] Iter: 646400 Loss: 0.02356128767132759  PSNR: 19.534868240356445
[TRAIN] Iter: 646500 Loss: 0.02648109570145607  PSNR: 18.912355422973633
[TRAIN] Iter: 646600 Loss: 0.026419438421726227  PSNR: 19.185434341430664
[TRAIN] Iter: 646700 Loss: 0.030097000300884247  PSNR: 18.421064376831055
[TRAIN] Iter: 646800 Loss: 0.0324225053191185  PSNR: 18.088685989379883
[TRAIN] Iter: 646900 Loss: 0.03320256620645523  PSNR: 17.891592025756836
[TRAIN] Iter: 647000 Loss: 0.02827918902039528  PSNR: 18.69600486755371
[TRAIN] Iter: 647100 Loss: 0.023822007700800896  PSNR: 19.316144943237305
[TRAIN] Iter: 647200 Loss: 0.030414894223213196  PSNR: 18.360292434692383
[TRAIN] Iter: 647300 Loss: 0.026404988020658493  PSNR: 19.222013473510742
[TRAIN] Iter: 647400 Loss: 0.024519937112927437  PSNR: 19.36840057373047
[TRAIN] Iter: 647500 Loss: 0.024008944630622864  PSNR: 19.088579177856445
[TRAIN] Iter: 647600 Loss: 0.028374386951327324  PSNR: 18.764923095703125
[TRAIN] Iter: 647700 Loss: 0.026699595153331757  PSNR: 19.12204933166504
[TRAIN] Iter: 647800 Loss: 0.020803647115826607  PSNR: 20.136741638183594
[TRAIN] Iter: 647900 Loss: 0.02732485719025135  PSNR: 18.614402770996094
[TRAIN] Iter: 648000 Loss: 0.02795654721558094  PSNR: 18.776796340942383
[TRAIN] Iter: 648100 Loss: 0.026190219447016716  PSNR: 18.751298904418945
[TRAIN] Iter: 648200 Loss: 0.026531198993325233  PSNR: 18.986331939697266
[TRAIN] Iter: 648300 Loss: 0.02839563600718975  PSNR: 18.676349639892578
[TRAIN] Iter: 648400 Loss: 0.025676365941762924  PSNR: 19.219051361083984
[TRAIN] Iter: 648500 Loss: 0.020793527364730835  PSNR: 20.002269744873047
[TRAIN] Iter: 648600 Loss: 0.02850927971303463  PSNR: 18.533605575561523
[TRAIN] Iter: 648700 Loss: 0.02451794221997261  PSNR: 19.589176177978516
[TRAIN] Iter: 648800 Loss: 0.02828686311841011  PSNR: 18.776697158813477
[TRAIN] Iter: 648900 Loss: 0.028159188106656075  PSNR: 18.513080596923828
[TRAIN] Iter: 649000 Loss: 0.028572505339980125  PSNR: 18.551918029785156
[TRAIN] Iter: 649100 Loss: 0.029143309220671654  PSNR: 18.716516494750977
[TRAIN] Iter: 649200 Loss: 0.0265461765229702  PSNR: 18.965383529663086
[TRAIN] Iter: 649300 Loss: 0.03074766881763935  PSNR: 18.3544864654541
[TRAIN] Iter: 649400 Loss: 0.03284096717834473  PSNR: 17.95040512084961
[TRAIN] Iter: 649500 Loss: 0.02743380144238472  PSNR: 18.858081817626953
[TRAIN] Iter: 649600 Loss: 0.03265498951077461  PSNR: 18.128971099853516
[TRAIN] Iter: 649700 Loss: 0.027790114283561707  PSNR: 18.68525505065918
[TRAIN] Iter: 649800 Loss: 0.0245641078799963  PSNR: 18.957361221313477
[TRAIN] Iter: 649900 Loss: 0.020423240959644318  PSNR: 20.217124938964844
Saved checkpoints at ./logs/TUT-LAB-nerf/650000.tar
0 0.00040841102600097656
torch.Size([320, 640, 3]) torch.Size([320, 640])
1 15.458080053329468
2 17.163892030715942
3 15.096407651901245
4 17.710910081863403
5 15.392773151397705
6 15.223207473754883
7 17.515660047531128
8 15.363811731338501
9 17.38748860359192
10 15.603595733642578
11 17.734621047973633
12 15.746793270111084
13 17.728251218795776
14 15.543232679367065
15 17.462172508239746
16 15.375118017196655
17 17.46707057952881
18 15.357392072677612
19 17.38840889930725
20 15.373593807220459
21 17.411442279815674
22 15.388391971588135
23 17.416789293289185
24 15.400537014007568
25 17.339661359786987
26 15.35128664970398
27 17.415143251419067
28 15.315091848373413
29 15.380152225494385
30 17.4173846244812
31 15.084898233413696
32 18.234487056732178
33 15.338661432266235
34 16.9269917011261
35 15.298360347747803
36 17.496375560760498
37 15.359797239303589
38 17.382035493850708
39 15.287506580352783
40 17.45348620414734
41 15.290827751159668
42 17.47939157485962
43 15.281349182128906
44 17.497056484222412
45 15.288702487945557
46 17.482404708862305
47 15.28126835823059
48 17.557578802108765
49 15.263695001602173
50 15.393829822540283
51 17.320844173431396
52 15.443219423294067
53 17.399739027023315
54 15.364662885665894
55 17.507017135620117
56 15.327704429626465
57 17.54121470451355
58 15.17421293258667
59 17.746334552764893
60 14.809055089950562
61 18.102877140045166
62 14.798423051834106
63 18.076372861862183
64 14.901276588439941
65 15.46806025505066
66 15.221902847290039
67 15.102226257324219
68 17.403778553009033
69 14.669589757919312
70 15.919891834259033
71 13.410204887390137
72 15.901638984680176
73 13.268812656402588
74 15.89673900604248
75 13.442513227462769
76 13.397661685943604
77 16.10688352584839
78 13.18863296508789
79 16.03938126564026
80 13.423102855682373
81 13.584959030151367
82 15.701472520828247
83 13.526331901550293
84 15.677349090576172
85 13.48466444015503
86 15.779284954071045
87 13.72470760345459
88 13.55435061454773
89 15.726797103881836
90 13.530691623687744
91 15.69984769821167
92 13.524384498596191
93 13.513745784759521
94 15.781338453292847
95 13.552634000778198
96 15.719098567962646
97 13.539852619171143
98 15.719277143478394
99 13.520221948623657
100 13.5127112865448
101 15.758267641067505
102 13.482666730880737
103 15.748591661453247
104 13.505428791046143
105 15.783381462097168
106 13.660822629928589
107 13.618308544158936
108 15.45201563835144
109 13.556196451187134
110 15.642149686813354
111 13.40787649154663
112 13.567623853683472
113 15.710592031478882
114 13.49381947517395
115 15.85503077507019
116 13.450704097747803
117 15.753958225250244
118 13.492749691009521
119 13.558392524719238
Done, saving (120, 320, 640, 3) (120, 320, 640)
extras:{'raw': tensor([[[ 5.9124e-01,  4.7849e-01,  5.9873e-01,  1.3596e+01],
         [ 2.7988e-01,  3.0977e-01,  6.3128e-01, -8.4388e-01],
         [ 1.1171e+00,  7.8969e-01,  6.8863e-01, -2.1457e+01],
         ...,
         [-1.9469e+00, -7.1680e-01,  2.7035e+00, -1.6185e+01],
         [-3.5927e+00, -1.9565e+00,  2.2833e+00, -3.3582e+01],
         [-3.7129e+00, -2.2212e+00,  1.6092e+00, -3.2441e+01]],

        [[-2.4241e+00, -1.9109e+00, -6.2848e-01, -1.4796e+01],
         [-4.7621e-01, -5.7418e-01, -9.5153e-02, -1.7231e+01],
         [ 1.8587e-01, -3.4067e-01, -8.1004e-01, -2.3937e+01],
         ...,
         [-3.9691e+00, -2.9268e+00, -1.6150e+00,  8.9722e+00],
         [-4.5017e+00, -3.0728e+00, -7.6572e-01,  1.8573e+00],
         [-4.1404e+00, -2.8395e+00, -8.2844e-01,  3.0415e+00]],

        [[ 1.8750e-01,  3.7685e-04, -1.4040e-01,  1.7146e+01],
         [ 2.9037e-01,  1.2042e-01, -7.5755e-02,  1.6304e+00],
         [ 3.0891e-01,  1.3898e-01, -5.4476e-02,  1.2661e+00],
         ...,
         [ 2.2515e+01,  1.9356e+01,  1.7877e+01,  2.8329e+02],
         [ 2.7061e+01,  2.3468e+01,  2.1892e+01,  3.1223e+02],
         [ 2.6704e+01,  2.3202e+01,  2.1746e+01,  3.1627e+02]],

        ...,

        [[ 9.2286e-01,  6.7417e-01,  5.6621e-01, -1.0125e+01],
         [ 2.0286e-01,  4.0856e-02,  3.4664e-03, -1.3434e+01],
         [ 1.9275e-01,  1.8005e-02, -1.1114e-01,  6.2383e+00],
         ...,
         [ 3.2505e+01,  2.6403e+01,  2.4941e+01,  2.5148e+02],
         [ 2.9377e+01,  2.3529e+01,  2.1905e+01,  2.5659e+02],
         [ 3.0466e+01,  2.4497e+01,  2.2899e+01,  2.4226e+02]],

        [[-2.9869e-01, -4.3788e-01, -6.0412e-01,  2.0401e+01],
         [-1.5641e-01, -3.7647e-01, -6.1188e-01,  6.7883e+00],
         [-1.5664e-01, -3.7662e-01, -6.1001e-01,  6.7389e+00],
         ...,
         [-2.9263e+00, -3.4609e+00, -8.7186e+00,  7.6443e+02],
         [-4.0033e+00, -4.7157e+00, -1.0604e+01,  7.8683e+02],
         [-4.3224e+00, -5.2115e+00, -1.1551e+01,  7.8742e+02]],

        [[-2.4813e-01, -3.7811e-01, -5.2442e-01,  8.1848e+00],
         [-4.7616e-02, -1.6464e-01, -2.7293e-01,  1.4270e+01],
         [-4.6401e-02, -1.6377e-01, -2.7223e-01,  1.4213e+01],
         ...,
         [-2.5560e+00, -2.8479e+00, -6.7210e+00,  5.5715e+02],
         [-1.2724e+00, -1.9772e+00, -6.5696e+00,  5.6148e+02],
         [-2.1402e+00, -2.8803e+00, -7.8318e+00,  5.4840e+02]]],
       grad_fn=<ReshapeAliasBackward0>), 'rgb0': tensor([[0.4924, 0.4780, 0.4937],
        [0.2116, 0.3435, 0.6988],
        [0.5569, 0.5128, 0.4702],
        ...,
        [0.5393, 0.4942, 0.4673],
        [0.4099, 0.3791, 0.3511],
        [0.4777, 0.4512, 0.4397]], grad_fn=<ReshapeAliasBackward0>), 'disp0': tensor([ 21.8209,  18.3111,  80.9476,  ...,  54.9827,  85.1382, 144.5524],
       grad_fn=<ReshapeAliasBackward0>), 'acc0': tensor([1., 1., 1.,  ..., 1., 1., 1.], grad_fn=<ReshapeAliasBackward0>), 'z_std': tensor([0.0031, 0.0468, 0.1054,  ..., 0.0026, 0.0022, 0.0017])}
0 0.0005710124969482422
torch.Size([320, 640, 3]) torch.Size([320, 640])
1 13.529325723648071
2 15.75748872756958
3 13.468853235244751
4 15.729493618011475
5 13.507314205169678
6 13.581318140029907
7 15.712784767150879
8 13.503252744674683
9 15.710111856460571
10 13.487577199935913
11 15.784065961837769
12 13.526653289794922
13 13.591358184814453
14 15.761377811431885
15 13.38483452796936
16 15.785140991210938
17 13.329807758331299
18 13.636719226837158
19 15.740321397781372
20 13.487192392349243
21 15.877440214157104
22 13.288162469863892
23 15.806939840316772
24 13.466981172561646
25 13.353878021240234
26 16.190709829330444
27 13.02182388305664
28 16.14394760131836
29 13.396549701690674
30 13.407189846038818
31 15.8037691116333
32 13.288405179977417
33 16.050991535186768
34 13.322204828262329
35 15.685456991195679
36 13.290661096572876
37 13.572702169418335
38 16.058265447616577
39 13.420098066329956
40 15.650992155075073
41 13.280425786972046
42 15.874415159225464
43 13.379148244857788
44 13.615589380264282
45 15.837252616882324
46 13.512707948684692
47 15.60347032546997
48 13.524379968643188
49 13.61089563369751
50 15.68148398399353
51 13.5342698097229
52 15.688005447387695
53 13.566205024719238
54 15.659240007400513
55 13.547277212142944
56 13.539188623428345
57 15.708956956863403
58 13.517716646194458
59 15.674561738967896
60 13.551055431365967
61 15.672437906265259
62 13.569282054901123
63 13.54308032989502
64 15.71579909324646
65 13.505743980407715
66 15.727381944656372
67 13.496475458145142
68 13.56346607208252
69 15.760517358779907
70 13.216155767440796
71 16.057487964630127
72 13.340325593948364
73 15.879889488220215
74 13.86830735206604
75 13.774341821670532
76 15.285236835479736
77 13.145259141921997
78 16.153597593307495
79 13.655261993408203
80 15.394335746765137
81 13.55947756767273
82 13.209316968917847
83 16.054083108901978
84 13.491349220275879
85 15.753260374069214
86 13.474216222763062
87 13.623817443847656
88 15.7138192653656
89 13.532016515731812
90 15.698779344558716
91 13.470016956329346
92 15.758885145187378
93 13.603160381317139
94 13.644452571868896
95 15.879462957382202
96 13.395384550094604
97 15.836073875427246
98 13.363629817962646
99 13.68100905418396
100 15.573318243026733
101 13.572696208953857
102 15.84439992904663
103 13.328548908233643
104 15.8381507396698
105 13.331257104873657
106 13.692214488983154
107 15.800339937210083
108 13.12738037109375
109 16.36390209197998
110 13.135057210922241
111 15.850309133529663
112 13.78867220878601
113 13.251217603683472
114 15.95961594581604
115 13.015299081802368
116 16.326125621795654
117 13.141314506530762
118 13.429105997085571
119 16.003637313842773
test poses shape torch.Size([13, 3, 4])
0 0.0006806850433349609
torch.Size([320, 640, 3]) torch.Size([320, 640])
1 16.140264749526978
2 13.008814573287964
3 16.179924726486206
4 13.247841119766235
5 13.498127222061157
6 16.10777473449707
7 13.162755250930786
8 16.067229747772217
9 13.524128198623657
10 13.529012680053711
11 15.774772882461548
12 13.530505418777466
Saved test set
[TRAIN] Iter: 650000 Loss: 0.0265660397708416  PSNR: 18.56186866760254
[TRAIN] Iter: 650100 Loss: 0.024023236706852913  PSNR: 19.463607788085938
[TRAIN] Iter: 650200 Loss: 0.027256637811660767  PSNR: 18.646503448486328
[TRAIN] Iter: 650300 Loss: 0.022305386140942574  PSNR: 20.187244415283203
[TRAIN] Iter: 650400 Loss: 0.02793790027499199  PSNR: 18.689393997192383
[TRAIN] Iter: 650500 Loss: 0.024831529706716537  PSNR: 19.241498947143555
[TRAIN] Iter: 650600 Loss: 0.03179633617401123  PSNR: 18.088733673095703
[TRAIN] Iter: 650700 Loss: 0.021931637078523636  PSNR: 19.853527069091797
[TRAIN] Iter: 650800 Loss: 0.02641366794705391  PSNR: 18.971084594726562
[TRAIN] Iter: 650900 Loss: 0.02628069743514061  PSNR: 19.10487174987793
[TRAIN] Iter: 651000 Loss: 0.029431618750095367  PSNR: 18.493196487426758
[TRAIN] Iter: 651100 Loss: 0.027465997263789177  PSNR: 18.74457550048828
[TRAIN] Iter: 651200 Loss: 0.02648492157459259  PSNR: 18.8913631439209
[TRAIN] Iter: 651300 Loss: 0.03198510408401489  PSNR: 18.176055908203125
[TRAIN] Iter: 651400 Loss: 0.030363067984580994  PSNR: 18.330629348754883
[TRAIN] Iter: 651500 Loss: 0.02556333690881729  PSNR: 19.064945220947266
[TRAIN] Iter: 651600 Loss: 0.020122302696108818  PSNR: 19.95377540588379
[TRAIN] Iter: 651700 Loss: 0.02798675373196602  PSNR: 18.664169311523438
[TRAIN] Iter: 651800 Loss: 0.027096817269921303  PSNR: 18.733488082885742
[TRAIN] Iter: 651900 Loss: 0.026830341666936874  PSNR: 18.766786575317383
[TRAIN] Iter: 652000 Loss: 0.025487057864665985  PSNR: 19.171579360961914
[TRAIN] Iter: 652100 Loss: 0.027169961482286453  PSNR: 18.901687622070312
[TRAIN] Iter: 652200 Loss: 0.02851114049553871  PSNR: 18.62859535217285
[TRAIN] Iter: 652300 Loss: 0.027806570753455162  PSNR: 18.889394760131836
[TRAIN] Iter: 652400 Loss: 0.022737368941307068  PSNR: 19.705297470092773
[TRAIN] Iter: 652500 Loss: 0.02243541181087494  PSNR: 19.667224884033203
[TRAIN] Iter: 652600 Loss: 0.02912845090031624  PSNR: 18.565397262573242
[TRAIN] Iter: 652700 Loss: 0.028805796056985855  PSNR: 18.580995559692383
[TRAIN] Iter: 652800 Loss: 0.029024755582213402  PSNR: 18.64164161682129
[TRAIN] Iter: 652900 Loss: 0.019882604479789734  PSNR: 20.270549774169922
[TRAIN] Iter: 653000 Loss: 0.022744696587324142  PSNR: 19.645137786865234
[TRAIN] Iter: 653100 Loss: 0.03009398654103279  PSNR: 18.47092628479004
[TRAIN] Iter: 653200 Loss: 0.024679172784090042  PSNR: 19.362022399902344
[TRAIN] Iter: 653300 Loss: 0.02391952835023403  PSNR: 19.377864837646484
[TRAIN] Iter: 653400 Loss: 0.024739375337958336  PSNR: 19.378095626831055
[TRAIN] Iter: 653500 Loss: 0.027073798701167107  PSNR: 18.815319061279297
[TRAIN] Iter: 653600 Loss: 0.023066770285367966  PSNR: 19.7297306060791
[TRAIN] Iter: 653700 Loss: 0.02888612076640129  PSNR: 18.63576316833496
[TRAIN] Iter: 653800 Loss: 0.024286340922117233  PSNR: 19.319976806640625
[TRAIN] Iter: 653900 Loss: 0.029004234820604324  PSNR: 18.56592559814453
[TRAIN] Iter: 654000 Loss: 0.03282454237341881  PSNR: 18.030485153198242
[TRAIN] Iter: 654100 Loss: 0.023573998361825943  PSNR: 19.236412048339844
[TRAIN] Iter: 654200 Loss: 0.028333526104688644  PSNR: 18.60017967224121
[TRAIN] Iter: 654300 Loss: 0.019748399034142494  PSNR: 20.336149215698242
[TRAIN] Iter: 654400 Loss: 0.03249230235815048  PSNR: 17.939544677734375
[TRAIN] Iter: 654500 Loss: 0.022109147161245346  PSNR: 19.93590545654297
[TRAIN] Iter: 654600 Loss: 0.02769152820110321  PSNR: 18.604557037353516
[TRAIN] Iter: 654700 Loss: 0.02634263038635254  PSNR: 18.574535369873047
[TRAIN] Iter: 654800 Loss: 0.02293773740530014  PSNR: 19.286775588989258
[TRAIN] Iter: 654900 Loss: 0.024132754653692245  PSNR: 19.35728645324707
[TRAIN] Iter: 655000 Loss: 0.03394825756549835  PSNR: 17.90021514892578
[TRAIN] Iter: 655100 Loss: 0.029488716274499893  PSNR: 18.402956008911133
[TRAIN] Iter: 655200 Loss: 0.027818337082862854  PSNR: 18.81830596923828
[TRAIN] Iter: 655300 Loss: 0.023378871381282806  PSNR: 19.62090301513672
[TRAIN] Iter: 655400 Loss: 0.0272606760263443  PSNR: 18.825227737426758
[TRAIN] Iter: 655500 Loss: 0.03084203228354454  PSNR: 18.464927673339844
[TRAIN] Iter: 655600 Loss: 0.018462300300598145  PSNR: 20.50652503967285
[TRAIN] Iter: 655700 Loss: 0.022644314914941788  PSNR: 19.660696029663086
[TRAIN] Iter: 655800 Loss: 0.02967558614909649  PSNR: 18.507816314697266
[TRAIN] Iter: 655900 Loss: 0.022577321156859398  PSNR: 19.181011199951172
[TRAIN] Iter: 656000 Loss: 0.03258928284049034  PSNR: 18.10321617126465
[TRAIN] Iter: 656100 Loss: 0.018364468589425087  PSNR: 20.457998275756836
[TRAIN] Iter: 656200 Loss: 0.026269719004631042  PSNR: 18.792818069458008
[TRAIN] Iter: 656300 Loss: 0.0260153915733099  PSNR: 18.704586029052734
[TRAIN] Iter: 656400 Loss: 0.024741290137171745  PSNR: 19.73441505432129
[TRAIN] Iter: 656500 Loss: 0.025904033333063126  PSNR: 19.248207092285156
[TRAIN] Iter: 656600 Loss: 0.022758368402719498  PSNR: 19.658525466918945
[TRAIN] Iter: 656700 Loss: 0.028526606038212776  PSNR: 18.9910831451416
[TRAIN] Iter: 656800 Loss: 0.029304631054401398  PSNR: 18.68783950805664
[TRAIN] Iter: 656900 Loss: 0.027900079265236855  PSNR: 18.6597843170166
[TRAIN] Iter: 657000 Loss: 0.026376353576779366  PSNR: 18.799440383911133
[TRAIN] Iter: 657100 Loss: 0.028712620958685875  PSNR: 18.685453414916992
[TRAIN] Iter: 657200 Loss: 0.02619507536292076  PSNR: 18.529237747192383
[TRAIN] Iter: 657300 Loss: 0.028148237615823746  PSNR: 18.519081115722656
[TRAIN] Iter: 657400 Loss: 0.030096139758825302  PSNR: 18.485872268676758
[TRAIN] Iter: 657500 Loss: 0.03565151244401932  PSNR: 17.665132522583008
[TRAIN] Iter: 657600 Loss: 0.025658588856458664  PSNR: 19.3048038482666
[TRAIN] Iter: 657700 Loss: 0.03351235389709473  PSNR: 17.958465576171875
[TRAIN] Iter: 657800 Loss: 0.023673946037888527  PSNR: 19.515748977661133
[TRAIN] Iter: 657900 Loss: 0.02611568011343479  PSNR: 19.529762268066406
[TRAIN] Iter: 658000 Loss: 0.024607548490166664  PSNR: 18.66591453552246
[TRAIN] Iter: 658100 Loss: 0.024829108268022537  PSNR: 19.180782318115234
[TRAIN] Iter: 658200 Loss: 0.025843854993581772  PSNR: 19.09606170654297
[TRAIN] Iter: 658300 Loss: 0.028210008516907692  PSNR: 18.683837890625
[TRAIN] Iter: 658400 Loss: 0.024175677448511124  PSNR: 19.301912307739258
[TRAIN] Iter: 658500 Loss: 0.028171338140964508  PSNR: 18.720386505126953
[TRAIN] Iter: 658600 Loss: 0.02330409735441208  PSNR: 19.248291015625
[TRAIN] Iter: 658700 Loss: 0.02418701723217964  PSNR: 19.910261154174805
[TRAIN] Iter: 658800 Loss: 0.02451184205710888  PSNR: 19.3714599609375
[TRAIN] Iter: 658900 Loss: 0.02914091758430004  PSNR: 18.502975463867188
[TRAIN] Iter: 659000 Loss: 0.027813855558633804  PSNR: 18.691925048828125
[TRAIN] Iter: 659100 Loss: 0.027101702988147736  PSNR: 18.765661239624023
[TRAIN] Iter: 659200 Loss: 0.025240762159228325  PSNR: 19.31585121154785
[TRAIN] Iter: 659300 Loss: 0.02064111828804016  PSNR: 20.166536331176758
[TRAIN] Iter: 659400 Loss: 0.033071763813495636  PSNR: 17.985811233520508
[TRAIN] Iter: 659500 Loss: 0.023983409628272057  PSNR: 19.275123596191406
[TRAIN] Iter: 659600 Loss: 0.023619823157787323  PSNR: 19.19610595703125
[TRAIN] Iter: 659700 Loss: 0.02590095065534115  PSNR: 19.007600784301758
[TRAIN] Iter: 659800 Loss: 0.01995769329369068  PSNR: 20.149837493896484
[TRAIN] Iter: 659900 Loss: 0.02785010077059269  PSNR: 18.727333068847656
Saved checkpoints at ./logs/TUT-LAB-nerf/660000.tar
[TRAIN] Iter: 660000 Loss: 0.02381434105336666  PSNR: 19.306344985961914
[TRAIN] Iter: 660100 Loss: 0.025889653712511063  PSNR: 18.791868209838867
[TRAIN] Iter: 660200 Loss: 0.022823750972747803  PSNR: 19.579143524169922
[TRAIN] Iter: 660300 Loss: 0.03015056997537613  PSNR: 18.404691696166992
[TRAIN] Iter: 660400 Loss: 0.027788417413830757  PSNR: 18.860614776611328
[TRAIN] Iter: 660500 Loss: 0.026377422735095024  PSNR: 19.563636779785156
[TRAIN] Iter: 660600 Loss: 0.024590972810983658  PSNR: 19.288236618041992
[TRAIN] Iter: 660700 Loss: 0.028151052072644234  PSNR: 18.91880989074707
[TRAIN] Iter: 660800 Loss: 0.019119225442409515  PSNR: 20.30853271484375
[TRAIN] Iter: 660900 Loss: 0.02899010106921196  PSNR: 18.727994918823242
[TRAIN] Iter: 661000 Loss: 0.022849781438708305  PSNR: 19.406641006469727
[TRAIN] Iter: 661100 Loss: 0.026890601962804794  PSNR: 19.297834396362305
[TRAIN] Iter: 661200 Loss: 0.025069281458854675  PSNR: 19.01239585876465
[TRAIN] Iter: 661300 Loss: 0.023160085082054138  PSNR: 19.2387638092041
[TRAIN] Iter: 661400 Loss: 0.02853088080883026  PSNR: 18.748170852661133
[TRAIN] Iter: 661500 Loss: 0.021993689239025116  PSNR: 19.821321487426758
[TRAIN] Iter: 661600 Loss: 0.0298154279589653  PSNR: 18.42540168762207
[TRAIN] Iter: 661700 Loss: 0.024500880390405655  PSNR: 19.529150009155273
[TRAIN] Iter: 661800 Loss: 0.03120807558298111  PSNR: 18.156179428100586
[TRAIN] Iter: 661900 Loss: 0.022185876965522766  PSNR: 19.743139266967773
[TRAIN] Iter: 662000 Loss: 0.026410963386297226  PSNR: 18.515666961669922
[TRAIN] Iter: 662100 Loss: 0.03364219143986702  PSNR: 17.969470977783203
[TRAIN] Iter: 662200 Loss: 0.02450699917972088  PSNR: 19.06783676147461
[TRAIN] Iter: 662300 Loss: 0.021009845659136772  PSNR: 19.93583869934082
[TRAIN] Iter: 662400 Loss: 0.02437518909573555  PSNR: 19.028438568115234
[TRAIN] Iter: 662500 Loss: 0.023206066340208054  PSNR: 19.275829315185547
[TRAIN] Iter: 662600 Loss: 0.03233198821544647  PSNR: 18.129661560058594
[TRAIN] Iter: 662700 Loss: 0.02857600897550583  PSNR: 18.90376091003418
[TRAIN] Iter: 662800 Loss: 0.022440453991293907  PSNR: 19.64866065979004
[TRAIN] Iter: 662900 Loss: 0.027441155165433884  PSNR: 18.794660568237305
[TRAIN] Iter: 663000 Loss: 0.030621279031038284  PSNR: 18.398239135742188
[TRAIN] Iter: 663100 Loss: 0.03165269270539284  PSNR: 18.097036361694336
[TRAIN] Iter: 663200 Loss: 0.029644301161170006  PSNR: 18.416757583618164
[TRAIN] Iter: 663300 Loss: 0.02969212271273136  PSNR: 18.402496337890625
[TRAIN] Iter: 663400 Loss: 0.027882298454642296  PSNR: 18.814361572265625
[TRAIN] Iter: 663500 Loss: 0.030674314126372337  PSNR: 18.306001663208008
[TRAIN] Iter: 663600 Loss: 0.03332575783133507  PSNR: 17.927221298217773
[TRAIN] Iter: 663700 Loss: 0.030822038650512695  PSNR: 18.18065643310547
[TRAIN] Iter: 663800 Loss: 0.027845706790685654  PSNR: 18.666858673095703
[TRAIN] Iter: 663900 Loss: 0.02811303175985813  PSNR: 18.817127227783203
[TRAIN] Iter: 664000 Loss: 0.026401475071907043  PSNR: 18.993240356445312
[TRAIN] Iter: 664100 Loss: 0.020415663719177246  PSNR: 20.158000946044922
[TRAIN] Iter: 664200 Loss: 0.028520412743091583  PSNR: 18.62762451171875
[TRAIN] Iter: 664300 Loss: 0.02840230241417885  PSNR: 18.766189575195312
[TRAIN] Iter: 664400 Loss: 0.03204930201172829  PSNR: 18.047386169433594
[TRAIN] Iter: 664500 Loss: 0.023982178419828415  PSNR: 19.50802993774414
[TRAIN] Iter: 664600 Loss: 0.024593621492385864  PSNR: 19.24308204650879
[TRAIN] Iter: 664700 Loss: 0.027394652366638184  PSNR: 19.04231834411621
[TRAIN] Iter: 664800 Loss: 0.024850673973560333  PSNR: 19.27399444580078
[TRAIN] Iter: 664900 Loss: 0.02518710307776928  PSNR: 19.16141700744629
[TRAIN] Iter: 665000 Loss: 0.024214670062065125  PSNR: 19.629898071289062
[TRAIN] Iter: 665100 Loss: 0.028207750990986824  PSNR: 18.761417388916016
[TRAIN] Iter: 665200 Loss: 0.019855961203575134  PSNR: 19.89035987854004
[TRAIN] Iter: 665300 Loss: 0.019442997872829437  PSNR: 20.25676155090332
[TRAIN] Iter: 665400 Loss: 0.03048163279891014  PSNR: 18.24872398376465
[TRAIN] Iter: 665500 Loss: 0.027788635343313217  PSNR: 18.82600212097168
[TRAIN] Iter: 665600 Loss: 0.026205234229564667  PSNR: 19.05755615234375
[TRAIN] Iter: 665700 Loss: 0.024826399981975555  PSNR: 19.358558654785156
[TRAIN] Iter: 665800 Loss: 0.023935414850711823  PSNR: 19.452465057373047
[TRAIN] Iter: 665900 Loss: 0.026302635669708252  PSNR: 18.480777740478516
[TRAIN] Iter: 666000 Loss: 0.024424854665994644  PSNR: 19.251171112060547
[TRAIN] Iter: 666100 Loss: 0.0294044092297554  PSNR: 18.42759132385254
[TRAIN] Iter: 666200 Loss: 0.026475924998521805  PSNR: 18.89947509765625
[TRAIN] Iter: 666300 Loss: 0.019464030861854553  PSNR: 20.39158058166504
[TRAIN] Iter: 666400 Loss: 0.0334501713514328  PSNR: 17.939743041992188
[TRAIN] Iter: 666500 Loss: 0.024999190121889114  PSNR: 19.229610443115234
[TRAIN] Iter: 666600 Loss: 0.021941490471363068  PSNR: 19.8769474029541
[TRAIN] Iter: 666700 Loss: 0.022493690252304077  PSNR: 19.429561614990234
[TRAIN] Iter: 666800 Loss: 0.025320231914520264  PSNR: 19.177169799804688
[TRAIN] Iter: 666900 Loss: 0.027606703341007233  PSNR: 18.6942081451416
[TRAIN] Iter: 667000 Loss: 0.027061941102147102  PSNR: 18.454830169677734
[TRAIN] Iter: 667100 Loss: 0.02968977764248848  PSNR: 18.53923988342285
[TRAIN] Iter: 667200 Loss: 0.026914943009614944  PSNR: 18.913984298706055
[TRAIN] Iter: 667300 Loss: 0.028535103425383568  PSNR: 18.513669967651367
[TRAIN] Iter: 667400 Loss: 0.021984416991472244  PSNR: 19.829666137695312
[TRAIN] Iter: 667500 Loss: 0.0262397900223732  PSNR: 19.010414123535156
[TRAIN] Iter: 667600 Loss: 0.026550564914941788  PSNR: 19.009231567382812
[TRAIN] Iter: 667700 Loss: 0.030418287962675095  PSNR: 18.456186294555664
[TRAIN] Iter: 667800 Loss: 0.0244387686252594  PSNR: 19.17348861694336
[TRAIN] Iter: 667900 Loss: 0.027712058275938034  PSNR: 18.75858497619629
[TRAIN] Iter: 668000 Loss: 0.028971001505851746  PSNR: 18.5766544342041
[TRAIN] Iter: 668100 Loss: 0.0275249183177948  PSNR: 18.862796783447266
[TRAIN] Iter: 668200 Loss: 0.025886911898851395  PSNR: 19.24772071838379
[TRAIN] Iter: 668300 Loss: 0.02948779985308647  PSNR: 18.573808670043945
[TRAIN] Iter: 668400 Loss: 0.0207115076482296  PSNR: 20.148221969604492
[TRAIN] Iter: 668500 Loss: 0.03177555650472641  PSNR: 18.140424728393555
[TRAIN] Iter: 668600 Loss: 0.02132490463554859  PSNR: 19.930938720703125
[TRAIN] Iter: 668700 Loss: 0.02875099703669548  PSNR: 18.570077896118164
[TRAIN] Iter: 668800 Loss: 0.02410430833697319  PSNR: 19.428373336791992
[TRAIN] Iter: 668900 Loss: 0.029129449278116226  PSNR: 18.554317474365234
[TRAIN] Iter: 669000 Loss: 0.023736480623483658  PSNR: 19.708499908447266
[TRAIN] Iter: 669100 Loss: 0.026750735938549042  PSNR: 18.681974411010742
[TRAIN] Iter: 669200 Loss: 0.03239569067955017  PSNR: 18.089853286743164
[TRAIN] Iter: 669300 Loss: 0.030441150069236755  PSNR: 18.333152770996094
[TRAIN] Iter: 669400 Loss: 0.02204418182373047  PSNR: 19.835153579711914
[TRAIN] Iter: 669500 Loss: 0.02601075917482376  PSNR: 19.020233154296875
[TRAIN] Iter: 669600 Loss: 0.021187562495470047  PSNR: 19.89634132385254
[TRAIN] Iter: 669700 Loss: 0.028794538229703903  PSNR: 18.468547821044922
[TRAIN] Iter: 669800 Loss: 0.026854680851101875  PSNR: 19.02182960510254
[TRAIN] Iter: 669900 Loss: 0.026240572333335876  PSNR: 18.952091217041016
Saved checkpoints at ./logs/TUT-LAB-nerf/670000.tar
[TRAIN] Iter: 670000 Loss: 0.035682618618011475  PSNR: 17.61597442626953
[TRAIN] Iter: 670100 Loss: 0.025841977447271347  PSNR: 18.848346710205078
[TRAIN] Iter: 670200 Loss: 0.026613760739564896  PSNR: 18.877199172973633
[TRAIN] Iter: 670300 Loss: 0.025445356965065002  PSNR: 19.117778778076172
[TRAIN] Iter: 670400 Loss: 0.03167148679494858  PSNR: 18.260828018188477
[TRAIN] Iter: 670500 Loss: 0.027692710980772972  PSNR: 18.764326095581055
[TRAIN] Iter: 670600 Loss: 0.023748967796564102  PSNR: 19.457948684692383
[TRAIN] Iter: 670700 Loss: 0.02542201429605484  PSNR: 19.178495407104492
[TRAIN] Iter: 670800 Loss: 0.02415016107261181  PSNR: 19.14295196533203
[TRAIN] Iter: 670900 Loss: 0.02730400301516056  PSNR: 18.814388275146484
[TRAIN] Iter: 671000 Loss: 0.02392568252980709  PSNR: 19.444107055664062
[TRAIN] Iter: 671100 Loss: 0.02706211432814598  PSNR: 18.95066261291504
[TRAIN] Iter: 671200 Loss: 0.0233268104493618  PSNR: 19.289649963378906
[TRAIN] Iter: 671300 Loss: 0.03254089504480362  PSNR: 17.948116302490234
[TRAIN] Iter: 671400 Loss: 0.030448229983448982  PSNR: 18.249616622924805
[TRAIN] Iter: 671500 Loss: 0.027702700346708298  PSNR: 19.22651481628418
[TRAIN] Iter: 671600 Loss: 0.029678426682949066  PSNR: 18.282846450805664
[TRAIN] Iter: 671700 Loss: 0.028125321492552757  PSNR: 18.862871170043945
[TRAIN] Iter: 671800 Loss: 0.02342892438173294  PSNR: 19.35276222229004
[TRAIN] Iter: 671900 Loss: 0.027733109891414642  PSNR: 18.85963249206543
[TRAIN] Iter: 672000 Loss: 0.02134733833372593  PSNR: 19.9116153717041
[TRAIN] Iter: 672100 Loss: 0.027327274903655052  PSNR: 18.782968521118164
[TRAIN] Iter: 672200 Loss: 0.02851945534348488  PSNR: 18.631168365478516
[TRAIN] Iter: 672300 Loss: 0.03463464975357056  PSNR: 17.821157455444336
[TRAIN] Iter: 672400 Loss: 0.020391007885336876  PSNR: 20.1102237701416
[TRAIN] Iter: 672500 Loss: 0.031665533781051636  PSNR: 18.14000129699707
[TRAIN] Iter: 672600 Loss: 0.028493449091911316  PSNR: 18.693166732788086
[TRAIN] Iter: 672700 Loss: 0.029911784455180168  PSNR: 18.499113082885742
[TRAIN] Iter: 672800 Loss: 0.02418082021176815  PSNR: 19.667713165283203
[TRAIN] Iter: 672900 Loss: 0.02847824990749359  PSNR: 18.557632446289062
[TRAIN] Iter: 673000 Loss: 0.02160509303212166  PSNR: 19.699243545532227
[TRAIN] Iter: 673100 Loss: 0.022506000474095345  PSNR: 19.687110900878906
[TRAIN] Iter: 673200 Loss: 0.02987438067793846  PSNR: 18.47907829284668
[TRAIN] Iter: 673300 Loss: 0.023054396733641624  PSNR: 19.579679489135742
[TRAIN] Iter: 673400 Loss: 0.030684009194374084  PSNR: 18.371519088745117
[TRAIN] Iter: 673500 Loss: 0.021890023723244667  PSNR: 19.427579879760742
[TRAIN] Iter: 673600 Loss: 0.025402992963790894  PSNR: 18.97052001953125
[TRAIN] Iter: 673700 Loss: 0.024491915479302406  PSNR: 19.287452697753906
[TRAIN] Iter: 673800 Loss: 0.028252573683857918  PSNR: 18.81407928466797
[TRAIN] Iter: 673900 Loss: 0.02950403094291687  PSNR: 18.405155181884766
[TRAIN] Iter: 674000 Loss: 0.025634191930294037  PSNR: 18.80910873413086
[TRAIN] Iter: 674100 Loss: 0.02908417396247387  PSNR: 18.507810592651367
[TRAIN] Iter: 674200 Loss: 0.022955307736992836  PSNR: 19.723173141479492
[TRAIN] Iter: 674300 Loss: 0.026628810912370682  PSNR: 18.935592651367188
[TRAIN] Iter: 674400 Loss: 0.027078991755843163  PSNR: 18.725521087646484
[TRAIN] Iter: 674500 Loss: 0.02581319585442543  PSNR: 19.193340301513672
[TRAIN] Iter: 674600 Loss: 0.021994253620505333  PSNR: 19.800771713256836
[TRAIN] Iter: 674700 Loss: 0.0269547738134861  PSNR: 18.694456100463867
[TRAIN] Iter: 674800 Loss: 0.029469303786754608  PSNR: 18.493833541870117
[TRAIN] Iter: 674900 Loss: 0.024503756314516068  PSNR: 19.24378204345703
[TRAIN] Iter: 675000 Loss: 0.029872998595237732  PSNR: 18.377702713012695
[TRAIN] Iter: 675100 Loss: 0.02277366816997528  PSNR: 19.490171432495117
[TRAIN] Iter: 675200 Loss: 0.02073778212070465  PSNR: 19.90382194519043
[TRAIN] Iter: 675300 Loss: 0.02097288891673088  PSNR: 19.973594665527344
[TRAIN] Iter: 675400 Loss: 0.028666384518146515  PSNR: 18.593835830688477
[TRAIN] Iter: 675500 Loss: 0.03168891370296478  PSNR: 18.216510772705078
[TRAIN] Iter: 675600 Loss: 0.03090314008295536  PSNR: 18.251758575439453
[TRAIN] Iter: 675700 Loss: 0.026153575628995895  PSNR: 19.082767486572266
[TRAIN] Iter: 675800 Loss: 0.0251314677298069  PSNR: 19.21599578857422
[TRAIN] Iter: 675900 Loss: 0.032018765807151794  PSNR: 18.21952247619629
[TRAIN] Iter: 676000 Loss: 0.026728834956884384  PSNR: 18.71544075012207
[TRAIN] Iter: 676100 Loss: 0.02667773887515068  PSNR: 19.262357711791992
[TRAIN] Iter: 676200 Loss: 0.02509508654475212  PSNR: 18.57265853881836
[TRAIN] Iter: 676300 Loss: 0.024102218449115753  PSNR: 19.343826293945312
[TRAIN] Iter: 676400 Loss: 0.028483841568231583  PSNR: 18.541404724121094
[TRAIN] Iter: 676500 Loss: 0.03196335583925247  PSNR: 18.037391662597656
[TRAIN] Iter: 676600 Loss: 0.02254154160618782  PSNR: 19.47611427307129
[TRAIN] Iter: 676700 Loss: 0.025199295952916145  PSNR: 19.325511932373047
[TRAIN] Iter: 676800 Loss: 0.022286100313067436  PSNR: 19.603395462036133
[TRAIN] Iter: 676900 Loss: 0.027536731213331223  PSNR: 18.60233497619629
[TRAIN] Iter: 677000 Loss: 0.030301328748464584  PSNR: 18.285682678222656
[TRAIN] Iter: 677100 Loss: 0.023472655564546585  PSNR: 19.410785675048828
[TRAIN] Iter: 677200 Loss: 0.030910691246390343  PSNR: 18.38633918762207
[TRAIN] Iter: 677300 Loss: 0.028458548709750175  PSNR: 18.627981185913086
[TRAIN] Iter: 677400 Loss: 0.027273893356323242  PSNR: 18.933870315551758
[TRAIN] Iter: 677500 Loss: 0.026307355612516403  PSNR: 19.006324768066406
[TRAIN] Iter: 677600 Loss: 0.02598794922232628  PSNR: 19.416301727294922
[TRAIN] Iter: 677700 Loss: 0.02343485690653324  PSNR: 19.567296981811523
[TRAIN] Iter: 677800 Loss: 0.027259783819317818  PSNR: 18.892412185668945
[TRAIN] Iter: 677900 Loss: 0.024486050009727478  PSNR: 19.477928161621094
[TRAIN] Iter: 678000 Loss: 0.025051895529031754  PSNR: 19.20857048034668
[TRAIN] Iter: 678100 Loss: 0.02117464318871498  PSNR: 19.64327049255371
[TRAIN] Iter: 678200 Loss: 0.024812011048197746  PSNR: 19.799013137817383
[TRAIN] Iter: 678300 Loss: 0.028702769428491592  PSNR: 18.682621002197266
[TRAIN] Iter: 678400 Loss: 0.027915451675653458  PSNR: 18.757286071777344
[TRAIN] Iter: 678500 Loss: 0.022799134254455566  PSNR: 19.47686004638672
[TRAIN] Iter: 678600 Loss: 0.02506273239850998  PSNR: 19.325773239135742
[TRAIN] Iter: 678700 Loss: 0.031155336648225784  PSNR: 18.191329956054688
[TRAIN] Iter: 678800 Loss: 0.022459018975496292  PSNR: 19.83123016357422
[TRAIN] Iter: 678900 Loss: 0.018353823572397232  PSNR: 20.366403579711914
[TRAIN] Iter: 679000 Loss: 0.025402575731277466  PSNR: 19.336366653442383
[TRAIN] Iter: 679100 Loss: 0.02406882308423519  PSNR: 19.25522804260254
[TRAIN] Iter: 679200 Loss: 0.029747504740953445  PSNR: 18.44585418701172
[TRAIN] Iter: 679300 Loss: 0.025099201127886772  PSNR: 19.218324661254883
[TRAIN] Iter: 679400 Loss: 0.026257049292325974  PSNR: 19.10605812072754
[TRAIN] Iter: 679500 Loss: 0.02526606246829033  PSNR: 19.832216262817383
[TRAIN] Iter: 679600 Loss: 0.025305796414613724  PSNR: 19.267656326293945
[TRAIN] Iter: 679700 Loss: 0.028764788061380386  PSNR: 18.70220947265625
[TRAIN] Iter: 679800 Loss: 0.02511594258248806  PSNR: 19.247974395751953
[TRAIN] Iter: 679900 Loss: 0.022076081484556198  PSNR: 19.811552047729492
Saved checkpoints at ./logs/TUT-LAB-nerf/680000.tar
[TRAIN] Iter: 680000 Loss: 0.024563148617744446  PSNR: 19.573575973510742
[TRAIN] Iter: 680100 Loss: 0.02589433826506138  PSNR: 19.229816436767578
[TRAIN] Iter: 680200 Loss: 0.03198706731200218  PSNR: 18.11531639099121
[TRAIN] Iter: 680300 Loss: 0.028295237571001053  PSNR: 18.484699249267578
[TRAIN] Iter: 680400 Loss: 0.028762025758624077  PSNR: 18.510223388671875
[TRAIN] Iter: 680500 Loss: 0.02415427193045616  PSNR: 19.324981689453125
[TRAIN] Iter: 680600 Loss: 0.029436644166707993  PSNR: 18.65220832824707
[TRAIN] Iter: 680700 Loss: 0.023406099528074265  PSNR: 19.297670364379883
[TRAIN] Iter: 680800 Loss: 0.030329275876283646  PSNR: 18.42226791381836
[TRAIN] Iter: 680900 Loss: 0.03657439351081848  PSNR: 17.543230056762695
[TRAIN] Iter: 681000 Loss: 0.023315979167819023  PSNR: 19.529003143310547
[TRAIN] Iter: 681100 Loss: 0.020174212753772736  PSNR: 20.132274627685547
[TRAIN] Iter: 681200 Loss: 0.03178109973669052  PSNR: 18.003299713134766
[TRAIN] Iter: 681300 Loss: 0.030249765142798424  PSNR: 18.459579467773438
[TRAIN] Iter: 681400 Loss: 0.026818566024303436  PSNR: 19.021806716918945
[TRAIN] Iter: 681500 Loss: 0.029180657118558884  PSNR: 18.623022079467773
[TRAIN] Iter: 681600 Loss: 0.03517867624759674  PSNR: 17.788135528564453
[TRAIN] Iter: 681700 Loss: 0.02723841182887554  PSNR: 19.10187530517578
[TRAIN] Iter: 681800 Loss: 0.026368122547864914  PSNR: 18.745689392089844
[TRAIN] Iter: 681900 Loss: 0.024159975349903107  PSNR: 19.40581512451172
[TRAIN] Iter: 682000 Loss: 0.03137839585542679  PSNR: 18.220605850219727
[TRAIN] Iter: 682100 Loss: 0.025024186819791794  PSNR: 18.961498260498047
[TRAIN] Iter: 682200 Loss: 0.02595396712422371  PSNR: 19.21137046813965
[TRAIN] Iter: 682300 Loss: 0.03395886719226837  PSNR: 18.055038452148438
[TRAIN] Iter: 682400 Loss: 0.031681373715400696  PSNR: 18.211143493652344
[TRAIN] Iter: 682500 Loss: 0.03201001510024071  PSNR: 18.1379337310791
[TRAIN] Iter: 682600 Loss: 0.023072823882102966  PSNR: 19.657176971435547
[TRAIN] Iter: 682700 Loss: 0.022417090833187103  PSNR: 19.59711265563965
[TRAIN] Iter: 682800 Loss: 0.024874746799468994  PSNR: 19.174776077270508
[TRAIN] Iter: 682900 Loss: 0.020507201552391052  PSNR: 20.102462768554688
[TRAIN] Iter: 683000 Loss: 0.026841947808861732  PSNR: 19.01047134399414
[TRAIN] Iter: 683100 Loss: 0.030445022508502007  PSNR: 18.412120819091797
[TRAIN] Iter: 683200 Loss: 0.029157381504774094  PSNR: 18.26366424560547
[TRAIN] Iter: 683300 Loss: 0.025118393823504448  PSNR: 19.3037166595459
[TRAIN] Iter: 683400 Loss: 0.02738969773054123  PSNR: 18.792139053344727
[TRAIN] Iter: 683500 Loss: 0.026577498763799667  PSNR: 19.081830978393555
[TRAIN] Iter: 683600 Loss: 0.02583182044327259  PSNR: 19.47492027282715
[TRAIN] Iter: 683700 Loss: 0.02927391789853573  PSNR: 18.529390335083008
[TRAIN] Iter: 683800 Loss: 0.022800568491220474  PSNR: 19.631694793701172
[TRAIN] Iter: 683900 Loss: 0.027097780257463455  PSNR: 18.752094268798828
[TRAIN] Iter: 684000 Loss: 0.02617534250020981  PSNR: 18.734249114990234
[TRAIN] Iter: 684100 Loss: 0.02599932812154293  PSNR: 18.990598678588867
[TRAIN] Iter: 684200 Loss: 0.0252685509622097  PSNR: 19.380664825439453
[TRAIN] Iter: 684300 Loss: 0.0301576629281044  PSNR: 18.471147537231445
[TRAIN] Iter: 684400 Loss: 0.026002848520874977  PSNR: 19.015104293823242
[TRAIN] Iter: 684500 Loss: 0.029493339359760284  PSNR: 18.93424415588379
[TRAIN] Iter: 684600 Loss: 0.024307522922754288  PSNR: 19.635976791381836
[TRAIN] Iter: 684700 Loss: 0.028831176459789276  PSNR: 18.69443130493164
[TRAIN] Iter: 684800 Loss: 0.02377963997423649  PSNR: 19.50680160522461
[TRAIN] Iter: 684900 Loss: 0.025710297748446465  PSNR: 19.2800350189209
[TRAIN] Iter: 685000 Loss: 0.03241780400276184  PSNR: 18.041271209716797
[TRAIN] Iter: 685100 Loss: 0.03200627118349075  PSNR: 18.163005828857422
[TRAIN] Iter: 685200 Loss: 0.02747134491801262  PSNR: 18.88313102722168
[TRAIN] Iter: 685300 Loss: 0.021372482180595398  PSNR: 19.86138153076172
[TRAIN] Iter: 685400 Loss: 0.03025386668741703  PSNR: 18.462072372436523
[TRAIN] Iter: 685500 Loss: 0.02384086698293686  PSNR: 19.67127227783203
[TRAIN] Iter: 685600 Loss: 0.02357858419418335  PSNR: 19.62799072265625
[TRAIN] Iter: 685700 Loss: 0.026583882048726082  PSNR: 18.939590454101562
[TRAIN] Iter: 685800 Loss: 0.02529917098581791  PSNR: 19.228944778442383
[TRAIN] Iter: 685900 Loss: 0.026830021291971207  PSNR: 18.849281311035156
[TRAIN] Iter: 686000 Loss: 0.031747858971357346  PSNR: 18.1986083984375
[TRAIN] Iter: 686100 Loss: 0.024479078128933907  PSNR: 19.47309112548828
[TRAIN] Iter: 686200 Loss: 0.024576716125011444  PSNR: 19.50432014465332
[TRAIN] Iter: 686300 Loss: 0.019788289442658424  PSNR: 20.192543029785156
[TRAIN] Iter: 686400 Loss: 0.026216812431812286  PSNR: 19.045801162719727
[TRAIN] Iter: 686500 Loss: 0.024547535926103592  PSNR: 19.45296287536621
[TRAIN] Iter: 686600 Loss: 0.02775413542985916  PSNR: 18.655054092407227
[TRAIN] Iter: 686700 Loss: 0.020617935806512833  PSNR: 19.891389846801758
[TRAIN] Iter: 686800 Loss: 0.0319259837269783  PSNR: 18.149871826171875
[TRAIN] Iter: 686900 Loss: 0.0215233713388443  PSNR: 19.517175674438477
[TRAIN] Iter: 687000 Loss: 0.03043614886701107  PSNR: 18.446264266967773
[TRAIN] Iter: 687100 Loss: 0.02329837903380394  PSNR: 19.569351196289062
[TRAIN] Iter: 687200 Loss: 0.02676059678196907  PSNR: 18.985340118408203
[TRAIN] Iter: 687300 Loss: 0.02768700197339058  PSNR: 18.819852828979492
[TRAIN] Iter: 687400 Loss: 0.030334018170833588  PSNR: 18.50523567199707
[TRAIN] Iter: 687500 Loss: 0.03309287130832672  PSNR: 18.080293655395508
[TRAIN] Iter: 687600 Loss: 0.02326406165957451  PSNR: 19.446138381958008
[TRAIN] Iter: 687700 Loss: 0.027246376499533653  PSNR: 18.858030319213867
[TRAIN] Iter: 687800 Loss: 0.02894851565361023  PSNR: 18.54861831665039
[TRAIN] Iter: 687900 Loss: 0.02952362783253193  PSNR: 18.587806701660156
[TRAIN] Iter: 688000 Loss: 0.02888145111501217  PSNR: 18.48280906677246
[TRAIN] Iter: 688100 Loss: 0.028612200170755386  PSNR: 18.73347282409668
[TRAIN] Iter: 688200 Loss: 0.026856761425733566  PSNR: 18.82870101928711
[TRAIN] Iter: 688300 Loss: 0.026192594319581985  PSNR: 18.925153732299805
[TRAIN] Iter: 688400 Loss: 0.02699713408946991  PSNR: 19.063980102539062
[TRAIN] Iter: 688500 Loss: 0.019720066338777542  PSNR: 20.302101135253906
[TRAIN] Iter: 688600 Loss: 0.022295217961072922  PSNR: 19.515825271606445
[TRAIN] Iter: 688700 Loss: 0.030180197209119797  PSNR: 18.342416763305664
[TRAIN] Iter: 688800 Loss: 0.02881598100066185  PSNR: 18.80559730529785
[TRAIN] Iter: 688900 Loss: 0.025912072509527206  PSNR: 18.913270950317383
[TRAIN] Iter: 689000 Loss: 0.028469648212194443  PSNR: 18.749435424804688
[TRAIN] Iter: 689100 Loss: 0.03310953453183174  PSNR: 17.849292755126953
[TRAIN] Iter: 689200 Loss: 0.02300434187054634  PSNR: 19.624343872070312
[TRAIN] Iter: 689300 Loss: 0.030016960576176643  PSNR: 18.689958572387695
[TRAIN] Iter: 689400 Loss: 0.024985596537590027  PSNR: 19.317302703857422
[TRAIN] Iter: 689500 Loss: 0.025204163044691086  PSNR: 19.183971405029297
[TRAIN] Iter: 689600 Loss: 0.030639903619885445  PSNR: 18.288616180419922
[TRAIN] Iter: 689700 Loss: 0.02743593603372574  PSNR: 18.82064437866211
[TRAIN] Iter: 689800 Loss: 0.026416178792715073  PSNR: 18.870174407958984
[TRAIN] Iter: 689900 Loss: 0.025263216346502304  PSNR: 19.235164642333984
Saved checkpoints at ./logs/TUT-LAB-nerf/690000.tar
[TRAIN] Iter: 690000 Loss: 0.025201275944709778  PSNR: 19.171680450439453
[TRAIN] Iter: 690100 Loss: 0.026335272938013077  PSNR: 18.896137237548828
[TRAIN] Iter: 690200 Loss: 0.025583988055586815  PSNR: 19.11746597290039
[TRAIN] Iter: 690300 Loss: 0.031905077397823334  PSNR: 18.16729164123535
[TRAIN] Iter: 690400 Loss: 0.03238844871520996  PSNR: 18.120988845825195
[TRAIN] Iter: 690500 Loss: 0.031249944120645523  PSNR: 18.207950592041016
[TRAIN] Iter: 690600 Loss: 0.022413402795791626  PSNR: 19.496414184570312
[TRAIN] Iter: 690700 Loss: 0.03035878576338291  PSNR: 18.62980842590332
[TRAIN] Iter: 690800 Loss: 0.02524475008249283  PSNR: 19.22559356689453
[TRAIN] Iter: 690900 Loss: 0.025989968329668045  PSNR: 19.119600296020508
[TRAIN] Iter: 691000 Loss: 0.02164647728204727  PSNR: 20.142484664916992
[TRAIN] Iter: 691100 Loss: 0.024209756404161453  PSNR: 19.408050537109375
[TRAIN] Iter: 691200 Loss: 0.026074843481183052  PSNR: 19.067983627319336
[TRAIN] Iter: 691300 Loss: 0.02075401321053505  PSNR: 19.976764678955078
[TRAIN] Iter: 691400 Loss: 0.03254653513431549  PSNR: 18.195405960083008
[TRAIN] Iter: 691500 Loss: 0.031130850315093994  PSNR: 18.372262954711914
[TRAIN] Iter: 691600 Loss: 0.021986505016684532  PSNR: 19.75333023071289
[TRAIN] Iter: 691700 Loss: 0.028767071664333344  PSNR: 18.7419376373291
[TRAIN] Iter: 691800 Loss: 0.024523645639419556  PSNR: 19.3184871673584
[TRAIN] Iter: 691900 Loss: 0.025841249153017998  PSNR: 19.110612869262695
[TRAIN] Iter: 692000 Loss: 0.02988518960773945  PSNR: 18.461742401123047
[TRAIN] Iter: 692100 Loss: 0.027976397424936295  PSNR: 18.732515335083008
[TRAIN] Iter: 692200 Loss: 0.03195594251155853  PSNR: 18.275615692138672
[TRAIN] Iter: 692300 Loss: 0.03301072120666504  PSNR: 17.866090774536133
[TRAIN] Iter: 692400 Loss: 0.02592707797884941  PSNR: 19.109981536865234
[TRAIN] Iter: 692500 Loss: 0.021709758788347244  PSNR: 19.914039611816406
[TRAIN] Iter: 692600 Loss: 0.026269838213920593  PSNR: 19.08755874633789
[TRAIN] Iter: 692700 Loss: 0.032430365681648254  PSNR: 18.01815414428711
[TRAIN] Iter: 692800 Loss: 0.030804354697465897  PSNR: 18.325241088867188
[TRAIN] Iter: 692900 Loss: 0.020323965698480606  PSNR: 20.169261932373047
[TRAIN] Iter: 693000 Loss: 0.02040516957640648  PSNR: 20.131301879882812
[TRAIN] Iter: 693100 Loss: 0.02403908036649227  PSNR: 19.359041213989258
[TRAIN] Iter: 693200 Loss: 0.028803467750549316  PSNR: 18.622329711914062
[TRAIN] Iter: 693300 Loss: 0.02684648334980011  PSNR: 19.057937622070312
[TRAIN] Iter: 693400 Loss: 0.025983920320868492  PSNR: 19.072282791137695
[TRAIN] Iter: 693500 Loss: 0.022951725870370865  PSNR: 19.283113479614258
[TRAIN] Iter: 693600 Loss: 0.02052350342273712  PSNR: 20.222946166992188
[TRAIN] Iter: 693700 Loss: 0.024949587881565094  PSNR: 19.28024673461914
[TRAIN] Iter: 693800 Loss: 0.02884002774953842  PSNR: 18.92303466796875
[TRAIN] Iter: 693900 Loss: 0.02793603017926216  PSNR: 18.600645065307617
[TRAIN] Iter: 694000 Loss: 0.03161217272281647  PSNR: 18.0992431640625
[TRAIN] Iter: 694100 Loss: 0.026514917612075806  PSNR: 19.220500946044922
[TRAIN] Iter: 694200 Loss: 0.027045417577028275  PSNR: 19.33045196533203
[TRAIN] Iter: 694300 Loss: 0.027170123532414436  PSNR: 18.844480514526367
[TRAIN] Iter: 694400 Loss: 0.01958649232983589  PSNR: 20.23252296447754
[TRAIN] Iter: 694500 Loss: 0.02396474778652191  PSNR: 19.399595260620117
[TRAIN] Iter: 694600 Loss: 0.026217572391033173  PSNR: 19.151559829711914
[TRAIN] Iter: 694700 Loss: 0.021935341879725456  PSNR: 19.812658309936523
[TRAIN] Iter: 694800 Loss: 0.028146052733063698  PSNR: 18.73552894592285
[TRAIN] Iter: 694900 Loss: 0.034085147082805634  PSNR: 17.83002471923828
[TRAIN] Iter: 695000 Loss: 0.033784449100494385  PSNR: 17.908035278320312
[TRAIN] Iter: 695100 Loss: 0.027548637241125107  PSNR: 18.710403442382812
[TRAIN] Iter: 695200 Loss: 0.03223297372460365  PSNR: 18.0836181640625
[TRAIN] Iter: 695300 Loss: 0.02200441248714924  PSNR: 19.54262351989746
[TRAIN] Iter: 695400 Loss: 0.026903513818979263  PSNR: 18.845909118652344
[TRAIN] Iter: 695500 Loss: 0.020922798663377762  PSNR: 19.752473831176758
[TRAIN] Iter: 695600 Loss: 0.022227071225643158  PSNR: 19.5780029296875
[TRAIN] Iter: 695700 Loss: 0.029524635523557663  PSNR: 18.73082733154297
[TRAIN] Iter: 695800 Loss: 0.02245694398880005  PSNR: 19.635099411010742
[TRAIN] Iter: 695900 Loss: 0.029116537421941757  PSNR: 18.476146697998047
[TRAIN] Iter: 696000 Loss: 0.02465592697262764  PSNR: 19.364229202270508
[TRAIN] Iter: 696100 Loss: 0.024146471172571182  PSNR: 19.58081817626953
[TRAIN] Iter: 696200 Loss: 0.023268135264515877  PSNR: 19.615915298461914
[TRAIN] Iter: 696300 Loss: 0.029144566506147385  PSNR: 18.511581420898438
[TRAIN] Iter: 696400 Loss: 0.023638781160116196  PSNR: 19.09400177001953
[TRAIN] Iter: 696500 Loss: 0.025902286171913147  PSNR: 18.98340606689453
[TRAIN] Iter: 696600 Loss: 0.02888583391904831  PSNR: 18.433048248291016
[TRAIN] Iter: 696700 Loss: 0.022569192573428154  PSNR: 19.453683853149414
[TRAIN] Iter: 696800 Loss: 0.03130774199962616  PSNR: 18.097795486450195
[TRAIN] Iter: 696900 Loss: 0.028718138113617897  PSNR: 18.52543830871582
[TRAIN] Iter: 697000 Loss: 0.025740988552570343  PSNR: 19.1108455657959
[TRAIN] Iter: 697100 Loss: 0.024141384288668633  PSNR: 19.28171157836914
[TRAIN] Iter: 697200 Loss: 0.03280021995306015  PSNR: 17.95443344116211
[TRAIN] Iter: 697300 Loss: 0.030296772718429565  PSNR: 18.287473678588867
[TRAIN] Iter: 697400 Loss: 0.025858614593744278  PSNR: 18.87040901184082
[TRAIN] Iter: 697500 Loss: 0.022287486121058464  PSNR: 19.89921760559082
[TRAIN] Iter: 697600 Loss: 0.029349390417337418  PSNR: 18.55752944946289
[TRAIN] Iter: 697700 Loss: 0.024166226387023926  PSNR: 19.360553741455078
[TRAIN] Iter: 697800 Loss: 0.02402915433049202  PSNR: 19.02964973449707
[TRAIN] Iter: 697900 Loss: 0.03050929680466652  PSNR: 18.377378463745117
[TRAIN] Iter: 698000 Loss: 0.027459923177957535  PSNR: 18.797576904296875
[TRAIN] Iter: 698100 Loss: 0.022482942789793015  PSNR: 19.677566528320312
[TRAIN] Iter: 698200 Loss: 0.028380636125802994  PSNR: 18.66077995300293
[TRAIN] Iter: 698300 Loss: 0.026622414588928223  PSNR: 18.967464447021484
[TRAIN] Iter: 698400 Loss: 0.03099600225687027  PSNR: 18.32423973083496
[TRAIN] Iter: 698500 Loss: 0.028591686859726906  PSNR: 18.715248107910156
[TRAIN] Iter: 698600 Loss: 0.028074387460947037  PSNR: 18.77926254272461
[TRAIN] Iter: 698700 Loss: 0.026724468916654587  PSNR: 18.980558395385742
[TRAIN] Iter: 698800 Loss: 0.024249497801065445  PSNR: 19.340356826782227
[TRAIN] Iter: 698900 Loss: 0.029011359438300133  PSNR: 18.524852752685547
[TRAIN] Iter: 699000 Loss: 0.027586326003074646  PSNR: 18.645349502563477
[TRAIN] Iter: 699100 Loss: 0.023416539654135704  PSNR: 19.53814125061035
[TRAIN] Iter: 699200 Loss: 0.020083529874682426  PSNR: 20.047531127929688
[TRAIN] Iter: 699300 Loss: 0.028866220265626907  PSNR: 18.652599334716797
[TRAIN] Iter: 699400 Loss: 0.02845008298754692  PSNR: 18.568119049072266
[TRAIN] Iter: 699500 Loss: 0.02443019673228264  PSNR: 19.047527313232422
[TRAIN] Iter: 699600 Loss: 0.024525796994566917  PSNR: 19.7725887298584
[TRAIN] Iter: 699700 Loss: 0.02638174407184124  PSNR: 18.932308197021484
[TRAIN] Iter: 699800 Loss: 0.033683545887470245  PSNR: 17.966991424560547
[TRAIN] Iter: 699900 Loss: 0.025268319994211197  PSNR: 19.47517967224121
Saved checkpoints at ./logs/TUT-LAB-nerf/700000.tar
0 0.0003933906555175781
torch.Size([320, 640, 3]) torch.Size([320, 640])
1 17.53609013557434
2 15.235516786575317
3 17.331222534179688
4 15.302181005477905
5 15.573997974395752
6 18.83704423904419
7 16.049224138259888
8 17.033082008361816
9 15.520195484161377
10 17.709405660629272
11 15.59689474105835
12 15.46060562133789
13 17.262104749679565
14 15.268818140029907
15 17.42387318611145
16 15.070961475372314
17 17.5869460105896
18 15.505027532577515
19 17.141486644744873
20 15.437861680984497
21 17.237957239151
22 15.451092720031738
23 17.12572193145752
24 15.382120370864868
25 17.180259704589844
26 15.412060260772705
27 17.213853359222412
28 15.436654806137085
29 17.137508392333984
30 15.438288688659668
31 17.12999939918518
32 15.481292486190796
33 17.17840051651001
34 15.459228515625
35 15.416350841522217
36 17.19130778312683
37 15.412684202194214
38 17.229034900665283
39 15.404229640960693
40 17.315320014953613
41 15.418355941772461
42 16.928720951080322
43 15.633243083953857
44 17.467921018600464
45 15.217533349990845
46 16.84381651878357
47 15.46846890449524
48 17.393399477005005
49 15.372175931930542
50 17.287779331207275
51 15.40112018585205
52 17.31403660774231
53 15.371096849441528
54 15.253137826919556
55 15.1277437210083
56 15.123229265213013
57 17.24311876296997
58 15.320785760879517
59 15.731471061706543
60 13.43665599822998
61 15.70480751991272
62 13.144343614578247
63 13.497581243515015
64 16.18227791786194
65 13.052680730819702
66 16.147720336914062
67 12.967689037322998
68 16.041091918945312
69 13.352107763290405
70 13.254023313522339
71 16.184738159179688
72 12.914069652557373
73 16.20276427268982
74 13.15454649925232
75 13.301865339279175
76 16.055447816848755
77 13.04733395576477
78 16.32000994682312
79 12.942901849746704
80 16.071606397628784
81 13.37982702255249
82 13.166305541992188
83 16.112974405288696
84 12.969560146331787
85 16.249881744384766
86 13.657928705215454
87 13.509911060333252
88 15.363324880599976
89 13.577470541000366
90 15.6235990524292
91 13.531555652618408
92 15.593759298324585
93 13.492125511169434
94 13.510578155517578
95 15.612958908081055
96 13.469367027282715
97 15.720815181732178
98 13.509114980697632
99 13.524546146392822
100 15.647467613220215
101 13.526236295700073
102 15.668287992477417
103 13.473881244659424
104 15.570916652679443
105 13.51741623878479
106 13.49685287475586
107 15.662222623825073
108 13.507263422012329
109 15.692550420761108
110 13.51543116569519
111 15.62953805923462
112 13.51559329032898
113 13.489200592041016
114 15.706405401229858
115 13.584065675735474
116 15.410752296447754
117 13.208523988723755
118 13.473367929458618
119 16.10781240463257
Done, saving (120, 320, 640, 3) (120, 320, 640)
extras:{'raw': tensor([[[-1.3711e-01, -3.2191e-01, -8.0611e-02, -6.9266e+00],
         [-1.4561e-01, -2.4823e-01,  3.2589e-01, -2.8645e+00],
         [-1.2448e+00, -9.7929e-01, -8.7550e-02, -5.2816e+00],
         ...,
         [-3.8427e+00, -3.0743e+00, -1.1533e+00, -4.6011e+00],
         [-3.4231e+00, -2.5563e+00, -1.0067e+00,  3.1940e+00],
         [-4.3638e+00, -3.1678e+00, -1.3866e+00, -4.1556e+00]],

        [[-8.9167e-01, -9.5009e-01, -1.0664e+00, -2.1144e+01],
         [-1.0113e+00, -9.6716e-01, -8.3429e-01, -5.6598e+00],
         [-1.4590e+00, -1.4052e+00, -1.3427e+00, -4.1838e-01],
         ...,
         [ 7.9438e+00, -1.0388e+01, -3.5489e+01,  3.0603e+02],
         [ 5.9882e+00, -1.1189e+01, -3.5610e+01,  2.9290e+02],
         [ 5.0022e+00, -1.2221e+01, -3.7092e+01,  2.9594e+02]],

        [[ 3.9763e-02,  2.4277e-01,  5.4908e-01,  3.3547e+00],
         [-5.2299e-01, -1.3876e-01, -4.7100e-01, -9.2676e+00],
         [-4.3899e-01, -2.3479e-01, -5.2505e-01, -5.4125e+00],
         ...,
         [ 7.5354e+00,  2.5444e+00, -5.5926e+00,  4.2592e+02],
         [ 6.5099e+00,  1.6014e+00, -6.4454e+00,  4.4623e+02],
         [ 8.7274e+00,  2.8142e+00, -6.5526e+00,  4.3233e+02]],

        ...,

        [[ 1.4138e-01, -1.1071e-01, -2.7734e-01,  6.7225e+00],
         [ 4.0898e-01,  1.7304e-01,  8.8260e-02, -9.1805e+00],
         [ 5.1422e-01,  2.8111e-01,  2.1888e-01, -1.2760e+01],
         ...,
         [ 1.1948e+00,  5.8084e-01, -4.8770e+00,  5.1886e+02],
         [-3.4610e-03, -3.8947e-01, -6.4390e+00,  5.1467e+02],
         [ 1.1957e-02, -3.6270e-01, -6.4213e+00,  5.2030e+02]],

        [[-9.5742e-01, -1.1438e+00, -1.4348e+00, -2.0023e+01],
         [-6.9826e-01, -7.3557e-01, -8.6883e-01, -9.6267e+00],
         [ 2.0277e-01,  1.8502e-01,  3.7473e-01, -3.4031e+00],
         ...,
         [ 2.2080e+00, -1.1061e+01, -3.1320e+01,  2.6268e+02],
         [ 2.1766e+00, -1.1336e+01, -3.2733e+01,  2.6076e+02],
         [ 3.8144e-02, -1.2562e+01, -3.3413e+01,  2.4799e+02]],

        [[-7.5895e-01, -7.7026e-01, -7.6287e-01, -8.0539e+00],
         [-7.2115e-01, -7.4052e-01, -6.0368e-01,  5.5435e+00],
         [-7.4706e-01, -7.6458e-01, -6.2405e-01,  5.5763e+00],
         ...,
         [ 3.9078e+00, -3.5126e+00, -1.3507e+01,  1.2502e+02],
         [ 5.3162e+00, -2.8365e+00, -1.2596e+01,  1.4105e+02],
         [ 4.5474e+00, -3.4360e+00, -1.3304e+01,  1.2926e+02]]],
       grad_fn=<ReshapeAliasBackward0>), 'rgb0': tensor([[0.2963, 0.3283, 0.4619],
        [0.2079, 0.2107, 0.2231],
        [0.5258, 0.6085, 0.5797],
        ...,
        [0.5526, 0.4978, 0.4763],
        [0.4825, 0.4791, 0.5349],
        [0.3735, 0.3567, 0.3592]], grad_fn=<ReshapeAliasBackward0>), 'disp0': tensor([ 68.0800,  42.6358,  37.8917,  ..., 409.4365,  50.4433, 112.5598],
       grad_fn=<ReshapeAliasBackward0>), 'acc0': tensor([1., 1., 1.,  ..., 1., 1., 1.], grad_fn=<ReshapeAliasBackward0>), 'z_std': tensor([0.0022, 0.0024, 0.0018,  ..., 0.2611, 0.0018, 0.0017])}
0 0.0004487037658691406
torch.Size([320, 640, 3]) torch.Size([320, 640])
1 15.687445163726807
2 13.466652631759644
3 15.689473628997803
4 13.240209817886353
5 13.472330570220947
6 15.959973096847534
7 13.482186317443848
8 15.695868492126465
9 13.446746349334717
10 13.44561767578125
11 15.734655380249023
12 13.416624307632446
13 15.840083837509155
14 13.135522365570068
15 16.034793376922607
16 13.446605920791626
17 13.182854175567627
18 16.26805853843689
19 12.98662781715393
20 16.101738691329956
21 13.25761604309082
22 13.223433494567871
23 16.224740028381348
24 12.994808673858643
25 16.28351068496704
26 13.068212747573853
27 15.881747961044312
28 13.517418384552002
29 13.085193395614624
30 16.394033432006836
31 13.25188684463501
32 15.643469333648682
33 13.166984558105469
34 13.185115814208984
35 16.25108051300049
36 13.023720979690552
37 16.350902557373047
38 13.037727117538452
39 13.328163146972656
40 16.056920289993286
41 13.078283548355103
42 16.333220720291138
43 12.958137512207031
44 16.3096981048584
45 13.269830465316772
46 13.139915227890015
47 16.184617280960083
48 12.947596073150635
49 16.23670792579651
50 13.054460048675537
51 13.272890329360962
52 16.35326623916626
53 13.016450643539429
54 16.163501977920532
55 12.967925310134888
56 13.572243213653564
57 16.48546290397644
58 13.532392024993896
59 15.09337592124939
60 13.245023488998413
61 16.035094738006592
62 13.254111051559448
63 13.237780332565308
64 16.076451778411865
65 13.080234050750732
66 16.10602617263794
67 13.15473222732544
68 13.369237661361694
69 15.906784057617188
70 13.062194347381592
71 16.21483826637268
72 13.024416446685791
73 16.163546800613403
74 13.096142292022705
75 13.213864088058472
76 16.403529405593872
77 13.436655521392822
78 15.62184762954712
79 13.414194345474243
80 13.413444995880127
81 15.686444759368896
82 13.393655776977539
83 15.740452527999878
84 13.388405323028564
85 15.643942832946777
86 13.44577670097351
87 13.349477767944336
88 15.717668056488037
89 13.371683359146118
90 15.761088848114014
91 13.438608407974243
92 13.361499071121216
93 15.831876277923584
94 13.286664962768555
95 15.866381645202637
96 13.2782564163208
97 13.32859492301941
98 15.820920944213867
99 13.243005514144897
100 16.15621018409729
101 13.020110845565796
102 15.985037803649902
103 13.327050685882568
104 13.083978414535522
105 16.11695122718811
106 13.00776481628418
107 16.21172070503235
108 13.105835676193237
109 13.23941445350647
110 16.05132555961609
111 12.965684413909912
112 16.431344747543335
113 13.005759954452515
114 13.285489559173584
115 15.998492002487183
116 13.062911748886108
117 16.268144845962524
118 12.962698459625244
119 16.16642451286316
test poses shape torch.Size([13, 3, 4])
0 0.0006020069122314453
torch.Size([320, 640, 3]) torch.Size([320, 640])
1 13.188106298446655
2 16.189945459365845
3 13.00793981552124
4 16.317248344421387
5 13.07238507270813
6 13.43362021446228
7 16.15807318687439
8 13.104727506637573
9 16.295734643936157
10 12.964258670806885
11 16.220850706100464
12 13.28279972076416
Saved test set
[TRAIN] Iter: 700000 Loss: 0.02666359394788742  PSNR: 18.91724967956543
[TRAIN] Iter: 700100 Loss: 0.025206618010997772  PSNR: 19.335988998413086
[TRAIN] Iter: 700200 Loss: 0.02779129520058632  PSNR: 18.85047721862793
[TRAIN] Iter: 700300 Loss: 0.030007831752300262  PSNR: 18.477703094482422
[TRAIN] Iter: 700400 Loss: 0.024060310795903206  PSNR: 19.348569869995117
[TRAIN] Iter: 700500 Loss: 0.02613418735563755  PSNR: 19.101974487304688
[TRAIN] Iter: 700600 Loss: 0.02969050034880638  PSNR: 18.528839111328125
[TRAIN] Iter: 700700 Loss: 0.03283332660794258  PSNR: 18.081974029541016
[TRAIN] Iter: 700800 Loss: 0.029003974050283432  PSNR: 18.43555450439453
[TRAIN] Iter: 700900 Loss: 0.03113887459039688  PSNR: 18.29023551940918
[TRAIN] Iter: 701000 Loss: 0.026554249227046967  PSNR: 18.999441146850586
[TRAIN] Iter: 701100 Loss: 0.02722809836268425  PSNR: 18.960363388061523
[TRAIN] Iter: 701200 Loss: 0.027197685092687607  PSNR: 18.80451774597168
[TRAIN] Iter: 701300 Loss: 0.028022825717926025  PSNR: 18.927825927734375
[TRAIN] Iter: 701400 Loss: 0.02830508165061474  PSNR: 18.722734451293945
[TRAIN] Iter: 701500 Loss: 0.02669362910091877  PSNR: 18.634449005126953
[TRAIN] Iter: 701600 Loss: 0.02676490880548954  PSNR: 18.87000846862793
[TRAIN] Iter: 701700 Loss: 0.028987236320972443  PSNR: 18.523759841918945
[TRAIN] Iter: 701800 Loss: 0.025088254362344742  PSNR: 19.310565948486328
[TRAIN] Iter: 701900 Loss: 0.021326471120119095  PSNR: 19.727943420410156
[TRAIN] Iter: 702000 Loss: 0.023824285715818405  PSNR: 19.381147384643555
[TRAIN] Iter: 702100 Loss: 0.026981621980667114  PSNR: 18.718244552612305
[TRAIN] Iter: 702200 Loss: 0.026196464896202087  PSNR: 19.075828552246094
[TRAIN] Iter: 702300 Loss: 0.02909887209534645  PSNR: 18.536155700683594
[TRAIN] Iter: 702400 Loss: 0.02289983630180359  PSNR: 19.555620193481445
[TRAIN] Iter: 702500 Loss: 0.031098991632461548  PSNR: 18.26717758178711
[TRAIN] Iter: 702600 Loss: 0.02279036119580269  PSNR: 19.893306732177734
[TRAIN] Iter: 702700 Loss: 0.02228923700749874  PSNR: 19.441238403320312
[TRAIN] Iter: 702800 Loss: 0.028742678463459015  PSNR: 18.62175941467285
[TRAIN] Iter: 702900 Loss: 0.023471955209970474  PSNR: 19.462993621826172
[TRAIN] Iter: 703000 Loss: 0.022857386618852615  PSNR: 19.388534545898438
[TRAIN] Iter: 703100 Loss: 0.025602661073207855  PSNR: 19.098127365112305
[TRAIN] Iter: 703200 Loss: 0.030070744454860687  PSNR: 18.509546279907227
[TRAIN] Iter: 703300 Loss: 0.025516141206026077  PSNR: 19.236526489257812
[TRAIN] Iter: 703400 Loss: 0.027473382651805878  PSNR: 18.696922302246094
[TRAIN] Iter: 703500 Loss: 0.024329908192157745  PSNR: 18.96881675720215
[TRAIN] Iter: 703600 Loss: 0.021129336208105087  PSNR: 20.075780868530273
[TRAIN] Iter: 703700 Loss: 0.022678770124912262  PSNR: 19.503509521484375
[TRAIN] Iter: 703800 Loss: 0.031505078077316284  PSNR: 18.156673431396484
[TRAIN] Iter: 703900 Loss: 0.026897747069597244  PSNR: 18.539506912231445
[TRAIN] Iter: 704000 Loss: 0.023677263408899307  PSNR: 19.21731185913086
[TRAIN] Iter: 704100 Loss: 0.020673509687185287  PSNR: 19.847875595092773
[TRAIN] Iter: 704200 Loss: 0.02920837700366974  PSNR: 18.593042373657227
[TRAIN] Iter: 704300 Loss: 0.02875245362520218  PSNR: 18.456871032714844
[TRAIN] Iter: 704400 Loss: 0.0224977545440197  PSNR: 19.501131057739258
[TRAIN] Iter: 704500 Loss: 0.024266455322504044  PSNR: 19.548786163330078
[TRAIN] Iter: 704600 Loss: 0.03039649687707424  PSNR: 18.371273040771484
[TRAIN] Iter: 704700 Loss: 0.023085318505764008  PSNR: 19.37487030029297
[TRAIN] Iter: 704800 Loss: 0.032556354999542236  PSNR: 17.988327026367188
[TRAIN] Iter: 704900 Loss: 0.03465937077999115  PSNR: 17.81489372253418
[TRAIN] Iter: 705000 Loss: 0.02725059911608696  PSNR: 18.86039924621582
[TRAIN] Iter: 705100 Loss: 0.02608383633196354  PSNR: 19.078449249267578
[TRAIN] Iter: 705200 Loss: 0.026008833199739456  PSNR: 18.920576095581055
[TRAIN] Iter: 705300 Loss: 0.02689949795603752  PSNR: 19.108184814453125
[TRAIN] Iter: 705400 Loss: 0.02854974754154682  PSNR: 18.75351905822754
[TRAIN] Iter: 705500 Loss: 0.030417494475841522  PSNR: 18.381746292114258
[TRAIN] Iter: 705600 Loss: 0.02136048674583435  PSNR: 19.754398345947266
[TRAIN] Iter: 705700 Loss: 0.028917089104652405  PSNR: 18.59906578063965
[TRAIN] Iter: 705800 Loss: 0.024431616067886353  PSNR: 19.21994972229004
[TRAIN] Iter: 705900 Loss: 0.02291198819875717  PSNR: 19.933849334716797
[TRAIN] Iter: 706000 Loss: 0.024929555132985115  PSNR: 19.007404327392578
[TRAIN] Iter: 706100 Loss: 0.027050627395510674  PSNR: 18.866100311279297
[TRAIN] Iter: 706200 Loss: 0.021835902705788612  PSNR: 19.81587028503418
[TRAIN] Iter: 706300 Loss: 0.025709930807352066  PSNR: 18.724637985229492
[TRAIN] Iter: 706400 Loss: 0.03083665296435356  PSNR: 18.257709503173828
[TRAIN] Iter: 706500 Loss: 0.02181221917271614  PSNR: 19.8897762298584
[TRAIN] Iter: 706600 Loss: 0.03030085191130638  PSNR: 18.300039291381836
[TRAIN] Iter: 706700 Loss: 0.0292020533233881  PSNR: 18.3925724029541
[TRAIN] Iter: 706800 Loss: 0.024875104427337646  PSNR: 19.002958297729492
[TRAIN] Iter: 706900 Loss: 0.027532702311873436  PSNR: 18.810548782348633
[TRAIN] Iter: 707000 Loss: 0.02719048596918583  PSNR: 18.8175106048584
[TRAIN] Iter: 707100 Loss: 0.03295046091079712  PSNR: 18.063758850097656
[TRAIN] Iter: 707200 Loss: 0.020847991108894348  PSNR: 20.026575088500977
[TRAIN] Iter: 707300 Loss: 0.023491015657782555  PSNR: 19.34276580810547
[TRAIN] Iter: 707400 Loss: 0.02752365916967392  PSNR: 18.690181732177734
[TRAIN] Iter: 707500 Loss: 0.03360580652952194  PSNR: 17.962472915649414
[TRAIN] Iter: 707600 Loss: 0.031527891755104065  PSNR: 18.10931968688965
[TRAIN] Iter: 707700 Loss: 0.02668926492333412  PSNR: 18.906795501708984
[TRAIN] Iter: 707800 Loss: 0.02520850859582424  PSNR: 19.002992630004883
[TRAIN] Iter: 707900 Loss: 0.02771681547164917  PSNR: 18.430675506591797
[TRAIN] Iter: 708000 Loss: 0.022047311067581177  PSNR: 19.778169631958008
[TRAIN] Iter: 708100 Loss: 0.02648990973830223  PSNR: 18.969968795776367
[TRAIN] Iter: 708200 Loss: 0.027050450444221497  PSNR: 18.86704444885254
[TRAIN] Iter: 708300 Loss: 0.02377980388700962  PSNR: 19.107873916625977
[TRAIN] Iter: 708400 Loss: 0.02323785237967968  PSNR: 19.36669921875
[TRAIN] Iter: 708500 Loss: 0.029870184138417244  PSNR: 18.41485595703125
[TRAIN] Iter: 708600 Loss: 0.025722378864884377  PSNR: 19.28573989868164
[TRAIN] Iter: 708700 Loss: 0.026781009510159492  PSNR: 19.285123825073242
[TRAIN] Iter: 708800 Loss: 0.019412338733673096  PSNR: 20.342281341552734
[TRAIN] Iter: 708900 Loss: 0.026232535019516945  PSNR: 19.131786346435547
[TRAIN] Iter: 709000 Loss: 0.03211456164717674  PSNR: 18.080446243286133
[TRAIN] Iter: 709100 Loss: 0.026799798011779785  PSNR: 19.00629425048828
[TRAIN] Iter: 709200 Loss: 0.0309019573032856  PSNR: 18.221967697143555
[TRAIN] Iter: 709300 Loss: 0.029798178002238274  PSNR: 18.445850372314453
[TRAIN] Iter: 709400 Loss: 0.027402078732848167  PSNR: 18.75366973876953
[TRAIN] Iter: 709500 Loss: 0.0277421772480011  PSNR: 19.0255126953125
[TRAIN] Iter: 709600 Loss: 0.03469458967447281  PSNR: 17.793027877807617
[TRAIN] Iter: 709700 Loss: 0.03066175803542137  PSNR: 18.366628646850586
[TRAIN] Iter: 709800 Loss: 0.03140538930892944  PSNR: 18.234588623046875
[TRAIN] Iter: 709900 Loss: 0.02434540167450905  PSNR: 19.514339447021484
Saved checkpoints at ./logs/TUT-LAB-nerf/710000.tar
[TRAIN] Iter: 710000 Loss: 0.028181936591863632  PSNR: 18.660249710083008
[TRAIN] Iter: 710100 Loss: 0.020727938041090965  PSNR: 19.965303421020508
[TRAIN] Iter: 710200 Loss: 0.030845271423459053  PSNR: 18.38519287109375
[TRAIN] Iter: 710300 Loss: 0.027181770652532578  PSNR: 18.935209274291992
[TRAIN] Iter: 710400 Loss: 0.025133397430181503  PSNR: 19.353633880615234
[TRAIN] Iter: 710500 Loss: 0.021459823474287987  PSNR: 20.05498695373535
[TRAIN] Iter: 710600 Loss: 0.027324240654706955  PSNR: 18.968257904052734
[TRAIN] Iter: 710700 Loss: 0.025565922260284424  PSNR: 19.259424209594727
[TRAIN] Iter: 710800 Loss: 0.024390630424022675  PSNR: 18.996957778930664
[TRAIN] Iter: 710900 Loss: 0.022177178412675858  PSNR: 19.834718704223633
[TRAIN] Iter: 711000 Loss: 0.028427910059690475  PSNR: 18.744138717651367
[TRAIN] Iter: 711100 Loss: 0.02679496817290783  PSNR: 18.949487686157227
[TRAIN] Iter: 711200 Loss: 0.02028527855873108  PSNR: 20.18291664123535
[TRAIN] Iter: 711300 Loss: 0.023299165070056915  PSNR: 19.566896438598633
[TRAIN] Iter: 711400 Loss: 0.02658131532371044  PSNR: 18.895099639892578
[TRAIN] Iter: 711500 Loss: 0.02791508659720421  PSNR: 18.71515464782715
[TRAIN] Iter: 711600 Loss: 0.026973828673362732  PSNR: 19.254257202148438
[TRAIN] Iter: 711700 Loss: 0.026341598480939865  PSNR: 19.15207290649414
[TRAIN] Iter: 711800 Loss: 0.023686660453677177  PSNR: 19.310129165649414
[TRAIN] Iter: 711900 Loss: 0.03391191363334656  PSNR: 17.977481842041016
[TRAIN] Iter: 712000 Loss: 0.02410907670855522  PSNR: 19.08550453186035
[TRAIN] Iter: 712100 Loss: 0.02247783914208412  PSNR: 19.64731788635254
[TRAIN] Iter: 712200 Loss: 0.025608807802200317  PSNR: 19.170412063598633
[TRAIN] Iter: 712300 Loss: 0.023539943620562553  PSNR: 19.50248908996582
[TRAIN] Iter: 712400 Loss: 0.03145825117826462  PSNR: 18.306997299194336
[TRAIN] Iter: 712500 Loss: 0.024712927639484406  PSNR: 19.272207260131836
[TRAIN] Iter: 712600 Loss: 0.03045823983848095  PSNR: 18.39549446105957
[TRAIN] Iter: 712700 Loss: 0.027044083923101425  PSNR: 18.940841674804688
[TRAIN] Iter: 712800 Loss: 0.027749940752983093  PSNR: 18.74372100830078
[TRAIN] Iter: 712900 Loss: 0.026316780596971512  PSNR: 18.765899658203125
[TRAIN] Iter: 713000 Loss: 0.02278105542063713  PSNR: 19.48436164855957
[TRAIN] Iter: 713100 Loss: 0.025046130642294884  PSNR: 19.73177146911621
[TRAIN] Iter: 713200 Loss: 0.029613815248012543  PSNR: 18.48106575012207
[TRAIN] Iter: 713300 Loss: 0.025712870061397552  PSNR: 18.66026496887207
[TRAIN] Iter: 713400 Loss: 0.024923644959926605  PSNR: 19.35293197631836
[TRAIN] Iter: 713500 Loss: 0.022918540984392166  PSNR: 19.576152801513672
[TRAIN] Iter: 713600 Loss: 0.031231176108121872  PSNR: 18.42413330078125
[TRAIN] Iter: 713700 Loss: 0.02459580823779106  PSNR: 19.371524810791016
[TRAIN] Iter: 713800 Loss: 0.032147809863090515  PSNR: 18.230281829833984
[TRAIN] Iter: 713900 Loss: 0.025491490960121155  PSNR: 19.180192947387695
[TRAIN] Iter: 714000 Loss: 0.025770341977477074  PSNR: 18.936906814575195
[TRAIN] Iter: 714100 Loss: 0.023352734744548798  PSNR: 19.60801124572754
[TRAIN] Iter: 714200 Loss: 0.02614954113960266  PSNR: 18.94828224182129
[TRAIN] Iter: 714300 Loss: 0.024472758173942566  PSNR: 19.228893280029297
[TRAIN] Iter: 714400 Loss: 0.027475593611598015  PSNR: 18.78926658630371
[TRAIN] Iter: 714500 Loss: 0.02502923086285591  PSNR: 19.012462615966797
[TRAIN] Iter: 714600 Loss: 0.021784957498311996  PSNR: 19.948776245117188
[TRAIN] Iter: 714700 Loss: 0.03224196285009384  PSNR: 18.008739471435547
[TRAIN] Iter: 714800 Loss: 0.028918392956256866  PSNR: 18.542810440063477
[TRAIN] Iter: 714900 Loss: 0.025534525513648987  PSNR: 19.098108291625977
[TRAIN] Iter: 715000 Loss: 0.02314443141222  PSNR: 19.435970306396484
[TRAIN] Iter: 715100 Loss: 0.02996171824634075  PSNR: 18.260364532470703
[TRAIN] Iter: 715200 Loss: 0.025918787345290184  PSNR: 19.156030654907227
[TRAIN] Iter: 715300 Loss: 0.024287888780236244  PSNR: 19.310592651367188
[TRAIN] Iter: 715400 Loss: 0.026892980560660362  PSNR: 18.96184730529785
[TRAIN] Iter: 715500 Loss: 0.028763486072421074  PSNR: 18.531538009643555
[TRAIN] Iter: 715600 Loss: 0.029189467430114746  PSNR: 18.50872802734375
[TRAIN] Iter: 715700 Loss: 0.026717212051153183  PSNR: 18.86640739440918
[TRAIN] Iter: 715800 Loss: 0.02943282201886177  PSNR: 18.72066879272461
[TRAIN] Iter: 715900 Loss: 0.027267586439847946  PSNR: 18.830142974853516
[TRAIN] Iter: 716000 Loss: 0.024343647062778473  PSNR: 18.988561630249023
[TRAIN] Iter: 716100 Loss: 0.03410149738192558  PSNR: 17.811840057373047
[TRAIN] Iter: 716200 Loss: 0.022574957460165024  PSNR: 19.621431350708008
[TRAIN] Iter: 716300 Loss: 0.031066598370671272  PSNR: 18.367671966552734
[TRAIN] Iter: 716400 Loss: 0.02890200726687908  PSNR: 18.58156967163086
[TRAIN] Iter: 716500 Loss: 0.02147921919822693  PSNR: 20.17516326904297
[TRAIN] Iter: 716600 Loss: 0.02643713727593422  PSNR: 18.800769805908203
[TRAIN] Iter: 716700 Loss: 0.03324596956372261  PSNR: 17.97018814086914
[TRAIN] Iter: 716800 Loss: 0.02791888639330864  PSNR: 18.573768615722656
[TRAIN] Iter: 716900 Loss: 0.023709312081336975  PSNR: 19.42343521118164
[TRAIN] Iter: 717000 Loss: 0.03302093222737312  PSNR: 18.005939483642578
[TRAIN] Iter: 717100 Loss: 0.02587340772151947  PSNR: 19.050439834594727
[TRAIN] Iter: 717200 Loss: 0.032209012657403946  PSNR: 18.067480087280273
[TRAIN] Iter: 717300 Loss: 0.0187444519251585  PSNR: 20.42850112915039
[TRAIN] Iter: 717400 Loss: 0.03386381268501282  PSNR: 17.82632827758789
[TRAIN] Iter: 717500 Loss: 0.023019490763545036  PSNR: 19.64041519165039
[TRAIN] Iter: 717600 Loss: 0.027941755950450897  PSNR: 18.636398315429688
[TRAIN] Iter: 717700 Loss: 0.030399257317185402  PSNR: 18.345144271850586
[TRAIN] Iter: 717800 Loss: 0.028158038854599  PSNR: 18.679088592529297
[TRAIN] Iter: 717900 Loss: 0.026804370805621147  PSNR: 18.65798568725586
[TRAIN] Iter: 718000 Loss: 0.02201663702726364  PSNR: 19.765411376953125
[TRAIN] Iter: 718100 Loss: 0.02553083747625351  PSNR: 19.090316772460938
[TRAIN] Iter: 718200 Loss: 0.02947729080915451  PSNR: 18.528743743896484
[TRAIN] Iter: 718300 Loss: 0.021991387009620667  PSNR: 19.731637954711914
[TRAIN] Iter: 718400 Loss: 0.03035097010433674  PSNR: 18.295839309692383
[TRAIN] Iter: 718500 Loss: 0.032700709998607635  PSNR: 18.053537368774414
[TRAIN] Iter: 718600 Loss: 0.02490229904651642  PSNR: 19.41556167602539
[TRAIN] Iter: 718700 Loss: 0.02534295991063118  PSNR: 19.188379287719727
[TRAIN] Iter: 718800 Loss: 0.024517454206943512  PSNR: 19.293170928955078
[TRAIN] Iter: 718900 Loss: 0.028331290930509567  PSNR: 18.65833282470703
[TRAIN] Iter: 719000 Loss: 0.02318042889237404  PSNR: 19.520902633666992
[TRAIN] Iter: 719100 Loss: 0.022771140560507774  PSNR: 19.644947052001953
[TRAIN] Iter: 719200 Loss: 0.03071860782802105  PSNR: 18.220190048217773
[TRAIN] Iter: 719300 Loss: 0.01987871527671814  PSNR: 20.131196975708008
[TRAIN] Iter: 719400 Loss: 0.025432243943214417  PSNR: 19.194082260131836
[TRAIN] Iter: 719500 Loss: 0.022300876677036285  PSNR: 19.554113388061523
[TRAIN] Iter: 719600 Loss: 0.027600722387433052  PSNR: 18.829673767089844
[TRAIN] Iter: 719700 Loss: 0.022760728374123573  PSNR: 19.60191535949707
[TRAIN] Iter: 719800 Loss: 0.024653969332575798  PSNR: 19.029476165771484
[TRAIN] Iter: 719900 Loss: 0.026893120259046555  PSNR: 18.78935432434082
Saved checkpoints at ./logs/TUT-LAB-nerf/720000.tar
[TRAIN] Iter: 720000 Loss: 0.02526896819472313  PSNR: 19.39605712890625
[TRAIN] Iter: 720100 Loss: 0.027736637741327286  PSNR: 18.87124252319336
[TRAIN] Iter: 720200 Loss: 0.02385781891644001  PSNR: 19.346174240112305
[TRAIN] Iter: 720300 Loss: 0.02233980968594551  PSNR: 19.738351821899414
[TRAIN] Iter: 720400 Loss: 0.030321773141622543  PSNR: 18.523372650146484
[TRAIN] Iter: 720500 Loss: 0.0325668640434742  PSNR: 18.06853675842285
[TRAIN] Iter: 720600 Loss: 0.02578592672944069  PSNR: 19.08860206604004
[TRAIN] Iter: 720700 Loss: 0.02364889532327652  PSNR: 19.557085037231445
[TRAIN] Iter: 720800 Loss: 0.02452872507274151  PSNR: 19.205474853515625
[TRAIN] Iter: 720900 Loss: 0.03350241854786873  PSNR: 17.821352005004883
[TRAIN] Iter: 721000 Loss: 0.029960673302412033  PSNR: 18.385591506958008
[TRAIN] Iter: 721100 Loss: 0.02698677033185959  PSNR: 18.727439880371094
[TRAIN] Iter: 721200 Loss: 0.029488837346434593  PSNR: 18.48317527770996
[TRAIN] Iter: 721300 Loss: 0.026681719347834587  PSNR: 18.971956253051758
[TRAIN] Iter: 721400 Loss: 0.025727415457367897  PSNR: 19.09494972229004
[TRAIN] Iter: 721500 Loss: 0.030164599418640137  PSNR: 18.41199493408203
[TRAIN] Iter: 721600 Loss: 0.030798448249697685  PSNR: 18.26280975341797
[TRAIN] Iter: 721700 Loss: 0.026492763310670853  PSNR: 18.88200569152832
[TRAIN] Iter: 721800 Loss: 0.030173558741807938  PSNR: 18.365459442138672
[TRAIN] Iter: 721900 Loss: 0.028059208765625954  PSNR: 18.68680191040039
[TRAIN] Iter: 722000 Loss: 0.026724740862846375  PSNR: 18.857101440429688
[TRAIN] Iter: 722100 Loss: 0.031469158828258514  PSNR: 18.222183227539062
[TRAIN] Iter: 722200 Loss: 0.027863476425409317  PSNR: 18.658878326416016
[TRAIN] Iter: 722300 Loss: 0.0286327563226223  PSNR: 19.175151824951172
[TRAIN] Iter: 722400 Loss: 0.027863092720508575  PSNR: 18.840011596679688
[TRAIN] Iter: 722500 Loss: 0.02802523598074913  PSNR: 18.931434631347656
[TRAIN] Iter: 722600 Loss: 0.025078359991312027  PSNR: 19.144947052001953
[TRAIN] Iter: 722700 Loss: 0.022192781791090965  PSNR: 19.746360778808594
[TRAIN] Iter: 722800 Loss: 0.029798664152622223  PSNR: 18.472883224487305
[TRAIN] Iter: 722900 Loss: 0.02053523063659668  PSNR: 20.159955978393555
[TRAIN] Iter: 723000 Loss: 0.020954107865691185  PSNR: 19.989612579345703
[TRAIN] Iter: 723100 Loss: 0.032240547239780426  PSNR: 18.02667999267578
[TRAIN] Iter: 723200 Loss: 0.021541837602853775  PSNR: 19.830074310302734
[TRAIN] Iter: 723300 Loss: 0.02580086886882782  PSNR: 19.186344146728516
[TRAIN] Iter: 723400 Loss: 0.023593716323375702  PSNR: 19.496475219726562
[TRAIN] Iter: 723500 Loss: 0.029829837381839752  PSNR: 18.38045310974121
[TRAIN] Iter: 723600 Loss: 0.02133149281144142  PSNR: 19.768218994140625
[TRAIN] Iter: 723700 Loss: 0.03133701533079147  PSNR: 18.274757385253906
[TRAIN] Iter: 723800 Loss: 0.02231574058532715  PSNR: 19.596500396728516
[TRAIN] Iter: 723900 Loss: 0.023722315207123756  PSNR: 19.70476531982422
[TRAIN] Iter: 724000 Loss: 0.032094817608594894  PSNR: 18.13973617553711
[TRAIN] Iter: 724100 Loss: 0.030006518587470055  PSNR: 18.367698669433594
[TRAIN] Iter: 724200 Loss: 0.029700301587581635  PSNR: 18.407833099365234
[TRAIN] Iter: 724300 Loss: 0.031856950372457504  PSNR: 18.062881469726562
[TRAIN] Iter: 724400 Loss: 0.02914803847670555  PSNR: 18.388063430786133
[TRAIN] Iter: 724500 Loss: 0.017211906611919403  PSNR: 20.89002799987793
[TRAIN] Iter: 724600 Loss: 0.02550612762570381  PSNR: 19.285686492919922
[TRAIN] Iter: 724700 Loss: 0.029081759974360466  PSNR: 18.40611457824707
[TRAIN] Iter: 724800 Loss: 0.025069337338209152  PSNR: 19.195205688476562
[TRAIN] Iter: 724900 Loss: 0.02938448265194893  PSNR: 18.62133026123047
[TRAIN] Iter: 725000 Loss: 0.031133970245718956  PSNR: 18.15226936340332
[TRAIN] Iter: 725100 Loss: 0.023342248052358627  PSNR: 19.323440551757812
[TRAIN] Iter: 725200 Loss: 0.02810206077992916  PSNR: 18.862388610839844
[TRAIN] Iter: 725300 Loss: 0.0263249259442091  PSNR: 18.827232360839844
[TRAIN] Iter: 725400 Loss: 0.033118054270744324  PSNR: 18.027372360229492
[TRAIN] Iter: 725500 Loss: 0.02973567135632038  PSNR: 18.03582763671875
[TRAIN] Iter: 725600 Loss: 0.03059353679418564  PSNR: 18.28716468811035
[TRAIN] Iter: 725700 Loss: 0.02572236768901348  PSNR: 18.95825958251953
[TRAIN] Iter: 725800 Loss: 0.028497667983174324  PSNR: 19.013458251953125
[TRAIN] Iter: 725900 Loss: 0.027522046118974686  PSNR: 18.76766014099121
[TRAIN] Iter: 726000 Loss: 0.030521972104907036  PSNR: 18.4320011138916
[TRAIN] Iter: 726100 Loss: 0.02938627079129219  PSNR: 18.402652740478516
[TRAIN] Iter: 726200 Loss: 0.021800339221954346  PSNR: 19.761022567749023
[TRAIN] Iter: 726300 Loss: 0.027233945205807686  PSNR: 18.791034698486328
[TRAIN] Iter: 726400 Loss: 0.03073738142848015  PSNR: 18.383045196533203
[TRAIN] Iter: 726500 Loss: 0.024277111515402794  PSNR: 19.389833450317383
[TRAIN] Iter: 726600 Loss: 0.023610837757587433  PSNR: 19.741886138916016
[TRAIN] Iter: 726700 Loss: 0.023556888103485107  PSNR: 19.551103591918945
[TRAIN] Iter: 726800 Loss: 0.027119075879454613  PSNR: 18.8054256439209
[TRAIN] Iter: 726900 Loss: 0.021864425390958786  PSNR: 19.611553192138672
[TRAIN] Iter: 727000 Loss: 0.027266550809144974  PSNR: 18.88040542602539
[TRAIN] Iter: 727100 Loss: 0.02666660025715828  PSNR: 18.94158935546875
[TRAIN] Iter: 727200 Loss: 0.02282198891043663  PSNR: 19.584766387939453
[TRAIN] Iter: 727300 Loss: 0.026759706437587738  PSNR: 18.798952102661133
[TRAIN] Iter: 727400 Loss: 0.03309042379260063  PSNR: 17.957523345947266
[TRAIN] Iter: 727500 Loss: 0.02960638701915741  PSNR: 18.308795928955078
[TRAIN] Iter: 727600 Loss: 0.025668350979685783  PSNR: 19.598581314086914
[TRAIN] Iter: 727700 Loss: 0.021285710856318474  PSNR: 19.794687271118164
[TRAIN] Iter: 727800 Loss: 0.024740181863307953  PSNR: 19.49519920349121
[TRAIN] Iter: 727900 Loss: 0.022349612787365913  PSNR: 19.755870819091797
[TRAIN] Iter: 728000 Loss: 0.025747152045369148  PSNR: 19.137542724609375
[TRAIN] Iter: 728100 Loss: 0.026287171989679337  PSNR: 19.048725128173828
[TRAIN] Iter: 728200 Loss: 0.02976888045668602  PSNR: 18.28772735595703
[TRAIN] Iter: 728300 Loss: 0.030100107192993164  PSNR: 18.41495704650879
[TRAIN] Iter: 728400 Loss: 0.027489179745316505  PSNR: 18.768747329711914
[TRAIN] Iter: 728500 Loss: 0.025060031563043594  PSNR: 19.02923011779785
[TRAIN] Iter: 728600 Loss: 0.02717871218919754  PSNR: 18.73041534423828
[TRAIN] Iter: 728700 Loss: 0.02722156047821045  PSNR: 18.87663459777832
[TRAIN] Iter: 728800 Loss: 0.024930184707045555  PSNR: 19.128459930419922
[TRAIN] Iter: 728900 Loss: 0.03393647074699402  PSNR: 17.894088745117188
[TRAIN] Iter: 729000 Loss: 0.0312708243727684  PSNR: 18.12220573425293
[TRAIN] Iter: 729100 Loss: 0.02382214181125164  PSNR: 19.580810546875
[TRAIN] Iter: 729200 Loss: 0.031814996153116226  PSNR: 18.141277313232422
[TRAIN] Iter: 729300 Loss: 0.023192284628748894  PSNR: 19.65093421936035
[TRAIN] Iter: 729400 Loss: 0.025123590603470802  PSNR: 19.667739868164062
[TRAIN] Iter: 729500 Loss: 0.026531226933002472  PSNR: 18.93918228149414
[TRAIN] Iter: 729600 Loss: 0.030911967158317566  PSNR: 18.34524154663086
[TRAIN] Iter: 729700 Loss: 0.02114216983318329  PSNR: 19.97444725036621
[TRAIN] Iter: 729800 Loss: 0.02341773360967636  PSNR: 19.51761817932129
[TRAIN] Iter: 729900 Loss: 0.027576595544815063  PSNR: 18.759904861450195
Saved checkpoints at ./logs/TUT-LAB-nerf/730000.tar
[TRAIN] Iter: 730000 Loss: 0.026662688702344894  PSNR: 19.1011962890625
[TRAIN] Iter: 730100 Loss: 0.026506181806325912  PSNR: 18.897193908691406
[TRAIN] Iter: 730200 Loss: 0.027781955897808075  PSNR: 18.63267707824707
[TRAIN] Iter: 730300 Loss: 0.020246949046850204  PSNR: 20.086868286132812
[TRAIN] Iter: 730400 Loss: 0.02883000671863556  PSNR: 18.61644172668457
[TRAIN] Iter: 730500 Loss: 0.021688178181648254  PSNR: 19.852087020874023
[TRAIN] Iter: 730600 Loss: 0.030801938846707344  PSNR: 18.281885147094727
[TRAIN] Iter: 730700 Loss: 0.02813669480383396  PSNR: 18.62002182006836
[TRAIN] Iter: 730800 Loss: 0.02561781369149685  PSNR: 19.101015090942383
[TRAIN] Iter: 730900 Loss: 0.0334586426615715  PSNR: 17.873424530029297
[TRAIN] Iter: 731000 Loss: 0.025984901934862137  PSNR: 19.06354522705078
[TRAIN] Iter: 731100 Loss: 0.02436625398695469  PSNR: 19.368148803710938
[TRAIN] Iter: 731200 Loss: 0.033676356077194214  PSNR: 18.012601852416992
[TRAIN] Iter: 731300 Loss: 0.02335219830274582  PSNR: 19.320764541625977
[TRAIN] Iter: 731400 Loss: 0.025643115863204002  PSNR: 19.039283752441406
[TRAIN] Iter: 731500 Loss: 0.029254112392663956  PSNR: 18.46651840209961
[TRAIN] Iter: 731600 Loss: 0.03067196160554886  PSNR: 18.208824157714844
[TRAIN] Iter: 731700 Loss: 0.02537030540406704  PSNR: 19.00755500793457
[TRAIN] Iter: 731800 Loss: 0.02658046782016754  PSNR: 19.023040771484375
[TRAIN] Iter: 731900 Loss: 0.023133795708417892  PSNR: 19.739778518676758
[TRAIN] Iter: 732000 Loss: 0.01848924532532692  PSNR: 20.757694244384766
[TRAIN] Iter: 732100 Loss: 0.029841020703315735  PSNR: 18.495817184448242
[TRAIN] Iter: 732200 Loss: 0.027876362204551697  PSNR: 18.844465255737305
[TRAIN] Iter: 732300 Loss: 0.025573205202817917  PSNR: 19.09743881225586
[TRAIN] Iter: 732400 Loss: 0.023069018498063087  PSNR: 19.717933654785156
[TRAIN] Iter: 732500 Loss: 0.03223482146859169  PSNR: 18.089292526245117
[TRAIN] Iter: 732600 Loss: 0.03067346289753914  PSNR: 18.36823844909668
[TRAIN] Iter: 732700 Loss: 0.02664869837462902  PSNR: 18.96135902404785
[TRAIN] Iter: 732800 Loss: 0.025013813748955727  PSNR: 19.260902404785156
[TRAIN] Iter: 732900 Loss: 0.028579503297805786  PSNR: 18.627483367919922
[TRAIN] Iter: 733000 Loss: 0.02409830316901207  PSNR: 19.50173568725586
[TRAIN] Iter: 733100 Loss: 0.025183459743857384  PSNR: 19.26410675048828
[TRAIN] Iter: 733200 Loss: 0.02866048738360405  PSNR: 18.649492263793945
[TRAIN] Iter: 733300 Loss: 0.025906989350914955  PSNR: 18.90515899658203
[TRAIN] Iter: 733400 Loss: 0.027448538690805435  PSNR: 19.160825729370117
[TRAIN] Iter: 733500 Loss: 0.025125747546553612  PSNR: 19.219270706176758
[TRAIN] Iter: 733600 Loss: 0.029956795275211334  PSNR: 18.344911575317383
[TRAIN] Iter: 733700 Loss: 0.028406739234924316  PSNR: 18.491254806518555
[TRAIN] Iter: 733800 Loss: 0.02143147774040699  PSNR: 19.947904586791992
[TRAIN] Iter: 733900 Loss: 0.026316635310649872  PSNR: 19.09321403503418
[TRAIN] Iter: 734000 Loss: 0.029160086065530777  PSNR: 18.468210220336914
[TRAIN] Iter: 734100 Loss: 0.03152577951550484  PSNR: 18.13457679748535
[TRAIN] Iter: 734200 Loss: 0.02299247682094574  PSNR: 19.449209213256836
[TRAIN] Iter: 734300 Loss: 0.02154054120182991  PSNR: 19.956764221191406
[TRAIN] Iter: 734400 Loss: 0.031796667724847794  PSNR: 18.273700714111328
[TRAIN] Iter: 734500 Loss: 0.024872250854969025  PSNR: 19.11478614807129
[TRAIN] Iter: 734600 Loss: 0.023559246212244034  PSNR: 19.45354461669922
[TRAIN] Iter: 734700 Loss: 0.025250595062971115  PSNR: 19.316347122192383
[TRAIN] Iter: 734800 Loss: 0.023948926478624344  PSNR: 19.53717041015625
[TRAIN] Iter: 734900 Loss: 0.02802668884396553  PSNR: 18.714635848999023
[TRAIN] Iter: 735000 Loss: 0.02482800930738449  PSNR: 19.113981246948242
[TRAIN] Iter: 735100 Loss: 0.023013664409518242  PSNR: 19.765871047973633
[TRAIN] Iter: 735200 Loss: 0.024034328758716583  PSNR: 19.295244216918945
[TRAIN] Iter: 735300 Loss: 0.023133069276809692  PSNR: 20.026472091674805
[TRAIN] Iter: 735400 Loss: 0.02340725064277649  PSNR: 19.094409942626953
[TRAIN] Iter: 735500 Loss: 0.026198817417025566  PSNR: 18.67582893371582
[TRAIN] Iter: 735600 Loss: 0.02711629681289196  PSNR: 18.781631469726562
[TRAIN] Iter: 735700 Loss: 0.021325064823031425  PSNR: 20.0108585357666
[TRAIN] Iter: 735800 Loss: 0.02758173644542694  PSNR: 18.815757751464844
[TRAIN] Iter: 735900 Loss: 0.026459334418177605  PSNR: 18.968852996826172
[TRAIN] Iter: 736000 Loss: 0.029534928500652313  PSNR: 18.455251693725586
[TRAIN] Iter: 736100 Loss: 0.029005689546465874  PSNR: 18.491127014160156
[TRAIN] Iter: 736200 Loss: 0.026924939826130867  PSNR: 19.049169540405273
[TRAIN] Iter: 736300 Loss: 0.03281887620687485  PSNR: 17.97882843017578
[TRAIN] Iter: 736400 Loss: 0.02752513252198696  PSNR: 18.733755111694336
[TRAIN] Iter: 736500 Loss: 0.02449360489845276  PSNR: 19.316679000854492
[TRAIN] Iter: 736600 Loss: 0.030154379084706306  PSNR: 18.39029312133789
[TRAIN] Iter: 736700 Loss: 0.021215930581092834  PSNR: 19.853214263916016
[TRAIN] Iter: 736800 Loss: 0.027174804359674454  PSNR: 19.139814376831055
[TRAIN] Iter: 736900 Loss: 0.026712603867053986  PSNR: 18.81304931640625
[TRAIN] Iter: 737000 Loss: 0.03222007304430008  PSNR: 18.137540817260742
[TRAIN] Iter: 737100 Loss: 0.027507420629262924  PSNR: 19.006668090820312
[TRAIN] Iter: 737200 Loss: 0.03211072459816933  PSNR: 18.173564910888672
[TRAIN] Iter: 737300 Loss: 0.028482288122177124  PSNR: 18.629863739013672
[TRAIN] Iter: 737400 Loss: 0.02150885760784149  PSNR: 19.887189865112305
[TRAIN] Iter: 737500 Loss: 0.031104469671845436  PSNR: 18.320722579956055
[TRAIN] Iter: 737600 Loss: 0.025677550584077835  PSNR: 19.057523727416992
[TRAIN] Iter: 737700 Loss: 0.029532596468925476  PSNR: 18.524662017822266
[TRAIN] Iter: 737800 Loss: 0.02986135706305504  PSNR: 18.444860458374023
[TRAIN] Iter: 737900 Loss: 0.023891590535640717  PSNR: 19.312580108642578
[TRAIN] Iter: 738000 Loss: 0.029927469789981842  PSNR: 18.72247886657715
[TRAIN] Iter: 738100 Loss: 0.0309713464230299  PSNR: 18.263803482055664
[TRAIN] Iter: 738200 Loss: 0.029477175325155258  PSNR: 18.529159545898438
[TRAIN] Iter: 738300 Loss: 0.028127772733569145  PSNR: 18.58867835998535
[TRAIN] Iter: 738400 Loss: 0.028693968430161476  PSNR: 18.609254837036133
[TRAIN] Iter: 738500 Loss: 0.02476123534142971  PSNR: 19.259424209594727
[TRAIN] Iter: 738600 Loss: 0.03182769566774368  PSNR: 18.295265197753906
[TRAIN] Iter: 738700 Loss: 0.02291519194841385  PSNR: 19.707693099975586
[TRAIN] Iter: 738800 Loss: 0.0273460503667593  PSNR: 18.70067024230957
[TRAIN] Iter: 738900 Loss: 0.026996755972504616  PSNR: 18.781145095825195
[TRAIN] Iter: 739000 Loss: 0.02108652889728546  PSNR: 19.883020401000977
[TRAIN] Iter: 739100 Loss: 0.020272579044103622  PSNR: 19.9652042388916
[TRAIN] Iter: 739200 Loss: 0.02593391388654709  PSNR: 18.901281356811523
[TRAIN] Iter: 739300 Loss: 0.024515824392437935  PSNR: 19.094865798950195
[TRAIN] Iter: 739400 Loss: 0.022058522328734398  PSNR: 19.791202545166016
[TRAIN] Iter: 739500 Loss: 0.02880178950726986  PSNR: 18.61081886291504
[TRAIN] Iter: 739600 Loss: 0.03223535418510437  PSNR: 18.147022247314453
[TRAIN] Iter: 739700 Loss: 0.020760875195264816  PSNR: 19.810928344726562
[TRAIN] Iter: 739800 Loss: 0.026528283953666687  PSNR: 18.604358673095703
[TRAIN] Iter: 739900 Loss: 0.02488204464316368  PSNR: 19.061887741088867
Saved checkpoints at ./logs/TUT-LAB-nerf/740000.tar
[TRAIN] Iter: 740000 Loss: 0.03229566663503647  PSNR: 18.025449752807617
[TRAIN] Iter: 740100 Loss: 0.02552424743771553  PSNR: 19.288084030151367
[TRAIN] Iter: 740200 Loss: 0.02807215042412281  PSNR: 18.168859481811523
[TRAIN] Iter: 740300 Loss: 0.023097513243556023  PSNR: 19.63751983642578
[TRAIN] Iter: 740400 Loss: 0.024668751284480095  PSNR: 19.09991455078125
[TRAIN] Iter: 740500 Loss: 0.028446972370147705  PSNR: 18.75577735900879
[TRAIN] Iter: 740600 Loss: 0.02236800640821457  PSNR: 19.701644897460938
[TRAIN] Iter: 740700 Loss: 0.031782977283000946  PSNR: 18.17116355895996
[TRAIN] Iter: 740800 Loss: 0.027315495535731316  PSNR: 18.80026626586914
[TRAIN] Iter: 740900 Loss: 0.02826714515686035  PSNR: 18.79997444152832
[TRAIN] Iter: 741000 Loss: 0.02053261362016201  PSNR: 20.156200408935547
[TRAIN] Iter: 741100 Loss: 0.024045413359999657  PSNR: 19.19428062438965
[TRAIN] Iter: 741200 Loss: 0.023682527244091034  PSNR: 19.70322608947754
[TRAIN] Iter: 741300 Loss: 0.024579163640737534  PSNR: 19.342906951904297
[TRAIN] Iter: 741400 Loss: 0.030247552320361137  PSNR: 18.356653213500977
[TRAIN] Iter: 741500 Loss: 0.03108309954404831  PSNR: 18.334684371948242
[TRAIN] Iter: 741600 Loss: 0.022841926664114  PSNR: 19.41166877746582
[TRAIN] Iter: 741700 Loss: 0.022484242916107178  PSNR: 19.611236572265625
[TRAIN] Iter: 741800 Loss: 0.02708594501018524  PSNR: 18.42046356201172
[TRAIN] Iter: 741900 Loss: 0.028396811336278915  PSNR: 18.736255645751953
[TRAIN] Iter: 742000 Loss: 0.02311728149652481  PSNR: 19.671907424926758
[TRAIN] Iter: 742100 Loss: 0.023240763694047928  PSNR: 19.721393585205078
[TRAIN] Iter: 742200 Loss: 0.029705768451094627  PSNR: 18.437044143676758
[TRAIN] Iter: 742300 Loss: 0.030604127794504166  PSNR: 18.685710906982422
[TRAIN] Iter: 742400 Loss: 0.026406897231936455  PSNR: 18.939128875732422
[TRAIN] Iter: 742500 Loss: 0.025492390617728233  PSNR: 19.293241500854492
[TRAIN] Iter: 742600 Loss: 0.025213781744241714  PSNR: 19.326059341430664
[TRAIN] Iter: 742700 Loss: 0.03315410017967224  PSNR: 17.966033935546875
[TRAIN] Iter: 742800 Loss: 0.03075864166021347  PSNR: 18.24481964111328
[TRAIN] Iter: 742900 Loss: 0.025498542934656143  PSNR: 19.298580169677734
[TRAIN] Iter: 743000 Loss: 0.033101700246334076  PSNR: 17.89543342590332
[TRAIN] Iter: 743100 Loss: 0.02217278629541397  PSNR: 19.81329345703125
[TRAIN] Iter: 743200 Loss: 0.025768183171749115  PSNR: 19.101268768310547
[TRAIN] Iter: 743300 Loss: 0.0300455242395401  PSNR: 18.41765785217285
[TRAIN] Iter: 743400 Loss: 0.026153773069381714  PSNR: 18.8723201751709
[TRAIN] Iter: 743500 Loss: 0.031235290691256523  PSNR: 18.210172653198242
[TRAIN] Iter: 743600 Loss: 0.022512897849082947  PSNR: 19.713115692138672
[TRAIN] Iter: 743700 Loss: 0.023186368867754936  PSNR: 19.89146614074707
[TRAIN] Iter: 743800 Loss: 0.03076910227537155  PSNR: 18.363380432128906
[TRAIN] Iter: 743900 Loss: 0.02228950336575508  PSNR: 19.77462387084961
[TRAIN] Iter: 744000 Loss: 0.02738291025161743  PSNR: 18.933521270751953
[TRAIN] Iter: 744100 Loss: 0.03268689662218094  PSNR: 18.061328887939453
[TRAIN] Iter: 744200 Loss: 0.02828928828239441  PSNR: 18.475759506225586
[TRAIN] Iter: 744300 Loss: 0.02490518055856228  PSNR: 19.624927520751953
[TRAIN] Iter: 744400 Loss: 0.02860214374959469  PSNR: 18.712926864624023
[TRAIN] Iter: 744500 Loss: 0.02366669289767742  PSNR: 19.786422729492188
[TRAIN] Iter: 744600 Loss: 0.02812473475933075  PSNR: 18.99545669555664
[TRAIN] Iter: 744700 Loss: 0.028898537158966064  PSNR: 18.589208602905273
[TRAIN] Iter: 744800 Loss: 0.031582579016685486  PSNR: 18.26048469543457
[TRAIN] Iter: 744900 Loss: 0.017880678176879883  PSNR: 20.698989868164062
[TRAIN] Iter: 745000 Loss: 0.019152982160449028  PSNR: 20.21136474609375
[TRAIN] Iter: 745100 Loss: 0.027108628302812576  PSNR: 18.856630325317383
[TRAIN] Iter: 745200 Loss: 0.024142540991306305  PSNR: 19.321237564086914
[TRAIN] Iter: 745300 Loss: 0.02068471908569336  PSNR: 20.147050857543945
[TRAIN] Iter: 745400 Loss: 0.025187497958540916  PSNR: 18.940275192260742
[TRAIN] Iter: 745500 Loss: 0.02570369280874729  PSNR: 19.064937591552734
[TRAIN] Iter: 745600 Loss: 0.032983750104904175  PSNR: 17.881454467773438
[TRAIN] Iter: 745700 Loss: 0.02676927112042904  PSNR: 18.858783721923828
[TRAIN] Iter: 745800 Loss: 0.0266239196062088  PSNR: 18.67998504638672
[TRAIN] Iter: 745900 Loss: 0.030312664806842804  PSNR: 18.396780014038086
[TRAIN] Iter: 746000 Loss: 0.024201063439249992  PSNR: 19.380510330200195
[TRAIN] Iter: 746100 Loss: 0.028544647619128227  PSNR: 18.50948715209961
[TRAIN] Iter: 746200 Loss: 0.030146196484565735  PSNR: 18.27082061767578
[TRAIN] Iter: 746300 Loss: 0.022930806502699852  PSNR: 19.582530975341797
[TRAIN] Iter: 746400 Loss: 0.02390209026634693  PSNR: 19.401628494262695
[TRAIN] Iter: 746500 Loss: 0.023972561582922935  PSNR: 19.117708206176758
[TRAIN] Iter: 746600 Loss: 0.025154639035463333  PSNR: 19.180997848510742
[TRAIN] Iter: 746700 Loss: 0.029362188652157784  PSNR: 18.47330093383789
[TRAIN] Iter: 746800 Loss: 0.028236959129571915  PSNR: 18.630508422851562
[TRAIN] Iter: 746900 Loss: 0.023532956838607788  PSNR: 19.09940528869629
[TRAIN] Iter: 747000 Loss: 0.02711356244981289  PSNR: 18.877117156982422
[TRAIN] Iter: 747100 Loss: 0.03502710163593292  PSNR: 17.71935272216797
[TRAIN] Iter: 747200 Loss: 0.024977272376418114  PSNR: 19.143081665039062
[TRAIN] Iter: 747300 Loss: 0.032128289341926575  PSNR: 18.100183486938477
[TRAIN] Iter: 747400 Loss: 0.025900809094309807  PSNR: 19.098812103271484
[TRAIN] Iter: 747500 Loss: 0.02395082637667656  PSNR: 19.5674991607666
[TRAIN] Iter: 747600 Loss: 0.03341160714626312  PSNR: 17.95032501220703
[TRAIN] Iter: 747700 Loss: 0.029586229473352432  PSNR: 18.411935806274414
[TRAIN] Iter: 747800 Loss: 0.023752888664603233  PSNR: 19.48760223388672
[TRAIN] Iter: 747900 Loss: 0.02914472296833992  PSNR: 18.833982467651367
[TRAIN] Iter: 748000 Loss: 0.027202915400266647  PSNR: 18.80625343322754
[TRAIN] Iter: 748100 Loss: 0.027458947151899338  PSNR: 19.196533203125
[TRAIN] Iter: 748200 Loss: 0.03202509135007858  PSNR: 18.1357479095459
[TRAIN] Iter: 748300 Loss: 0.023432213813066483  PSNR: 19.542428970336914
[TRAIN] Iter: 748400 Loss: 0.02859615348279476  PSNR: 18.625778198242188
[TRAIN] Iter: 748500 Loss: 0.0251581110060215  PSNR: 19.043224334716797
[TRAIN] Iter: 748600 Loss: 0.024781029671430588  PSNR: 19.239459991455078
[TRAIN] Iter: 748700 Loss: 0.02877684123814106  PSNR: 18.56517791748047
[TRAIN] Iter: 748800 Loss: 0.022542394697666168  PSNR: 19.73682403564453
[TRAIN] Iter: 748900 Loss: 0.023041829466819763  PSNR: 19.507326126098633
[TRAIN] Iter: 749000 Loss: 0.019506128504872322  PSNR: 20.28062629699707
[TRAIN] Iter: 749100 Loss: 0.03015579655766487  PSNR: 18.479049682617188
[TRAIN] Iter: 749200 Loss: 0.027870740741491318  PSNR: 18.72169303894043
[TRAIN] Iter: 749300 Loss: 0.021187886595726013  PSNR: 20.01339340209961
[TRAIN] Iter: 749400 Loss: 0.03155430033802986  PSNR: 18.297422409057617
[TRAIN] Iter: 749500 Loss: 0.023940643295645714  PSNR: 19.501394271850586
[TRAIN] Iter: 749600 Loss: 0.025474274531006813  PSNR: 18.944942474365234
[TRAIN] Iter: 749700 Loss: 0.026991985738277435  PSNR: 18.895219802856445
[TRAIN] Iter: 749800 Loss: 0.030689440667629242  PSNR: 18.305742263793945
[TRAIN] Iter: 749900 Loss: 0.021226916462183  PSNR: 19.75682258605957
Saved checkpoints at ./logs/TUT-LAB-nerf/750000.tar
0 0.0004086494445800781
torch.Size([320, 640, 3]) torch.Size([320, 640])
1 16.322962760925293
2 16.197443962097168
3 16.28895926475525
4 16.277695655822754
5 16.268298864364624
6 16.267781257629395
7 16.314544677734375
8 16.304765224456787
9 16.21553659439087
10 16.275014638900757
11 16.28299045562744
12 16.26167631149292
13 16.29797101020813
14 16.29486894607544
15 16.158270597457886
16 16.248997449874878
17 16.236467599868774
18 16.242066144943237
19 16.259631395339966
20 16.252957105636597
21 16.27125573158264
22 16.264379501342773
23 16.281907558441162
24 16.280925273895264
25 16.28610348701477
26 16.348336219787598
27 16.19180130958557
28 16.234095573425293
29 16.253628492355347
30 16.273266077041626
31 16.267611742019653
32 16.26822328567505
33 16.358532428741455
34 16.252283573150635
35 16.345480918884277
36 16.47783350944519
37 16.52778697013855
38 16.60914635658264
39 16.731698751449585
40 16.553001880645752
41 16.512039184570312
42 16.4280788898468
43 16.321035146713257
44 14.890557527542114
45 16.18681025505066
46 16.21120810508728
47 16.08784508705139
48 14.594342231750488
49 14.425734043121338
50 14.335498332977295
51 14.457801342010498
52 14.4142427444458
53 14.40933632850647
54 14.289553880691528
55 14.389337301254272
56 14.411680698394775
57 14.43041729927063
58 14.41214632987976
59 14.39180064201355
60 14.406371831893921
61 14.392517566680908
62 14.402301549911499
63 14.381706953048706
64 14.354194164276123
65 14.428918361663818
66 14.391179323196411
67 14.36839246749878
68 14.3703932762146
69 14.335574626922607
70 14.401535034179688
71 14.429956912994385
72 14.412966966629028
73 14.39842438697815
74 14.369560718536377
75 14.384155988693237
76 14.385586261749268
77 14.346400022506714
78 14.39235520362854
79 14.357569456100464
80 14.380152702331543
81 14.371390104293823
82 14.376669645309448
83 14.427530765533447
84 14.424073934555054
85 14.390033960342407
86 14.396947622299194
87 14.395468711853027
88 14.379724264144897
89 14.378253698348999
90 14.382657527923584
91 14.391548156738281
92 14.384880304336548
93 14.369033813476562
94 14.405343532562256
95 14.408884525299072
96 14.373602151870728
97 14.422397375106812
98 14.390350103378296
99 14.556949377059937
100 14.403737306594849
101 14.42483401298523
102 14.400410890579224
103 14.449451923370361
104 14.44847583770752
105 14.44959831237793
106 14.447803735733032
107 14.450540781021118
108 14.457188129425049
109 14.47609567642212
110 14.403784275054932
111 14.446836709976196
112 14.443474054336548
113 14.441275835037231
114 14.405749320983887
115 14.483222246170044
116 14.406769514083862
117 14.236741542816162
118 14.385125875473022
119 14.383013248443604
Done, saving (120, 320, 640, 3) (120, 320, 640)
extras:{'raw': tensor([[[ 8.2767e-02, -8.6503e-02, -3.2883e-01, -7.3056e-02],
         [ 1.0122e-01, -7.9230e-02, -3.6948e-01,  4.8062e+00],
         [ 9.4301e-02, -7.9257e-02, -3.0446e-01,  5.1597e+00],
         ...,
         [ 3.0475e+01,  2.5683e+01,  2.5566e+01,  2.0076e+02],
         [ 2.7473e+01,  2.3231e+01,  2.3734e+01,  1.8586e+02],
         [ 2.8560e+01,  2.4562e+01,  2.5723e+01,  1.8810e+02]],

        [[ 3.2546e-01,  1.4908e-01, -7.5071e-02,  8.2398e+00],
         [ 3.2709e-01,  2.7480e-01, -5.1304e-02, -1.9899e+01],
         [ 3.3145e-01,  2.7814e-01, -5.6435e-02, -2.0386e+01],
         ...,
         [ 3.2273e+00,  1.9765e+00, -6.4695e-01,  2.6912e+02],
         [ 2.6850e+00,  1.4320e+00, -1.2788e+00,  2.8110e+02],
         [ 2.9944e+00,  1.7260e+00, -1.0163e+00,  2.7760e+02]],

        [[-3.1473e-01, -2.1495e-01, -1.1916e-02, -8.2447e+00],
         [-7.8787e-01, -5.4204e-01, -6.0488e-01, -1.3553e+01],
         [-7.3584e-01, -5.4365e-01, -4.4406e-01, -1.2218e+01],
         ...,
         [-5.0104e+00, -5.0547e+00, -5.8245e+00,  2.8403e+02],
         [-5.9270e+00, -6.4494e+00, -8.6818e+00,  3.2178e+02],
         [-6.1026e+00, -6.3848e+00, -8.2503e+00,  3.2464e+02]],

        ...,

        [[-7.1000e-01, -7.5271e-01, -7.4162e-01,  1.6201e+01],
         [ 3.5235e-01,  2.0277e-01,  2.2314e-01,  7.4463e-01],
         [ 2.9734e-01,  1.3398e-01,  1.0887e-01,  1.4291e-02],
         ...,
         [-3.7708e-01,  1.7936e+00,  1.2295e+01,  2.7711e+02],
         [ 9.9149e-01,  3.1626e+00,  1.3781e+01,  2.7285e+02],
         [-1.2712e-01,  2.1871e+00,  1.2826e+01,  2.8169e+02]],

        [[ 1.5369e-01, -8.7812e-02, -1.4373e-01, -1.8722e+01],
         [-5.7192e-01, -8.1347e-01, -6.3865e-01, -1.5987e+01],
         [-1.3825e+00, -1.6841e+00, -2.2639e+00, -2.0766e+01],
         ...,
         [-2.9124e+01, -2.5149e+01, -2.2277e+01,  6.9710e+01],
         [-2.6844e+01, -2.2995e+01, -2.0467e+01,  7.9930e+01],
         [-2.7094e+01, -2.3192e+01, -2.0620e+01,  8.2844e+01]],

        [[-1.8388e+00, -1.8022e+00, -1.8190e+00,  1.9021e+01],
         [-3.9321e+00, -3.6103e+00, -3.7230e+00,  1.7951e+01],
         [-2.3603e+00, -2.0067e+00, -1.7202e+00, -4.2648e+01],
         ...,
         [ 1.8618e+00,  4.2180e+00,  1.7580e+01,  3.2012e+02],
         [ 4.5954e+00,  6.7676e+00,  2.0465e+01,  3.1128e+02],
         [ 4.3165e+00,  7.0554e+00,  2.4070e+01,  3.1315e+02]]],
       grad_fn=<ReshapeAliasBackward0>), 'rgb0': tensor([[0.5450, 0.5000, 0.4329],
        [0.5839, 0.5387, 0.4855],
        [0.4189, 0.4591, 0.4863],
        ...,
        [0.2857, 0.2796, 0.3103],
        [0.5111, 0.4098, 0.3465],
        [0.1426, 0.1436, 0.1568]], grad_fn=<ReshapeAliasBackward0>), 'disp0': tensor([1325.2240,   34.3032, 1644.8604,  ...,  874.2842,   33.1596,
        1060.1882], grad_fn=<ReshapeAliasBackward0>), 'acc0': tensor([1., 1., 1.,  ..., 1., 1., 1.], grad_fn=<ReshapeAliasBackward0>), 'z_std': tensor([0.1156, 0.0231, 0.1331,  ..., 0.2931, 0.0026, 0.2741])}
0 0.0006101131439208984
torch.Size([320, 640, 3]) torch.Size([320, 640])
1 14.365185022354126
2 14.423908233642578
3 14.439367294311523
4 14.421990633010864
5 14.422969818115234
6 14.38943600654602
7 14.377482414245605
8 14.395086526870728
9 14.391990184783936
10 14.419904232025146
11 14.365902662277222
12 14.40838360786438
13 14.423965692520142
14 14.405263900756836
15 14.396548509597778
16 14.397435903549194
17 14.395167112350464
18 14.401185035705566
19 14.397517681121826
20 14.377107381820679
21 14.375880718231201
22 14.395533800125122
23 14.384072303771973
24 14.359583616256714
25 14.375877141952515
26 14.392346382141113
27 14.412647485733032
28 14.393592596054077
29 14.383772850036621
30 14.405579090118408
31 14.388615608215332
32 14.333767175674438
33 14.403425216674805
34 14.405683279037476
35 14.423122882843018
36 14.385075330734253
37 14.380926847457886
38 14.419164180755615
39 14.394681453704834
40 14.397461891174316
41 14.440216779708862
42 14.363348960876465
43 14.390750408172607
44 14.390165090560913
45 14.427536725997925
46 14.410605907440186
47 14.417369365692139
48 14.424931526184082
49 14.413194417953491
50 14.401926040649414
51 14.42798924446106
52 14.396543979644775
53 14.405166864395142
54 14.37837553024292
55 14.398603439331055
56 14.390376567840576
57 14.38516116142273
58 14.394100427627563
59 14.387674808502197
60 14.438644409179688
61 14.375467538833618
62 14.391227960586548
63 14.374114990234375
64 14.40447187423706
65 14.377788066864014
66 14.410295724868774
67 14.401179552078247
68 14.391439437866211
69 14.391756534576416
70 14.37688422203064
71 14.377660989761353
72 14.3967866897583
73 14.412145376205444
74 14.25648307800293
75 14.41457486152649
76 14.382144451141357
77 14.393425941467285
78 14.366554498672485
79 14.379145860671997
80 14.37647819519043
81 14.404047727584839
82 14.366187334060669
83 14.37595009803772
84 14.34469723701477
85 14.425937175750732
86 14.38476014137268
87 14.407920122146606
88 14.369528770446777
89 14.393341064453125
90 14.389333963394165
91 14.393546104431152
92 14.455593347549438
93 14.416836977005005
94 14.402286052703857
95 14.419409275054932
96 14.407727241516113
97 14.430423259735107
98 14.404239416122437
99 14.429413795471191
100 14.400249242782593
101 14.399574279785156
102 14.415034055709839
103 14.408581018447876
104 14.40603518486023
105 14.391313552856445
106 14.39599871635437
107 14.372661113739014
108 14.402997970581055
109 14.413755416870117
110 14.437709331512451
111 14.41202998161316
112 14.417214155197144
113 14.419355869293213
114 14.402974605560303
115 14.357003688812256
116 14.39023208618164
117 14.470687866210938
118 14.408322811126709
119 14.419540882110596
test poses shape torch.Size([13, 3, 4])
0 0.0007195472717285156
torch.Size([320, 640, 3]) torch.Size([320, 640])
1 14.463884353637695
2 14.384554862976074
3 14.44293212890625
4 14.472075700759888
5 14.424226760864258
6 14.478445768356323
7 14.468358755111694
8 14.407346963882446
9 14.466404914855957
10 14.42648959159851
11 14.515367269515991
12 14.457350492477417
Saved test set
[TRAIN] Iter: 750000 Loss: 0.025738323107361794  PSNR: 19.100305557250977
[TRAIN] Iter: 750100 Loss: 0.020509421825408936  PSNR: 20.10626792907715
[TRAIN] Iter: 750200 Loss: 0.027634860947728157  PSNR: 18.798795700073242
[TRAIN] Iter: 750300 Loss: 0.021509665995836258  PSNR: 19.930295944213867
[TRAIN] Iter: 750400 Loss: 0.02795599028468132  PSNR: 18.76107406616211
[TRAIN] Iter: 750500 Loss: 0.02409687638282776  PSNR: 19.402263641357422
[TRAIN] Iter: 750600 Loss: 0.02454078570008278  PSNR: 19.40343475341797
[TRAIN] Iter: 750700 Loss: 0.031668953597545624  PSNR: 18.119251251220703
[TRAIN] Iter: 750800 Loss: 0.02500958740711212  PSNR: 19.093082427978516
[TRAIN] Iter: 750900 Loss: 0.033670708537101746  PSNR: 17.965686798095703
[TRAIN] Iter: 751000 Loss: 0.03007393144071102  PSNR: 18.703170776367188
[TRAIN] Iter: 751100 Loss: 0.018264517188072205  PSNR: 20.6082763671875
[TRAIN] Iter: 751200 Loss: 0.03129648417234421  PSNR: 18.245126724243164
[TRAIN] Iter: 751300 Loss: 0.024801086634397507  PSNR: 19.049373626708984
[TRAIN] Iter: 751400 Loss: 0.022954581305384636  PSNR: 19.377206802368164
[TRAIN] Iter: 751500 Loss: 0.026478592306375504  PSNR: 19.07847785949707
[TRAIN] Iter: 751600 Loss: 0.022609131410717964  PSNR: 19.701383590698242
[TRAIN] Iter: 751700 Loss: 0.02883625030517578  PSNR: 18.476825714111328
[TRAIN] Iter: 751800 Loss: 0.026320919394493103  PSNR: 19.19161605834961
[TRAIN] Iter: 751900 Loss: 0.03035990707576275  PSNR: 18.40650177001953
[TRAIN] Iter: 752000 Loss: 0.026273787021636963  PSNR: 19.022619247436523
[TRAIN] Iter: 752100 Loss: 0.02540394850075245  PSNR: 19.083398818969727
[TRAIN] Iter: 752200 Loss: 0.02630743384361267  PSNR: 18.997089385986328
[TRAIN] Iter: 752300 Loss: 0.025296131148934364  PSNR: 19.43767738342285
[TRAIN] Iter: 752400 Loss: 0.03240398317575455  PSNR: 18.35541343688965
[TRAIN] Iter: 752500 Loss: 0.020423024892807007  PSNR: 19.98261260986328
[TRAIN] Iter: 752600 Loss: 0.025592530146241188  PSNR: 18.989591598510742
[TRAIN] Iter: 752700 Loss: 0.028481479734182358  PSNR: 18.518951416015625
[TRAIN] Iter: 752800 Loss: 0.021106770262122154  PSNR: 20.161218643188477
[TRAIN] Iter: 752900 Loss: 0.03335949033498764  PSNR: 18.027435302734375
[TRAIN] Iter: 753000 Loss: 0.027860870584845543  PSNR: 18.70749855041504
[TRAIN] Iter: 753100 Loss: 0.024851642549037933  PSNR: 19.3646297454834
[TRAIN] Iter: 753200 Loss: 0.027128593996167183  PSNR: 19.0566349029541
[TRAIN] Iter: 753300 Loss: 0.031168561428785324  PSNR: 18.264930725097656
[TRAIN] Iter: 753400 Loss: 0.028067711740732193  PSNR: 18.558673858642578
[TRAIN] Iter: 753500 Loss: 0.021770047023892403  PSNR: 19.911792755126953
[TRAIN] Iter: 753600 Loss: 0.023292891681194305  PSNR: 19.3531436920166
[TRAIN] Iter: 753700 Loss: 0.023451214656233788  PSNR: 19.31948471069336
[TRAIN] Iter: 753800 Loss: 0.03283326327800751  PSNR: 17.999637603759766
[TRAIN] Iter: 753900 Loss: 0.02459827810525894  PSNR: 19.0911865234375
[TRAIN] Iter: 754000 Loss: 0.026005428284406662  PSNR: 19.162864685058594
[TRAIN] Iter: 754100 Loss: 0.026473291218280792  PSNR: 19.05579948425293
[TRAIN] Iter: 754200 Loss: 0.0313536711037159  PSNR: 18.238218307495117
[TRAIN] Iter: 754300 Loss: 0.030324436724185944  PSNR: 18.338380813598633
[TRAIN] Iter: 754400 Loss: 0.030005697160959244  PSNR: 18.40378189086914
[TRAIN] Iter: 754500 Loss: 0.022626100108027458  PSNR: 19.77111053466797
[TRAIN] Iter: 754600 Loss: 0.027172941714525223  PSNR: 18.946565628051758
[TRAIN] Iter: 754700 Loss: 0.025617437437176704  PSNR: 19.225360870361328
[TRAIN] Iter: 754800 Loss: 0.024727502837777138  PSNR: 19.137495040893555
[TRAIN] Iter: 754900 Loss: 0.028309501707553864  PSNR: 18.610727310180664
[TRAIN] Iter: 755000 Loss: 0.02521149441599846  PSNR: 18.86165428161621
[TRAIN] Iter: 755100 Loss: 0.03089604154229164  PSNR: 18.283849716186523
[TRAIN] Iter: 755200 Loss: 0.030357960611581802  PSNR: 18.352874755859375
[TRAIN] Iter: 755300 Loss: 0.02189907431602478  PSNR: 19.8272762298584
[TRAIN] Iter: 755400 Loss: 0.02897430583834648  PSNR: 18.517593383789062
[TRAIN] Iter: 755500 Loss: 0.0213515292853117  PSNR: 19.926801681518555
[TRAIN] Iter: 755600 Loss: 0.026836711913347244  PSNR: 18.967193603515625
[TRAIN] Iter: 755700 Loss: 0.028510544449090958  PSNR: 18.66069221496582
[TRAIN] Iter: 755800 Loss: 0.024168293923139572  PSNR: 19.25071144104004
[TRAIN] Iter: 755900 Loss: 0.026949476450681686  PSNR: 18.79084587097168
[TRAIN] Iter: 756000 Loss: 0.0269272830337286  PSNR: 18.951274871826172
[TRAIN] Iter: 756100 Loss: 0.03027888387441635  PSNR: 18.341205596923828
[TRAIN] Iter: 756200 Loss: 0.022624410688877106  PSNR: 19.631662368774414
[TRAIN] Iter: 756300 Loss: 0.02683146297931671  PSNR: 18.98393440246582
[TRAIN] Iter: 756400 Loss: 0.03288712352514267  PSNR: 18.064592361450195
[TRAIN] Iter: 756500 Loss: 0.033127717673778534  PSNR: 18.001510620117188
[TRAIN] Iter: 756600 Loss: 0.02970184199512005  PSNR: 18.524429321289062
[TRAIN] Iter: 756700 Loss: 0.02208053693175316  PSNR: 19.744705200195312
[TRAIN] Iter: 756800 Loss: 0.01924782618880272  PSNR: 20.432247161865234
[TRAIN] Iter: 756900 Loss: 0.03311385214328766  PSNR: 17.9550724029541
[TRAIN] Iter: 757000 Loss: 0.026434341445565224  PSNR: 19.080684661865234
[TRAIN] Iter: 757100 Loss: 0.02095925807952881  PSNR: 19.96739959716797
[TRAIN] Iter: 757200 Loss: 0.03176618367433548  PSNR: 18.197032928466797
[TRAIN] Iter: 757300 Loss: 0.026185397058725357  PSNR: 18.851293563842773
[TRAIN] Iter: 757400 Loss: 0.02542657032608986  PSNR: 19.405941009521484
[TRAIN] Iter: 757500 Loss: 0.028739653527736664  PSNR: 18.506450653076172
[TRAIN] Iter: 757600 Loss: 0.024480324238538742  PSNR: 19.02688980102539
[TRAIN] Iter: 757700 Loss: 0.02662537805736065  PSNR: 18.847278594970703
[TRAIN] Iter: 757800 Loss: 0.025748787447810173  PSNR: 19.174545288085938
[TRAIN] Iter: 757900 Loss: 0.026066817343235016  PSNR: 19.326091766357422
[TRAIN] Iter: 758000 Loss: 0.02475031465291977  PSNR: 19.662633895874023
[TRAIN] Iter: 758100 Loss: 0.031896915286779404  PSNR: 18.070072174072266
[TRAIN] Iter: 758200 Loss: 0.02188260480761528  PSNR: 19.89886474609375
[TRAIN] Iter: 758300 Loss: 0.02080494910478592  PSNR: 20.113767623901367
[TRAIN] Iter: 758400 Loss: 0.031078346073627472  PSNR: 18.279659271240234
[TRAIN] Iter: 758500 Loss: 0.02382928878068924  PSNR: 19.25432014465332
[TRAIN] Iter: 758600 Loss: 0.02555915340781212  PSNR: 18.960132598876953
[TRAIN] Iter: 758700 Loss: 0.02773074433207512  PSNR: 18.914981842041016
[TRAIN] Iter: 758800 Loss: 0.026611309498548508  PSNR: 18.465179443359375
[TRAIN] Iter: 758900 Loss: 0.02389218844473362  PSNR: 19.37007713317871
[TRAIN] Iter: 759000 Loss: 0.03039480373263359  PSNR: 18.398754119873047
[TRAIN] Iter: 759100 Loss: 0.020660070702433586  PSNR: 20.05852508544922
[TRAIN] Iter: 759200 Loss: 0.024701742455363274  PSNR: 19.15193748474121
[TRAIN] Iter: 759300 Loss: 0.02416299283504486  PSNR: 19.514432907104492
[TRAIN] Iter: 759400 Loss: 0.03103620558977127  PSNR: 18.432315826416016
[TRAIN] Iter: 759500 Loss: 0.025630168616771698  PSNR: 19.28143310546875
[TRAIN] Iter: 759600 Loss: 0.027558183297514915  PSNR: 18.851083755493164
[TRAIN] Iter: 759700 Loss: 0.02439219504594803  PSNR: 19.17044448852539
[TRAIN] Iter: 759800 Loss: 0.02569608762860298  PSNR: 19.277652740478516
[TRAIN] Iter: 759900 Loss: 0.025849349796772003  PSNR: 18.922714233398438
Saved checkpoints at ./logs/TUT-LAB-nerf/760000.tar
[TRAIN] Iter: 760000 Loss: 0.02115609124302864  PSNR: 19.93433952331543
[TRAIN] Iter: 760100 Loss: 0.027535399422049522  PSNR: 18.828807830810547
[TRAIN] Iter: 760200 Loss: 0.02228327840566635  PSNR: 19.693605422973633
[TRAIN] Iter: 760300 Loss: 0.02522243931889534  PSNR: 19.136762619018555
[TRAIN] Iter: 760400 Loss: 0.025036407634615898  PSNR: 19.577449798583984
[TRAIN] Iter: 760500 Loss: 0.02390320971608162  PSNR: 19.42026710510254
[TRAIN] Iter: 760600 Loss: 0.02863239496946335  PSNR: 18.718128204345703
[TRAIN] Iter: 760700 Loss: 0.027802173048257828  PSNR: 19.302940368652344
[TRAIN] Iter: 760800 Loss: 0.026791926473379135  PSNR: 18.705413818359375
[TRAIN] Iter: 760900 Loss: 0.030188094824552536  PSNR: 18.290903091430664
[TRAIN] Iter: 761000 Loss: 0.029299337416887283  PSNR: 18.459409713745117
[TRAIN] Iter: 761100 Loss: 0.0254395492374897  PSNR: 19.248807907104492
[TRAIN] Iter: 761200 Loss: 0.02630385011434555  PSNR: 19.188196182250977
[TRAIN] Iter: 761300 Loss: 0.025289133191108704  PSNR: 19.267324447631836
[TRAIN] Iter: 761400 Loss: 0.030561096966266632  PSNR: 18.4617919921875
[TRAIN] Iter: 761500 Loss: 0.031625427305698395  PSNR: 18.191865921020508
[TRAIN] Iter: 761600 Loss: 0.028017042204737663  PSNR: 18.60133934020996
[TRAIN] Iter: 761700 Loss: 0.025367990136146545  PSNR: 19.28145980834961
[TRAIN] Iter: 761800 Loss: 0.026054617017507553  PSNR: 19.408510208129883
[TRAIN] Iter: 761900 Loss: 0.025293398648500443  PSNR: 19.196521759033203
[TRAIN] Iter: 762000 Loss: 0.029300179332494736  PSNR: 18.48196792602539
[TRAIN] Iter: 762100 Loss: 0.023912647739052773  PSNR: 19.337696075439453
[TRAIN] Iter: 762200 Loss: 0.025458645075559616  PSNR: 19.287582397460938
[TRAIN] Iter: 762300 Loss: 0.027937566861510277  PSNR: 18.53029441833496
[TRAIN] Iter: 762400 Loss: 0.028670094907283783  PSNR: 18.597370147705078
[TRAIN] Iter: 762500 Loss: 0.02411525510251522  PSNR: 19.184194564819336
[TRAIN] Iter: 762600 Loss: 0.030557386577129364  PSNR: 18.201416015625
[TRAIN] Iter: 762700 Loss: 0.024585507810115814  PSNR: 19.294918060302734
[TRAIN] Iter: 762800 Loss: 0.030064139515161514  PSNR: 18.58696746826172
[TRAIN] Iter: 762900 Loss: 0.03194302320480347  PSNR: 18.14890480041504
[TRAIN] Iter: 763000 Loss: 0.02606009878218174  PSNR: 19.12171745300293
[TRAIN] Iter: 763100 Loss: 0.026420731097459793  PSNR: 19.27370262145996
[TRAIN] Iter: 763200 Loss: 0.026456404477357864  PSNR: 19.114755630493164
[TRAIN] Iter: 763300 Loss: 0.023643750697374344  PSNR: 19.28253936767578
[TRAIN] Iter: 763400 Loss: 0.032377324998378754  PSNR: 17.985782623291016
[TRAIN] Iter: 763500 Loss: 0.029820211231708527  PSNR: 18.402189254760742
[TRAIN] Iter: 763600 Loss: 0.026006847620010376  PSNR: 19.063308715820312
[TRAIN] Iter: 763700 Loss: 0.025352608412504196  PSNR: 19.314603805541992
[TRAIN] Iter: 763800 Loss: 0.026775218546390533  PSNR: 19.025800704956055
[TRAIN] Iter: 763900 Loss: 0.026502301916480064  PSNR: 18.83622932434082
[TRAIN] Iter: 764000 Loss: 0.02941880002617836  PSNR: 18.474802017211914
[TRAIN] Iter: 764100 Loss: 0.02423267811536789  PSNR: 19.327091217041016
[TRAIN] Iter: 764200 Loss: 0.035126350820064545  PSNR: 17.55461883544922
[TRAIN] Iter: 764300 Loss: 0.02443087473511696  PSNR: 19.844141006469727
[TRAIN] Iter: 764400 Loss: 0.02478352189064026  PSNR: 19.595258712768555
[TRAIN] Iter: 764500 Loss: 0.02666907012462616  PSNR: 18.840280532836914
[TRAIN] Iter: 764600 Loss: 0.03014366701245308  PSNR: 18.354074478149414
[TRAIN] Iter: 764700 Loss: 0.023669887334108353  PSNR: 19.16970443725586
[TRAIN] Iter: 764800 Loss: 0.02466643787920475  PSNR: 19.194087982177734
[TRAIN] Iter: 764900 Loss: 0.0265938900411129  PSNR: 18.808334350585938
[TRAIN] Iter: 765000 Loss: 0.02507077157497406  PSNR: 19.040740966796875
[TRAIN] Iter: 765100 Loss: 0.01792469434440136  PSNR: 20.732086181640625
[TRAIN] Iter: 765200 Loss: 0.02825254015624523  PSNR: 18.606943130493164
[TRAIN] Iter: 765300 Loss: 0.02381524257361889  PSNR: 19.49411964416504
[TRAIN] Iter: 765400 Loss: 0.029549360275268555  PSNR: 18.46497344970703
[TRAIN] Iter: 765500 Loss: 0.030122317373752594  PSNR: 18.320354461669922
[TRAIN] Iter: 765600 Loss: 0.02804959937930107  PSNR: 18.788997650146484
[TRAIN] Iter: 765700 Loss: 0.02377273142337799  PSNR: 19.241750717163086
[TRAIN] Iter: 765800 Loss: 0.028008967638015747  PSNR: 18.626169204711914
[TRAIN] Iter: 765900 Loss: 0.024806631729006767  PSNR: 19.319889068603516
[TRAIN] Iter: 766000 Loss: 0.02471049502491951  PSNR: 19.231700897216797
[TRAIN] Iter: 766100 Loss: 0.027261510491371155  PSNR: 18.84501838684082
[TRAIN] Iter: 766200 Loss: 0.02933293953537941  PSNR: 18.495864868164062
[TRAIN] Iter: 766300 Loss: 0.019198406487703323  PSNR: 20.28591537475586
[TRAIN] Iter: 766400 Loss: 0.031010601669549942  PSNR: 18.260601043701172
[TRAIN] Iter: 766500 Loss: 0.031223682686686516  PSNR: 18.51278305053711
[TRAIN] Iter: 766600 Loss: 0.030126355588436127  PSNR: 18.34488868713379
[TRAIN] Iter: 766700 Loss: 0.02043108269572258  PSNR: 20.08766746520996
[TRAIN] Iter: 766800 Loss: 0.023728132247924805  PSNR: 19.538818359375
[TRAIN] Iter: 766900 Loss: 0.024343285709619522  PSNR: 19.100122451782227
[TRAIN] Iter: 767000 Loss: 0.026638304814696312  PSNR: 19.08576202392578
[TRAIN] Iter: 767100 Loss: 0.02620181255042553  PSNR: 18.977481842041016
[TRAIN] Iter: 767200 Loss: 0.029812108725309372  PSNR: 18.56208038330078
[TRAIN] Iter: 767300 Loss: 0.02854991890490055  PSNR: 19.026988983154297
[TRAIN] Iter: 767400 Loss: 0.021582048386335373  PSNR: 19.885887145996094
[TRAIN] Iter: 767500 Loss: 0.026632364839315414  PSNR: 19.186742782592773
[TRAIN] Iter: 767600 Loss: 0.02761469967663288  PSNR: 18.8382625579834
[TRAIN] Iter: 767700 Loss: 0.025275660678744316  PSNR: 19.11351203918457
[TRAIN] Iter: 767800 Loss: 0.026205800473690033  PSNR: 19.223743438720703
[TRAIN] Iter: 767900 Loss: 0.02471442148089409  PSNR: 19.08725929260254
[TRAIN] Iter: 768000 Loss: 0.031228849664330482  PSNR: 18.253826141357422
[TRAIN] Iter: 768100 Loss: 0.027348307892680168  PSNR: 19.011354446411133
[TRAIN] Iter: 768200 Loss: 0.024671390652656555  PSNR: 19.48650550842285
[TRAIN] Iter: 768300 Loss: 0.028844818472862244  PSNR: 18.708438873291016
[TRAIN] Iter: 768400 Loss: 0.03233975172042847  PSNR: 18.06975555419922
[TRAIN] Iter: 768500 Loss: 0.023511061444878578  PSNR: 19.596826553344727
[TRAIN] Iter: 768600 Loss: 0.03295532613992691  PSNR: 18.108339309692383
[TRAIN] Iter: 768700 Loss: 0.025452833622694016  PSNR: 19.304187774658203
[TRAIN] Iter: 768800 Loss: 0.02876337431371212  PSNR: 18.559110641479492
[TRAIN] Iter: 768900 Loss: 0.025918377563357353  PSNR: 19.058746337890625
[TRAIN] Iter: 769000 Loss: 0.030031155794858932  PSNR: 18.411657333374023
[TRAIN] Iter: 769100 Loss: 0.03314429521560669  PSNR: 17.96605682373047
[TRAIN] Iter: 769200 Loss: 0.022111494094133377  PSNR: 19.838605880737305
[TRAIN] Iter: 769300 Loss: 0.029880760237574577  PSNR: 18.453519821166992
[TRAIN] Iter: 769400 Loss: 0.03344298154115677  PSNR: 17.897367477416992
[TRAIN] Iter: 769500 Loss: 0.02454419434070587  PSNR: 19.219146728515625
[TRAIN] Iter: 769600 Loss: 0.02872546575963497  PSNR: 19.061080932617188
[TRAIN] Iter: 769700 Loss: 0.03202551603317261  PSNR: 18.126155853271484
[TRAIN] Iter: 769800 Loss: 0.023924512788653374  PSNR: 19.377843856811523
[TRAIN] Iter: 769900 Loss: 0.029329989105463028  PSNR: 18.487741470336914
Saved checkpoints at ./logs/TUT-LAB-nerf/770000.tar
[TRAIN] Iter: 770000 Loss: 0.02843066118657589  PSNR: 18.61931800842285
[TRAIN] Iter: 770100 Loss: 0.02195664867758751  PSNR: 20.216753005981445
[TRAIN] Iter: 770200 Loss: 0.027385465800762177  PSNR: 18.812963485717773
[TRAIN] Iter: 770300 Loss: 0.022474810481071472  PSNR: 19.895627975463867
[TRAIN] Iter: 770400 Loss: 0.024602510035037994  PSNR: 18.869279861450195
[TRAIN] Iter: 770500 Loss: 0.03132345899939537  PSNR: 18.314699172973633
[TRAIN] Iter: 770600 Loss: 0.028300262987613678  PSNR: 18.645126342773438
[TRAIN] Iter: 770700 Loss: 0.025167737156152725  PSNR: 19.396085739135742
[TRAIN] Iter: 770800 Loss: 0.028818555176258087  PSNR: 18.488039016723633
[TRAIN] Iter: 770900 Loss: 0.020966317504644394  PSNR: 19.66853904724121
[TRAIN] Iter: 771000 Loss: 0.03185950219631195  PSNR: 18.131187438964844
[TRAIN] Iter: 771100 Loss: 0.021915502846240997  PSNR: 19.987646102905273
[TRAIN] Iter: 771200 Loss: 0.02388400211930275  PSNR: 19.352201461791992
[TRAIN] Iter: 771300 Loss: 0.022092368453741074  PSNR: 19.67252540588379
[TRAIN] Iter: 771400 Loss: 0.028356213122606277  PSNR: 18.655071258544922
[TRAIN] Iter: 771500 Loss: 0.03146562725305557  PSNR: 18.20000648498535
[TRAIN] Iter: 771600 Loss: 0.021443119272589684  PSNR: 19.823017120361328
[TRAIN] Iter: 771700 Loss: 0.019870780408382416  PSNR: 20.196447372436523
[TRAIN] Iter: 771800 Loss: 0.03000805154442787  PSNR: 18.32029914855957
[TRAIN] Iter: 771900 Loss: 0.02388499677181244  PSNR: 19.46537971496582
[TRAIN] Iter: 772000 Loss: 0.021172959357500076  PSNR: 19.89755630493164
[TRAIN] Iter: 772100 Loss: 0.031129959970712662  PSNR: 18.259950637817383
[TRAIN] Iter: 772200 Loss: 0.03523535653948784  PSNR: 17.673959732055664
[TRAIN] Iter: 772300 Loss: 0.019515886902809143  PSNR: 20.38675880432129
[TRAIN] Iter: 772400 Loss: 0.02958519756793976  PSNR: 18.505043029785156
[TRAIN] Iter: 772500 Loss: 0.032143790274858475  PSNR: 18.165849685668945
[TRAIN] Iter: 772600 Loss: 0.03132627159357071  PSNR: 18.308446884155273
[TRAIN] Iter: 772700 Loss: 0.03312988206744194  PSNR: 18.132020950317383
[TRAIN] Iter: 772800 Loss: 0.024992194026708603  PSNR: 18.893632888793945
[TRAIN] Iter: 772900 Loss: 0.019623950123786926  PSNR: 20.29257583618164
[TRAIN] Iter: 773000 Loss: 0.024128124117851257  PSNR: 19.317657470703125
[TRAIN] Iter: 773100 Loss: 0.031428392976522446  PSNR: 18.34959602355957
[TRAIN] Iter: 773200 Loss: 0.028679532930254936  PSNR: 18.52902603149414
[TRAIN] Iter: 773300 Loss: 0.022637581452727318  PSNR: 19.915586471557617
[TRAIN] Iter: 773400 Loss: 0.027487684041261673  PSNR: 18.96579360961914
[TRAIN] Iter: 773500 Loss: 0.0258328840136528  PSNR: 19.26512336730957
[TRAIN] Iter: 773600 Loss: 0.02387947589159012  PSNR: 19.523462295532227
[TRAIN] Iter: 773700 Loss: 0.0285282451659441  PSNR: 18.480087280273438
[TRAIN] Iter: 773800 Loss: 0.03208991140127182  PSNR: 18.308780670166016
[TRAIN] Iter: 773900 Loss: 0.02615286037325859  PSNR: 18.941892623901367
[TRAIN] Iter: 774000 Loss: 0.022399138659238815  PSNR: 19.336957931518555
[TRAIN] Iter: 774100 Loss: 0.030893078073859215  PSNR: 18.236377716064453
[TRAIN] Iter: 774200 Loss: 0.026138346642255783  PSNR: 19.18002700805664
[TRAIN] Iter: 774300 Loss: 0.03193831443786621  PSNR: 18.086275100708008
[TRAIN] Iter: 774400 Loss: 0.028477514162659645  PSNR: 18.57270622253418
[TRAIN] Iter: 774500 Loss: 0.02764563448727131  PSNR: 18.89560317993164
[TRAIN] Iter: 774600 Loss: 0.03000827692449093  PSNR: 18.344194412231445
[TRAIN] Iter: 774700 Loss: 0.03301366791129112  PSNR: 18.184375762939453
[TRAIN] Iter: 774800 Loss: 0.018914468586444855  PSNR: 20.448741912841797
[TRAIN] Iter: 774900 Loss: 0.028697725385427475  PSNR: 18.603078842163086
[TRAIN] Iter: 775000 Loss: 0.02824956178665161  PSNR: 18.72901725769043
[TRAIN] Iter: 775100 Loss: 0.032613709568977356  PSNR: 17.93575096130371
[TRAIN] Iter: 775200 Loss: 0.02725154533982277  PSNR: 18.919401168823242
[TRAIN] Iter: 775300 Loss: 0.023103829473257065  PSNR: 19.591230392456055
[TRAIN] Iter: 775400 Loss: 0.022491609677672386  PSNR: 19.681184768676758
[TRAIN] Iter: 775500 Loss: 0.02046125754714012  PSNR: 20.156024932861328
[TRAIN] Iter: 775600 Loss: 0.024142730981111526  PSNR: 19.142452239990234
[TRAIN] Iter: 775700 Loss: 0.027440084144473076  PSNR: 18.490785598754883
[TRAIN] Iter: 775800 Loss: 0.025013845413923264  PSNR: 19.200040817260742
[TRAIN] Iter: 775900 Loss: 0.02634338103234768  PSNR: 18.90012550354004
[TRAIN] Iter: 776000 Loss: 0.030851732939481735  PSNR: 18.334531784057617
[TRAIN] Iter: 776100 Loss: 0.027987170964479446  PSNR: 18.733951568603516
[TRAIN] Iter: 776200 Loss: 0.03191232681274414  PSNR: 18.189504623413086
[TRAIN] Iter: 776300 Loss: 0.021406657993793488  PSNR: 19.80330467224121
[TRAIN] Iter: 776400 Loss: 0.03257285803556442  PSNR: 18.14508819580078
[TRAIN] Iter: 776500 Loss: 0.023314958438277245  PSNR: 19.417116165161133
[TRAIN] Iter: 776600 Loss: 0.03202579915523529  PSNR: 18.009225845336914
[TRAIN] Iter: 776700 Loss: 0.029592525213956833  PSNR: 18.529253005981445
[TRAIN] Iter: 776800 Loss: 0.029132835566997528  PSNR: 18.49335289001465
[TRAIN] Iter: 776900 Loss: 0.028705216944217682  PSNR: 18.70228385925293
[TRAIN] Iter: 777000 Loss: 0.0288703553378582  PSNR: 18.523651123046875
[TRAIN] Iter: 777100 Loss: 0.026392862200737  PSNR: 18.90442657470703
[TRAIN] Iter: 777200 Loss: 0.023050351068377495  PSNR: 19.619014739990234
[TRAIN] Iter: 777300 Loss: 0.0258488692343235  PSNR: 19.34579086303711
[TRAIN] Iter: 777400 Loss: 0.02319944277405739  PSNR: 19.62473487854004
[TRAIN] Iter: 777500 Loss: 0.029779179021716118  PSNR: 18.64885711669922
[TRAIN] Iter: 777600 Loss: 0.026853540912270546  PSNR: 18.763355255126953
[TRAIN] Iter: 777700 Loss: 0.022668464109301567  PSNR: 19.700944900512695
[TRAIN] Iter: 777800 Loss: 0.029766149818897247  PSNR: 18.547815322875977
[TRAIN] Iter: 777900 Loss: 0.02540215104818344  PSNR: 19.233367919921875
[TRAIN] Iter: 778000 Loss: 0.02452767640352249  PSNR: 19.276857376098633
[TRAIN] Iter: 778100 Loss: 0.02839655801653862  PSNR: 18.58007049560547
[TRAIN] Iter: 778200 Loss: 0.02887674793601036  PSNR: 18.93869400024414
[TRAIN] Iter: 778300 Loss: 0.028703391551971436  PSNR: 18.62348175048828
[TRAIN] Iter: 778400 Loss: 0.02443601004779339  PSNR: 19.426124572753906
[TRAIN] Iter: 778500 Loss: 0.02959226444363594  PSNR: 18.484045028686523
[TRAIN] Iter: 778600 Loss: 0.03208789974451065  PSNR: 18.102924346923828
[TRAIN] Iter: 778700 Loss: 0.0322590097784996  PSNR: 18.103670120239258
[TRAIN] Iter: 778800 Loss: 0.022709038108587265  PSNR: 19.60323143005371
[TRAIN] Iter: 778900 Loss: 0.02485169842839241  PSNR: 19.06766128540039
[TRAIN] Iter: 779000 Loss: 0.031641870737075806  PSNR: 18.130401611328125
[TRAIN] Iter: 779100 Loss: 0.02862272784113884  PSNR: 18.556501388549805
[TRAIN] Iter: 779200 Loss: 0.02408914640545845  PSNR: 19.368648529052734
[TRAIN] Iter: 779300 Loss: 0.029188398271799088  PSNR: 18.596012115478516
[TRAIN] Iter: 779400 Loss: 0.029013536870479584  PSNR: 18.82269859313965
[TRAIN] Iter: 779500 Loss: 0.025179240852594376  PSNR: 19.293012619018555
[TRAIN] Iter: 779600 Loss: 0.026375599205493927  PSNR: 19.09705924987793
[TRAIN] Iter: 779700 Loss: 0.025987137109041214  PSNR: 19.08092498779297
[TRAIN] Iter: 779800 Loss: 0.02275410294532776  PSNR: 19.32741355895996
[TRAIN] Iter: 779900 Loss: 0.024141833186149597  PSNR: 19.24679183959961
Saved checkpoints at ./logs/TUT-LAB-nerf/780000.tar
[TRAIN] Iter: 780000 Loss: 0.027509596198797226  PSNR: 18.771835327148438
[TRAIN] Iter: 780100 Loss: 0.025516441091895103  PSNR: 19.42629051208496
[TRAIN] Iter: 780200 Loss: 0.023676756769418716  PSNR: 19.443540573120117
[TRAIN] Iter: 780300 Loss: 0.023213721811771393  PSNR: 19.600811004638672
[TRAIN] Iter: 780400 Loss: 0.030017035081982613  PSNR: 18.446895599365234
[TRAIN] Iter: 780500 Loss: 0.02271980233490467  PSNR: 19.36452293395996
[TRAIN] Iter: 780600 Loss: 0.027659043669700623  PSNR: 18.67528533935547
[TRAIN] Iter: 780700 Loss: 0.025944234803318977  PSNR: 19.190105438232422
[TRAIN] Iter: 780800 Loss: 0.024161094799637794  PSNR: 19.39716339111328
[TRAIN] Iter: 780900 Loss: 0.03373682126402855  PSNR: 17.843849182128906
[TRAIN] Iter: 781000 Loss: 0.024269476532936096  PSNR: 19.164350509643555
[TRAIN] Iter: 781100 Loss: 0.022710397839546204  PSNR: 19.578214645385742
[TRAIN] Iter: 781200 Loss: 0.020712530240416527  PSNR: 20.076940536499023
[TRAIN] Iter: 781300 Loss: 0.03284579515457153  PSNR: 18.051599502563477
[TRAIN] Iter: 781400 Loss: 0.025285251438617706  PSNR: 19.24832534790039
[TRAIN] Iter: 781500 Loss: 0.028161413967609406  PSNR: 18.6982421875
[TRAIN] Iter: 781600 Loss: 0.021900054067373276  PSNR: 19.71097183227539
[TRAIN] Iter: 781700 Loss: 0.028069298714399338  PSNR: 18.74858856201172
[TRAIN] Iter: 781800 Loss: 0.029419388622045517  PSNR: 18.51451873779297
[TRAIN] Iter: 781900 Loss: 0.024723827838897705  PSNR: 19.342361450195312
[TRAIN] Iter: 782000 Loss: 0.0258558951318264  PSNR: 18.907623291015625
[TRAIN] Iter: 782100 Loss: 0.0265305545181036  PSNR: 18.460779190063477
[TRAIN] Iter: 782200 Loss: 0.029918383806943893  PSNR: 18.470577239990234
[TRAIN] Iter: 782300 Loss: 0.0260164737701416  PSNR: 19.05670166015625
[TRAIN] Iter: 782400 Loss: 0.025491073727607727  PSNR: 19.023527145385742
[TRAIN] Iter: 782500 Loss: 0.02575407363474369  PSNR: 18.788175582885742
[TRAIN] Iter: 782600 Loss: 0.026308394968509674  PSNR: 19.017181396484375
[TRAIN] Iter: 782700 Loss: 0.02558916248381138  PSNR: 19.404834747314453
[TRAIN] Iter: 782800 Loss: 0.027292447164654732  PSNR: 18.821456909179688
[TRAIN] Iter: 782900 Loss: 0.02835150808095932  PSNR: 18.780996322631836
[TRAIN] Iter: 783000 Loss: 0.03074675053358078  PSNR: 18.296630859375
[TRAIN] Iter: 783100 Loss: 0.02057677134871483  PSNR: 19.978111267089844
[TRAIN] Iter: 783200 Loss: 0.0268930122256279  PSNR: 18.687196731567383
[TRAIN] Iter: 783300 Loss: 0.024924777448177338  PSNR: 19.092288970947266
[TRAIN] Iter: 783400 Loss: 0.02899404801428318  PSNR: 18.405601501464844
[TRAIN] Iter: 783500 Loss: 0.03276059776544571  PSNR: 17.991466522216797
[TRAIN] Iter: 783600 Loss: 0.031725525856018066  PSNR: 18.174707412719727
[TRAIN] Iter: 783700 Loss: 0.027332544326782227  PSNR: 18.97295570373535
[TRAIN] Iter: 783800 Loss: 0.03178006038069725  PSNR: 18.121234893798828
[TRAIN] Iter: 783900 Loss: 0.026085076853632927  PSNR: 18.98158073425293
[TRAIN] Iter: 784000 Loss: 0.01982010342180729  PSNR: 20.37301254272461
[TRAIN] Iter: 784100 Loss: 0.022169187664985657  PSNR: 19.683101654052734
[TRAIN] Iter: 784200 Loss: 0.026256632059812546  PSNR: 18.798398971557617
[TRAIN] Iter: 784300 Loss: 0.026499180123209953  PSNR: 19.099506378173828
[TRAIN] Iter: 784400 Loss: 0.03231649100780487  PSNR: 18.18210792541504
[TRAIN] Iter: 784500 Loss: 0.028378084301948547  PSNR: 18.684663772583008
[TRAIN] Iter: 784600 Loss: 0.02277994528412819  PSNR: 19.589954376220703
[TRAIN] Iter: 784700 Loss: 0.03036491945385933  PSNR: 18.410316467285156
[TRAIN] Iter: 784800 Loss: 0.021760929375886917  PSNR: 19.74851417541504
[TRAIN] Iter: 784900 Loss: 0.0252088513225317  PSNR: 19.341318130493164
[TRAIN] Iter: 785000 Loss: 0.022889191284775734  PSNR: 19.449716567993164
[TRAIN] Iter: 785100 Loss: 0.026060499250888824  PSNR: 18.95167350769043
[TRAIN] Iter: 785200 Loss: 0.028003448620438576  PSNR: 19.185256958007812
[TRAIN] Iter: 785300 Loss: 0.02446134015917778  PSNR: 19.57924461364746
[TRAIN] Iter: 785400 Loss: 0.027337025851011276  PSNR: 19.06694984436035
[TRAIN] Iter: 785500 Loss: 0.029295094311237335  PSNR: 18.495868682861328
[TRAIN] Iter: 785600 Loss: 0.02898886427283287  PSNR: 18.471620559692383
[TRAIN] Iter: 785700 Loss: 0.031945694237947464  PSNR: 18.004106521606445
[TRAIN] Iter: 785800 Loss: 0.03078305721282959  PSNR: 18.287214279174805
[TRAIN] Iter: 785900 Loss: 0.028328843414783478  PSNR: 18.63486671447754
[TRAIN] Iter: 786000 Loss: 0.02615683153271675  PSNR: 18.67930030822754
[TRAIN] Iter: 786100 Loss: 0.028103038668632507  PSNR: 19.149288177490234
[TRAIN] Iter: 786200 Loss: 0.027629030868411064  PSNR: 18.727493286132812
[TRAIN] Iter: 786300 Loss: 0.02713056653738022  PSNR: 18.656343460083008
[TRAIN] Iter: 786400 Loss: 0.02412542700767517  PSNR: 18.896955490112305
[TRAIN] Iter: 786500 Loss: 0.028730275109410286  PSNR: 18.62879180908203
[TRAIN] Iter: 786600 Loss: 0.022773273289203644  PSNR: 19.75444793701172
[TRAIN] Iter: 786700 Loss: 0.025678997859358788  PSNR: 19.03978157043457
[TRAIN] Iter: 786800 Loss: 0.029726844280958176  PSNR: 18.440370559692383
[TRAIN] Iter: 786900 Loss: 0.02843218669295311  PSNR: 18.5589656829834
[TRAIN] Iter: 787000 Loss: 0.029262717813253403  PSNR: 18.48175621032715
[TRAIN] Iter: 787100 Loss: 0.023306671530008316  PSNR: 19.704219818115234
[TRAIN] Iter: 787200 Loss: 0.024826448410749435  PSNR: 19.366273880004883
[TRAIN] Iter: 787300 Loss: 0.028734488412737846  PSNR: 18.58683967590332
[TRAIN] Iter: 787400 Loss: 0.02759234607219696  PSNR: 18.760526657104492
[TRAIN] Iter: 787500 Loss: 0.03132522851228714  PSNR: 18.215015411376953
[TRAIN] Iter: 787600 Loss: 0.023486964404582977  PSNR: 19.417116165161133
[TRAIN] Iter: 787700 Loss: 0.025073744356632233  PSNR: 19.156192779541016
[TRAIN] Iter: 787800 Loss: 0.03315295651555061  PSNR: 18.08888816833496
[TRAIN] Iter: 787900 Loss: 0.029561569914221764  PSNR: 18.618616104125977
[TRAIN] Iter: 788000 Loss: 0.025866011157631874  PSNR: 19.134756088256836
[TRAIN] Iter: 788100 Loss: 0.02721869945526123  PSNR: 18.964012145996094
[TRAIN] Iter: 788200 Loss: 0.02256288379430771  PSNR: 19.31843376159668
[TRAIN] Iter: 788300 Loss: 0.0249030701816082  PSNR: 19.416271209716797
[TRAIN] Iter: 788400 Loss: 0.02993081696331501  PSNR: 18.385671615600586
[TRAIN] Iter: 788500 Loss: 0.02009371668100357  PSNR: 20.301626205444336
[TRAIN] Iter: 788600 Loss: 0.02401968464255333  PSNR: 19.529830932617188
[TRAIN] Iter: 788700 Loss: 0.029914896935224533  PSNR: 18.358686447143555
[TRAIN] Iter: 788800 Loss: 0.026151414960622787  PSNR: 19.005285263061523
[TRAIN] Iter: 788900 Loss: 0.02540013939142227  PSNR: 18.939435958862305
[TRAIN] Iter: 789000 Loss: 0.022947128862142563  PSNR: 19.303800582885742
[TRAIN] Iter: 789100 Loss: 0.03359834477305412  PSNR: 17.89670753479004
[TRAIN] Iter: 789200 Loss: 0.031274110078811646  PSNR: 18.22352409362793
[TRAIN] Iter: 789300 Loss: 0.02255946770310402  PSNR: 19.58293914794922
[TRAIN] Iter: 789400 Loss: 0.028535785153508186  PSNR: 18.850990295410156
[TRAIN] Iter: 789500 Loss: 0.02909458614885807  PSNR: 18.534547805786133
[TRAIN] Iter: 789600 Loss: 0.02683204784989357  PSNR: 18.94662094116211
[TRAIN] Iter: 789700 Loss: 0.027004394680261612  PSNR: 18.947818756103516
[TRAIN] Iter: 789800 Loss: 0.02385057881474495  PSNR: 19.041278839111328
[TRAIN] Iter: 789900 Loss: 0.03406637907028198  PSNR: 17.810897827148438
Saved checkpoints at ./logs/TUT-LAB-nerf/790000.tar
[TRAIN] Iter: 790000 Loss: 0.023856617510318756  PSNR: 19.32648468017578
[TRAIN] Iter: 790100 Loss: 0.028638066723942757  PSNR: 18.53990936279297
[TRAIN] Iter: 790200 Loss: 0.03029579669237137  PSNR: 18.275909423828125
[TRAIN] Iter: 790300 Loss: 0.02021443098783493  PSNR: 20.057891845703125
[TRAIN] Iter: 790400 Loss: 0.029631342738866806  PSNR: 18.34946632385254
[TRAIN] Iter: 790500 Loss: 0.02431553229689598  PSNR: 19.41582489013672
[TRAIN] Iter: 790600 Loss: 0.026384849101305008  PSNR: 18.993770599365234
[TRAIN] Iter: 790700 Loss: 0.02912217564880848  PSNR: 18.502487182617188
[TRAIN] Iter: 790800 Loss: 0.033673860132694244  PSNR: 17.828325271606445
[TRAIN] Iter: 790900 Loss: 0.02222958207130432  PSNR: 19.659881591796875
[TRAIN] Iter: 791000 Loss: 0.029992401599884033  PSNR: 18.72616958618164
[TRAIN] Iter: 791100 Loss: 0.03417772799730301  PSNR: 17.77382469177246
[TRAIN] Iter: 791200 Loss: 0.03153780847787857  PSNR: 18.28818702697754
[TRAIN] Iter: 791300 Loss: 0.020355654880404472  PSNR: 20.071308135986328
[TRAIN] Iter: 791400 Loss: 0.025870956480503082  PSNR: 18.981447219848633
[TRAIN] Iter: 791500 Loss: 0.027305537834763527  PSNR: 19.015888214111328
[TRAIN] Iter: 791600 Loss: 0.030678093433380127  PSNR: 18.33780288696289
[TRAIN] Iter: 791700 Loss: 0.02606423944234848  PSNR: 18.987760543823242
[TRAIN] Iter: 791800 Loss: 0.025617146864533424  PSNR: 19.130062103271484
[TRAIN] Iter: 791900 Loss: 0.03395935148000717  PSNR: 17.85578155517578
[TRAIN] Iter: 792000 Loss: 0.027631912380456924  PSNR: 18.807666778564453
[TRAIN] Iter: 792100 Loss: 0.027829837054014206  PSNR: 18.99700355529785
[TRAIN] Iter: 792200 Loss: 0.030179839581251144  PSNR: 18.317359924316406
[TRAIN] Iter: 792300 Loss: 0.029456578195095062  PSNR: 18.519668579101562
[TRAIN] Iter: 792400 Loss: 0.024676358327269554  PSNR: 19.28248405456543
[TRAIN] Iter: 792500 Loss: 0.028969261795282364  PSNR: 18.724884033203125
[TRAIN] Iter: 792600 Loss: 0.02786209061741829  PSNR: 18.734655380249023
[TRAIN] Iter: 792700 Loss: 0.03065999411046505  PSNR: 18.400846481323242
[TRAIN] Iter: 792800 Loss: 0.02527652680873871  PSNR: 18.828338623046875
[TRAIN] Iter: 792900 Loss: 0.03456418216228485  PSNR: 17.84305763244629
[TRAIN] Iter: 793000 Loss: 0.024286232888698578  PSNR: 19.326562881469727
[TRAIN] Iter: 793100 Loss: 0.03014604188501835  PSNR: 18.48786735534668
[TRAIN] Iter: 793200 Loss: 0.024241598322987556  PSNR: 19.334150314331055
[TRAIN] Iter: 793300 Loss: 0.027298111468553543  PSNR: 18.796722412109375
[TRAIN] Iter: 793400 Loss: 0.025030165910720825  PSNR: 19.313739776611328
[TRAIN] Iter: 793500 Loss: 0.0284566767513752  PSNR: 18.654117584228516
[TRAIN] Iter: 793600 Loss: 0.02458464726805687  PSNR: 19.083850860595703
[TRAIN] Iter: 793700 Loss: 0.027936341241002083  PSNR: 18.635007858276367
[TRAIN] Iter: 793800 Loss: 0.02812708355486393  PSNR: 18.80455207824707
[TRAIN] Iter: 793900 Loss: 0.029072852805256844  PSNR: 18.348651885986328
[TRAIN] Iter: 794000 Loss: 0.02292914129793644  PSNR: 19.6114501953125
[TRAIN] Iter: 794100 Loss: 0.02947174571454525  PSNR: 18.419742584228516
[TRAIN] Iter: 794200 Loss: 0.02626817673444748  PSNR: 19.05825424194336
[TRAIN] Iter: 794300 Loss: 0.0283828042447567  PSNR: 18.527402877807617
[TRAIN] Iter: 794400 Loss: 0.027211658656597137  PSNR: 18.91923713684082
[TRAIN] Iter: 794500 Loss: 0.02786365896463394  PSNR: 18.53608512878418
[TRAIN] Iter: 794600 Loss: 0.03088124468922615  PSNR: 18.27347183227539
[TRAIN] Iter: 794700 Loss: 0.032223984599113464  PSNR: 18.13497543334961
[TRAIN] Iter: 794800 Loss: 0.02773013710975647  PSNR: 18.62047576904297
[TRAIN] Iter: 794900 Loss: 0.0263662226498127  PSNR: 19.16033935546875
[TRAIN] Iter: 795000 Loss: 0.02481977827847004  PSNR: 19.37965965270996
[TRAIN] Iter: 795100 Loss: 0.021039724349975586  PSNR: 20.095056533813477
[TRAIN] Iter: 795200 Loss: 0.02675648406147957  PSNR: 18.947736740112305
[TRAIN] Iter: 795300 Loss: 0.021982798352837563  PSNR: 19.750385284423828
[TRAIN] Iter: 795400 Loss: 0.024715691804885864  PSNR: 19.400279998779297
[TRAIN] Iter: 795500 Loss: 0.02768194116652012  PSNR: 18.866153717041016
[TRAIN] Iter: 795600 Loss: 0.0288306325674057  PSNR: 18.36505889892578
[TRAIN] Iter: 795700 Loss: 0.030113108456134796  PSNR: 18.292755126953125
[TRAIN] Iter: 795800 Loss: 0.028953900560736656  PSNR: 18.52920913696289
[TRAIN] Iter: 795900 Loss: 0.023827625438570976  PSNR: 19.426694869995117
[TRAIN] Iter: 796000 Loss: 0.024604830890893936  PSNR: 19.309078216552734
[TRAIN] Iter: 796100 Loss: 0.020997775718569756  PSNR: 20.11141586303711
[TRAIN] Iter: 796200 Loss: 0.027571074664592743  PSNR: 18.940099716186523
[TRAIN] Iter: 796300 Loss: 0.02713422290980816  PSNR: 18.75777244567871
[TRAIN] Iter: 796400 Loss: 0.03125724568963051  PSNR: 18.080814361572266
[TRAIN] Iter: 796500 Loss: 0.02414734661579132  PSNR: 19.363597869873047
[TRAIN] Iter: 796600 Loss: 0.02138540893793106  PSNR: 19.77337074279785
[TRAIN] Iter: 796700 Loss: 0.026286285370588303  PSNR: 19.03546142578125
[TRAIN] Iter: 796800 Loss: 0.020084431394934654  PSNR: 20.106678009033203
[TRAIN] Iter: 796900 Loss: 0.025374865159392357  PSNR: 19.310527801513672
[TRAIN] Iter: 797000 Loss: 0.02756759151816368  PSNR: 18.5583553314209
[TRAIN] Iter: 797100 Loss: 0.028435800224542618  PSNR: 18.65595245361328
[TRAIN] Iter: 797200 Loss: 0.021073656156659126  PSNR: 20.006662368774414
[TRAIN] Iter: 797300 Loss: 0.02980245277285576  PSNR: 18.47553825378418
[TRAIN] Iter: 797400 Loss: 0.03031635656952858  PSNR: 18.429807662963867
[TRAIN] Iter: 797500 Loss: 0.029349714517593384  PSNR: 18.514427185058594
[TRAIN] Iter: 797600 Loss: 0.031571321189403534  PSNR: 18.378570556640625
[TRAIN] Iter: 797700 Loss: 0.029959265142679214  PSNR: 18.39784812927246
[TRAIN] Iter: 797800 Loss: 0.027840405702590942  PSNR: 18.83612823486328
[TRAIN] Iter: 797900 Loss: 0.02501429244875908  PSNR: 19.09563446044922
[TRAIN] Iter: 798000 Loss: 0.021384941413998604  PSNR: 19.76969337463379
[TRAIN] Iter: 798100 Loss: 0.026041528210043907  PSNR: 19.20281219482422
[TRAIN] Iter: 798200 Loss: 0.027806920930743217  PSNR: 18.555103302001953
[TRAIN] Iter: 798300 Loss: 0.027323827147483826  PSNR: 18.707958221435547
[TRAIN] Iter: 798400 Loss: 0.025819500908255577  PSNR: 18.724529266357422
[TRAIN] Iter: 798500 Loss: 0.021462153643369675  PSNR: 19.784629821777344
[TRAIN] Iter: 798600 Loss: 0.022460535168647766  PSNR: 19.773277282714844
[TRAIN] Iter: 798700 Loss: 0.025205887854099274  PSNR: 19.347509384155273
[TRAIN] Iter: 798800 Loss: 0.02960902266204357  PSNR: 18.497974395751953
[TRAIN] Iter: 798900 Loss: 0.024089397862553596  PSNR: 19.617650985717773
[TRAIN] Iter: 799000 Loss: 0.025105800479650497  PSNR: 19.177474975585938
[TRAIN] Iter: 799100 Loss: 0.024564294144511223  PSNR: 19.102684020996094
[TRAIN] Iter: 799200 Loss: 0.01863424852490425  PSNR: 20.460899353027344
[TRAIN] Iter: 799300 Loss: 0.028875675052404404  PSNR: 18.49837875366211
[TRAIN] Iter: 799400 Loss: 0.028727836906909943  PSNR: 18.745758056640625
[TRAIN] Iter: 799500 Loss: 0.018709801137447357  PSNR: 20.53278923034668
[TRAIN] Iter: 799600 Loss: 0.022448886185884476  PSNR: 19.720245361328125
[TRAIN] Iter: 799700 Loss: 0.033573493361473083  PSNR: 17.921567916870117
[TRAIN] Iter: 799800 Loss: 0.031219884753227234  PSNR: 18.241512298583984
[TRAIN] Iter: 799900 Loss: 0.023088380694389343  PSNR: 19.826385498046875
Saved checkpoints at ./logs/TUT-LAB-nerf/800000.tar
0 0.00042748451232910156
torch.Size([320, 640, 3]) torch.Size([320, 640])
1 16.50701332092285
2 15.997791290283203
3 16.558910608291626
4 16.07288432121277
5 16.564409971237183
6 15.94479513168335
7 16.501407861709595
8 15.982957124710083
9 16.49378752708435
10 16.006002187728882
11 16.48684549331665
12 16.037838220596313
13 16.524202346801758
14 16.02698278427124
15 16.516475677490234
16 16.05293369293213
17 16.059178829193115
18 16.481512784957886
19 16.06236457824707
20 16.557595252990723
21 15.943964958190918
22 16.59302544593811
23 16.041013956069946
24 16.552846908569336
25 15.952904224395752
26 16.56610107421875
27 16.01452898979187
28 16.53082585334778
29 16.031591176986694
30 16.512651681900024
31 16.01921057701111
32 16.513176441192627
33 13.622107744216919
34 16.449316263198853
35 15.921226978302002
36 16.458759307861328
37 15.141496419906616
38 14.75682282447815
39 14.158591270446777
40 14.723228693008423
41 14.121721744537354
42 14.680160284042358
43 14.031186819076538
44 14.228874444961548
45 14.801563501358032
46 14.078423738479614
47 14.696755647659302
48 13.980909585952759
49 14.79338002204895
50 14.157150983810425
51 14.033266067504883
52 14.814939737319946
53 13.9570951461792
54 14.815443992614746
55 14.00771164894104
56 14.773640394210815
57 14.052711963653564
58 14.787780046463013
59 14.216673612594604
60 13.98838496208191
61 14.840234756469727
62 13.935842514038086
63 14.866599559783936
64 14.014514207839966
65 14.855615377426147
66 14.083974838256836
67 14.043358564376831
68 14.808372497558594
69 14.003265857696533
70 14.814712762832642
71 13.99659514427185
72 14.834826707839966
73 14.028536796569824
74 14.734895944595337
75 14.18164610862732
76 14.175232410430908
77 14.676127672195435
78 14.110853910446167
79 14.734689712524414
80 14.116954565048218
81 14.751425504684448
82 14.147934675216675
83 14.159118890762329
84 14.679441452026367
85 14.141590595245361
86 14.717460870742798
87 14.141925573348999
88 14.719669103622437
89 14.100746870040894
90 14.772298812866211
91 14.137902736663818
92 14.162837028503418
93 14.736758470535278
94 14.137518405914307
95 14.661373376846313
96 14.183659315109253
97 14.750044584274292
98 14.158066987991333
99 14.144323825836182
100 14.72201657295227
101 14.153169631958008
102 14.674580097198486
103 14.12422490119934
104 14.704739093780518
105 14.051629304885864
106 14.752122402191162
107 14.113561630249023
108 14.179150104522705
109 14.704894304275513
110 14.115636825561523
111 14.712302446365356
112 14.136825323104858
113 14.748987436294556
114 14.12205696105957
115 14.141269445419312
116 14.700398445129395
117 14.104129314422607
118 14.732741594314575
119 14.126466512680054
Done, saving (120, 320, 640, 3) (120, 320, 640)
extras:{'raw': tensor([[[-1.6451e-01, -3.3263e-02,  6.7425e-01, -8.9621e+00],
         [-1.7724e+00, -1.9797e+00, -2.1977e+00, -5.1534e+01],
         [ 8.9868e-01,  8.8828e-01,  1.1464e+00, -1.3587e+01],
         ...,
         [-9.5143e+00, -7.3120e+00, -4.1459e+00, -6.9606e+00],
         [-6.4725e+00, -4.8558e+00, -2.0968e+00,  6.6820e+00],
         [-4.7260e+00, -3.2016e+00, -4.0580e-01,  3.5674e+00]],

        [[ 8.9566e-01,  8.2568e-01,  9.5003e-01,  1.3110e+01],
         [ 7.9129e-01,  7.8137e-01,  9.9997e-01,  1.5621e+01],
         [ 7.8432e-01,  7.7772e-01,  9.9972e-01,  1.5616e+01],
         ...,
         [ 1.1334e+01,  1.0457e+01,  1.1138e+01,  1.3599e+01],
         [ 8.3047e+00,  7.8497e+00,  8.6573e+00,  2.7839e+01],
         [ 8.2260e+00,  7.8147e+00,  8.7138e+00,  2.3672e+01]],

        [[ 4.6054e-01,  4.7770e-01,  1.1554e+00, -1.1646e+01],
         [-2.0672e+00, -2.1071e+00, -2.4134e+00, -4.5504e+01],
         [ 8.9950e-02,  6.4993e-03,  1.4821e-01, -3.0741e+00],
         ...,
         [-1.1441e+01, -8.4486e+00, -4.3603e+00, -1.4519e+01],
         [-6.8735e+00, -5.7345e+00, -4.6203e+00,  4.5791e+00],
         [-7.9618e+00, -6.9345e+00, -6.1797e+00,  1.6218e+00]],

        ...,

        [[-7.8737e-01, -4.9199e-01,  3.7756e-01, -1.3793e+01],
         [-4.9778e-01, -3.2126e-01,  2.1767e-01, -2.6813e+01],
         [ 3.2829e-01,  2.3821e-01,  2.4722e-01,  3.9107e+00],
         ...,
         [-8.9342e+00, -7.1273e+00, -4.7973e+00, -1.2355e+01],
         [-1.5668e+00, -1.2044e+00, -1.7258e-01,  4.6066e+00],
         [-1.2054e+00, -6.5843e-01,  1.0082e+00,  4.8532e+00]],

        [[-9.1049e-01, -1.0397e+00, -1.0666e+00, -1.5011e+01],
         [ 4.2422e-01,  1.0049e-01, -2.9953e-01, -3.4233e+01],
         [-1.8910e+00, -1.7054e+00, -1.6986e+00, -3.6650e+01],
         ...,
         [-1.8696e+01, -1.7074e+01, -1.7822e+01,  5.9156e+01],
         [-1.7233e+01, -1.5688e+01, -1.6234e+01,  6.2601e+01],
         [-1.8196e+01, -1.6558e+01, -1.7060e+01,  7.8409e+01]],

        [[-9.6332e-01, -1.2645e+00, -1.5736e+00, -1.4084e+01],
         [-6.8790e-01, -7.2586e-01, -6.9092e-01, -1.9008e+01],
         [-1.0356e+00, -9.4969e-01, -9.3278e-01, -1.4954e+01],
         ...,
         [ 3.9404e+00,  5.4140e+00,  1.5746e+01,  2.4065e+02],
         [ 3.0699e+00,  5.1654e+00,  1.7921e+01,  2.4374e+02],
         [ 4.1872e+00,  6.1933e+00,  1.9187e+01,  2.4049e+02]]],
       grad_fn=<ReshapeAliasBackward0>), 'rgb0': tensor([[0.6055, 0.5812, 0.5750],
        [0.6982, 0.6954, 0.7241],
        [0.5813, 0.5570, 0.5509],
        ...,
        [0.6068, 0.5822, 0.5832],
        [0.4057, 0.3657, 0.3473],
        [0.1541, 0.1461, 0.1297]], grad_fn=<ReshapeAliasBackward0>), 'disp0': tensor([ 13.2069, 296.8258,  13.6872,  ...,  14.6345,  17.0602,  17.5296],
       grad_fn=<ReshapeAliasBackward0>), 'acc0': tensor([1., 1., 1.,  ..., 1., 1., 1.], grad_fn=<ReshapeAliasBackward0>), 'z_std': tensor([0.0038, 0.0186, 0.0069,  ..., 0.0031, 0.0038, 0.0021])}
0 0.0004696846008300781
torch.Size([320, 640, 3]) torch.Size([320, 640])
1 14.115026950836182
2 14.728392362594604
3 14.108547687530518
4 14.099413871765137
5 14.673242807388306
6 14.044200897216797
7 14.81909728050232
8 14.119355916976929
9 14.773099184036255
10 14.128824472427368
11 14.684000492095947
12 14.1781485080719
13 13.978575944900513
14 14.87765097618103
15 13.960318803787231
16 14.832446098327637
17 13.982303857803345
18 14.856858730316162
19 14.08170223236084
20 13.962599992752075
21 14.829135179519653
22 13.978489637374878
23 14.870847225189209
24 13.960953950881958
25 14.861415147781372
26 14.016366720199585
27 14.702662706375122
28 14.042240858078003
29 14.108757019042969
30 14.831882953643799
31 14.05261778831482
32 14.846235513687134
33 14.092312812805176
34 14.662659168243408
35 14.032306671142578
36 14.115052938461304
37 14.705137252807617
38 14.121427774429321
39 14.8616042137146
40 14.12926173210144
41 14.682877540588379
42 14.115386486053467
43 14.725544214248657
44 14.087070226669312
45 14.108835220336914
46 14.684174537658691
47 14.12760329246521
48 14.72745394706726
49 14.119028329849243
50 14.698148012161255
51 14.101845741271973
52 14.117480754852295
53 14.770973205566406
54 14.178203821182251
55 14.713958501815796
56 14.115456581115723
57 14.734440326690674
58 14.049209833145142
59 14.750829935073853
60 14.191194534301758
61 14.147783041000366
62 14.766202688217163
63 14.164606094360352
64 14.626612186431885
65 14.062979221343994
66 14.788352489471436
67 14.073588132858276
68 14.00288462638855
69 14.848071813583374
70 14.084684371948242
71 14.705639600753784
72 14.168926239013672
73 14.726900815963745
74 14.013971090316772
75 14.88324761390686
76 14.116734743118286
77 14.12357497215271
78 14.699626922607422
79 14.104830265045166
80 14.705336093902588
81 14.084554195404053
82 14.722986936569214
83 14.092126369476318
84 14.063421249389648
85 14.756523847579956
86 14.14703917503357
87 14.70338773727417
88 14.057501554489136
89 14.755331754684448
90 14.059909343719482
91 14.768467903137207
92 14.097163915634155
93 14.099467515945435
94 14.90116286277771
95 14.026884317398071
96 14.892668008804321
97 14.020000457763672
98 14.820276021957397
99 14.1686270236969
100 13.95356822013855
101 14.897804260253906
102 13.94074010848999
103 14.961439371109009
104 13.924526691436768
105 14.921386957168579
106 14.000778436660767
107 14.748801946640015
108 14.137634038925171
109 13.951563596725464
110 14.846881628036499
111 13.998174667358398
112 14.939507722854614
113 13.92210578918457
114 14.893145322799683
115 14.003730773925781
116 13.994139909744263
117 14.83840560913086
118 14.089214086532593
119 14.85765290260315
test poses shape torch.Size([13, 3, 4])
0 0.0007522106170654297
torch.Size([320, 640, 3]) torch.Size([320, 640])
1 14.946113348007202
2 14.014638900756836
3 14.87921667098999
4 14.202763557434082
5 14.111883163452148
6 14.787490844726562
7 14.129910469055176
8 14.834694862365723
9 14.143097877502441
10 14.850921630859375
11 14.078030347824097
12 14.130708456039429
Saved test set
[TRAIN] Iter: 800000 Loss: 0.024639282375574112  PSNR: 19.167869567871094
[TRAIN] Iter: 800100 Loss: 0.025390643626451492  PSNR: 19.001625061035156
[TRAIN] Iter: 800200 Loss: 0.02046792022883892  PSNR: 20.078657150268555
[TRAIN] Iter: 800300 Loss: 0.028594957664608955  PSNR: 18.54813575744629
[TRAIN] Iter: 800400 Loss: 0.026434555649757385  PSNR: 18.813232421875
[TRAIN] Iter: 800500 Loss: 0.02451663836836815  PSNR: 19.44425392150879
[TRAIN] Iter: 800600 Loss: 0.02754983678460121  PSNR: 18.936519622802734
[TRAIN] Iter: 800700 Loss: 0.020876208320260048  PSNR: 20.053781509399414
[TRAIN] Iter: 800800 Loss: 0.023989815264940262  PSNR: 19.23225212097168
[TRAIN] Iter: 800900 Loss: 0.02326361835002899  PSNR: 19.679773330688477
[TRAIN] Iter: 801000 Loss: 0.027001850306987762  PSNR: 18.803590774536133
[TRAIN] Iter: 801100 Loss: 0.02605516090989113  PSNR: 18.67703628540039
[TRAIN] Iter: 801200 Loss: 0.026932507753372192  PSNR: 18.87289810180664
[TRAIN] Iter: 801300 Loss: 0.024162502959370613  PSNR: 19.358680725097656
[TRAIN] Iter: 801400 Loss: 0.0329366959631443  PSNR: 17.993257522583008
[TRAIN] Iter: 801500 Loss: 0.025010554119944572  PSNR: 19.136329650878906
[TRAIN] Iter: 801600 Loss: 0.019965885207057  PSNR: 20.064237594604492
[TRAIN] Iter: 801700 Loss: 0.02624720335006714  PSNR: 18.996959686279297
[TRAIN] Iter: 801800 Loss: 0.02504788339138031  PSNR: 19.120262145996094
[TRAIN] Iter: 801900 Loss: 0.03160737454891205  PSNR: 18.13094711303711
[TRAIN] Iter: 802000 Loss: 0.02583225443959236  PSNR: 18.927465438842773
[TRAIN] Iter: 802100 Loss: 0.026043865829706192  PSNR: 19.176448822021484
[TRAIN] Iter: 802200 Loss: 0.025942331179976463  PSNR: 18.826663970947266
[TRAIN] Iter: 802300 Loss: 0.03176739066839218  PSNR: 18.067630767822266
[TRAIN] Iter: 802400 Loss: 0.024742366746068  PSNR: 19.292070388793945
[TRAIN] Iter: 802500 Loss: 0.030230702832341194  PSNR: 18.47124671936035
[TRAIN] Iter: 802600 Loss: 0.027700109407305717  PSNR: 18.60305404663086
[TRAIN] Iter: 802700 Loss: 0.027886340394616127  PSNR: 18.63801383972168
[TRAIN] Iter: 802800 Loss: 0.025360671803355217  PSNR: 18.945758819580078
[TRAIN] Iter: 802900 Loss: 0.028067460283637047  PSNR: 18.792940139770508
[TRAIN] Iter: 803000 Loss: 0.024567915126681328  PSNR: 19.360544204711914
[TRAIN] Iter: 803100 Loss: 0.031066806986927986  PSNR: 18.252906799316406
[TRAIN] Iter: 803200 Loss: 0.029957417398691177  PSNR: 18.390602111816406
[TRAIN] Iter: 803300 Loss: 0.027146605774760246  PSNR: 18.915122985839844
[TRAIN] Iter: 803400 Loss: 0.025789283215999603  PSNR: 18.912973403930664
[TRAIN] Iter: 803500 Loss: 0.024488015100359917  PSNR: 19.421483993530273
[TRAIN] Iter: 803600 Loss: 0.03172500059008598  PSNR: 18.193323135375977
[TRAIN] Iter: 803700 Loss: 0.02450099214911461  PSNR: 19.29930305480957
[TRAIN] Iter: 803800 Loss: 0.02790418267250061  PSNR: 19.09657096862793
[TRAIN] Iter: 803900 Loss: 0.029697423800826073  PSNR: 18.388782501220703
[TRAIN] Iter: 804000 Loss: 0.03576749563217163  PSNR: 17.537277221679688
[TRAIN] Iter: 804100 Loss: 0.02720145881175995  PSNR: 18.898731231689453
[TRAIN] Iter: 804200 Loss: 0.026856189593672752  PSNR: 18.866220474243164
[TRAIN] Iter: 804300 Loss: 0.02824908122420311  PSNR: 18.64202117919922
[TRAIN] Iter: 804400 Loss: 0.02347203902900219  PSNR: 19.693828582763672
[TRAIN] Iter: 804500 Loss: 0.022160423919558525  PSNR: 20.021955490112305
[TRAIN] Iter: 804600 Loss: 0.02292383275926113  PSNR: 19.553564071655273
[TRAIN] Iter: 804700 Loss: 0.028218479827046394  PSNR: 18.98764419555664
[TRAIN] Iter: 804800 Loss: 0.028110070154070854  PSNR: 18.716493606567383
[TRAIN] Iter: 804900 Loss: 0.03154061734676361  PSNR: 18.140552520751953
[TRAIN] Iter: 805000 Loss: 0.02650521695613861  PSNR: 18.732501983642578
[TRAIN] Iter: 805100 Loss: 0.02829710952937603  PSNR: 18.647539138793945
[TRAIN] Iter: 805200 Loss: 0.03212781995534897  PSNR: 18.03775405883789
[TRAIN] Iter: 805300 Loss: 0.025803230702877045  PSNR: 19.21741485595703
[TRAIN] Iter: 805400 Loss: 0.025926902890205383  PSNR: 19.065528869628906
[TRAIN] Iter: 805500 Loss: 0.02303299494087696  PSNR: 19.498693466186523
[TRAIN] Iter: 805600 Loss: 0.022563550621271133  PSNR: 19.88734245300293
[TRAIN] Iter: 805700 Loss: 0.022848475724458694  PSNR: 19.695859909057617
[TRAIN] Iter: 805800 Loss: 0.02521342784166336  PSNR: 19.177732467651367
[TRAIN] Iter: 805900 Loss: 0.030273593962192535  PSNR: 18.275493621826172
[TRAIN] Iter: 806000 Loss: 0.020215772092342377  PSNR: 19.928468704223633
[TRAIN] Iter: 806100 Loss: 0.03195139020681381  PSNR: 18.135465621948242
[TRAIN] Iter: 806200 Loss: 0.021780626848340034  PSNR: 19.601491928100586
[TRAIN] Iter: 806300 Loss: 0.025194676592946053  PSNR: 19.56620979309082
[TRAIN] Iter: 806400 Loss: 0.02925407700240612  PSNR: 18.498912811279297
[TRAIN] Iter: 806500 Loss: 0.026244740933179855  PSNR: 18.798133850097656
[TRAIN] Iter: 806600 Loss: 0.03425511345267296  PSNR: 17.85624122619629
[TRAIN] Iter: 806700 Loss: 0.02471802569925785  PSNR: 19.140085220336914
[TRAIN] Iter: 806800 Loss: 0.03135693073272705  PSNR: 18.296716690063477
[TRAIN] Iter: 806900 Loss: 0.029113812372088432  PSNR: 18.523021697998047
[TRAIN] Iter: 807000 Loss: 0.02966286614537239  PSNR: 18.533018112182617
[TRAIN] Iter: 807100 Loss: 0.02673204056918621  PSNR: 19.318031311035156
[TRAIN] Iter: 807200 Loss: 0.027256546542048454  PSNR: 18.814350128173828
[TRAIN] Iter: 807300 Loss: 0.029330013319849968  PSNR: 18.407390594482422
[TRAIN] Iter: 807400 Loss: 0.02335331216454506  PSNR: 19.58161163330078
[TRAIN] Iter: 807500 Loss: 0.028119763359427452  PSNR: 18.651214599609375
[TRAIN] Iter: 807600 Loss: 0.022554339841008186  PSNR: 19.29994773864746
[TRAIN] Iter: 807700 Loss: 0.021132569760084152  PSNR: 20.0322208404541
[TRAIN] Iter: 807800 Loss: 0.022158155217766762  PSNR: 19.89499855041504
[TRAIN] Iter: 807900 Loss: 0.026862677186727524  PSNR: 18.860673904418945
[TRAIN] Iter: 808000 Loss: 0.028370430693030357  PSNR: 18.66749382019043
[TRAIN] Iter: 808100 Loss: 0.028886981308460236  PSNR: 18.58771514892578
[TRAIN] Iter: 808200 Loss: 0.024436455219984055  PSNR: 19.369089126586914
[TRAIN] Iter: 808300 Loss: 0.025014830753207207  PSNR: 19.085613250732422
[TRAIN] Iter: 808400 Loss: 0.029485955834388733  PSNR: 18.438180923461914
[TRAIN] Iter: 808500 Loss: 0.031169135123491287  PSNR: 18.323579788208008
[TRAIN] Iter: 808600 Loss: 0.028374670073390007  PSNR: 18.837621688842773
[TRAIN] Iter: 808700 Loss: 0.03155965730547905  PSNR: 18.246606826782227
[TRAIN] Iter: 808800 Loss: 0.02634454146027565  PSNR: 18.8054256439209
[TRAIN] Iter: 808900 Loss: 0.022495828568935394  PSNR: 19.708255767822266
[TRAIN] Iter: 809000 Loss: 0.027578936889767647  PSNR: 18.621610641479492
[TRAIN] Iter: 809100 Loss: 0.030737489461898804  PSNR: 18.522228240966797
[TRAIN] Iter: 809200 Loss: 0.024100756272673607  PSNR: 19.333818435668945
[TRAIN] Iter: 809300 Loss: 0.025187259539961815  PSNR: 19.243547439575195
[TRAIN] Iter: 809400 Loss: 0.02190108597278595  PSNR: 19.73801612854004
[TRAIN] Iter: 809500 Loss: 0.022462036460638046  PSNR: 19.177173614501953
[TRAIN] Iter: 809600 Loss: 0.02492918074131012  PSNR: 19.173336029052734
[TRAIN] Iter: 809700 Loss: 0.02277817204594612  PSNR: 19.691102981567383
[TRAIN] Iter: 809800 Loss: 0.029708776623010635  PSNR: 18.56818199157715
[TRAIN] Iter: 809900 Loss: 0.02457284927368164  PSNR: 19.397275924682617
Saved checkpoints at ./logs/TUT-LAB-nerf/810000.tar
[TRAIN] Iter: 810000 Loss: 0.02564861997961998  PSNR: 19.197032928466797
[TRAIN] Iter: 810100 Loss: 0.03210622817277908  PSNR: 18.160232543945312
[TRAIN] Iter: 810200 Loss: 0.03255332633852959  PSNR: 18.070369720458984
[TRAIN] Iter: 810300 Loss: 0.023289665579795837  PSNR: 19.2770938873291
[TRAIN] Iter: 810400 Loss: 0.030010519549250603  PSNR: 18.306148529052734
[TRAIN] Iter: 810500 Loss: 0.023613879457116127  PSNR: 19.291391372680664
[TRAIN] Iter: 810600 Loss: 0.031581804156303406  PSNR: 18.313968658447266
[TRAIN] Iter: 810700 Loss: 0.032377999275922775  PSNR: 18.25371742248535
[TRAIN] Iter: 810800 Loss: 0.026555761694908142  PSNR: 18.916969299316406
[TRAIN] Iter: 810900 Loss: 0.025270244106650352  PSNR: 19.44806671142578
[TRAIN] Iter: 811000 Loss: 0.0182432122528553  PSNR: 20.58817481994629
[TRAIN] Iter: 811100 Loss: 0.021420443430542946  PSNR: 19.952341079711914
[TRAIN] Iter: 811200 Loss: 0.023105032742023468  PSNR: 19.513751983642578
[TRAIN] Iter: 811300 Loss: 0.025057345628738403  PSNR: 19.30722427368164
[TRAIN] Iter: 811400 Loss: 0.024071823805570602  PSNR: 19.177570343017578
[TRAIN] Iter: 811500 Loss: 0.029321661219000816  PSNR: 18.574851989746094
[TRAIN] Iter: 811600 Loss: 0.026951609179377556  PSNR: 18.88780975341797
[TRAIN] Iter: 811700 Loss: 0.024473238736391068  PSNR: 19.350170135498047
[TRAIN] Iter: 811800 Loss: 0.024456579238176346  PSNR: 19.88796043395996
[TRAIN] Iter: 811900 Loss: 0.026962554082274437  PSNR: 19.000871658325195
[TRAIN] Iter: 812000 Loss: 0.02346881479024887  PSNR: 19.385168075561523
[TRAIN] Iter: 812100 Loss: 0.030791163444519043  PSNR: 18.308679580688477
[TRAIN] Iter: 812200 Loss: 0.025755999609827995  PSNR: 19.13484001159668
[TRAIN] Iter: 812300 Loss: 0.027429498732089996  PSNR: 18.664058685302734
[TRAIN] Iter: 812400 Loss: 0.023370634764432907  PSNR: 19.27850914001465
[TRAIN] Iter: 812500 Loss: 0.025090957060456276  PSNR: 18.956348419189453
[TRAIN] Iter: 812600 Loss: 0.028947727754712105  PSNR: 18.37189483642578
[TRAIN] Iter: 812700 Loss: 0.02508566528558731  PSNR: 19.132423400878906
[TRAIN] Iter: 812800 Loss: 0.025416184216737747  PSNR: 19.204265594482422
[TRAIN] Iter: 812900 Loss: 0.027751028537750244  PSNR: 19.00747299194336
[TRAIN] Iter: 813000 Loss: 0.02825225703418255  PSNR: 18.649003982543945
[TRAIN] Iter: 813100 Loss: 0.029379885643720627  PSNR: 18.3604679107666
[TRAIN] Iter: 813200 Loss: 0.02948925271630287  PSNR: 18.558700561523438
[TRAIN] Iter: 813300 Loss: 0.020654495805501938  PSNR: 19.91832733154297
[TRAIN] Iter: 813400 Loss: 0.02993856742978096  PSNR: 18.56717300415039
[TRAIN] Iter: 813500 Loss: 0.023956507444381714  PSNR: 19.311365127563477
[TRAIN] Iter: 813600 Loss: 0.02626004070043564  PSNR: 19.222963333129883
[TRAIN] Iter: 813700 Loss: 0.02162051759660244  PSNR: 19.866226196289062
[TRAIN] Iter: 813800 Loss: 0.028050672262907028  PSNR: 18.81932830810547
[TRAIN] Iter: 813900 Loss: 0.03022722899913788  PSNR: 18.278257369995117
[TRAIN] Iter: 814000 Loss: 0.029521966353058815  PSNR: 18.585359573364258
[TRAIN] Iter: 814100 Loss: 0.0284736230969429  PSNR: 18.80125617980957
[TRAIN] Iter: 814200 Loss: 0.030970189720392227  PSNR: 18.26325035095215
[TRAIN] Iter: 814300 Loss: 0.023137999698519707  PSNR: 19.216251373291016
[TRAIN] Iter: 814400 Loss: 0.03301002085208893  PSNR: 17.919286727905273
[TRAIN] Iter: 814500 Loss: 0.02924918383359909  PSNR: 18.440927505493164
[TRAIN] Iter: 814600 Loss: 0.030698619782924652  PSNR: 18.257671356201172
[TRAIN] Iter: 814700 Loss: 0.025543132796883583  PSNR: 18.781784057617188
[TRAIN] Iter: 814800 Loss: 0.02626049518585205  PSNR: 18.95921516418457
[TRAIN] Iter: 814900 Loss: 0.024335162714123726  PSNR: 19.20970916748047
[TRAIN] Iter: 815000 Loss: 0.032459765672683716  PSNR: 18.15691566467285
[TRAIN] Iter: 815100 Loss: 0.025368763133883476  PSNR: 19.029359817504883
[TRAIN] Iter: 815200 Loss: 0.03086571954190731  PSNR: 18.221925735473633
[TRAIN] Iter: 815300 Loss: 0.02258162572979927  PSNR: 19.485637664794922
[TRAIN] Iter: 815400 Loss: 0.027586882933974266  PSNR: 18.797407150268555
[TRAIN] Iter: 815500 Loss: 0.020052529871463776  PSNR: 20.113813400268555
[TRAIN] Iter: 815600 Loss: 0.02447386085987091  PSNR: 18.937545776367188
[TRAIN] Iter: 815700 Loss: 0.019933050498366356  PSNR: 20.11983299255371
[TRAIN] Iter: 815800 Loss: 0.02220209501683712  PSNR: 19.632457733154297
[TRAIN] Iter: 815900 Loss: 0.025279663503170013  PSNR: 19.127485275268555
[TRAIN] Iter: 816000 Loss: 0.03285594284534454  PSNR: 18.076074600219727
[TRAIN] Iter: 816100 Loss: 0.02478710561990738  PSNR: 19.317739486694336
[TRAIN] Iter: 816200 Loss: 0.02133786305785179  PSNR: 19.81960678100586
[TRAIN] Iter: 816300 Loss: 0.028692640364170074  PSNR: 18.683956146240234
[TRAIN] Iter: 816400 Loss: 0.02024814859032631  PSNR: 20.22624397277832
[TRAIN] Iter: 816500 Loss: 0.02735322341322899  PSNR: 18.859025955200195
[TRAIN] Iter: 816600 Loss: 0.020455598831176758  PSNR: 20.08428192138672
[TRAIN] Iter: 816700 Loss: 0.03291497379541397  PSNR: 18.078670501708984
[TRAIN] Iter: 816800 Loss: 0.028544537723064423  PSNR: 18.583450317382812
[TRAIN] Iter: 816900 Loss: 0.027062419801950455  PSNR: 18.914119720458984
[TRAIN] Iter: 817000 Loss: 0.028674185276031494  PSNR: 18.59225082397461
[TRAIN] Iter: 817100 Loss: 0.02397443912923336  PSNR: 19.51938247680664
[TRAIN] Iter: 817200 Loss: 0.02541312947869301  PSNR: 19.253358840942383
[TRAIN] Iter: 817300 Loss: 0.029518142342567444  PSNR: 18.52287483215332
[TRAIN] Iter: 817400 Loss: 0.028818238526582718  PSNR: 18.55942153930664
[TRAIN] Iter: 817500 Loss: 0.024932201951742172  PSNR: 19.050689697265625
[TRAIN] Iter: 817600 Loss: 0.028955858200788498  PSNR: 18.52177619934082
[TRAIN] Iter: 817700 Loss: 0.028864441439509392  PSNR: 18.535654067993164
[TRAIN] Iter: 817800 Loss: 0.017864376306533813  PSNR: 20.61687660217285
[TRAIN] Iter: 817900 Loss: 0.029273713007569313  PSNR: 18.481950759887695
[TRAIN] Iter: 818000 Loss: 0.029741302132606506  PSNR: 18.468507766723633
[TRAIN] Iter: 818100 Loss: 0.024052072316408157  PSNR: 19.423572540283203
[TRAIN] Iter: 818200 Loss: 0.028585724532604218  PSNR: 18.566905975341797
[TRAIN] Iter: 818300 Loss: 0.02770174667239189  PSNR: 18.61887550354004
[TRAIN] Iter: 818400 Loss: 0.025164835155010223  PSNR: 19.118541717529297
[TRAIN] Iter: 818500 Loss: 0.01986486092209816  PSNR: 20.248428344726562
[TRAIN] Iter: 818600 Loss: 0.019617831334471703  PSNR: 20.261333465576172
[TRAIN] Iter: 818700 Loss: 0.02045869082212448  PSNR: 20.021419525146484
[TRAIN] Iter: 818800 Loss: 0.02475615218281746  PSNR: 19.235355377197266
[TRAIN] Iter: 818900 Loss: 0.024228468537330627  PSNR: 19.314119338989258
[TRAIN] Iter: 819000 Loss: 0.02834269218146801  PSNR: 18.587949752807617
[TRAIN] Iter: 819100 Loss: 0.02406267449259758  PSNR: 19.498899459838867
[TRAIN] Iter: 819200 Loss: 0.028372034430503845  PSNR: 18.51917839050293
[TRAIN] Iter: 819300 Loss: 0.029330067336559296  PSNR: 18.306480407714844
[TRAIN] Iter: 819400 Loss: 0.03389688581228256  PSNR: 17.800052642822266
[TRAIN] Iter: 819500 Loss: 0.023793386295437813  PSNR: 19.33322525024414
[TRAIN] Iter: 819600 Loss: 0.023583177477121353  PSNR: 18.98390007019043
[TRAIN] Iter: 819700 Loss: 0.02804204262793064  PSNR: 18.848711013793945
[TRAIN] Iter: 819800 Loss: 0.026762213557958603  PSNR: 18.472862243652344
[TRAIN] Iter: 819900 Loss: 0.025102149695158005  PSNR: 19.26494026184082
Saved checkpoints at ./logs/TUT-LAB-nerf/820000.tar
[TRAIN] Iter: 820000 Loss: 0.031602296978235245  PSNR: 18.097484588623047
[TRAIN] Iter: 820100 Loss: 0.027116423472762108  PSNR: 18.777971267700195
[TRAIN] Iter: 820200 Loss: 0.022129612043499947  PSNR: 19.719791412353516
[TRAIN] Iter: 820300 Loss: 0.029808247461915016  PSNR: 18.478296279907227
[TRAIN] Iter: 820400 Loss: 0.03012806549668312  PSNR: 18.474260330200195
[TRAIN] Iter: 820500 Loss: 0.019463758915662766  PSNR: 20.191495895385742
[TRAIN] Iter: 820600 Loss: 0.023246698081493378  PSNR: 19.64620018005371
[TRAIN] Iter: 820700 Loss: 0.029396507889032364  PSNR: 18.65926170349121
[TRAIN] Iter: 820800 Loss: 0.023079290986061096  PSNR: 19.312379837036133
[TRAIN] Iter: 820900 Loss: 0.0220472514629364  PSNR: 19.966474533081055
[TRAIN] Iter: 821000 Loss: 0.02169840969145298  PSNR: 20.098676681518555
[TRAIN] Iter: 821100 Loss: 0.02646712213754654  PSNR: 18.90325164794922
[TRAIN] Iter: 821200 Loss: 0.028049306944012642  PSNR: 18.723299026489258
[TRAIN] Iter: 821300 Loss: 0.031542059034109116  PSNR: 18.20846176147461
[TRAIN] Iter: 821400 Loss: 0.026722872629761696  PSNR: 18.913515090942383
[TRAIN] Iter: 821500 Loss: 0.030598029494285583  PSNR: 18.29924201965332
[TRAIN] Iter: 821600 Loss: 0.02485312521457672  PSNR: 19.002254486083984
[TRAIN] Iter: 821700 Loss: 0.026694005355238914  PSNR: 18.727201461791992
[TRAIN] Iter: 821800 Loss: 0.020580176264047623  PSNR: 19.90592384338379
[TRAIN] Iter: 821900 Loss: 0.026273824274539948  PSNR: 18.992298126220703
[TRAIN] Iter: 822000 Loss: 0.018448147922754288  PSNR: 20.63741683959961
[TRAIN] Iter: 822100 Loss: 0.02285969816148281  PSNR: 19.40622901916504
[TRAIN] Iter: 822200 Loss: 0.029500190168619156  PSNR: 18.300443649291992
[TRAIN] Iter: 822300 Loss: 0.02830146811902523  PSNR: 18.74152374267578
[TRAIN] Iter: 822400 Loss: 0.02814944088459015  PSNR: 18.851600646972656
[TRAIN] Iter: 822500 Loss: 0.026887137442827225  PSNR: 18.881099700927734
[TRAIN] Iter: 822600 Loss: 0.024244587868452072  PSNR: 19.380786895751953
[TRAIN] Iter: 822700 Loss: 0.02946886420249939  PSNR: 18.547107696533203
[TRAIN] Iter: 822800 Loss: 0.027412381023168564  PSNR: 18.97296142578125
[TRAIN] Iter: 822900 Loss: 0.028075288981199265  PSNR: 18.715665817260742
[TRAIN] Iter: 823000 Loss: 0.021810537204146385  PSNR: 19.753868103027344
[TRAIN] Iter: 823100 Loss: 0.02896716073155403  PSNR: 18.770227432250977
[TRAIN] Iter: 823200 Loss: 0.024439159780740738  PSNR: 19.12127113342285
[TRAIN] Iter: 823300 Loss: 0.02835364267230034  PSNR: 18.64295196533203
[TRAIN] Iter: 823400 Loss: 0.026192013174295425  PSNR: 18.824426651000977
[TRAIN] Iter: 823500 Loss: 0.020395148545503616  PSNR: 20.174772262573242
[TRAIN] Iter: 823600 Loss: 0.02755153737962246  PSNR: 18.787080764770508
[TRAIN] Iter: 823700 Loss: 0.027823753654956818  PSNR: 18.763511657714844
[TRAIN] Iter: 823800 Loss: 0.024913765490055084  PSNR: 19.245054244995117
[TRAIN] Iter: 823900 Loss: 0.025138335302472115  PSNR: 19.210641860961914
[TRAIN] Iter: 824000 Loss: 0.032462239265441895  PSNR: 17.997140884399414
[TRAIN] Iter: 824100 Loss: 0.02404705621302128  PSNR: 19.53965950012207
[TRAIN] Iter: 824200 Loss: 0.031227651983499527  PSNR: 18.226274490356445
[TRAIN] Iter: 824300 Loss: 0.025930698961019516  PSNR: 19.21199607849121
[TRAIN] Iter: 824400 Loss: 0.027821317315101624  PSNR: 18.75191879272461
[TRAIN] Iter: 824500 Loss: 0.026371832937002182  PSNR: 19.257770538330078
[TRAIN] Iter: 824600 Loss: 0.02406517043709755  PSNR: 19.217609405517578
[TRAIN] Iter: 824700 Loss: 0.02221463993191719  PSNR: 19.407316207885742
[TRAIN] Iter: 824800 Loss: 0.022735480219125748  PSNR: 19.50507354736328
[TRAIN] Iter: 824900 Loss: 0.03125385567545891  PSNR: 18.16266441345215
[TRAIN] Iter: 825000 Loss: 0.01908170059323311  PSNR: 20.3956241607666
[TRAIN] Iter: 825100 Loss: 0.02904481813311577  PSNR: 18.50624656677246
[TRAIN] Iter: 825200 Loss: 0.02507246844470501  PSNR: 19.145362854003906
[TRAIN] Iter: 825300 Loss: 0.019685734063386917  PSNR: 20.18720245361328
[TRAIN] Iter: 825400 Loss: 0.03280958905816078  PSNR: 18.041685104370117
[TRAIN] Iter: 825500 Loss: 0.028622057288885117  PSNR: 18.771764755249023
[TRAIN] Iter: 825600 Loss: 0.02402474917471409  PSNR: 19.43732261657715
[TRAIN] Iter: 825700 Loss: 0.025800183415412903  PSNR: 18.91638946533203
[TRAIN] Iter: 825800 Loss: 0.03323356807231903  PSNR: 18.025930404663086
[TRAIN] Iter: 825900 Loss: 0.02653093822300434  PSNR: 19.09849739074707
[TRAIN] Iter: 826000 Loss: 0.027270274236798286  PSNR: 18.958383560180664
[TRAIN] Iter: 826100 Loss: 0.024096880108118057  PSNR: 19.38930320739746
[TRAIN] Iter: 826200 Loss: 0.02410942129790783  PSNR: 19.386577606201172
[TRAIN] Iter: 826300 Loss: 0.02458425983786583  PSNR: 19.11604118347168
[TRAIN] Iter: 826400 Loss: 0.025254981592297554  PSNR: 19.156709671020508
[TRAIN] Iter: 826500 Loss: 0.025287790223956108  PSNR: 19.182769775390625
[TRAIN] Iter: 826600 Loss: 0.022851459681987762  PSNR: 19.525657653808594
[TRAIN] Iter: 826700 Loss: 0.02485477551817894  PSNR: 18.808650970458984
[TRAIN] Iter: 826800 Loss: 0.024249983951449394  PSNR: 19.465373992919922
[TRAIN] Iter: 826900 Loss: 0.02404598891735077  PSNR: 19.56301498413086
[TRAIN] Iter: 827000 Loss: 0.02558700554072857  PSNR: 19.222126007080078
[TRAIN] Iter: 827100 Loss: 0.027293793857097626  PSNR: 18.806241989135742
[TRAIN] Iter: 827200 Loss: 0.023790165781974792  PSNR: 19.71023178100586
[TRAIN] Iter: 827300 Loss: 0.023505620658397675  PSNR: 19.656818389892578
[TRAIN] Iter: 827400 Loss: 0.02259226143360138  PSNR: 19.745174407958984
[TRAIN] Iter: 827500 Loss: 0.02705363556742668  PSNR: 18.877944946289062
[TRAIN] Iter: 827600 Loss: 0.020680081099271774  PSNR: 19.892955780029297
[TRAIN] Iter: 827700 Loss: 0.025593217462301254  PSNR: 19.06702995300293
[TRAIN] Iter: 827800 Loss: 0.02327369712293148  PSNR: 19.548648834228516
[TRAIN] Iter: 827900 Loss: 0.027543555945158005  PSNR: 18.86004066467285
[TRAIN] Iter: 828000 Loss: 0.026912588626146317  PSNR: 18.921249389648438
[TRAIN] Iter: 828100 Loss: 0.028948647901415825  PSNR: 18.46428108215332
[TRAIN] Iter: 828200 Loss: 0.02873411402106285  PSNR: 18.78125762939453
[TRAIN] Iter: 828300 Loss: 0.02131597325205803  PSNR: 19.970523834228516
[TRAIN] Iter: 828400 Loss: 0.0187152661383152  PSNR: 20.484901428222656
[TRAIN] Iter: 828500 Loss: 0.02512233518064022  PSNR: 18.965301513671875
[TRAIN] Iter: 828600 Loss: 0.026577213779091835  PSNR: 19.011474609375
[TRAIN] Iter: 828700 Loss: 0.028942763805389404  PSNR: 18.597900390625
[TRAIN] Iter: 828800 Loss: 0.022206587716937065  PSNR: 19.623531341552734
[TRAIN] Iter: 828900 Loss: 0.022829893976449966  PSNR: 19.515972137451172
[TRAIN] Iter: 829000 Loss: 0.029334546998143196  PSNR: 18.589397430419922
[TRAIN] Iter: 829100 Loss: 0.030388420447707176  PSNR: 18.28949546813965
[TRAIN] Iter: 829200 Loss: 0.030094368383288383  PSNR: 18.368515014648438
[TRAIN] Iter: 829300 Loss: 0.03437976539134979  PSNR: 17.839946746826172
[TRAIN] Iter: 829400 Loss: 0.022102115675807  PSNR: 19.735576629638672
[TRAIN] Iter: 829500 Loss: 0.021879278123378754  PSNR: 19.880929946899414
[TRAIN] Iter: 829600 Loss: 0.02639390341937542  PSNR: 19.01251983642578
[TRAIN] Iter: 829700 Loss: 0.029250934720039368  PSNR: 18.357868194580078
[TRAIN] Iter: 829800 Loss: 0.02240784838795662  PSNR: 19.68077278137207
[TRAIN] Iter: 829900 Loss: 0.01723804697394371  PSNR: 20.76863670349121
Saved checkpoints at ./logs/TUT-LAB-nerf/830000.tar
[TRAIN] Iter: 830000 Loss: 0.02478625997900963  PSNR: 19.43628692626953
[TRAIN] Iter: 830100 Loss: 0.03321053087711334  PSNR: 17.942386627197266
[TRAIN] Iter: 830200 Loss: 0.03130761906504631  PSNR: 18.241239547729492
[TRAIN] Iter: 830300 Loss: 0.026602203026413918  PSNR: 19.010032653808594
[TRAIN] Iter: 830400 Loss: 0.030192021280527115  PSNR: 18.245832443237305
[TRAIN] Iter: 830500 Loss: 0.020703069865703583  PSNR: 19.630624771118164
[TRAIN] Iter: 830600 Loss: 0.02569095604121685  PSNR: 19.277252197265625
[TRAIN] Iter: 830700 Loss: 0.023527581244707108  PSNR: 19.770652770996094
[TRAIN] Iter: 830800 Loss: 0.024058274924755096  PSNR: 19.487384796142578
[TRAIN] Iter: 830900 Loss: 0.024203132838010788  PSNR: 19.17137908935547
[TRAIN] Iter: 831000 Loss: 0.03587877005338669  PSNR: 17.613250732421875
[TRAIN] Iter: 831100 Loss: 0.02942376583814621  PSNR: 18.50643539428711
[TRAIN] Iter: 831200 Loss: 0.023977721109986305  PSNR: 19.2683048248291
[TRAIN] Iter: 831300 Loss: 0.02413773164153099  PSNR: 19.463289260864258
[TRAIN] Iter: 831400 Loss: 0.027374185621738434  PSNR: 18.848953247070312
[TRAIN] Iter: 831500 Loss: 0.028112053871154785  PSNR: 18.762544631958008
[TRAIN] Iter: 831600 Loss: 0.02667115069925785  PSNR: 18.876916885375977
[TRAIN] Iter: 831700 Loss: 0.025236764922738075  PSNR: 19.263553619384766
[TRAIN] Iter: 831800 Loss: 0.02361254021525383  PSNR: 19.159326553344727
[TRAIN] Iter: 831900 Loss: 0.0343194305896759  PSNR: 17.754602432250977
[TRAIN] Iter: 832000 Loss: 0.025097351521253586  PSNR: 19.34722137451172
[TRAIN] Iter: 832100 Loss: 0.026226937770843506  PSNR: 19.048656463623047
[TRAIN] Iter: 832200 Loss: 0.0302109457552433  PSNR: 18.402725219726562
[TRAIN] Iter: 832300 Loss: 0.026524938642978668  PSNR: 18.914522171020508
[TRAIN] Iter: 832400 Loss: 0.029164059087634087  PSNR: 18.8419189453125
[TRAIN] Iter: 832500 Loss: 0.03220834210515022  PSNR: 18.236852645874023
[TRAIN] Iter: 832600 Loss: 0.019675150513648987  PSNR: 20.19261932373047
[TRAIN] Iter: 832700 Loss: 0.023928891867399216  PSNR: 19.23166275024414
[TRAIN] Iter: 832800 Loss: 0.02222503162920475  PSNR: 19.489410400390625
[TRAIN] Iter: 832900 Loss: 0.026422660797834396  PSNR: 19.27836799621582
[TRAIN] Iter: 833000 Loss: 0.027089953422546387  PSNR: 18.85077667236328
[TRAIN] Iter: 833100 Loss: 0.026590833440423012  PSNR: 19.06561851501465
[TRAIN] Iter: 833200 Loss: 0.027924012392759323  PSNR: 18.770416259765625
[TRAIN] Iter: 833300 Loss: 0.028102798387408257  PSNR: 18.76978302001953
[TRAIN] Iter: 833400 Loss: 0.02641618810594082  PSNR: 19.16724395751953
[TRAIN] Iter: 833500 Loss: 0.032590121030807495  PSNR: 18.050771713256836
[TRAIN] Iter: 833600 Loss: 0.03305548429489136  PSNR: 18.05440902709961
[TRAIN] Iter: 833700 Loss: 0.02454555593430996  PSNR: 19.2897891998291
[TRAIN] Iter: 833800 Loss: 0.022190261632204056  PSNR: 19.766403198242188
[TRAIN] Iter: 833900 Loss: 0.028183862566947937  PSNR: 18.490812301635742
[TRAIN] Iter: 834000 Loss: 0.023255910724401474  PSNR: 19.5838565826416
[TRAIN] Iter: 834100 Loss: 0.028058413416147232  PSNR: 18.642850875854492
[TRAIN] Iter: 834200 Loss: 0.025647876784205437  PSNR: 19.245546340942383
[TRAIN] Iter: 834300 Loss: 0.02792505919933319  PSNR: 18.498138427734375
[TRAIN] Iter: 834400 Loss: 0.02206958457827568  PSNR: 19.610654830932617
[TRAIN] Iter: 834500 Loss: 0.029853302985429764  PSNR: 18.536678314208984
[TRAIN] Iter: 834600 Loss: 0.02412429079413414  PSNR: 19.282054901123047
[TRAIN] Iter: 834700 Loss: 0.03256198763847351  PSNR: 18.011621475219727
[TRAIN] Iter: 834800 Loss: 0.02562793716788292  PSNR: 19.54258918762207
[TRAIN] Iter: 834900 Loss: 0.026208270341157913  PSNR: 19.1152400970459
[TRAIN] Iter: 835000 Loss: 0.024500681087374687  PSNR: 18.969900131225586
[TRAIN] Iter: 835100 Loss: 0.0321868434548378  PSNR: 17.988628387451172
[TRAIN] Iter: 835200 Loss: 0.022972147911787033  PSNR: 19.601913452148438
[TRAIN] Iter: 835300 Loss: 0.020448513329029083  PSNR: 20.048751831054688
[TRAIN] Iter: 835400 Loss: 0.027449525892734528  PSNR: 18.909568786621094
[TRAIN] Iter: 835500 Loss: 0.021019963547587395  PSNR: 19.91567611694336
[TRAIN] Iter: 835600 Loss: 0.03073335438966751  PSNR: 18.483726501464844
[TRAIN] Iter: 835700 Loss: 0.020402275025844574  PSNR: 19.955677032470703
[TRAIN] Iter: 835800 Loss: 0.02734552137553692  PSNR: 18.867433547973633
[TRAIN] Iter: 835900 Loss: 0.029720619320869446  PSNR: 18.56009864807129
[TRAIN] Iter: 836000 Loss: 0.025070475414395332  PSNR: 19.227582931518555
[TRAIN] Iter: 836100 Loss: 0.02588089182972908  PSNR: 19.305648803710938
[TRAIN] Iter: 836200 Loss: 0.032042957842350006  PSNR: 18.066301345825195
[TRAIN] Iter: 836300 Loss: 0.023997437208890915  PSNR: 19.66300392150879
[TRAIN] Iter: 836400 Loss: 0.03002847172319889  PSNR: 18.511831283569336
[TRAIN] Iter: 836500 Loss: 0.027021531015634537  PSNR: 18.808223724365234
[TRAIN] Iter: 836600 Loss: 0.030481792986392975  PSNR: 18.440303802490234
[TRAIN] Iter: 836700 Loss: 0.020884208381175995  PSNR: 19.795713424682617
[TRAIN] Iter: 836800 Loss: 0.030788719654083252  PSNR: 18.36211585998535
[TRAIN] Iter: 836900 Loss: 0.022881459444761276  PSNR: 19.58842658996582
[TRAIN] Iter: 837000 Loss: 0.02835611253976822  PSNR: 18.63442611694336
[TRAIN] Iter: 837100 Loss: 0.021794218569993973  PSNR: 19.901098251342773
[TRAIN] Iter: 837200 Loss: 0.024021752178668976  PSNR: 19.50443458557129
[TRAIN] Iter: 837300 Loss: 0.02485562674701214  PSNR: 19.434654235839844
[TRAIN] Iter: 837400 Loss: 0.03257634490728378  PSNR: 18.01441764831543
[TRAIN] Iter: 837500 Loss: 0.02630036510527134  PSNR: 18.92604637145996
[TRAIN] Iter: 837600 Loss: 0.02572721615433693  PSNR: 19.19472885131836
[TRAIN] Iter: 837700 Loss: 0.026292741298675537  PSNR: 19.082462310791016
[TRAIN] Iter: 837800 Loss: 0.02695717290043831  PSNR: 18.907360076904297
[TRAIN] Iter: 837900 Loss: 0.028056779876351357  PSNR: 18.547948837280273
[TRAIN] Iter: 838000 Loss: 0.02396567165851593  PSNR: 19.21164894104004
[TRAIN] Iter: 838100 Loss: 0.023589201271533966  PSNR: 19.585329055786133
[TRAIN] Iter: 838200 Loss: 0.03160431236028671  PSNR: 18.190513610839844
[TRAIN] Iter: 838300 Loss: 0.03034663200378418  PSNR: 18.370534896850586
[TRAIN] Iter: 838400 Loss: 0.028494974598288536  PSNR: 18.892662048339844
[TRAIN] Iter: 838500 Loss: 0.023030152544379234  PSNR: 19.512420654296875
[TRAIN] Iter: 838600 Loss: 0.02423369139432907  PSNR: 19.243024826049805
[TRAIN] Iter: 838700 Loss: 0.02631414495408535  PSNR: 18.998687744140625
[TRAIN] Iter: 838800 Loss: 0.032745566219091415  PSNR: 17.880290985107422
[TRAIN] Iter: 838900 Loss: 0.02868746966123581  PSNR: 18.6751651763916
[TRAIN] Iter: 839000 Loss: 0.031208783388137817  PSNR: 18.24937629699707
[TRAIN] Iter: 839100 Loss: 0.026482494547963142  PSNR: 18.974620819091797
[TRAIN] Iter: 839200 Loss: 0.026878397911787033  PSNR: 18.915081024169922
[TRAIN] Iter: 839300 Loss: 0.026451721787452698  PSNR: 18.95306968688965
[TRAIN] Iter: 839400 Loss: 0.030458511784672737  PSNR: 18.5245418548584
[TRAIN] Iter: 839500 Loss: 0.03362379968166351  PSNR: 17.883386611938477
[TRAIN] Iter: 839600 Loss: 0.02699418179690838  PSNR: 18.80023765563965
[TRAIN] Iter: 839700 Loss: 0.032558195292949677  PSNR: 17.9935302734375
[TRAIN] Iter: 839800 Loss: 0.025577934458851814  PSNR: 19.142925262451172
[TRAIN] Iter: 839900 Loss: 0.026076342910528183  PSNR: 19.032485961914062
Saved checkpoints at ./logs/TUT-LAB-nerf/840000.tar
[TRAIN] Iter: 840000 Loss: 0.025903716683387756  PSNR: 19.032499313354492
[TRAIN] Iter: 840100 Loss: 0.024705899879336357  PSNR: 18.857425689697266
[TRAIN] Iter: 840200 Loss: 0.02313576266169548  PSNR: 19.758392333984375
[TRAIN] Iter: 840300 Loss: 0.027231022715568542  PSNR: 18.88083267211914
[TRAIN] Iter: 840400 Loss: 0.01746051013469696  PSNR: 20.860422134399414
[TRAIN] Iter: 840500 Loss: 0.018937261775135994  PSNR: 20.51608657836914
[TRAIN] Iter: 840600 Loss: 0.024508748203516006  PSNR: 19.123546600341797
[TRAIN] Iter: 840700 Loss: 0.020179087296128273  PSNR: 20.195289611816406
[TRAIN] Iter: 840800 Loss: 0.022665265947580338  PSNR: 19.65117073059082
[TRAIN] Iter: 840900 Loss: 0.030141320079565048  PSNR: 18.86602783203125
[TRAIN] Iter: 841000 Loss: 0.030941061675548553  PSNR: 18.278474807739258
[TRAIN] Iter: 841100 Loss: 0.02575698308646679  PSNR: 19.601808547973633
[TRAIN] Iter: 841200 Loss: 0.02863708883523941  PSNR: 18.733537673950195
[TRAIN] Iter: 841300 Loss: 0.030740758404135704  PSNR: 18.327857971191406
[TRAIN] Iter: 841400 Loss: 0.021377600729465485  PSNR: 19.887561798095703
[TRAIN] Iter: 841500 Loss: 0.03168785572052002  PSNR: 18.102741241455078
[TRAIN] Iter: 841600 Loss: 0.023455385118722916  PSNR: 19.770864486694336
[TRAIN] Iter: 841700 Loss: 0.028299976140260696  PSNR: 18.675661087036133
[TRAIN] Iter: 841800 Loss: 0.02714366279542446  PSNR: 18.955890655517578
[TRAIN] Iter: 841900 Loss: 0.024658381938934326  PSNR: 19.323333740234375
[TRAIN] Iter: 842000 Loss: 0.030876759439706802  PSNR: 18.30191993713379
[TRAIN] Iter: 842100 Loss: 0.027044404298067093  PSNR: 18.820999145507812
[TRAIN] Iter: 842200 Loss: 0.02146119996905327  PSNR: 19.92925262451172
[TRAIN] Iter: 842300 Loss: 0.030236700549721718  PSNR: 18.39933204650879
[TRAIN] Iter: 842400 Loss: 0.021879497915506363  PSNR: 20.17448616027832
[TRAIN] Iter: 842500 Loss: 0.023684516549110413  PSNR: 19.723636627197266
[TRAIN] Iter: 842600 Loss: 0.024470878764986992  PSNR: 19.256799697875977
[TRAIN] Iter: 842700 Loss: 0.02273177169263363  PSNR: 19.64946174621582
[TRAIN] Iter: 842800 Loss: 0.026894699782133102  PSNR: 18.90754508972168
[TRAIN] Iter: 842900 Loss: 0.020001113414764404  PSNR: 20.295230865478516
[TRAIN] Iter: 843000 Loss: 0.027831299230456352  PSNR: 18.958772659301758
[TRAIN] Iter: 843100 Loss: 0.02775135077536106  PSNR: 18.724458694458008
[TRAIN] Iter: 843200 Loss: 0.02573239803314209  PSNR: 19.33567237854004
[TRAIN] Iter: 843300 Loss: 0.02007436752319336  PSNR: 20.25661277770996
[TRAIN] Iter: 843400 Loss: 0.027150806039571762  PSNR: 18.641393661499023
[TRAIN] Iter: 843500 Loss: 0.03077618032693863  PSNR: 18.290231704711914
[TRAIN] Iter: 843600 Loss: 0.022707687690854073  PSNR: 19.68375015258789
[TRAIN] Iter: 843700 Loss: 0.031169096007943153  PSNR: 18.251670837402344
[TRAIN] Iter: 843800 Loss: 0.022811997681856155  PSNR: 19.563825607299805
[TRAIN] Iter: 843900 Loss: 0.0234786719083786  PSNR: 19.6451473236084
[TRAIN] Iter: 844000 Loss: 0.02200970984995365  PSNR: 19.723318099975586
[TRAIN] Iter: 844100 Loss: 0.026580926030874252  PSNR: 18.992733001708984
[TRAIN] Iter: 844200 Loss: 0.02580258809030056  PSNR: 19.013233184814453
[TRAIN] Iter: 844300 Loss: 0.026255369186401367  PSNR: 19.015972137451172
[TRAIN] Iter: 844400 Loss: 0.03237017244100571  PSNR: 18.071142196655273
[TRAIN] Iter: 844500 Loss: 0.027056682854890823  PSNR: 18.978023529052734
[TRAIN] Iter: 844600 Loss: 0.026050109416246414  PSNR: 19.041467666625977
[TRAIN] Iter: 844700 Loss: 0.026410557329654694  PSNR: 18.972658157348633
[TRAIN] Iter: 844800 Loss: 0.028707977384328842  PSNR: 18.683507919311523
[TRAIN] Iter: 844900 Loss: 0.02666410058736801  PSNR: 19.109891891479492
[TRAIN] Iter: 845000 Loss: 0.02642534300684929  PSNR: 19.175647735595703
[TRAIN] Iter: 845100 Loss: 0.028314432129263878  PSNR: 18.834102630615234
[TRAIN] Iter: 845200 Loss: 0.022715076804161072  PSNR: 19.68940544128418
[TRAIN] Iter: 845300 Loss: 0.027140874415636063  PSNR: 19.288373947143555
[TRAIN] Iter: 845400 Loss: 0.02192545309662819  PSNR: 19.57088851928711
[TRAIN] Iter: 845500 Loss: 0.031964197754859924  PSNR: 18.083051681518555
[TRAIN] Iter: 845600 Loss: 0.028577737510204315  PSNR: 18.680334091186523
[TRAIN] Iter: 845700 Loss: 0.025684623047709465  PSNR: 18.997121810913086
[TRAIN] Iter: 845800 Loss: 0.02749987319111824  PSNR: 19.101634979248047
[TRAIN] Iter: 845900 Loss: 0.031040843576192856  PSNR: 18.32636260986328
[TRAIN] Iter: 846000 Loss: 0.03280998766422272  PSNR: 18.093114852905273
[TRAIN] Iter: 846100 Loss: 0.02457534149289131  PSNR: 19.059734344482422
[TRAIN] Iter: 846200 Loss: 0.026951204985380173  PSNR: 18.846763610839844
[TRAIN] Iter: 846300 Loss: 0.02490311674773693  PSNR: 19.01439094543457
[TRAIN] Iter: 846400 Loss: 0.026388375088572502  PSNR: 18.98948860168457
[TRAIN] Iter: 846500 Loss: 0.024109721183776855  PSNR: 19.34436798095703
[TRAIN] Iter: 846600 Loss: 0.026586633175611496  PSNR: 18.960588455200195
[TRAIN] Iter: 846700 Loss: 0.03236741945147514  PSNR: 18.023601531982422
[TRAIN] Iter: 846800 Loss: 0.023326236754655838  PSNR: 19.51288414001465
[TRAIN] Iter: 846900 Loss: 0.025729116052389145  PSNR: 19.576946258544922
[TRAIN] Iter: 847000 Loss: 0.028253838419914246  PSNR: 18.657663345336914
[TRAIN] Iter: 847100 Loss: 0.02684132009744644  PSNR: 18.886093139648438
[TRAIN] Iter: 847200 Loss: 0.026235919445753098  PSNR: 19.006189346313477
[TRAIN] Iter: 847300 Loss: 0.032195478677749634  PSNR: 18.067577362060547
[TRAIN] Iter: 847400 Loss: 0.028100118041038513  PSNR: 18.735126495361328
[TRAIN] Iter: 847500 Loss: 0.032555364072322845  PSNR: 18.103288650512695
[TRAIN] Iter: 847600 Loss: 0.03227657079696655  PSNR: 17.985305786132812
[TRAIN] Iter: 847700 Loss: 0.03261847048997879  PSNR: 18.08596420288086
[TRAIN] Iter: 847800 Loss: 0.02692335471510887  PSNR: 18.844900131225586
[TRAIN] Iter: 847900 Loss: 0.03017931990325451  PSNR: 18.310565948486328
[TRAIN] Iter: 848000 Loss: 0.024669749662280083  PSNR: 19.347829818725586
[TRAIN] Iter: 848100 Loss: 0.028108028694987297  PSNR: 18.7652645111084
[TRAIN] Iter: 848200 Loss: 0.023683879524469376  PSNR: 19.248538970947266
[TRAIN] Iter: 848300 Loss: 0.025208011269569397  PSNR: 19.230552673339844
[TRAIN] Iter: 848400 Loss: 0.030407696962356567  PSNR: 18.392478942871094
[TRAIN] Iter: 848500 Loss: 0.02655685506761074  PSNR: 18.73909568786621
[TRAIN] Iter: 848600 Loss: 0.022290166467428207  PSNR: 19.452566146850586
[TRAIN] Iter: 848700 Loss: 0.031056512147188187  PSNR: 18.23952293395996
[TRAIN] Iter: 848800 Loss: 0.029044412076473236  PSNR: 18.09606170654297
[TRAIN] Iter: 848900 Loss: 0.0311141237616539  PSNR: 18.219449996948242
[TRAIN] Iter: 849000 Loss: 0.023768462240695953  PSNR: 19.450931549072266
[TRAIN] Iter: 849100 Loss: 0.028033725917339325  PSNR: 18.808509826660156
[TRAIN] Iter: 849200 Loss: 0.026874911040067673  PSNR: 18.73308753967285
[TRAIN] Iter: 849300 Loss: 0.028213977813720703  PSNR: 18.601716995239258
[TRAIN] Iter: 849400 Loss: 0.026552846655249596  PSNR: 18.87298583984375
[TRAIN] Iter: 849500 Loss: 0.032632745802402496  PSNR: 18.068557739257812
[TRAIN] Iter: 849600 Loss: 0.024111390113830566  PSNR: 19.338327407836914
[TRAIN] Iter: 849700 Loss: 0.022294584661722183  PSNR: 19.379953384399414
[TRAIN] Iter: 849800 Loss: 0.030540142208337784  PSNR: 18.392200469970703
[TRAIN] Iter: 849900 Loss: 0.03160017728805542  PSNR: 18.19840431213379
Saved checkpoints at ./logs/TUT-LAB-nerf/850000.tar
0 0.0004093647003173828
torch.Size([320, 640, 3]) torch.Size([320, 640])
1 15.694455862045288
2 16.923792123794556
3 15.797733068466187
4 16.740947008132935
5 15.665796279907227
6 16.892810344696045
7 15.88041877746582
8 16.705165147781372
9 15.88058853149414
10 16.675347089767456
11 15.864397764205933
12 15.910846710205078
13 16.86936092376709
14 16.144604682922363
15 17.117515802383423
16 17.123589515686035
17 16.244051456451416
18 16.974830150604248
19 15.942750215530396
20 15.856333494186401
21 16.97195315361023
22 15.907851696014404
23 15.784439325332642
24 15.98490834236145
25 16.94882822036743
26 16.052606105804443
27 15.326819896697998
28 14.21488070487976
29 15.189509868621826
30 14.272352695465088
31 15.223167181015015
32 14.36620569229126
33 15.079876184463501
34 14.227985143661499
35 14.25951099395752
36 15.155179977416992
37 14.325421333312988
38 15.09087872505188
39 14.277854442596436
40 15.154184103012085
41 14.236847877502441
42 15.1825852394104
43 14.263349533081055
44 14.240769863128662
45 15.183225154876709
46 14.260218381881714
47 15.167770624160767
48 14.293745994567871
49 15.1298987865448
50 14.26836085319519
51 15.12976622581482
52 14.276663064956665
53 14.240375995635986
54 15.25527572631836
55 14.221879005432129
56 15.363926649093628
57 14.199381113052368
58 15.228153467178345
59 14.27705717086792
60 14.153259038925171
61 15.435738325119019
62 14.016789674758911
63 15.288700342178345
64 14.047089338302612
65 15.579951763153076
66 14.044379711151123
67 15.344314575195312
68 14.324433088302612
69 14.08483600616455
70 15.34476113319397
71 14.076172828674316
72 15.368741750717163
73 14.153380155563354
74 15.298070669174194
75 14.077425479888916
76 15.492806673049927
77 14.16258430480957
78 14.274141550064087
79 15.2901291847229
80 14.137898683547974
81 15.429276943206787
82 13.985124111175537
83 15.041031122207642
84 13.763274192810059
85 14.038101196289062
86 14.986165046691895
87 13.985475540161133
88 15.070164203643799
89 13.769253015518188
90 15.186896324157715
91 13.741830587387085
92 15.109135150909424
93 13.724097728729248
94 14.05464482307434
95 14.854161262512207
96 14.038843870162964
97 15.156144142150879
98 14.043748140335083
99 14.898629188537598
100 13.995895385742188
101 13.969834566116333
102 14.903409004211426
103 13.946271896362305
104 14.878942251205444
105 14.022899627685547
106 14.94628357887268
107 14.001289367675781
108 13.99032473564148
109 14.915295600891113
110 14.022428750991821
111 14.907501220703125
112 13.949077606201172
113 14.971562623977661
114 13.96675157546997
115 14.016596794128418
116 14.962048768997192
117 13.991093635559082
118 14.936890602111816
119 13.97008204460144
Done, saving (120, 320, 640, 3) (120, 320, 640)
extras:{'raw': tensor([[[ 2.9705e-01,  1.2593e-01,  1.1237e-01, -1.2095e+01],
         [ 3.1506e-01,  1.2076e-01, -6.5086e-02, -4.8024e+00],
         [-5.7620e-01, -8.0714e-01, -1.2282e+00, -4.0006e+00],
         ...,
         [ 1.9344e+01,  1.9766e+01,  2.4629e+01,  2.1964e+02],
         [ 1.9064e+01,  1.9569e+01,  2.4523e+01,  2.1922e+02],
         [ 1.9746e+01,  1.9958e+01,  2.4467e+01,  2.0777e+02]],

        [[ 9.2416e-01,  6.7365e-01,  5.8087e-01, -1.2880e+01],
         [ 7.4567e-01,  5.4590e-01,  5.1639e-01, -4.6765e-01],
         [ 9.3130e-01,  8.1530e-01,  9.5132e-01,  5.5233e+00],
         ...,
         [ 1.5524e+01,  1.3716e+01,  1.3305e+01,  1.2116e+02],
         [ 1.6284e+01,  1.4423e+01,  1.4026e+01,  1.1898e+02],
         [ 1.6178e+01,  1.4364e+01,  1.4028e+01,  1.1951e+02]],

        [[-1.2303e+00, -7.5448e-01,  5.7273e-01, -1.0532e+01],
         [-1.6818e+00, -9.8840e-01,  4.1496e-01, -1.7216e+01],
         [-1.2966e+00, -1.0268e+00, -1.7465e-01, -8.7974e+00],
         ...,
         [-5.4603e+00, -4.0677e+00, -1.8877e+00, -1.1166e+00],
         [-5.1998e+00, -3.8307e+00, -1.8096e+00,  4.5795e+00],
         [-6.3968e+00, -4.6075e+00, -2.1614e+00,  3.6864e+00]],

        ...,

        [[ 3.3787e-01,  2.0807e-01,  2.8460e-01, -1.4394e+01],
         [-3.6394e-01, -4.8447e-01, -5.5602e-01, -8.9140e+00],
         [ 2.5841e-01,  4.9054e-02, -2.0749e-02,  4.6389e+00],
         ...,
         [ 4.4043e+00,  3.4549e+00,  4.7946e-01,  3.3673e+02],
         [ 5.2965e+00,  4.3192e+00,  1.4722e+00,  3.3964e+02],
         [ 4.4906e+00,  3.7848e+00,  1.3376e+00,  3.4383e+02]],

        [[ 7.4534e-01,  6.1773e-01,  6.2503e-01, -1.6494e+01],
         [ 5.4752e-01,  3.5001e-01,  2.2667e-01, -1.7109e+01],
         [ 1.9660e-01,  3.3020e-02, -7.3366e-02, -3.8931e+00],
         ...,
         [ 1.1878e+01,  5.6801e+00, -1.6921e+00,  2.5153e+02],
         [ 9.1280e+00,  3.8316e+00, -2.4049e+00,  2.4316e+02],
         [ 1.1493e+01,  5.5788e+00, -1.3177e+00,  2.4031e+02]],

        [[-1.2884e-01,  1.2003e-01,  5.8836e-01,  3.3912e+00],
         [ 4.6780e-01,  4.3806e-01,  5.3770e-01, -7.0583e+00],
         [ 3.7349e-01,  3.7812e-01,  4.2887e-01, -1.1949e+01],
         ...,
         [ 1.1198e+01,  3.8599e+00, -7.7920e+00,  4.6687e+02],
         [ 9.7956e+00,  3.5996e+00, -6.1899e+00,  4.7396e+02],
         [ 1.0547e+01,  3.6161e+00, -7.2256e+00,  4.4349e+02]]],
       grad_fn=<ReshapeAliasBackward0>), 'rgb0': tensor([[0.5837, 0.5300, 0.5043],
        [0.6555, 0.6378, 0.6467],
        [0.1550, 0.2886, 0.6823],
        ...,
        [0.5734, 0.5348, 0.5082],
        [0.6022, 0.5528, 0.5233],
        [0.5406, 0.5837, 0.6733]], grad_fn=<ReshapeAliasBackward0>), 'disp0': tensor([61.1139, 53.8265, 15.9700,  ..., 37.9465, 50.5054, 48.2921],
       grad_fn=<ReshapeAliasBackward0>), 'acc0': tensor([1., 1., 1.,  ..., 1., 1., 1.], grad_fn=<ReshapeAliasBackward0>), 'z_std': tensor([0.0032, 0.0027, 0.0683,  ..., 0.0025, 0.0031, 0.0029])}
0 0.0005347728729248047
torch.Size([320, 640, 3]) torch.Size([320, 640])
1 14.000685453414917
2 14.932177305221558
3 13.903155326843262
4 13.923008441925049
5 15.019678115844727
6 13.982069730758667
7 14.919348955154419
8 13.954529047012329
9 14.915486097335815
10 13.909136533737183
11 14.02896785736084
12 14.960660219192505
13 13.93207311630249
14 15.0004243850708
15 13.8526930809021
16 14.965675354003906
17 13.90431809425354
18 13.869765043258667
19 15.021301984786987
20 13.936343669891357
21 14.968010902404785
22 13.77821683883667
23 15.224748849868774
24 13.793867588043213
25 14.90423846244812
26 14.150200843811035
27 13.590680837631226
28 15.241822242736816
29 13.60753083229065
30 15.29146957397461
31 13.739613771438599
32 15.036261320114136
33 13.904026985168457
34 13.767704248428345
35 15.118166208267212
36 13.857000589370728
37 14.999353408813477
38 13.815124988555908
39 15.081050395965576
40 13.879253387451172
41 13.888041973114014
42 15.039672613143921
43 13.81401515007019
44 15.21703052520752
45 13.934832572937012
46 14.841960668563843
47 13.91975212097168
48 13.893671751022339
49 15.018593788146973
50 13.909298181533813
51 14.991539716720581
52 13.917824268341064
53 14.976047039031982
54 13.938404321670532
55 13.919214487075806
56 14.954167604446411
57 13.970056056976318
58 14.99661660194397
59 13.907560348510742
60 15.028100728988647
61 13.907247066497803
62 13.904316902160645
63 15.012499332427979
64 13.89664912223816
65 15.027724504470825
66 13.897235870361328
67 14.972715616226196
68 13.948503971099854
69 13.961364030838013
70 15.058007955551147
71 13.856424570083618
72 15.123486518859863
73 13.737277030944824
74 15.087487936019897
75 13.894700288772583
76 13.959501266479492
77 14.969943761825562
78 13.89356541633606
79 15.010666847229004
80 13.916017293930054
81 14.986986637115479
82 13.897070169448853
83 15.021149396896362
84 13.909929990768433
85 13.918876647949219
86 15.013724088668823
87 13.948622226715088
88 15.051255702972412
89 13.888962984085083
90 15.017660856246948
91 13.901723623275757
92 13.914704084396362
93 15.020237922668457
94 13.848021745681763
95 15.136026859283447
96 13.76621961593628
97 15.072769165039062
98 13.972105503082275
99 13.794955968856812
100 15.1029953956604
101 13.7849600315094
102 15.172160148620605
103 13.814836740493774
104 15.029801368713379
105 13.89981746673584
106 13.937009572982788
107 15.114107131958008
108 13.686767339706421
109 15.209420919418335
110 13.733479261398315
111 15.168492555618286
112 13.738068103790283
113 13.906570672988892
114 15.103795289993286
115 13.748009204864502
116 15.329441785812378
117 13.546064138412476
118 15.442099332809448
119 13.602490663528442
test poses shape torch.Size([13, 3, 4])
0 0.0007519721984863281
torch.Size([320, 640, 3]) torch.Size([320, 640])
1 15.171630382537842
2 13.796606540679932
3 15.275110483169556
4 13.62441635131836
5 15.388477087020874
6 13.741107702255249
7 15.269793272018433
8 13.865560531616211
9 13.823466539382935
10 15.12791132926941
11 13.866726160049438
12 15.287394046783447
Saved test set
[TRAIN] Iter: 850000 Loss: 0.025889229029417038  PSNR: 19.040180206298828
[TRAIN] Iter: 850100 Loss: 0.024559108540415764  PSNR: 19.12896728515625
[TRAIN] Iter: 850200 Loss: 0.026656650006771088  PSNR: 19.031230926513672
[TRAIN] Iter: 850300 Loss: 0.027753714472055435  PSNR: 18.68408203125
[TRAIN] Iter: 850400 Loss: 0.02886108309030533  PSNR: 18.62421417236328
[TRAIN] Iter: 850500 Loss: 0.024087125435471535  PSNR: 19.2030086517334
[TRAIN] Iter: 850600 Loss: 0.024261463433504105  PSNR: 19.183740615844727
[TRAIN] Iter: 850700 Loss: 0.03005673550069332  PSNR: 18.48978042602539
[TRAIN] Iter: 850800 Loss: 0.024196768179535866  PSNR: 19.45182228088379
[TRAIN] Iter: 850900 Loss: 0.03503246232867241  PSNR: 17.678585052490234
[TRAIN] Iter: 851000 Loss: 0.0277594942599535  PSNR: 18.690818786621094
[TRAIN] Iter: 851100 Loss: 0.03082607313990593  PSNR: 18.273807525634766
[TRAIN] Iter: 851200 Loss: 0.02220388501882553  PSNR: 19.763187408447266
[TRAIN] Iter: 851300 Loss: 0.03147002309560776  PSNR: 18.124616622924805
[TRAIN] Iter: 851400 Loss: 0.030031543225049973  PSNR: 18.55635643005371
[TRAIN] Iter: 851500 Loss: 0.02879071608185768  PSNR: 18.636474609375
[TRAIN] Iter: 851600 Loss: 0.028606422245502472  PSNR: 18.783580780029297
[TRAIN] Iter: 851700 Loss: 0.02032303437590599  PSNR: 19.970197677612305
[TRAIN] Iter: 851800 Loss: 0.029212361201643944  PSNR: 18.50835609436035
[TRAIN] Iter: 851900 Loss: 0.024356504902243614  PSNR: 19.547426223754883
[TRAIN] Iter: 852000 Loss: 0.03173450380563736  PSNR: 18.159399032592773
[TRAIN] Iter: 852100 Loss: 0.02527305856347084  PSNR: 19.313777923583984
[TRAIN] Iter: 852200 Loss: 0.029542462900280952  PSNR: 18.626998901367188
[TRAIN] Iter: 852300 Loss: 0.025513192638754845  PSNR: 19.0512752532959
[TRAIN] Iter: 852400 Loss: 0.02538362890481949  PSNR: 19.274856567382812
[TRAIN] Iter: 852500 Loss: 0.028642766177654266  PSNR: 18.473737716674805
[TRAIN] Iter: 852600 Loss: 0.029070759192109108  PSNR: 18.46324348449707
[TRAIN] Iter: 852700 Loss: 0.03390415757894516  PSNR: 17.86841583251953
[TRAIN] Iter: 852800 Loss: 0.027018602937459946  PSNR: 18.838457107543945
[TRAIN] Iter: 852900 Loss: 0.02786928042769432  PSNR: 18.809246063232422
[TRAIN] Iter: 853000 Loss: 0.0231102854013443  PSNR: 19.469680786132812
[TRAIN] Iter: 853100 Loss: 0.023254968225955963  PSNR: 19.71731185913086
[TRAIN] Iter: 853200 Loss: 0.025318514555692673  PSNR: 19.109352111816406
[TRAIN] Iter: 853300 Loss: 0.02856423519551754  PSNR: 18.80161476135254
[TRAIN] Iter: 853400 Loss: 0.02307852730154991  PSNR: 19.419649124145508
[TRAIN] Iter: 853500 Loss: 0.023029157891869545  PSNR: 19.72845458984375
[TRAIN] Iter: 853600 Loss: 0.02697077766060829  PSNR: 18.887819290161133
[TRAIN] Iter: 853700 Loss: 0.024143271148204803  PSNR: 19.293455123901367
[TRAIN] Iter: 853800 Loss: 0.031187238171696663  PSNR: 18.500356674194336
[TRAIN] Iter: 853900 Loss: 0.022619761526584625  PSNR: 19.53425407409668
[TRAIN] Iter: 854000 Loss: 0.02333185449242592  PSNR: 19.584880828857422
[TRAIN] Iter: 854100 Loss: 0.0253654383122921  PSNR: 18.66524887084961
[TRAIN] Iter: 854200 Loss: 0.03144366666674614  PSNR: 18.195613861083984
[TRAIN] Iter: 854300 Loss: 0.029980521649122238  PSNR: 18.355371475219727
[TRAIN] Iter: 854400 Loss: 0.030180834233760834  PSNR: 18.265947341918945
[TRAIN] Iter: 854500 Loss: 0.025282226502895355  PSNR: 19.11395263671875
[TRAIN] Iter: 854600 Loss: 0.02760428935289383  PSNR: 18.801509857177734
[TRAIN] Iter: 854700 Loss: 0.03220157325267792  PSNR: 18.054662704467773
[TRAIN] Iter: 854800 Loss: 0.02048325538635254  PSNR: 19.979076385498047
[TRAIN] Iter: 854900 Loss: 0.027737915515899658  PSNR: 18.77789878845215
[TRAIN] Iter: 855000 Loss: 0.02097194269299507  PSNR: 19.984703063964844
[TRAIN] Iter: 855100 Loss: 0.024549800902605057  PSNR: 19.21077537536621
[TRAIN] Iter: 855200 Loss: 0.02443973533809185  PSNR: 19.422382354736328
[TRAIN] Iter: 855300 Loss: 0.029671454802155495  PSNR: 18.53822135925293
[TRAIN] Iter: 855400 Loss: 0.03219342231750488  PSNR: 18.02060890197754
[TRAIN] Iter: 855500 Loss: 0.023645859211683273  PSNR: 19.632505416870117
[TRAIN] Iter: 855600 Loss: 0.023595185950398445  PSNR: 19.964950561523438
[TRAIN] Iter: 855700 Loss: 0.023917842656373978  PSNR: 19.334033966064453
[TRAIN] Iter: 855800 Loss: 0.03173834830522537  PSNR: 18.17455291748047
[TRAIN] Iter: 855900 Loss: 0.02804204821586609  PSNR: 18.707942962646484
[TRAIN] Iter: 856000 Loss: 0.02469494380056858  PSNR: 19.5231990814209
[TRAIN] Iter: 856100 Loss: 0.026768550276756287  PSNR: 19.313888549804688
[TRAIN] Iter: 856200 Loss: 0.025942500680685043  PSNR: 18.846141815185547
[TRAIN] Iter: 856300 Loss: 0.021159131079912186  PSNR: 19.867229461669922
[TRAIN] Iter: 856400 Loss: 0.02437102049589157  PSNR: 19.34217643737793
[TRAIN] Iter: 856500 Loss: 0.028945283964276314  PSNR: 18.56156349182129
[TRAIN] Iter: 856600 Loss: 0.023262135684490204  PSNR: 19.390853881835938
[TRAIN] Iter: 856700 Loss: 0.025900516659021378  PSNR: 19.20611572265625
[TRAIN] Iter: 856800 Loss: 0.027397386729717255  PSNR: 18.94430160522461
[TRAIN] Iter: 856900 Loss: 0.030874714255332947  PSNR: 18.463726043701172
[TRAIN] Iter: 857000 Loss: 0.031136171892285347  PSNR: 18.29461097717285
[TRAIN] Iter: 857100 Loss: 0.029153842478990555  PSNR: 18.60031509399414
[TRAIN] Iter: 857200 Loss: 0.02509535849094391  PSNR: 19.358312606811523
[TRAIN] Iter: 857300 Loss: 0.030839595943689346  PSNR: 18.485265731811523
[TRAIN] Iter: 857400 Loss: 0.02345965802669525  PSNR: 19.60890007019043
[TRAIN] Iter: 857500 Loss: 0.027111057192087173  PSNR: 18.922466278076172
[TRAIN] Iter: 857600 Loss: 0.026754237711429596  PSNR: 19.003267288208008
[TRAIN] Iter: 857700 Loss: 0.02756640315055847  PSNR: 18.663898468017578
[TRAIN] Iter: 857800 Loss: 0.028762109577655792  PSNR: 18.61699867248535
[TRAIN] Iter: 857900 Loss: 0.02199985831975937  PSNR: 19.72102165222168
[TRAIN] Iter: 858000 Loss: 0.026413338258862495  PSNR: 19.03638458251953
[TRAIN] Iter: 858100 Loss: 0.02430533803999424  PSNR: 19.335371017456055
[TRAIN] Iter: 858200 Loss: 0.028281133621931076  PSNR: 18.672521591186523
[TRAIN] Iter: 858300 Loss: 0.021497944369912148  PSNR: 19.86764144897461
[TRAIN] Iter: 858400 Loss: 0.028888946399092674  PSNR: 18.569664001464844
[TRAIN] Iter: 858500 Loss: 0.024332016706466675  PSNR: 19.44358253479004
[TRAIN] Iter: 858600 Loss: 0.023479705676436424  PSNR: 19.40296173095703
[TRAIN] Iter: 858700 Loss: 0.028801241889595985  PSNR: 18.539188385009766
[TRAIN] Iter: 858800 Loss: 0.029530860483646393  PSNR: 18.379150390625
[TRAIN] Iter: 858900 Loss: 0.026010673493146896  PSNR: 19.441659927368164
[TRAIN] Iter: 859000 Loss: 0.029649894684553146  PSNR: 18.33869743347168
[TRAIN] Iter: 859100 Loss: 0.03332889825105667  PSNR: 17.938688278198242
[TRAIN] Iter: 859200 Loss: 0.024483148008584976  PSNR: 19.30974769592285
[TRAIN] Iter: 859300 Loss: 0.029195599257946014  PSNR: 18.53268051147461
[TRAIN] Iter: 859400 Loss: 0.03296056389808655  PSNR: 17.8665771484375
[TRAIN] Iter: 859500 Loss: 0.02488148957490921  PSNR: 19.421098709106445
[TRAIN] Iter: 859600 Loss: 0.02646811679005623  PSNR: 19.27007484436035
[TRAIN] Iter: 859700 Loss: 0.023164644837379456  PSNR: 19.11062240600586
[TRAIN] Iter: 859800 Loss: 0.02972174622118473  PSNR: 18.79329490661621
[TRAIN] Iter: 859900 Loss: 0.02680245041847229  PSNR: 18.9630184173584
Saved checkpoints at ./logs/TUT-LAB-nerf/860000.tar
[TRAIN] Iter: 860000 Loss: 0.024291343986988068  PSNR: 19.514110565185547
[TRAIN] Iter: 860100 Loss: 0.02188006229698658  PSNR: 19.851463317871094
[TRAIN] Iter: 860200 Loss: 0.031007252633571625  PSNR: 18.33489990234375
[TRAIN] Iter: 860300 Loss: 0.030027110129594803  PSNR: 18.47138023376465
[TRAIN] Iter: 860400 Loss: 0.024458028376102448  PSNR: 19.4088191986084
[TRAIN] Iter: 860500 Loss: 0.02520441822707653  PSNR: 19.264928817749023
[TRAIN] Iter: 860600 Loss: 0.026715237647294998  PSNR: 19.000165939331055
[TRAIN] Iter: 860700 Loss: 0.024707434698939323  PSNR: 19.228525161743164
[TRAIN] Iter: 860800 Loss: 0.023779001086950302  PSNR: 19.631208419799805
[TRAIN] Iter: 860900 Loss: 0.023879509419202805  PSNR: 19.46373176574707
[TRAIN] Iter: 861000 Loss: 0.03260139375925064  PSNR: 18.19640350341797
[TRAIN] Iter: 861100 Loss: 0.025757944211363792  PSNR: 19.10652732849121
[TRAIN] Iter: 861200 Loss: 0.02800968661904335  PSNR: 18.74070930480957
[TRAIN] Iter: 861300 Loss: 0.02377282828092575  PSNR: 19.448402404785156
[TRAIN] Iter: 861400 Loss: 0.02629079483449459  PSNR: 18.890512466430664
[TRAIN] Iter: 861500 Loss: 0.027187595143914223  PSNR: 18.897422790527344
[TRAIN] Iter: 861600 Loss: 0.025361772626638412  PSNR: 19.189619064331055
[TRAIN] Iter: 861700 Loss: 0.028699588030576706  PSNR: 18.50946617126465
[TRAIN] Iter: 861800 Loss: 0.029378391802310944  PSNR: 18.626110076904297
[TRAIN] Iter: 861900 Loss: 0.027829084545373917  PSNR: 18.900049209594727
[TRAIN] Iter: 862000 Loss: 0.02267691120505333  PSNR: 19.575176239013672
[TRAIN] Iter: 862100 Loss: 0.027514353394508362  PSNR: 18.669401168823242
[TRAIN] Iter: 862200 Loss: 0.025928981602191925  PSNR: 18.99845314025879
[TRAIN] Iter: 862300 Loss: 0.02122344821691513  PSNR: 19.994762420654297
[TRAIN] Iter: 862400 Loss: 0.02206292562186718  PSNR: 19.695350646972656
[TRAIN] Iter: 862500 Loss: 0.02540591172873974  PSNR: 19.24322509765625
[TRAIN] Iter: 862600 Loss: 0.02744966372847557  PSNR: 19.13385581970215
[TRAIN] Iter: 862700 Loss: 0.025467704981565475  PSNR: 19.0778751373291
[TRAIN] Iter: 862800 Loss: 0.025950275361537933  PSNR: 18.867223739624023
[TRAIN] Iter: 862900 Loss: 0.029601609334349632  PSNR: 18.49257469177246
[TRAIN] Iter: 863000 Loss: 0.030016465112566948  PSNR: 18.50554084777832
[TRAIN] Iter: 863100 Loss: 0.020328542217612267  PSNR: 19.8005313873291
[TRAIN] Iter: 863200 Loss: 0.021983759477734566  PSNR: 19.778135299682617
[TRAIN] Iter: 863300 Loss: 0.03026089444756508  PSNR: 18.337684631347656
[TRAIN] Iter: 863400 Loss: 0.024632038548588753  PSNR: 19.00361442565918
[TRAIN] Iter: 863500 Loss: 0.03021245077252388  PSNR: 18.319944381713867
[TRAIN] Iter: 863600 Loss: 0.023832693696022034  PSNR: 19.331436157226562
[TRAIN] Iter: 863700 Loss: 0.033553559333086014  PSNR: 17.973169326782227
[TRAIN] Iter: 863800 Loss: 0.025443613529205322  PSNR: 19.18410873413086
[TRAIN] Iter: 863900 Loss: 0.025306519120931625  PSNR: 18.75368309020996
[TRAIN] Iter: 864000 Loss: 0.028979331254959106  PSNR: 18.542654037475586
[TRAIN] Iter: 864100 Loss: 0.03310316056013107  PSNR: 18.006851196289062
[TRAIN] Iter: 864200 Loss: 0.024803927168250084  PSNR: 19.15266990661621
[TRAIN] Iter: 864300 Loss: 0.020138591527938843  PSNR: 20.088998794555664
[TRAIN] Iter: 864400 Loss: 0.022737815976142883  PSNR: 19.51066780090332
[TRAIN] Iter: 864500 Loss: 0.022601602599024773  PSNR: 19.72939109802246
[TRAIN] Iter: 864600 Loss: 0.025637708604335785  PSNR: 19.18112564086914
[TRAIN] Iter: 864700 Loss: 0.027013644576072693  PSNR: 18.908910751342773
[TRAIN] Iter: 864800 Loss: 0.02441343292593956  PSNR: 19.347795486450195
[TRAIN] Iter: 864900 Loss: 0.024729449301958084  PSNR: 18.99166488647461
[TRAIN] Iter: 865000 Loss: 0.022638987749814987  PSNR: 19.73589515686035
[TRAIN] Iter: 865100 Loss: 0.03187794238328934  PSNR: 18.065629959106445
[TRAIN] Iter: 865200 Loss: 0.028579019010066986  PSNR: 18.61956024169922
[TRAIN] Iter: 865300 Loss: 0.018428761512041092  PSNR: 20.63712501525879
[TRAIN] Iter: 865400 Loss: 0.027735408395528793  PSNR: 18.558040618896484
[TRAIN] Iter: 865500 Loss: 0.02478044480085373  PSNR: 19.538738250732422
[TRAIN] Iter: 865600 Loss: 0.0265829898416996  PSNR: 19.03374671936035
[TRAIN] Iter: 865700 Loss: 0.019587039947509766  PSNR: 20.24745750427246
[TRAIN] Iter: 865800 Loss: 0.022353554144501686  PSNR: 19.78879737854004
[TRAIN] Iter: 865900 Loss: 0.02746289223432541  PSNR: 18.76603889465332
[TRAIN] Iter: 866000 Loss: 0.026547439396381378  PSNR: 18.96560287475586
[TRAIN] Iter: 866100 Loss: 0.023635465651750565  PSNR: 19.387697219848633
[TRAIN] Iter: 866200 Loss: 0.020087020471692085  PSNR: 20.144201278686523
[TRAIN] Iter: 866300 Loss: 0.026477014645934105  PSNR: 18.808635711669922
[TRAIN] Iter: 866400 Loss: 0.028136050328612328  PSNR: 18.619665145874023
[TRAIN] Iter: 866500 Loss: 0.025144122540950775  PSNR: 19.376461029052734
[TRAIN] Iter: 866600 Loss: 0.02053830400109291  PSNR: 20.020490646362305
[TRAIN] Iter: 866700 Loss: 0.03186251223087311  PSNR: 18.155717849731445
[TRAIN] Iter: 866800 Loss: 0.024577034637331963  PSNR: 19.171974182128906
[TRAIN] Iter: 866900 Loss: 0.022762157022953033  PSNR: 19.438283920288086
[TRAIN] Iter: 867000 Loss: 0.026065152138471603  PSNR: 19.183435440063477
[TRAIN] Iter: 867100 Loss: 0.0249177273362875  PSNR: 19.38260269165039
[TRAIN] Iter: 867200 Loss: 0.02814006805419922  PSNR: 18.92676544189453
[TRAIN] Iter: 867300 Loss: 0.022694643586874008  PSNR: 19.774904251098633
[TRAIN] Iter: 867400 Loss: 0.024448510259389877  PSNR: 19.429410934448242
[TRAIN] Iter: 867500 Loss: 0.024384524673223495  PSNR: 19.40760040283203
[TRAIN] Iter: 867600 Loss: 0.02946324087679386  PSNR: 18.41231918334961
[TRAIN] Iter: 867700 Loss: 0.021960504353046417  PSNR: 19.627155303955078
[TRAIN] Iter: 867800 Loss: 0.02432245761156082  PSNR: 19.366125106811523
[TRAIN] Iter: 867900 Loss: 0.026023047044873238  PSNR: 19.162738800048828
[TRAIN] Iter: 868000 Loss: 0.02667691931128502  PSNR: 19.125484466552734
[TRAIN] Iter: 868100 Loss: 0.02432280220091343  PSNR: 19.661630630493164
[TRAIN] Iter: 868200 Loss: 0.03433549031615257  PSNR: 17.853191375732422
[TRAIN] Iter: 868300 Loss: 0.028384501114487648  PSNR: 18.85146141052246
[TRAIN] Iter: 868400 Loss: 0.03180224448442459  PSNR: 18.104402542114258
[TRAIN] Iter: 868500 Loss: 0.026193611323833466  PSNR: 19.134798049926758
[TRAIN] Iter: 868600 Loss: 0.027952007949352264  PSNR: 19.03080177307129
[TRAIN] Iter: 868700 Loss: 0.02348933555185795  PSNR: 19.54827880859375
[TRAIN] Iter: 868800 Loss: 0.021570447832345963  PSNR: 19.895164489746094
[TRAIN] Iter: 868900 Loss: 0.02385394275188446  PSNR: 19.21661376953125
[TRAIN] Iter: 869000 Loss: 0.02409503236413002  PSNR: 19.109872817993164
[TRAIN] Iter: 869100 Loss: 0.02494092658162117  PSNR: 19.434226989746094
[TRAIN] Iter: 869200 Loss: 0.023640070110559464  PSNR: 19.40105438232422
[TRAIN] Iter: 869300 Loss: 0.026243802160024643  PSNR: 18.87215805053711
[TRAIN] Iter: 869400 Loss: 0.029053114354610443  PSNR: 18.623619079589844
[TRAIN] Iter: 869500 Loss: 0.025677109137177467  PSNR: 19.060302734375
[TRAIN] Iter: 869600 Loss: 0.031254298985004425  PSNR: 18.182283401489258
[TRAIN] Iter: 869700 Loss: 0.034275203943252563  PSNR: 17.81805419921875
[TRAIN] Iter: 869800 Loss: 0.021289274096488953  PSNR: 19.961889266967773
[TRAIN] Iter: 869900 Loss: 0.027368800714612007  PSNR: 18.5665225982666
Saved checkpoints at ./logs/TUT-LAB-nerf/870000.tar
[TRAIN] Iter: 870000 Loss: 0.03139283135533333  PSNR: 18.229251861572266
[TRAIN] Iter: 870100 Loss: 0.02645135670900345  PSNR: 19.365676879882812
[TRAIN] Iter: 870200 Loss: 0.030238064005970955  PSNR: 18.461185455322266
[TRAIN] Iter: 870300 Loss: 0.0249493271112442  PSNR: 19.1977481842041
[TRAIN] Iter: 870400 Loss: 0.022854629904031754  PSNR: 19.43233871459961
[TRAIN] Iter: 870500 Loss: 0.024829259142279625  PSNR: 19.432153701782227
[TRAIN] Iter: 870600 Loss: 0.020939618349075317  PSNR: 20.08596420288086
[TRAIN] Iter: 870700 Loss: 0.031142020598053932  PSNR: 18.354623794555664
[TRAIN] Iter: 870800 Loss: 0.024958722293376923  PSNR: 19.127758026123047
[TRAIN] Iter: 870900 Loss: 0.023385420441627502  PSNR: 20.073678970336914
[TRAIN] Iter: 871000 Loss: 0.02601519227027893  PSNR: 19.127685546875
[TRAIN] Iter: 871100 Loss: 0.020247753709554672  PSNR: 20.116174697875977
[TRAIN] Iter: 871200 Loss: 0.029968027025461197  PSNR: 18.540605545043945
[TRAIN] Iter: 871300 Loss: 0.028232449665665627  PSNR: 18.688058853149414
[TRAIN] Iter: 871400 Loss: 0.028077613562345505  PSNR: 18.60803985595703
[TRAIN] Iter: 871500 Loss: 0.020134031772613525  PSNR: 20.070390701293945
[TRAIN] Iter: 871600 Loss: 0.026196839287877083  PSNR: 19.109243392944336
[TRAIN] Iter: 871700 Loss: 0.03139615058898926  PSNR: 18.286767959594727
[TRAIN] Iter: 871800 Loss: 0.025368493050336838  PSNR: 19.062864303588867
[TRAIN] Iter: 871900 Loss: 0.02945561334490776  PSNR: 18.403039932250977
[TRAIN] Iter: 872000 Loss: 0.022636735811829567  PSNR: 19.36981964111328
[TRAIN] Iter: 872100 Loss: 0.03204333409667015  PSNR: 18.183809280395508
[TRAIN] Iter: 872200 Loss: 0.023654837161302567  PSNR: 19.849224090576172
[TRAIN] Iter: 872300 Loss: 0.02823874168097973  PSNR: 18.659957885742188
[TRAIN] Iter: 872400 Loss: 0.024254368618130684  PSNR: 19.607349395751953
[TRAIN] Iter: 872500 Loss: 0.02931024134159088  PSNR: 18.461891174316406
[TRAIN] Iter: 872600 Loss: 0.02517087012529373  PSNR: 19.113142013549805
[TRAIN] Iter: 872700 Loss: 0.024503709748387337  PSNR: 19.515867233276367
[TRAIN] Iter: 872800 Loss: 0.026742998510599136  PSNR: 18.94874382019043
[TRAIN] Iter: 872900 Loss: 0.025785572826862335  PSNR: 19.148923873901367
[TRAIN] Iter: 873000 Loss: 0.030592843890190125  PSNR: 18.35918617248535
[TRAIN] Iter: 873100 Loss: 0.030822331085801125  PSNR: 18.19610023498535
[TRAIN] Iter: 873200 Loss: 0.028988860547542572  PSNR: 18.656230926513672
[TRAIN] Iter: 873300 Loss: 0.03161022439599037  PSNR: 18.05398941040039
[TRAIN] Iter: 873400 Loss: 0.021704021841287613  PSNR: 19.901464462280273
[TRAIN] Iter: 873500 Loss: 0.030751176178455353  PSNR: 18.300416946411133
[TRAIN] Iter: 873600 Loss: 0.022960800677537918  PSNR: 19.732547760009766
[TRAIN] Iter: 873700 Loss: 0.020654620602726936  PSNR: 19.952880859375
[TRAIN] Iter: 873800 Loss: 0.028897222131490707  PSNR: 18.97089385986328
[TRAIN] Iter: 873900 Loss: 0.02794904075562954  PSNR: 18.779300689697266
[TRAIN] Iter: 874000 Loss: 0.025064527988433838  PSNR: 19.162683486938477
[TRAIN] Iter: 874100 Loss: 0.029096081852912903  PSNR: 18.658096313476562
[TRAIN] Iter: 874200 Loss: 0.027096495032310486  PSNR: 18.632545471191406
[TRAIN] Iter: 874300 Loss: 0.03141193091869354  PSNR: 18.151819229125977
[TRAIN] Iter: 874400 Loss: 0.02915695682168007  PSNR: 18.502962112426758
[TRAIN] Iter: 874500 Loss: 0.023844778537750244  PSNR: 19.63081169128418
[TRAIN] Iter: 874600 Loss: 0.03176131844520569  PSNR: 18.241931915283203
[TRAIN] Iter: 874700 Loss: 0.02483450248837471  PSNR: 19.492387771606445
[TRAIN] Iter: 874800 Loss: 0.02752560004591942  PSNR: 18.78750991821289
[TRAIN] Iter: 874900 Loss: 0.02272902987897396  PSNR: 19.408472061157227
[TRAIN] Iter: 875000 Loss: 0.02487785927951336  PSNR: 19.19350242614746
[TRAIN] Iter: 875100 Loss: 0.02909754030406475  PSNR: 18.585041046142578
[TRAIN] Iter: 875200 Loss: 0.025290213525295258  PSNR: 19.529747009277344
[TRAIN] Iter: 875300 Loss: 0.022081833332777023  PSNR: 19.49762725830078
[TRAIN] Iter: 875400 Loss: 0.024234212934970856  PSNR: 19.800535202026367
[TRAIN] Iter: 875500 Loss: 0.02859286591410637  PSNR: 18.969261169433594
[TRAIN] Iter: 875600 Loss: 0.026713257655501366  PSNR: 18.962026596069336
[TRAIN] Iter: 875700 Loss: 0.023132149130105972  PSNR: 19.397485733032227
[TRAIN] Iter: 875800 Loss: 0.021666690707206726  PSNR: 20.009885787963867
[TRAIN] Iter: 875900 Loss: 0.025780759751796722  PSNR: 18.795852661132812
[TRAIN] Iter: 876000 Loss: 0.025935931131243706  PSNR: 18.951507568359375
[TRAIN] Iter: 876100 Loss: 0.027795791625976562  PSNR: 18.744592666625977
[TRAIN] Iter: 876200 Loss: 0.024240165948867798  PSNR: 19.526206970214844
[TRAIN] Iter: 876300 Loss: 0.025896992534399033  PSNR: 19.220191955566406
[TRAIN] Iter: 876400 Loss: 0.024358782917261124  PSNR: 19.247739791870117
[TRAIN] Iter: 876500 Loss: 0.027977462857961655  PSNR: 18.718584060668945
[TRAIN] Iter: 876600 Loss: 0.02357407659292221  PSNR: 19.674123764038086
[TRAIN] Iter: 876700 Loss: 0.024159274995326996  PSNR: 19.350587844848633
[TRAIN] Iter: 876800 Loss: 0.028195712715387344  PSNR: 18.616226196289062
[TRAIN] Iter: 876900 Loss: 0.02936249040067196  PSNR: 18.4695987701416
[TRAIN] Iter: 877000 Loss: 0.02472643367946148  PSNR: 19.06498146057129
[TRAIN] Iter: 877100 Loss: 0.022432440891861916  PSNR: 19.517194747924805
[TRAIN] Iter: 877200 Loss: 0.026967911049723625  PSNR: 18.711318969726562
[TRAIN] Iter: 877300 Loss: 0.025044025853276253  PSNR: 19.3282413482666
[TRAIN] Iter: 877400 Loss: 0.026464972645044327  PSNR: 18.91349983215332
[TRAIN] Iter: 877500 Loss: 0.02437383122742176  PSNR: 19.667152404785156
[TRAIN] Iter: 877600 Loss: 0.022186269983649254  PSNR: 19.758464813232422
[TRAIN] Iter: 877700 Loss: 0.032550614327192307  PSNR: 18.141489028930664
[TRAIN] Iter: 877800 Loss: 0.029864009469747543  PSNR: 18.354352951049805
[TRAIN] Iter: 877900 Loss: 0.02535630203783512  PSNR: 19.478069305419922
[TRAIN] Iter: 878000 Loss: 0.03166269510984421  PSNR: 18.23714828491211
[TRAIN] Iter: 878100 Loss: 0.021148543804883957  PSNR: 20.0087947845459
[TRAIN] Iter: 878200 Loss: 0.024842645972967148  PSNR: 19.10985565185547
[TRAIN] Iter: 878300 Loss: 0.024404175579547882  PSNR: 19.445846557617188
[TRAIN] Iter: 878400 Loss: 0.020965076982975006  PSNR: 20.092174530029297
[TRAIN] Iter: 878500 Loss: 0.024084750562906265  PSNR: 19.355920791625977
[TRAIN] Iter: 878600 Loss: 0.028084730729460716  PSNR: 18.723648071289062
[TRAIN] Iter: 878700 Loss: 0.028891101479530334  PSNR: 18.497310638427734
[TRAIN] Iter: 878800 Loss: 0.026983335614204407  PSNR: 18.634878158569336
[TRAIN] Iter: 878900 Loss: 0.02332748845219612  PSNR: 19.502538681030273
[TRAIN] Iter: 879000 Loss: 0.026483211666345596  PSNR: 18.967561721801758
[TRAIN] Iter: 879100 Loss: 0.02602817490696907  PSNR: 19.2022762298584
[TRAIN] Iter: 879200 Loss: 0.026179006323218346  PSNR: 19.12051773071289
[TRAIN] Iter: 879300 Loss: 0.020626887679100037  PSNR: 20.06869125366211
[TRAIN] Iter: 879400 Loss: 0.029245389625430107  PSNR: 18.29216957092285
[TRAIN] Iter: 879500 Loss: 0.024454912170767784  PSNR: 19.40019416809082
[TRAIN] Iter: 879600 Loss: 0.028727492317557335  PSNR: 18.658533096313477
[TRAIN] Iter: 879700 Loss: 0.02398809790611267  PSNR: 19.434856414794922
[TRAIN] Iter: 879800 Loss: 0.03289481997489929  PSNR: 18.023874282836914
[TRAIN] Iter: 879900 Loss: 0.02518056333065033  PSNR: 19.188934326171875
Saved checkpoints at ./logs/TUT-LAB-nerf/880000.tar
[TRAIN] Iter: 880000 Loss: 0.022799260914325714  PSNR: 19.46714973449707
[TRAIN] Iter: 880100 Loss: 0.033913370221853256  PSNR: 17.857765197753906
[TRAIN] Iter: 880200 Loss: 0.029324300587177277  PSNR: 18.5450496673584
[TRAIN] Iter: 880300 Loss: 0.026473945006728172  PSNR: 18.958250045776367
[TRAIN] Iter: 880400 Loss: 0.0232301764190197  PSNR: 19.574966430664062
[TRAIN] Iter: 880500 Loss: 0.02148144133388996  PSNR: 19.72714614868164
[TRAIN] Iter: 880600 Loss: 0.029799867421388626  PSNR: 18.339067459106445
[TRAIN] Iter: 880700 Loss: 0.03292170912027359  PSNR: 18.075523376464844
[TRAIN] Iter: 880800 Loss: 0.030659690499305725  PSNR: 18.323659896850586
[TRAIN] Iter: 880900 Loss: 0.030538568273186684  PSNR: 18.44585418701172
[TRAIN] Iter: 881000 Loss: 0.024977870285511017  PSNR: 19.198457717895508
[TRAIN] Iter: 881100 Loss: 0.023981856182217598  PSNR: 19.431541442871094
[TRAIN] Iter: 881200 Loss: 0.025832466781139374  PSNR: 19.40756607055664
[TRAIN] Iter: 881300 Loss: 0.027527224272489548  PSNR: 18.644086837768555
[TRAIN] Iter: 881400 Loss: 0.023168787360191345  PSNR: 19.725299835205078
[TRAIN] Iter: 881500 Loss: 0.028197789564728737  PSNR: 18.723848342895508
[TRAIN] Iter: 881600 Loss: 0.03120753914117813  PSNR: 18.296924591064453
[TRAIN] Iter: 881700 Loss: 0.021991398185491562  PSNR: 19.498748779296875
[TRAIN] Iter: 881800 Loss: 0.023453257977962494  PSNR: 19.634124755859375
[TRAIN] Iter: 881900 Loss: 0.028235819190740585  PSNR: 18.965734481811523
[TRAIN] Iter: 882000 Loss: 0.027551252394914627  PSNR: 19.087827682495117
[TRAIN] Iter: 882100 Loss: 0.030629634857177734  PSNR: 18.422508239746094
[TRAIN] Iter: 882200 Loss: 0.029575947672128677  PSNR: 18.572425842285156
[TRAIN] Iter: 882300 Loss: 0.028190676122903824  PSNR: 18.74945068359375
[TRAIN] Iter: 882400 Loss: 0.0303292628377676  PSNR: 18.280990600585938
[TRAIN] Iter: 882500 Loss: 0.020955178886651993  PSNR: 20.052370071411133
[TRAIN] Iter: 882600 Loss: 0.025296367704868317  PSNR: 19.045270919799805
[TRAIN] Iter: 882700 Loss: 0.030427642166614532  PSNR: 18.48057746887207
[TRAIN] Iter: 882800 Loss: 0.030239511281251907  PSNR: 18.36434555053711
[TRAIN] Iter: 882900 Loss: 0.02971402183175087  PSNR: 18.41176414489746
[TRAIN] Iter: 883000 Loss: 0.024408191442489624  PSNR: 19.29410743713379
[TRAIN] Iter: 883100 Loss: 0.02805856242775917  PSNR: 18.718143463134766
[TRAIN] Iter: 883200 Loss: 0.028582390397787094  PSNR: 18.711944580078125
[TRAIN] Iter: 883300 Loss: 0.029505282640457153  PSNR: 18.512033462524414
[TRAIN] Iter: 883400 Loss: 0.025547537952661514  PSNR: 18.978086471557617
[TRAIN] Iter: 883500 Loss: 0.030127983540296555  PSNR: 18.415111541748047
[TRAIN] Iter: 883600 Loss: 0.031234797090291977  PSNR: 18.095664978027344
[TRAIN] Iter: 883700 Loss: 0.029548604041337967  PSNR: 18.396432876586914
[TRAIN] Iter: 883800 Loss: 0.025341983884572983  PSNR: 19.251989364624023
[TRAIN] Iter: 883900 Loss: 0.0327349528670311  PSNR: 17.987424850463867
[TRAIN] Iter: 884000 Loss: 0.031003709882497787  PSNR: 18.275209426879883
[TRAIN] Iter: 884100 Loss: 0.02963925153017044  PSNR: 18.41035270690918
[TRAIN] Iter: 884200 Loss: 0.030703891068696976  PSNR: 18.561635971069336
[TRAIN] Iter: 884300 Loss: 0.02135937288403511  PSNR: 20.04145622253418
[TRAIN] Iter: 884400 Loss: 0.02674845978617668  PSNR: 18.896772384643555
[TRAIN] Iter: 884500 Loss: 0.03240704908967018  PSNR: 18.020490646362305
[TRAIN] Iter: 884600 Loss: 0.02569960616528988  PSNR: 19.02018928527832
[TRAIN] Iter: 884700 Loss: 0.027223244309425354  PSNR: 18.778615951538086
[TRAIN] Iter: 884800 Loss: 0.02885490469634533  PSNR: 18.520063400268555
[TRAIN] Iter: 884900 Loss: 0.030643006786704063  PSNR: 18.34520721435547
[TRAIN] Iter: 885000 Loss: 0.025203492492437363  PSNR: 19.202255249023438
[TRAIN] Iter: 885100 Loss: 0.024782128632068634  PSNR: 19.105316162109375
[TRAIN] Iter: 885200 Loss: 0.023816227912902832  PSNR: 19.363143920898438
[TRAIN] Iter: 885300 Loss: 0.024829886853694916  PSNR: 19.525117874145508
[TRAIN] Iter: 885400 Loss: 0.020482568070292473  PSNR: 20.207284927368164
[TRAIN] Iter: 885500 Loss: 0.033298008143901825  PSNR: 17.918746948242188
[TRAIN] Iter: 885600 Loss: 0.024315591901540756  PSNR: 19.47846031188965
[TRAIN] Iter: 885700 Loss: 0.029085487127304077  PSNR: 18.491147994995117
[TRAIN] Iter: 885800 Loss: 0.024732142686843872  PSNR: 19.369260787963867
[TRAIN] Iter: 885900 Loss: 0.02724255435168743  PSNR: 18.777950286865234
[TRAIN] Iter: 886000 Loss: 0.02879374660551548  PSNR: 18.810731887817383
[TRAIN] Iter: 886100 Loss: 0.027027811855077744  PSNR: 18.95955467224121
[TRAIN] Iter: 886200 Loss: 0.02978523075580597  PSNR: 18.461217880249023
[TRAIN] Iter: 886300 Loss: 0.02521803416311741  PSNR: 19.414369583129883
[TRAIN] Iter: 886400 Loss: 0.03331737592816353  PSNR: 17.9484806060791
[TRAIN] Iter: 886500 Loss: 0.02473733387887478  PSNR: 19.305255889892578
[TRAIN] Iter: 886600 Loss: 0.028118809685111046  PSNR: 18.835344314575195
[TRAIN] Iter: 886700 Loss: 0.025708379223942757  PSNR: 19.09547996520996
[TRAIN] Iter: 886800 Loss: 0.027218161150813103  PSNR: 19.012895584106445
[TRAIN] Iter: 886900 Loss: 0.03142990916967392  PSNR: 18.167551040649414
[TRAIN] Iter: 887000 Loss: 0.026387635618448257  PSNR: 18.875757217407227
[TRAIN] Iter: 887100 Loss: 0.03331335261464119  PSNR: 17.995986938476562
[TRAIN] Iter: 887200 Loss: 0.02428612858057022  PSNR: 19.32282829284668
[TRAIN] Iter: 887300 Loss: 0.025332100689411163  PSNR: 18.845422744750977
[TRAIN] Iter: 887400 Loss: 0.025221791118383408  PSNR: 19.339994430541992
[TRAIN] Iter: 887500 Loss: 0.027305372059345245  PSNR: 18.80837059020996
[TRAIN] Iter: 887600 Loss: 0.030297119170427322  PSNR: 18.23372459411621
[TRAIN] Iter: 887700 Loss: 0.023751383647322655  PSNR: 19.51412582397461
[TRAIN] Iter: 887800 Loss: 0.02607954852283001  PSNR: 19.027971267700195
[TRAIN] Iter: 887900 Loss: 0.03736977279186249  PSNR: 17.475662231445312
[TRAIN] Iter: 888000 Loss: 0.02849913015961647  PSNR: 18.579986572265625
[TRAIN] Iter: 888100 Loss: 0.021386317908763885  PSNR: 20.04532814025879
[TRAIN] Iter: 888200 Loss: 0.022651955485343933  PSNR: 19.63286018371582
[TRAIN] Iter: 888300 Loss: 0.023876499384641647  PSNR: 19.615808486938477
[TRAIN] Iter: 888400 Loss: 0.0326283797621727  PSNR: 17.983076095581055
[TRAIN] Iter: 888500 Loss: 0.02556748501956463  PSNR: 19.201824188232422
[TRAIN] Iter: 888600 Loss: 0.02364799752831459  PSNR: 19.519020080566406
[TRAIN] Iter: 888700 Loss: 0.02589772455394268  PSNR: 19.17164421081543
[TRAIN] Iter: 888800 Loss: 0.029352569952607155  PSNR: 18.685667037963867
[TRAIN] Iter: 888900 Loss: 0.027254614979028702  PSNR: 18.917774200439453
[TRAIN] Iter: 889000 Loss: 0.027745647355914116  PSNR: 18.793760299682617
[TRAIN] Iter: 889100 Loss: 0.028195783495903015  PSNR: 18.553319931030273
[TRAIN] Iter: 889200 Loss: 0.030522339046001434  PSNR: 18.299531936645508
[TRAIN] Iter: 889300 Loss: 0.02679327130317688  PSNR: 18.802471160888672
[TRAIN] Iter: 889400 Loss: 0.03469684720039368  PSNR: 17.755983352661133
[TRAIN] Iter: 889500 Loss: 0.025788363069295883  PSNR: 19.262601852416992
[TRAIN] Iter: 889600 Loss: 0.0235289316624403  PSNR: 19.49875831604004
[TRAIN] Iter: 889700 Loss: 0.02859608642756939  PSNR: 18.514263153076172
[TRAIN] Iter: 889800 Loss: 0.03174194321036339  PSNR: 18.179636001586914
[TRAIN] Iter: 889900 Loss: 0.025585699826478958  PSNR: 19.291725158691406
Saved checkpoints at ./logs/TUT-LAB-nerf/890000.tar
[TRAIN] Iter: 890000 Loss: 0.02153037115931511  PSNR: 20.104557037353516
[TRAIN] Iter: 890100 Loss: 0.03157602623105049  PSNR: 18.18260955810547
[TRAIN] Iter: 890200 Loss: 0.020665455609560013  PSNR: 20.026552200317383
[TRAIN] Iter: 890300 Loss: 0.02714148908853531  PSNR: 18.803794860839844
[TRAIN] Iter: 890400 Loss: 0.019736021757125854  PSNR: 20.212739944458008
[TRAIN] Iter: 890500 Loss: 0.025769464671611786  PSNR: 18.97361183166504
[TRAIN] Iter: 890600 Loss: 0.03154096007347107  PSNR: 18.22393035888672
[TRAIN] Iter: 890700 Loss: 0.023662788793444633  PSNR: 19.61346435546875
[TRAIN] Iter: 890800 Loss: 0.02929042838513851  PSNR: 18.558591842651367
[TRAIN] Iter: 890900 Loss: 0.017814677208662033  PSNR: 20.761821746826172
[TRAIN] Iter: 891000 Loss: 0.022497300058603287  PSNR: 19.64645004272461
[TRAIN] Iter: 891100 Loss: 0.030762556940317154  PSNR: 18.297618865966797
[TRAIN] Iter: 891200 Loss: 0.023595819249749184  PSNR: 19.260387420654297
[TRAIN] Iter: 891300 Loss: 0.02806127443909645  PSNR: 18.83176612854004
[TRAIN] Iter: 891400 Loss: 0.025232654064893723  PSNR: 19.043787002563477
[TRAIN] Iter: 891500 Loss: 0.024030834436416626  PSNR: 19.336559295654297
[TRAIN] Iter: 891600 Loss: 0.024960966780781746  PSNR: 18.994535446166992
[TRAIN] Iter: 891700 Loss: 0.024259988218545914  PSNR: 19.42565155029297
[TRAIN] Iter: 891800 Loss: 0.027353398501873016  PSNR: 18.846040725708008
[TRAIN] Iter: 891900 Loss: 0.03066062740981579  PSNR: 18.243101119995117
[TRAIN] Iter: 892000 Loss: 0.02629322186112404  PSNR: 18.814268112182617
[TRAIN] Iter: 892100 Loss: 0.031593840569257736  PSNR: 18.29408073425293
[TRAIN] Iter: 892200 Loss: 0.03207401558756828  PSNR: 18.137929916381836
[TRAIN] Iter: 892300 Loss: 0.03426629677414894  PSNR: 17.7736873626709
[TRAIN] Iter: 892400 Loss: 0.025115391239523888  PSNR: 19.065265655517578
[TRAIN] Iter: 892500 Loss: 0.03129544109106064  PSNR: 18.290124893188477
[TRAIN] Iter: 892600 Loss: 0.021595541387796402  PSNR: 19.734848022460938
[TRAIN] Iter: 892700 Loss: 0.025126293301582336  PSNR: 19.103084564208984
[TRAIN] Iter: 892800 Loss: 0.0310443714261055  PSNR: 18.300195693969727
[TRAIN] Iter: 892900 Loss: 0.02341165952384472  PSNR: 19.790754318237305
[TRAIN] Iter: 893000 Loss: 0.02804974839091301  PSNR: 18.619821548461914
[TRAIN] Iter: 893100 Loss: 0.02153279259800911  PSNR: 20.047792434692383
[TRAIN] Iter: 893200 Loss: 0.020449865609407425  PSNR: 20.039426803588867
[TRAIN] Iter: 893300 Loss: 0.023741330951452255  PSNR: 19.42169952392578
[TRAIN] Iter: 893400 Loss: 0.02307811751961708  PSNR: 19.39923667907715
[TRAIN] Iter: 893500 Loss: 0.020436614751815796  PSNR: 20.010278701782227
[TRAIN] Iter: 893600 Loss: 0.021911587566137314  PSNR: 19.80681800842285
[TRAIN] Iter: 893700 Loss: 0.027478404343128204  PSNR: 18.83181381225586
[TRAIN] Iter: 893800 Loss: 0.019958145916461945  PSNR: 20.431411743164062
[TRAIN] Iter: 893900 Loss: 0.023620331659913063  PSNR: 19.304828643798828
[TRAIN] Iter: 894000 Loss: 0.027081044390797615  PSNR: 19.05769157409668
[TRAIN] Iter: 894100 Loss: 0.018635008484125137  PSNR: 20.482820510864258
[TRAIN] Iter: 894200 Loss: 0.026999257504940033  PSNR: 18.856002807617188
[TRAIN] Iter: 894300 Loss: 0.027174990624189377  PSNR: 18.7877197265625
[TRAIN] Iter: 894400 Loss: 0.02752886898815632  PSNR: 18.581356048583984
[TRAIN] Iter: 894500 Loss: 0.02503509819507599  PSNR: 19.36421012878418
[TRAIN] Iter: 894600 Loss: 0.025383517146110535  PSNR: 19.25689125061035
[TRAIN] Iter: 894700 Loss: 0.02191701903939247  PSNR: 19.61627197265625
[TRAIN] Iter: 894800 Loss: 0.023076187819242477  PSNR: 19.561134338378906
[TRAIN] Iter: 894900 Loss: 0.029441118240356445  PSNR: 18.702438354492188
[TRAIN] Iter: 895000 Loss: 0.028336508199572563  PSNR: 18.64742660522461
[TRAIN] Iter: 895100 Loss: 0.028802283108234406  PSNR: 18.65115737915039
[TRAIN] Iter: 895200 Loss: 0.029343491420149803  PSNR: 18.488977432250977
[TRAIN] Iter: 895300 Loss: 0.025716014206409454  PSNR: 19.34715461730957
[TRAIN] Iter: 895400 Loss: 0.02667188085615635  PSNR: 18.78131675720215
[TRAIN] Iter: 895500 Loss: 0.025254052132368088  PSNR: 19.18495750427246
[TRAIN] Iter: 895600 Loss: 0.025848008692264557  PSNR: 19.132095336914062
[TRAIN] Iter: 895700 Loss: 0.025342488661408424  PSNR: 18.923030853271484
[TRAIN] Iter: 895800 Loss: 0.022487331181764603  PSNR: 19.66053009033203
[TRAIN] Iter: 895900 Loss: 0.024691147729754448  PSNR: 19.344430923461914
[TRAIN] Iter: 896000 Loss: 0.029965225607156754  PSNR: 18.611276626586914
[TRAIN] Iter: 896100 Loss: 0.024612735956907272  PSNR: 19.32275390625
[TRAIN] Iter: 896200 Loss: 0.025847535580396652  PSNR: 19.164216995239258
[TRAIN] Iter: 896300 Loss: 0.02040730230510235  PSNR: 20.0325984954834
[TRAIN] Iter: 896400 Loss: 0.02618705853819847  PSNR: 19.01790428161621
[TRAIN] Iter: 896500 Loss: 0.025114238262176514  PSNR: 19.2347469329834
[TRAIN] Iter: 896600 Loss: 0.02950330078601837  PSNR: 18.4125919342041
[TRAIN] Iter: 896700 Loss: 0.03205209970474243  PSNR: 17.97331428527832
[TRAIN] Iter: 896800 Loss: 0.023379674181342125  PSNR: 19.549985885620117
[TRAIN] Iter: 896900 Loss: 0.021886948496103287  PSNR: 19.72966957092285
[TRAIN] Iter: 897000 Loss: 0.027787279337644577  PSNR: 18.726097106933594
[TRAIN] Iter: 897100 Loss: 0.03131430596113205  PSNR: 18.374958038330078
[TRAIN] Iter: 897200 Loss: 0.035923656076192856  PSNR: 17.63543701171875
[TRAIN] Iter: 897300 Loss: 0.02724359929561615  PSNR: 19.066667556762695
[TRAIN] Iter: 897400 Loss: 0.028382115066051483  PSNR: 18.603986740112305
[TRAIN] Iter: 897500 Loss: 0.02677447535097599  PSNR: 18.86669158935547
[TRAIN] Iter: 897600 Loss: 0.02323809452354908  PSNR: 19.45521354675293
[TRAIN] Iter: 897700 Loss: 0.026912571862339973  PSNR: 18.973472595214844
[TRAIN] Iter: 897800 Loss: 0.023245330899953842  PSNR: 19.605785369873047
[TRAIN] Iter: 897900 Loss: 0.024455411359667778  PSNR: 19.323951721191406
[TRAIN] Iter: 898000 Loss: 0.025239575654268265  PSNR: 19.16917610168457
[TRAIN] Iter: 898100 Loss: 0.032221242785453796  PSNR: 18.106428146362305
[TRAIN] Iter: 898200 Loss: 0.027872135862708092  PSNR: 18.5767879486084
[TRAIN] Iter: 898300 Loss: 0.021839607506990433  PSNR: 19.706377029418945
[TRAIN] Iter: 898400 Loss: 0.027652742341160774  PSNR: 18.678569793701172
[TRAIN] Iter: 898500 Loss: 0.02130666747689247  PSNR: 19.910724639892578
[TRAIN] Iter: 898600 Loss: 0.022973448038101196  PSNR: 19.406858444213867
[TRAIN] Iter: 898700 Loss: 0.032273054122924805  PSNR: 18.074586868286133
[TRAIN] Iter: 898800 Loss: 0.03143581748008728  PSNR: 18.19879150390625
[TRAIN] Iter: 898900 Loss: 0.02391747385263443  PSNR: 19.400897979736328
[TRAIN] Iter: 899000 Loss: 0.03269519656896591  PSNR: 18.050424575805664
[TRAIN] Iter: 899100 Loss: 0.029888680204749107  PSNR: 18.52405548095703
[TRAIN] Iter: 899200 Loss: 0.025570422410964966  PSNR: 19.227083206176758
[TRAIN] Iter: 899300 Loss: 0.029540833085775375  PSNR: 18.84793472290039
[TRAIN] Iter: 899400 Loss: 0.029874514788389206  PSNR: 18.451557159423828
[TRAIN] Iter: 899500 Loss: 0.025758981704711914  PSNR: 19.002538681030273
[TRAIN] Iter: 899600 Loss: 0.031121395528316498  PSNR: 18.244611740112305
[TRAIN] Iter: 899700 Loss: 0.023796867579221725  PSNR: 19.432912826538086
[TRAIN] Iter: 899800 Loss: 0.0254141166806221  PSNR: 18.91193389892578
[TRAIN] Iter: 899900 Loss: 0.023487141355872154  PSNR: 19.217893600463867
Saved checkpoints at ./logs/TUT-LAB-nerf/900000.tar
0 0.0004436969757080078
torch.Size([320, 640, 3]) torch.Size([320, 640])
1 15.59092664718628
2 16.96302342414856
3 15.560466289520264
4 17.002734661102295
5 15.543205499649048
6 17.011218309402466
7 15.558986902236938
8 17.02306628227234
9 15.479464054107666
10 16.990294694900513
11 15.539875745773315
12 12.383378505706787
13 16.879857778549194
14 15.441101312637329
15 16.816533088684082
16 15.03757095336914
17 15.208779335021973
18 13.657993078231812
19 15.370641708374023
20 13.878590106964111
21 13.820686101913452
22 14.993719339370728
23 13.743233680725098
24 15.205166339874268
25 13.740551233291626
26 15.206815004348755
27 13.735870361328125
28 13.727200031280518
29 15.25434684753418
30 13.651408910751343
31 15.36863112449646
32 13.648608207702637
33 15.328054904937744
34 13.870062351226807
35 13.631238222122192
36 15.33765721321106
37 13.587314128875732
38 15.42755126953125
39 13.66966462135315
40 15.250252962112427
41 13.797388553619385
42 13.648630857467651
43 15.486597776412964
44 13.517044067382812
45 15.553430557250977
46 13.520534992218018
47 15.373181819915771
48 13.66049599647522
49 13.570717334747314
50 14.102678298950195
51 16.49574089050293
52 13.173505067825317
53 15.157784223556519
54 13.491641998291016
55 15.305574655532837
56 13.741560697555542
57 13.860532760620117
58 15.515945672988892
59 14.047407150268555
60 15.698153257369995
61 13.726023197174072
62 15.68221402168274
63 13.883584022521973
64 14.405064105987549
65 15.207525253295898
66 14.185592651367188
67 15.458818435668945
68 13.914737462997437
69 15.618571281433105
70 13.770109176635742
71 15.65125060081482
72 13.889612436294556
73 14.52198338508606
74 15.121113777160645
75 14.204488754272461
76 15.575186967849731
77 14.377628803253174
78 14.914199352264404
79 13.72703218460083
80 15.712994813919067
81 14.299115419387817
82 14.251536846160889
83 15.092634916305542
84 14.22437834739685
85 15.221452951431274
86 14.145816326141357
87 15.280259609222412
88 14.165170907974243
89 15.242970943450928
90 14.20585322380066
91 14.239014148712158
92 15.156186819076538
93 14.166276693344116
94 15.18458104133606
95 14.236093521118164
96 15.406898260116577
97 13.94191026687622
98 14.269631624221802
99 15.474162578582764
100 14.13587498664856
101 15.195289373397827
102 13.894436120986938
103 15.537011861801147
104 14.056899547576904
105 15.29000449180603
106 14.106385231018066
107 14.064851999282837
108 15.485334396362305
109 14.042567729949951
110 15.39724063873291
111 14.072092056274414
112 15.44603705406189
113 14.04625678062439
114 15.389940738677979
115 14.09893274307251
116 14.163948059082031
117 15.38357424736023
118 14.071442127227783
119 15.324025630950928
Done, saving (120, 320, 640, 3) (120, 320, 640)
extras:{'raw': tensor([[[ 1.9317e-02,  1.3478e-01,  2.9498e-01,  3.7304e+00],
         [-1.4186e-01, -4.5306e-02,  2.0943e-01,  1.3293e+01],
         [ 2.1452e-01,  3.3578e-01,  6.1776e-01, -2.3217e+01],
         ...,
         [ 1.1835e+01,  4.7100e+00, -5.9733e+00,  3.9468e+02],
         [ 1.2309e+01,  5.1423e+00, -5.1404e+00,  3.3535e+02],
         [ 1.1827e+01,  4.7703e+00, -5.4791e+00,  3.4889e+02]],

        [[ 2.3297e-01,  3.6551e-02, -1.9825e-02, -1.2026e+00],
         [-5.2634e-02, -2.0293e-01, -2.6700e-01,  7.3094e+00],
         [-3.5430e-01, -4.6894e-01, -5.5154e-01,  5.5978e+00],
         ...,
         [ 4.9695e+00,  3.7214e+00, -1.1771e+00,  5.6391e+02],
         [ 5.2245e+00,  4.1253e+00, -6.8462e-01,  5.6058e+02],
         [ 6.1541e+00,  4.9768e+00,  1.8610e-02,  5.7472e+02]],

        [[ 2.3403e-01,  4.7931e-02, -1.7914e-01,  2.2013e+01],
         [ 1.1434e-01, -5.5609e-02, -2.8038e-01,  1.9838e+01],
         [ 1.2272e+00,  1.0751e+00,  1.0235e+00, -2.6873e+01],
         ...,
         [ 2.0671e+01,  1.7772e+01,  1.6628e+01,  1.9972e+02],
         [ 1.9907e+01,  1.6985e+01,  1.5690e+01,  1.9405e+02],
         [ 1.8937e+01,  1.5887e+01,  1.4278e+01,  1.8921e+02]],

        ...,

        [[ 2.0870e-01,  3.8020e-02, -1.4825e-01,  1.8202e+01],
         [ 2.1433e-02, -1.3480e-01, -3.0991e-01,  1.2233e+01],
         [ 7.5179e-01,  5.7549e-01,  3.7823e-01, -3.5887e+00],
         ...,
         [ 1.3677e+01,  1.1511e+01,  1.0125e+01,  1.9264e+02],
         [ 1.6820e+01,  1.4474e+01,  1.3162e+01,  2.0468e+02],
         [ 1.8565e+01,  1.6034e+01,  1.4675e+01,  2.2011e+02]],

        [[ 3.0133e-01,  1.2370e-01,  5.5060e-02,  1.9759e+01],
         [ 3.5703e-01,  1.2852e-01, -1.3826e-01,  1.2345e+01],
         [ 2.8403e-01,  1.3529e-01, -3.5307e-02, -1.7755e+00],
         ...,
         [ 8.5328e+00,  4.4403e+00, -4.5430e-01,  1.7431e+02],
         [ 6.8174e+00,  2.9694e+00, -1.8473e+00,  1.7604e+02],
         [ 6.0754e+00,  2.2106e+00, -2.7580e+00,  1.6823e+02]],

        [[-3.5632e-01, -4.3378e-01, -4.7557e-01,  2.8589e+01],
         [-3.3160e-01, -4.3162e-01, -4.8890e-01,  8.4021e+00],
         [-3.2759e-01, -4.2816e-01, -4.7921e-01,  8.5983e+00],
         ...,
         [-3.9448e+00, -4.5582e+00, -1.0040e+01,  7.3344e+02],
         [-3.8701e+00, -4.4129e+00, -9.7912e+00,  7.4771e+02],
         [-3.8691e+00, -4.4173e+00, -9.9341e+00,  7.4049e+02]]],
       grad_fn=<ReshapeAliasBackward0>), 'rgb0': tensor([[0.4958, 0.5139, 0.5696],
        [0.4887, 0.4494, 0.4038],
        [0.5440, 0.5035, 0.4520],
        ...,
        [0.5579, 0.5135, 0.4722],
        [0.5873, 0.5473, 0.5268],
        [0.4155, 0.3798, 0.3562]], grad_fn=<ReshapeAliasBackward0>), 'disp0': tensor([ 23.3341,  61.2633,  51.2037,  ...,  51.3183, 302.5460, 145.5312],
       grad_fn=<ReshapeAliasBackward0>), 'acc0': tensor([1., 1., 1.,  ..., 1., 1., 1.], grad_fn=<ReshapeAliasBackward0>), 'z_std': tensor([0.0031, 0.0019, 0.0032,  ..., 0.0135, 0.2851, 0.0024])}
0 0.0005695819854736328
torch.Size([320, 640, 3]) torch.Size([320, 640])
1 15.12931489944458
2 13.824334859848022
3 13.836106538772583
4 14.998618602752686
5 13.876417875289917
6 15.152989387512207
7 13.688218116760254
8 15.242148637771606
9 13.642010688781738
10 13.716163396835327
11 15.4784255027771
12 13.403819561004639
13 15.410411357879639
14 13.298045873641968
15 15.539227724075317
16 13.500451564788818
17 13.777066946029663
18 15.325761079788208
19 13.470595121383667
20 15.697096824645996
21 13.33773684501648
22 15.357237339019775
23 13.871687650680542
24 13.48369550704956
25 15.413487195968628
26 13.437987089157104
27 15.457584381103516
28 13.432994842529297
29 15.404576539993286
30 13.671468019485474
31 13.777136325836182
32 15.125952005386353
33 13.671396017074585
34 15.26432728767395
35 13.56275224685669
36 15.34003734588623
37 13.609764814376831
38 13.807283163070679
39 15.372671604156494
40 13.749662160873413
41 15.01194977760315
42 13.770899295806885
43 13.638618469238281
44 15.200754165649414
45 13.742809772491455
46 15.201044797897339
47 13.72693943977356
48 15.211270093917847
49 13.760449886322021
50 13.744710206985474
51 15.164927959442139
52 13.72675085067749
53 15.139266729354858
54 13.782408237457275
55 15.167677402496338
56 13.714780569076538
57 13.72627305984497
58 15.19234848022461
59 13.675739288330078
60 15.207676887512207
61 13.767103672027588
62 15.197136640548706
63 13.714396238327026
64 13.794991493225098
65 15.287293910980225
66 13.476564407348633
67 15.427470684051514
68 13.82997989654541
69 15.098087310791016
70 13.74966549873352
71 13.732362985610962
72 15.209882974624634
73 13.748682975769043
74 15.229599475860596
75 13.750723361968994
76 15.160195589065552
77 13.748761892318726
78 13.750302791595459
79 15.286784648895264
80 13.741445779800415
81 15.73276972770691
82 14.176907300949097
83 15.595091819763184
84 14.081519842147827
85 14.125056266784668
86 15.701210498809814
87 14.148802757263184
88 15.691153526306152
89 14.088172197341919
90 15.7348051071167
91 14.067458629608154
92 15.778167963027954
93 14.561864852905273
94 13.765193223953247
95 15.551105499267578
96 13.661056518554688
97 15.832852363586426
98 13.719837427139282
99 15.396364450454712
100 14.129997253417969
101 13.656842708587646
102 15.645775556564331
103 13.580024242401123
104 15.730905771255493
105 13.503555297851562
106 15.758385181427002
107 13.705229043960571
108 13.797199010848999
109 15.485785722732544
110 13.640910387039185
111 15.767988681793213
112 13.643647193908691
113 15.668593645095825
114 13.72754192352295
115 13.686936378479004
116 15.574642658233643
117 13.705012559890747
118 15.744057416915894
119 13.608849048614502
test poses shape torch.Size([13, 3, 4])
0 0.0008409023284912109
torch.Size([320, 640, 3]) torch.Size([320, 640])
1 13.602589845657349
2 14.028665781021118
3 15.411719799041748
4 13.849871397018433
5 15.63927435874939
6 13.738515615463257
7 15.671961069107056
8 13.918776035308838
9 13.861215353012085
10 15.46077275276184
11 13.89764666557312
12 15.526642084121704
Saved test set
[TRAIN] Iter: 900000 Loss: 0.027210932224988937  PSNR: 18.50118637084961
[TRAIN] Iter: 900100 Loss: 0.02951558120548725  PSNR: 18.497106552124023
[TRAIN] Iter: 900200 Loss: 0.02466490864753723  PSNR: 19.472652435302734
[TRAIN] Iter: 900300 Loss: 0.020642632618546486  PSNR: 19.99473762512207
[TRAIN] Iter: 900400 Loss: 0.022488407790660858  PSNR: 19.51346778869629
[TRAIN] Iter: 900500 Loss: 0.025792796164751053  PSNR: 19.146024703979492
[TRAIN] Iter: 900600 Loss: 0.023256614804267883  PSNR: 19.130552291870117
[TRAIN] Iter: 900700 Loss: 0.024057216942310333  PSNR: 19.486295700073242
[TRAIN] Iter: 900800 Loss: 0.025403350591659546  PSNR: 19.147401809692383
[TRAIN] Iter: 900900 Loss: 0.027058027684688568  PSNR: 18.80428695678711
[TRAIN] Iter: 901000 Loss: 0.02372855134308338  PSNR: 19.4527645111084
[TRAIN] Iter: 901100 Loss: 0.02712872624397278  PSNR: 18.918684005737305
[TRAIN] Iter: 901200 Loss: 0.03259090334177017  PSNR: 18.051603317260742
[TRAIN] Iter: 901300 Loss: 0.01941562071442604  PSNR: 20.203676223754883
[TRAIN] Iter: 901400 Loss: 0.02171945944428444  PSNR: 19.764097213745117
[TRAIN] Iter: 901500 Loss: 0.022233134135603905  PSNR: 19.54523277282715
[TRAIN] Iter: 901600 Loss: 0.024736087769269943  PSNR: 19.307554244995117
[TRAIN] Iter: 901700 Loss: 0.022440902888774872  PSNR: 19.631488800048828
[TRAIN] Iter: 901800 Loss: 0.025036431849002838  PSNR: 19.446430206298828
[TRAIN] Iter: 901900 Loss: 0.02839682251214981  PSNR: 18.711105346679688
[TRAIN] Iter: 902000 Loss: 0.03426257520914078  PSNR: 17.814294815063477
[TRAIN] Iter: 902100 Loss: 0.02927856147289276  PSNR: 18.52140998840332
[TRAIN] Iter: 902200 Loss: 0.024871153756976128  PSNR: 19.08208465576172
[TRAIN] Iter: 902300 Loss: 0.026454096660017967  PSNR: 19.115480422973633
[TRAIN] Iter: 902400 Loss: 0.029824016615748405  PSNR: 18.36814308166504
[TRAIN] Iter: 902500 Loss: 0.026652492582798004  PSNR: 18.806529998779297
[TRAIN] Iter: 902600 Loss: 0.024446340277791023  PSNR: 19.042837142944336
[TRAIN] Iter: 902700 Loss: 0.023457692936062813  PSNR: 19.447410583496094
[TRAIN] Iter: 902800 Loss: 0.028729736804962158  PSNR: 18.544572830200195
[TRAIN] Iter: 902900 Loss: 0.024839993566274643  PSNR: 19.093027114868164
[TRAIN] Iter: 903000 Loss: 0.02919730916619301  PSNR: 18.472444534301758
[TRAIN] Iter: 903100 Loss: 0.02529389038681984  PSNR: 19.108169555664062
[TRAIN] Iter: 903200 Loss: 0.020823216065764427  PSNR: 20.03606414794922
[TRAIN] Iter: 903300 Loss: 0.023447874933481216  PSNR: 19.324655532836914
[TRAIN] Iter: 903400 Loss: 0.018972156569361687  PSNR: 20.411479949951172
[TRAIN] Iter: 903500 Loss: 0.02756631188094616  PSNR: 18.83776092529297
[TRAIN] Iter: 903600 Loss: 0.026901058852672577  PSNR: 18.979448318481445
[TRAIN] Iter: 903700 Loss: 0.028128650039434433  PSNR: 18.444162368774414
[TRAIN] Iter: 903800 Loss: 0.026437964290380478  PSNR: 19.063827514648438
[TRAIN] Iter: 903900 Loss: 0.03396090865135193  PSNR: 17.782119750976562
[TRAIN] Iter: 904000 Loss: 0.025564715266227722  PSNR: 19.185754776000977
[TRAIN] Iter: 904100 Loss: 0.02324664406478405  PSNR: 19.473800659179688
[TRAIN] Iter: 904200 Loss: 0.02827492170035839  PSNR: 18.750946044921875
[TRAIN] Iter: 904300 Loss: 0.029933171346783638  PSNR: 18.481185913085938
[TRAIN] Iter: 904400 Loss: 0.02909715846180916  PSNR: 18.46869468688965
[TRAIN] Iter: 904500 Loss: 0.026632333174347878  PSNR: 18.893957138061523
[TRAIN] Iter: 904600 Loss: 0.018437784165143967  PSNR: 20.601972579956055
[TRAIN] Iter: 904700 Loss: 0.032330162823200226  PSNR: 18.03978157043457
[TRAIN] Iter: 904800 Loss: 0.02665884792804718  PSNR: 18.87592315673828
[TRAIN] Iter: 904900 Loss: 0.03213001787662506  PSNR: 18.294797897338867
[TRAIN] Iter: 905000 Loss: 0.028661295771598816  PSNR: 18.66009521484375
[TRAIN] Iter: 905100 Loss: 0.02352357655763626  PSNR: 19.37244415283203
[TRAIN] Iter: 905200 Loss: 0.027092907577753067  PSNR: 18.90054702758789
[TRAIN] Iter: 905300 Loss: 0.025341857224702835  PSNR: 19.26575469970703
[TRAIN] Iter: 905400 Loss: 0.029943211004137993  PSNR: 18.30341911315918
[TRAIN] Iter: 905500 Loss: 0.03047531470656395  PSNR: 18.584999084472656
[TRAIN] Iter: 905600 Loss: 0.02231534942984581  PSNR: 19.71579360961914
[TRAIN] Iter: 905700 Loss: 0.027108609676361084  PSNR: 18.926719665527344
[TRAIN] Iter: 905800 Loss: 0.02794945053756237  PSNR: 19.07576560974121
[TRAIN] Iter: 905900 Loss: 0.03273172676563263  PSNR: 17.961530685424805
[TRAIN] Iter: 906000 Loss: 0.03045034408569336  PSNR: 18.37721061706543
[TRAIN] Iter: 906100 Loss: 0.023520804941654205  PSNR: 19.4097900390625
[TRAIN] Iter: 906200 Loss: 0.030728373676538467  PSNR: 18.23214340209961
[TRAIN] Iter: 906300 Loss: 0.02851119451224804  PSNR: 18.697330474853516
[TRAIN] Iter: 906400 Loss: 0.02145245112478733  PSNR: 19.852279663085938
[TRAIN] Iter: 906500 Loss: 0.025466438382864  PSNR: 18.923322677612305
[TRAIN] Iter: 906600 Loss: 0.02178744599223137  PSNR: 19.82686996459961
[TRAIN] Iter: 906700 Loss: 0.02790573239326477  PSNR: 18.828094482421875
[TRAIN] Iter: 906800 Loss: 0.02549276500940323  PSNR: 19.17019271850586
[TRAIN] Iter: 906900 Loss: 0.02317669987678528  PSNR: 19.59351921081543
[TRAIN] Iter: 907000 Loss: 0.024984773248434067  PSNR: 19.195463180541992
[TRAIN] Iter: 907100 Loss: 0.028249574825167656  PSNR: 18.631393432617188
[TRAIN] Iter: 907200 Loss: 0.026325082406401634  PSNR: 19.06970977783203
[TRAIN] Iter: 907300 Loss: 0.031740374863147736  PSNR: 18.234830856323242
[TRAIN] Iter: 907400 Loss: 0.022472120821475983  PSNR: 19.847759246826172
[TRAIN] Iter: 907500 Loss: 0.02942342683672905  PSNR: 18.572446823120117
[TRAIN] Iter: 907600 Loss: 0.02833244763314724  PSNR: 18.712648391723633
[TRAIN] Iter: 907700 Loss: 0.03453632444143295  PSNR: 17.740934371948242
[TRAIN] Iter: 907800 Loss: 0.0301514882594347  PSNR: 18.246450424194336
[TRAIN] Iter: 907900 Loss: 0.023381300270557404  PSNR: 19.59937858581543
[TRAIN] Iter: 908000 Loss: 0.022925810888409615  PSNR: 19.703292846679688
[TRAIN] Iter: 908100 Loss: 0.027311131358146667  PSNR: 18.9009952545166
[TRAIN] Iter: 908200 Loss: 0.03148539736866951  PSNR: 18.19073486328125
[TRAIN] Iter: 908300 Loss: 0.027760513126850128  PSNR: 18.810657501220703
[TRAIN] Iter: 908400 Loss: 0.03017346002161503  PSNR: 18.421701431274414
[TRAIN] Iter: 908500 Loss: 0.02810172364115715  PSNR: 18.608720779418945
[TRAIN] Iter: 908600 Loss: 0.024197790771722794  PSNR: 19.116003036499023
[TRAIN] Iter: 908700 Loss: 0.024463072419166565  PSNR: 19.293479919433594
[TRAIN] Iter: 908800 Loss: 0.02720336616039276  PSNR: 18.970966339111328
[TRAIN] Iter: 908900 Loss: 0.025928128510713577  PSNR: 19.100706100463867
[TRAIN] Iter: 909000 Loss: 0.02185642160475254  PSNR: 19.57816505432129
[TRAIN] Iter: 909100 Loss: 0.028305210173130035  PSNR: 18.64593505859375
[TRAIN] Iter: 909200 Loss: 0.023843269795179367  PSNR: 19.227977752685547
[TRAIN] Iter: 909300 Loss: 0.025312349200248718  PSNR: 19.23461151123047
[TRAIN] Iter: 909400 Loss: 0.026708753779530525  PSNR: 19.026824951171875
[TRAIN] Iter: 909500 Loss: 0.030878467485308647  PSNR: 18.311857223510742
[TRAIN] Iter: 909600 Loss: 0.030774202197790146  PSNR: 18.399349212646484
[TRAIN] Iter: 909700 Loss: 0.02292751893401146  PSNR: 19.573654174804688
[TRAIN] Iter: 909800 Loss: 0.02989458106458187  PSNR: 18.809152603149414
[TRAIN] Iter: 909900 Loss: 0.0327736996114254  PSNR: 18.090065002441406
Saved checkpoints at ./logs/TUT-LAB-nerf/910000.tar
[TRAIN] Iter: 910000 Loss: 0.027690201997756958  PSNR: 18.753061294555664
[TRAIN] Iter: 910100 Loss: 0.023627547547221184  PSNR: 19.30160140991211
[TRAIN] Iter: 910200 Loss: 0.030639775097370148  PSNR: 18.392288208007812
[TRAIN] Iter: 910300 Loss: 0.02514687553048134  PSNR: 19.485021591186523
[TRAIN] Iter: 910400 Loss: 0.026702437549829483  PSNR: 18.915359497070312
[TRAIN] Iter: 910500 Loss: 0.023614082485437393  PSNR: 19.217580795288086
[TRAIN] Iter: 910600 Loss: 0.02901865914463997  PSNR: 18.624195098876953
[TRAIN] Iter: 910700 Loss: 0.02806190773844719  PSNR: 18.512117385864258
[TRAIN] Iter: 910800 Loss: 0.02266993187367916  PSNR: 19.674419403076172
[TRAIN] Iter: 910900 Loss: 0.02767804265022278  PSNR: 18.69097137451172
[TRAIN] Iter: 911000 Loss: 0.031236400827765465  PSNR: 18.35466957092285
[TRAIN] Iter: 911100 Loss: 0.032505884766578674  PSNR: 18.15669059753418
[TRAIN] Iter: 911200 Loss: 0.02979273349046707  PSNR: 18.46735191345215
[TRAIN] Iter: 911300 Loss: 0.027778662741184235  PSNR: 18.83076286315918
[TRAIN] Iter: 911400 Loss: 0.02506537362933159  PSNR: 19.05265235900879
[TRAIN] Iter: 911500 Loss: 0.022561322897672653  PSNR: 19.31558609008789
[TRAIN] Iter: 911600 Loss: 0.025829091668128967  PSNR: 19.070541381835938
[TRAIN] Iter: 911700 Loss: 0.024622779339551926  PSNR: 19.183345794677734
[TRAIN] Iter: 911800 Loss: 0.024020448327064514  PSNR: 19.5612735748291
[TRAIN] Iter: 911900 Loss: 0.031713295727968216  PSNR: 18.134998321533203
[TRAIN] Iter: 912000 Loss: 0.03521403297781944  PSNR: 17.708419799804688
[TRAIN] Iter: 912100 Loss: 0.026004154235124588  PSNR: 18.9810733795166
[TRAIN] Iter: 912200 Loss: 0.025561178103089333  PSNR: 19.227684020996094
[TRAIN] Iter: 912300 Loss: 0.027180075645446777  PSNR: 19.097728729248047
[TRAIN] Iter: 912400 Loss: 0.030030149966478348  PSNR: 18.303939819335938
[TRAIN] Iter: 912500 Loss: 0.02787831611931324  PSNR: 18.7403507232666
[TRAIN] Iter: 912600 Loss: 0.023440692573785782  PSNR: 19.5213623046875
[TRAIN] Iter: 912700 Loss: 0.028406843543052673  PSNR: 18.51723289489746
[TRAIN] Iter: 912800 Loss: 0.02060774900019169  PSNR: 20.064682006835938
[TRAIN] Iter: 912900 Loss: 0.024291977286338806  PSNR: 19.754682540893555
[TRAIN] Iter: 913000 Loss: 0.024368947371840477  PSNR: 19.296493530273438
[TRAIN] Iter: 913100 Loss: 0.03519432991743088  PSNR: 17.66995620727539
[TRAIN] Iter: 913200 Loss: 0.02429812029004097  PSNR: 19.3969669342041
[TRAIN] Iter: 913300 Loss: 0.027750885114073753  PSNR: 18.76683235168457
[TRAIN] Iter: 913400 Loss: 0.026669733226299286  PSNR: 18.960756301879883
[TRAIN] Iter: 913500 Loss: 0.03108178824186325  PSNR: 18.375146865844727
[TRAIN] Iter: 913600 Loss: 0.02276846393942833  PSNR: 19.782596588134766
[TRAIN] Iter: 913700 Loss: 0.02204560488462448  PSNR: 19.47236442565918
[TRAIN] Iter: 913800 Loss: 0.027179136872291565  PSNR: 18.755027770996094
[TRAIN] Iter: 913900 Loss: 0.019576262682676315  PSNR: 20.36373519897461
[TRAIN] Iter: 914000 Loss: 0.024289077147841454  PSNR: 19.10578155517578
[TRAIN] Iter: 914100 Loss: 0.02848333679139614  PSNR: 18.616315841674805
[TRAIN] Iter: 914200 Loss: 0.025466587394475937  PSNR: 19.523056030273438
[TRAIN] Iter: 914300 Loss: 0.02935001440346241  PSNR: 18.46100425720215
[TRAIN] Iter: 914400 Loss: 0.024867765605449677  PSNR: 19.304540634155273
[TRAIN] Iter: 914500 Loss: 0.02595452219247818  PSNR: 19.075624465942383
[TRAIN] Iter: 914600 Loss: 0.02384466677904129  PSNR: 19.284997940063477
[TRAIN] Iter: 914700 Loss: 0.028496189042925835  PSNR: 18.686506271362305
[TRAIN] Iter: 914800 Loss: 0.02551959455013275  PSNR: 19.186031341552734
[TRAIN] Iter: 914900 Loss: 0.02895282953977585  PSNR: 18.594345092773438
[TRAIN] Iter: 915000 Loss: 0.02425013855099678  PSNR: 19.18518829345703
[TRAIN] Iter: 915100 Loss: 0.022739099338650703  PSNR: 19.492326736450195
[TRAIN] Iter: 915200 Loss: 0.027099687606096268  PSNR: 18.70524787902832
[TRAIN] Iter: 915300 Loss: 0.031234772875905037  PSNR: 18.338245391845703
[TRAIN] Iter: 915400 Loss: 0.024111492559313774  PSNR: 19.387653350830078
[TRAIN] Iter: 915500 Loss: 0.0264798142015934  PSNR: 18.733625411987305
[TRAIN] Iter: 915600 Loss: 0.022435694932937622  PSNR: 19.595529556274414
[TRAIN] Iter: 915700 Loss: 0.02371857315301895  PSNR: 19.374492645263672
[TRAIN] Iter: 915800 Loss: 0.02600724995136261  PSNR: 19.157772064208984
[TRAIN] Iter: 915900 Loss: 0.031114591285586357  PSNR: 18.126415252685547
[TRAIN] Iter: 916000 Loss: 0.03220100700855255  PSNR: 18.092065811157227
[TRAIN] Iter: 916100 Loss: 0.03425687551498413  PSNR: 17.942581176757812
[TRAIN] Iter: 916200 Loss: 0.027614004909992218  PSNR: 18.778242111206055
[TRAIN] Iter: 916300 Loss: 0.03267277032136917  PSNR: 18.198200225830078
[TRAIN] Iter: 916400 Loss: 0.027741333469748497  PSNR: 18.95011329650879
[TRAIN] Iter: 916500 Loss: 0.022842828184366226  PSNR: 19.497459411621094
[TRAIN] Iter: 916600 Loss: 0.028051624074578285  PSNR: 18.820913314819336
[TRAIN] Iter: 916700 Loss: 0.028384720906615257  PSNR: 18.50557518005371
[TRAIN] Iter: 916800 Loss: 0.031094670295715332  PSNR: 18.162790298461914
[TRAIN] Iter: 916900 Loss: 0.029764613136649132  PSNR: 18.33965492248535
[TRAIN] Iter: 917000 Loss: 0.026342488825321198  PSNR: 19.01409149169922
[TRAIN] Iter: 917100 Loss: 0.0325213298201561  PSNR: 18.0391788482666
[TRAIN] Iter: 917200 Loss: 0.026576006785035133  PSNR: 19.404483795166016
[TRAIN] Iter: 917300 Loss: 0.02310720831155777  PSNR: 19.399417877197266
[TRAIN] Iter: 917400 Loss: 0.029849395155906677  PSNR: 18.384687423706055
[TRAIN] Iter: 917500 Loss: 0.02218545228242874  PSNR: 19.61073112487793
[TRAIN] Iter: 917600 Loss: 0.023294828832149506  PSNR: 19.204374313354492
[TRAIN] Iter: 917700 Loss: 0.02726772427558899  PSNR: 18.870594024658203
[TRAIN] Iter: 917800 Loss: 0.01988545060157776  PSNR: 20.37385368347168
[TRAIN] Iter: 917900 Loss: 0.027546998113393784  PSNR: 18.866653442382812
[TRAIN] Iter: 918000 Loss: 0.024503648281097412  PSNR: 19.099063873291016
[TRAIN] Iter: 918100 Loss: 0.02754896506667137  PSNR: 18.909719467163086
[TRAIN] Iter: 918200 Loss: 0.02436060644686222  PSNR: 19.385042190551758
[TRAIN] Iter: 918300 Loss: 0.028244731947779655  PSNR: 18.65123176574707
[TRAIN] Iter: 918400 Loss: 0.02254677563905716  PSNR: 19.727169036865234
[TRAIN] Iter: 918500 Loss: 0.026296600699424744  PSNR: 19.00623893737793
[TRAIN] Iter: 918600 Loss: 0.028612541034817696  PSNR: 18.567230224609375
[TRAIN] Iter: 918700 Loss: 0.026836775243282318  PSNR: 18.78083610534668
[TRAIN] Iter: 918800 Loss: 0.025137031450867653  PSNR: 19.3089542388916
[TRAIN] Iter: 918900 Loss: 0.025804074481129646  PSNR: 18.925973892211914
[TRAIN] Iter: 919000 Loss: 0.0322408527135849  PSNR: 18.066434860229492
[TRAIN] Iter: 919100 Loss: 0.027082867920398712  PSNR: 18.6617374420166
[TRAIN] Iter: 919200 Loss: 0.02214990369975567  PSNR: 19.767236709594727
[TRAIN] Iter: 919300 Loss: 0.022673986852169037  PSNR: 19.48883056640625
[TRAIN] Iter: 919400 Loss: 0.026762772351503372  PSNR: 18.8646183013916
[TRAIN] Iter: 919500 Loss: 0.02415306493639946  PSNR: 19.36215591430664
[TRAIN] Iter: 919600 Loss: 0.034469012171030045  PSNR: 17.77239990234375
[TRAIN] Iter: 919700 Loss: 0.023001831024885178  PSNR: 19.661237716674805
[TRAIN] Iter: 919800 Loss: 0.023736927658319473  PSNR: 19.785388946533203
[TRAIN] Iter: 919900 Loss: 0.02425980381667614  PSNR: 19.4942626953125
Saved checkpoints at ./logs/TUT-LAB-nerf/920000.tar
[TRAIN] Iter: 920000 Loss: 0.026539690792560577  PSNR: 18.871362686157227
[TRAIN] Iter: 920100 Loss: 0.026365626603364944  PSNR: 19.12177848815918
[TRAIN] Iter: 920200 Loss: 0.030268771573901176  PSNR: 18.4691219329834
[TRAIN] Iter: 920300 Loss: 0.019822614267468452  PSNR: 20.260753631591797
[TRAIN] Iter: 920400 Loss: 0.024047808721661568  PSNR: 19.763492584228516
[TRAIN] Iter: 920500 Loss: 0.025395914912223816  PSNR: 19.2009334564209
[TRAIN] Iter: 920600 Loss: 0.02715163305401802  PSNR: 19.239625930786133
[TRAIN] Iter: 920700 Loss: 0.025032293051481247  PSNR: 19.405349731445312
[TRAIN] Iter: 920800 Loss: 0.02881792187690735  PSNR: 18.519241333007812
[TRAIN] Iter: 920900 Loss: 0.031326472759246826  PSNR: 18.254894256591797
[TRAIN] Iter: 921000 Loss: 0.023028787225484848  PSNR: 19.737791061401367
[TRAIN] Iter: 921100 Loss: 0.032016679644584656  PSNR: 18.050533294677734
[TRAIN] Iter: 921200 Loss: 0.02208801731467247  PSNR: 19.717313766479492
[TRAIN] Iter: 921300 Loss: 0.021176233887672424  PSNR: 19.836828231811523
[TRAIN] Iter: 921400 Loss: 0.025090202689170837  PSNR: 19.572154998779297
[TRAIN] Iter: 921500 Loss: 0.02227739244699478  PSNR: 19.497026443481445
[TRAIN] Iter: 921600 Loss: 0.024090323597192764  PSNR: 19.396665573120117
[TRAIN] Iter: 921700 Loss: 0.027130264788866043  PSNR: 18.942474365234375
[TRAIN] Iter: 921800 Loss: 0.026397090405225754  PSNR: 18.974224090576172
[TRAIN] Iter: 921900 Loss: 0.02492927387356758  PSNR: 18.986963272094727
[TRAIN] Iter: 922000 Loss: 0.030491365119814873  PSNR: 18.29983901977539
[TRAIN] Iter: 922100 Loss: 0.026968441903591156  PSNR: 18.948469161987305
[TRAIN] Iter: 922200 Loss: 0.02643457241356373  PSNR: 19.26927375793457
[TRAIN] Iter: 922300 Loss: 0.029293997213244438  PSNR: 18.60437774658203
[TRAIN] Iter: 922400 Loss: 0.023969069123268127  PSNR: 19.299781799316406
[TRAIN] Iter: 922500 Loss: 0.03266541659832001  PSNR: 18.055997848510742
[TRAIN] Iter: 922600 Loss: 0.023713812232017517  PSNR: 19.235496520996094
[TRAIN] Iter: 922700 Loss: 0.028116513043642044  PSNR: 18.521930694580078
[TRAIN] Iter: 922800 Loss: 0.018976977095007896  PSNR: 20.38648223876953
[TRAIN] Iter: 922900 Loss: 0.02151588909327984  PSNR: 19.76762580871582
[TRAIN] Iter: 923000 Loss: 0.022840188816189766  PSNR: 19.165189743041992
[TRAIN] Iter: 923100 Loss: 0.03039006143808365  PSNR: 18.336294174194336
[TRAIN] Iter: 923200 Loss: 0.02884894795715809  PSNR: 18.59398651123047
[TRAIN] Iter: 923300 Loss: 0.030897896736860275  PSNR: 18.185190200805664
[TRAIN] Iter: 923400 Loss: 0.02245495095849037  PSNR: 19.69938850402832
[TRAIN] Iter: 923500 Loss: 0.025215571746230125  PSNR: 19.23910140991211
[TRAIN] Iter: 923600 Loss: 0.02380586601793766  PSNR: 19.60742950439453
[TRAIN] Iter: 923700 Loss: 0.026686031371355057  PSNR: 18.945423126220703
[TRAIN] Iter: 923800 Loss: 0.022634394466876984  PSNR: 19.538063049316406
[TRAIN] Iter: 923900 Loss: 0.025377055630087852  PSNR: 19.10543441772461
[TRAIN] Iter: 924000 Loss: 0.024078764021396637  PSNR: 19.648099899291992
[TRAIN] Iter: 924100 Loss: 0.025099650025367737  PSNR: 19.13545036315918
[TRAIN] Iter: 924200 Loss: 0.031391676515340805  PSNR: 18.23455810546875
[TRAIN] Iter: 924300 Loss: 0.02759484574198723  PSNR: 18.690412521362305
[TRAIN] Iter: 924400 Loss: 0.0257174763828516  PSNR: 19.195974349975586
[TRAIN] Iter: 924500 Loss: 0.02717386558651924  PSNR: 18.746374130249023
[TRAIN] Iter: 924600 Loss: 0.029774248600006104  PSNR: 18.51506996154785
[TRAIN] Iter: 924700 Loss: 0.02512085996568203  PSNR: 19.314311981201172
[TRAIN] Iter: 924800 Loss: 0.027460385113954544  PSNR: 18.831674575805664
[TRAIN] Iter: 924900 Loss: 0.02614673227071762  PSNR: 18.9857234954834
[TRAIN] Iter: 925000 Loss: 0.030079135671257973  PSNR: 18.375207901000977
[TRAIN] Iter: 925100 Loss: 0.02018621750175953  PSNR: 19.826438903808594
[TRAIN] Iter: 925200 Loss: 0.03184156119823456  PSNR: 18.158018112182617
[TRAIN] Iter: 925300 Loss: 0.02851736545562744  PSNR: 18.656997680664062
[TRAIN] Iter: 925400 Loss: 0.024990687146782875  PSNR: 19.18303108215332
[TRAIN] Iter: 925500 Loss: 0.02524804323911667  PSNR: 19.29070281982422
[TRAIN] Iter: 925600 Loss: 0.028665782883763313  PSNR: 18.515766143798828
[TRAIN] Iter: 925700 Loss: 0.02212289720773697  PSNR: 19.391572952270508
[TRAIN] Iter: 925800 Loss: 0.020982496440410614  PSNR: 20.045682907104492
[TRAIN] Iter: 925900 Loss: 0.031193487346172333  PSNR: 18.201993942260742
[TRAIN] Iter: 926000 Loss: 0.030248591676354408  PSNR: 18.41985511779785
[TRAIN] Iter: 926100 Loss: 0.030924685299396515  PSNR: 18.228219985961914
[TRAIN] Iter: 926200 Loss: 0.026829620823264122  PSNR: 18.83941650390625
[TRAIN] Iter: 926300 Loss: 0.027346568182110786  PSNR: 18.97934913635254
[TRAIN] Iter: 926400 Loss: 0.0265370961278677  PSNR: 19.043087005615234
[TRAIN] Iter: 926500 Loss: 0.034893978387117386  PSNR: 17.82564926147461
[TRAIN] Iter: 926600 Loss: 0.028671741485595703  PSNR: 18.490615844726562
[TRAIN] Iter: 926700 Loss: 0.029370494186878204  PSNR: 18.674638748168945
[TRAIN] Iter: 926800 Loss: 0.024108868092298508  PSNR: 19.51741600036621
[TRAIN] Iter: 926900 Loss: 0.027238449081778526  PSNR: 18.844970703125
[TRAIN] Iter: 927000 Loss: 0.02452715113759041  PSNR: 19.344013214111328
[TRAIN] Iter: 927100 Loss: 0.030173614621162415  PSNR: 18.23919105529785
[TRAIN] Iter: 927200 Loss: 0.02637251280248165  PSNR: 19.235401153564453
[TRAIN] Iter: 927300 Loss: 0.03140360862016678  PSNR: 18.39457130432129
[TRAIN] Iter: 927400 Loss: 0.02507566101849079  PSNR: 19.108741760253906
[TRAIN] Iter: 927500 Loss: 0.019528701901435852  PSNR: 20.22455406188965
[TRAIN] Iter: 927600 Loss: 0.025898166000843048  PSNR: 18.948196411132812
[TRAIN] Iter: 927700 Loss: 0.022542811930179596  PSNR: 19.653987884521484
[TRAIN] Iter: 927800 Loss: 0.02703697234392166  PSNR: 18.83613395690918
[TRAIN] Iter: 927900 Loss: 0.028090927749872208  PSNR: 19.33901023864746
[TRAIN] Iter: 928000 Loss: 0.02784130536019802  PSNR: 18.825902938842773
[TRAIN] Iter: 928100 Loss: 0.025066349655389786  PSNR: 19.18347930908203
[TRAIN] Iter: 928200 Loss: 0.0246401559561491  PSNR: 19.26254653930664
[TRAIN] Iter: 928300 Loss: 0.027837634086608887  PSNR: 18.7750186920166
[TRAIN] Iter: 928400 Loss: 0.025749269872903824  PSNR: 18.490623474121094
[TRAIN] Iter: 928500 Loss: 0.03245755657553673  PSNR: 17.976465225219727
[TRAIN] Iter: 928600 Loss: 0.030510785058140755  PSNR: 18.348371505737305
[TRAIN] Iter: 928700 Loss: 0.027509817853569984  PSNR: 18.85276222229004
[TRAIN] Iter: 928800 Loss: 0.03207448124885559  PSNR: 17.98014259338379
[TRAIN] Iter: 928900 Loss: 0.024746783077716827  PSNR: 19.112689971923828
[TRAIN] Iter: 929000 Loss: 0.021772172302007675  PSNR: 19.645404815673828
[TRAIN] Iter: 929100 Loss: 0.025562584400177002  PSNR: 19.338581085205078
[TRAIN] Iter: 929200 Loss: 0.030691109597682953  PSNR: 18.281475067138672
[TRAIN] Iter: 929300 Loss: 0.031910210847854614  PSNR: 18.05641746520996
[TRAIN] Iter: 929400 Loss: 0.02334815450012684  PSNR: 19.082639694213867
[TRAIN] Iter: 929500 Loss: 0.021905656903982162  PSNR: 19.745019912719727
[TRAIN] Iter: 929600 Loss: 0.030387725681066513  PSNR: 18.272106170654297
[TRAIN] Iter: 929700 Loss: 0.0285171028226614  PSNR: 18.7438907623291
[TRAIN] Iter: 929800 Loss: 0.02657586894929409  PSNR: 19.024141311645508
[TRAIN] Iter: 929900 Loss: 0.026985444128513336  PSNR: 18.997434616088867
Saved checkpoints at ./logs/TUT-LAB-nerf/930000.tar
[TRAIN] Iter: 930000 Loss: 0.031546685844659805  PSNR: 18.075075149536133
[TRAIN] Iter: 930100 Loss: 0.02739114686846733  PSNR: 18.90484046936035
[TRAIN] Iter: 930200 Loss: 0.03052494116127491  PSNR: 18.357595443725586
[TRAIN] Iter: 930300 Loss: 0.02106400951743126  PSNR: 20.1270809173584
[TRAIN] Iter: 930400 Loss: 0.0246780626475811  PSNR: 19.33522605895996
[TRAIN] Iter: 930500 Loss: 0.03045179694890976  PSNR: 18.453645706176758
[TRAIN] Iter: 930600 Loss: 0.02301207184791565  PSNR: 19.685420989990234
[TRAIN] Iter: 930700 Loss: 0.02654755860567093  PSNR: 19.133808135986328
[TRAIN] Iter: 930800 Loss: 0.033396296203136444  PSNR: 17.964523315429688
[TRAIN] Iter: 930900 Loss: 0.02203993871808052  PSNR: 19.814661026000977
[TRAIN] Iter: 931000 Loss: 0.029800862073898315  PSNR: 18.383052825927734
[TRAIN] Iter: 931100 Loss: 0.025565264746546745  PSNR: 19.160274505615234
[TRAIN] Iter: 931200 Loss: 0.026589812710881233  PSNR: 18.781702041625977
[TRAIN] Iter: 931300 Loss: 0.030568189918994904  PSNR: 18.171533584594727
[TRAIN] Iter: 931400 Loss: 0.020879505202174187  PSNR: 20.00990867614746
[TRAIN] Iter: 931500 Loss: 0.02344111166894436  PSNR: 19.609506607055664
[TRAIN] Iter: 931600 Loss: 0.026380039751529694  PSNR: 19.12519073486328
[TRAIN] Iter: 931700 Loss: 0.024772318080067635  PSNR: 19.282777786254883
[TRAIN] Iter: 931800 Loss: 0.025886571034789085  PSNR: 19.04520606994629
[TRAIN] Iter: 931900 Loss: 0.025373151525855064  PSNR: 19.043638229370117
[TRAIN] Iter: 932000 Loss: 0.02743571437895298  PSNR: 18.801410675048828
[TRAIN] Iter: 932100 Loss: 0.02797640860080719  PSNR: 18.785123825073242
[TRAIN] Iter: 932200 Loss: 0.03688396140933037  PSNR: 17.45009422302246
[TRAIN] Iter: 932300 Loss: 0.023430611938238144  PSNR: 19.358049392700195
[TRAIN] Iter: 932400 Loss: 0.029000483453273773  PSNR: 18.58095359802246
[TRAIN] Iter: 932500 Loss: 0.02505376935005188  PSNR: 18.93800926208496
[TRAIN] Iter: 932600 Loss: 0.02881733886897564  PSNR: 19.104522705078125
[TRAIN] Iter: 932700 Loss: 0.02866595983505249  PSNR: 18.577608108520508
[TRAIN] Iter: 932800 Loss: 0.025095103308558464  PSNR: 19.219573974609375
[TRAIN] Iter: 932900 Loss: 0.025576751679182053  PSNR: 19.06618881225586
[TRAIN] Iter: 933000 Loss: 0.022433578968048096  PSNR: 19.850378036499023
[TRAIN] Iter: 933100 Loss: 0.029117830097675323  PSNR: 18.315967559814453
[TRAIN] Iter: 933200 Loss: 0.02231745608150959  PSNR: 19.729188919067383
[TRAIN] Iter: 933300 Loss: 0.024015288800001144  PSNR: 19.404754638671875
[TRAIN] Iter: 933400 Loss: 0.02593008242547512  PSNR: 18.519582748413086
[TRAIN] Iter: 933500 Loss: 0.022239496931433678  PSNR: 19.459325790405273
[TRAIN] Iter: 933600 Loss: 0.026782192289829254  PSNR: 18.737350463867188
[TRAIN] Iter: 933700 Loss: 0.029702985659241676  PSNR: 18.745092391967773
[TRAIN] Iter: 933800 Loss: 0.02523924969136715  PSNR: 19.13563346862793
[TRAIN] Iter: 933900 Loss: 0.023997562006115913  PSNR: 19.220369338989258
[TRAIN] Iter: 934000 Loss: 0.02409360744059086  PSNR: 19.203733444213867
[TRAIN] Iter: 934100 Loss: 0.030779607594013214  PSNR: 18.46485710144043
[TRAIN] Iter: 934200 Loss: 0.02832893468439579  PSNR: 18.642681121826172
[TRAIN] Iter: 934300 Loss: 0.02164486050605774  PSNR: 19.631324768066406
[TRAIN] Iter: 934400 Loss: 0.025082862004637718  PSNR: 19.286304473876953
[TRAIN] Iter: 934500 Loss: 0.029341530054807663  PSNR: 18.683198928833008
[TRAIN] Iter: 934600 Loss: 0.03084460087120533  PSNR: 18.21308135986328
[TRAIN] Iter: 934700 Loss: 0.026217497885227203  PSNR: 18.920455932617188
[TRAIN] Iter: 934800 Loss: 0.018172597512602806  PSNR: 20.549137115478516
[TRAIN] Iter: 934900 Loss: 0.03411388397216797  PSNR: 17.9427490234375
[TRAIN] Iter: 935000 Loss: 0.021382514387369156  PSNR: 19.867225646972656
[TRAIN] Iter: 935100 Loss: 0.032088037580251694  PSNR: 18.178401947021484
[TRAIN] Iter: 935200 Loss: 0.03447594866156578  PSNR: 17.763824462890625
[TRAIN] Iter: 935300 Loss: 0.028219034895300865  PSNR: 18.59825325012207
[TRAIN] Iter: 935400 Loss: 0.03212978318333626  PSNR: 18.12866973876953
[TRAIN] Iter: 935500 Loss: 0.02365768700838089  PSNR: 19.49131202697754
[TRAIN] Iter: 935600 Loss: 0.025130439549684525  PSNR: 18.996305465698242
[TRAIN] Iter: 935700 Loss: 0.028084134683012962  PSNR: 18.991792678833008
[TRAIN] Iter: 935800 Loss: 0.03314822167158127  PSNR: 17.98185920715332
[TRAIN] Iter: 935900 Loss: 0.024216320365667343  PSNR: 19.33523941040039
[TRAIN] Iter: 936000 Loss: 0.022019533440470695  PSNR: 19.917509078979492
[TRAIN] Iter: 936100 Loss: 0.028657451272010803  PSNR: 18.673625946044922
[TRAIN] Iter: 936200 Loss: 0.028940780088305473  PSNR: 18.50034523010254
[TRAIN] Iter: 936300 Loss: 0.03306505084037781  PSNR: 17.984148025512695
[TRAIN] Iter: 936400 Loss: 0.026180988177657127  PSNR: 19.092519760131836
[TRAIN] Iter: 936500 Loss: 0.027537472546100616  PSNR: 18.712158203125
[TRAIN] Iter: 936600 Loss: 0.022622888907790184  PSNR: 19.64232635498047
[TRAIN] Iter: 936700 Loss: 0.029114359989762306  PSNR: 18.4991455078125
[TRAIN] Iter: 936800 Loss: 0.022379465401172638  PSNR: 19.803234100341797
[TRAIN] Iter: 936900 Loss: 0.029956497251987457  PSNR: 18.343852996826172
[TRAIN] Iter: 937000 Loss: 0.02491055428981781  PSNR: 19.11905860900879
[TRAIN] Iter: 937100 Loss: 0.03106900118291378  PSNR: 18.250967025756836
[TRAIN] Iter: 937200 Loss: 0.029868975281715393  PSNR: 18.409679412841797
[TRAIN] Iter: 937300 Loss: 0.029394425451755524  PSNR: 18.91856575012207
[TRAIN] Iter: 937400 Loss: 0.0305744931101799  PSNR: 18.429506301879883
[TRAIN] Iter: 937500 Loss: 0.022531136870384216  PSNR: 19.753238677978516
[TRAIN] Iter: 937600 Loss: 0.02558058500289917  PSNR: 19.148366928100586
[TRAIN] Iter: 937700 Loss: 0.027902953326702118  PSNR: 18.703359603881836
[TRAIN] Iter: 937800 Loss: 0.02194402366876602  PSNR: 19.737194061279297
[TRAIN] Iter: 937900 Loss: 0.03007768653333187  PSNR: 18.47315216064453
[TRAIN] Iter: 938000 Loss: 0.028147811070084572  PSNR: 18.600465774536133
[TRAIN] Iter: 938100 Loss: 0.030221518129110336  PSNR: 18.236888885498047
[TRAIN] Iter: 938200 Loss: 0.03037995658814907  PSNR: 18.3817081451416
[TRAIN] Iter: 938300 Loss: 0.02489977702498436  PSNR: 19.2109317779541
[TRAIN] Iter: 938400 Loss: 0.024439174681901932  PSNR: 19.53272819519043
[TRAIN] Iter: 938500 Loss: 0.02425503544509411  PSNR: 19.26365852355957
[TRAIN] Iter: 938600 Loss: 0.02749779261648655  PSNR: 18.854122161865234
[TRAIN] Iter: 938700 Loss: 0.023388441652059555  PSNR: 19.481754302978516
[TRAIN] Iter: 938800 Loss: 0.021981658414006233  PSNR: 19.827640533447266
[TRAIN] Iter: 938900 Loss: 0.02477147802710533  PSNR: 19.207311630249023
[TRAIN] Iter: 939000 Loss: 0.024324772879481316  PSNR: 19.235239028930664
[TRAIN] Iter: 939100 Loss: 0.029369989410042763  PSNR: 18.4993953704834
[TRAIN] Iter: 939200 Loss: 0.029498985037207603  PSNR: 18.50164222717285
[TRAIN] Iter: 939300 Loss: 0.030638664960861206  PSNR: 18.32052993774414
[TRAIN] Iter: 939400 Loss: 0.022419661283493042  PSNR: 19.630435943603516
[TRAIN] Iter: 939500 Loss: 0.021985171362757683  PSNR: 19.86037826538086
[TRAIN] Iter: 939600 Loss: 0.025503452867269516  PSNR: 19.10643768310547
[TRAIN] Iter: 939700 Loss: 0.027997605502605438  PSNR: 18.845600128173828
[TRAIN] Iter: 939800 Loss: 0.02223147079348564  PSNR: 19.99197769165039
[TRAIN] Iter: 939900 Loss: 0.026965752243995667  PSNR: 19.142824172973633
Saved checkpoints at ./logs/TUT-LAB-nerf/940000.tar
[TRAIN] Iter: 940000 Loss: 0.023748312145471573  PSNR: 20.001928329467773
[TRAIN] Iter: 940100 Loss: 0.03115348145365715  PSNR: 18.285139083862305
[TRAIN] Iter: 940200 Loss: 0.023536069318652153  PSNR: 19.49448013305664
[TRAIN] Iter: 940300 Loss: 0.033457253128290176  PSNR: 18.14868927001953
[TRAIN] Iter: 940400 Loss: 0.027257053181529045  PSNR: 18.924449920654297
[TRAIN] Iter: 940500 Loss: 0.028405651450157166  PSNR: 18.72480010986328
[TRAIN] Iter: 940600 Loss: 0.03160964697599411  PSNR: 18.35880470275879
[TRAIN] Iter: 940700 Loss: 0.02366560697555542  PSNR: 19.811098098754883
[TRAIN] Iter: 940800 Loss: 0.0261467806994915  PSNR: 18.914899826049805
[TRAIN] Iter: 940900 Loss: 0.02791055105626583  PSNR: 18.763011932373047
[TRAIN] Iter: 941000 Loss: 0.022902043536305428  PSNR: 19.533981323242188
[TRAIN] Iter: 941100 Loss: 0.01997395232319832  PSNR: 20.16080093383789
[TRAIN] Iter: 941200 Loss: 0.03239462897181511  PSNR: 18.006980895996094
[TRAIN] Iter: 941300 Loss: 0.031122274696826935  PSNR: 18.353219985961914
[TRAIN] Iter: 941400 Loss: 0.023818181827664375  PSNR: 19.43442153930664
[TRAIN] Iter: 941500 Loss: 0.025065358728170395  PSNR: 19.277231216430664
[TRAIN] Iter: 941600 Loss: 0.02341095171868801  PSNR: 19.577056884765625
[TRAIN] Iter: 941700 Loss: 0.02258189767599106  PSNR: 19.640439987182617
[TRAIN] Iter: 941800 Loss: 0.03117797151207924  PSNR: 18.226398468017578
[TRAIN] Iter: 941900 Loss: 0.02483440563082695  PSNR: 19.697525024414062
[TRAIN] Iter: 942000 Loss: 0.02279854193329811  PSNR: 19.61553382873535
[TRAIN] Iter: 942100 Loss: 0.028244957327842712  PSNR: 18.619508743286133
[TRAIN] Iter: 942200 Loss: 0.02515222132205963  PSNR: 19.174240112304688
[TRAIN] Iter: 942300 Loss: 0.030042102560400963  PSNR: 18.291868209838867
[TRAIN] Iter: 942400 Loss: 0.022883104160428047  PSNR: 19.784038543701172
[TRAIN] Iter: 942500 Loss: 0.031011663377285004  PSNR: 18.238187789916992
[TRAIN] Iter: 942600 Loss: 0.022742856293916702  PSNR: 19.473371505737305
[TRAIN] Iter: 942700 Loss: 0.029443517327308655  PSNR: 18.849058151245117
[TRAIN] Iter: 942800 Loss: 0.022970028221607208  PSNR: 19.71028709411621
[TRAIN] Iter: 942900 Loss: 0.023447934538125992  PSNR: 19.075407028198242
[TRAIN] Iter: 943000 Loss: 0.024209922179579735  PSNR: 19.50884246826172
[TRAIN] Iter: 943100 Loss: 0.03044731728732586  PSNR: 18.23944091796875
[TRAIN] Iter: 943200 Loss: 0.025997431948781013  PSNR: 19.1141300201416
[TRAIN] Iter: 943300 Loss: 0.024104692041873932  PSNR: 19.272775650024414
[TRAIN] Iter: 943400 Loss: 0.023788228631019592  PSNR: 19.52945899963379
[TRAIN] Iter: 943500 Loss: 0.028742875903844833  PSNR: 18.504446029663086
[TRAIN] Iter: 943600 Loss: 0.030231866985559464  PSNR: 18.333511352539062
[TRAIN] Iter: 943700 Loss: 0.02767646126449108  PSNR: 18.677236557006836
[TRAIN] Iter: 943800 Loss: 0.02415853552520275  PSNR: 19.179195404052734
[TRAIN] Iter: 943900 Loss: 0.033135101199150085  PSNR: 17.81315040588379
[TRAIN] Iter: 944000 Loss: 0.027674198150634766  PSNR: 18.936405181884766
[TRAIN] Iter: 944100 Loss: 0.025673972442746162  PSNR: 19.08302116394043
[TRAIN] Iter: 944200 Loss: 0.027036340907216072  PSNR: 18.827924728393555
[TRAIN] Iter: 944300 Loss: 0.026291200891137123  PSNR: 18.963035583496094
[TRAIN] Iter: 944400 Loss: 0.02592441625893116  PSNR: 19.083913803100586
[TRAIN] Iter: 944500 Loss: 0.020687328651547432  PSNR: 20.081151962280273
[TRAIN] Iter: 944600 Loss: 0.022219404578208923  PSNR: 19.797636032104492
[TRAIN] Iter: 944700 Loss: 0.027513697743415833  PSNR: 18.70815086364746
[TRAIN] Iter: 944800 Loss: 0.023864539340138435  PSNR: 19.393064498901367
[TRAIN] Iter: 944900 Loss: 0.028621520847082138  PSNR: 18.573461532592773
[TRAIN] Iter: 945000 Loss: 0.021199993789196014  PSNR: 20.103351593017578
[TRAIN] Iter: 945100 Loss: 0.026687053963541985  PSNR: 18.67179298400879
[TRAIN] Iter: 945200 Loss: 0.02702755481004715  PSNR: 18.895505905151367
[TRAIN] Iter: 945300 Loss: 0.025641221553087234  PSNR: 19.21669578552246
[TRAIN] Iter: 945400 Loss: 0.030523579567670822  PSNR: 18.419559478759766
[TRAIN] Iter: 945500 Loss: 0.023669660091400146  PSNR: 19.616302490234375
[TRAIN] Iter: 945600 Loss: 0.028825562447309494  PSNR: 18.400318145751953
[TRAIN] Iter: 945700 Loss: 0.02476128377020359  PSNR: 19.068397521972656
[TRAIN] Iter: 945800 Loss: 0.023884572088718414  PSNR: 19.41847038269043
[TRAIN] Iter: 945900 Loss: 0.024634648114442825  PSNR: 19.281787872314453
[TRAIN] Iter: 946000 Loss: 0.02642187662422657  PSNR: 18.999731063842773
[TRAIN] Iter: 946100 Loss: 0.027093056589365005  PSNR: 19.16928482055664
[TRAIN] Iter: 946200 Loss: 0.025589970871806145  PSNR: 19.176206588745117
[TRAIN] Iter: 946300 Loss: 0.03148108348250389  PSNR: 18.561904907226562
[TRAIN] Iter: 946400 Loss: 0.021836567670106888  PSNR: 19.670137405395508
[TRAIN] Iter: 946500 Loss: 0.030644677579402924  PSNR: 18.182117462158203
[TRAIN] Iter: 946600 Loss: 0.025403207167983055  PSNR: 18.56133270263672
[TRAIN] Iter: 946700 Loss: 0.0332060381770134  PSNR: 17.95741081237793
[TRAIN] Iter: 946800 Loss: 0.025587555021047592  PSNR: 19.18436622619629
[TRAIN] Iter: 946900 Loss: 0.02531905472278595  PSNR: 19.42009162902832
[TRAIN] Iter: 947000 Loss: 0.026957228779792786  PSNR: 18.871932983398438
[TRAIN] Iter: 947100 Loss: 0.025449013337492943  PSNR: 19.20915412902832
[TRAIN] Iter: 947200 Loss: 0.030050627887248993  PSNR: 18.367631912231445
[TRAIN] Iter: 947300 Loss: 0.024582514539361  PSNR: 19.264734268188477
[TRAIN] Iter: 947400 Loss: 0.03085561841726303  PSNR: 18.246803283691406
[TRAIN] Iter: 947500 Loss: 0.029434021562337875  PSNR: 18.38363265991211
[TRAIN] Iter: 947600 Loss: 0.02006462961435318  PSNR: 20.194137573242188
[TRAIN] Iter: 947700 Loss: 0.028072426095604897  PSNR: 18.681072235107422
[TRAIN] Iter: 947800 Loss: 0.03185532987117767  PSNR: 18.321666717529297
[TRAIN] Iter: 947900 Loss: 0.026199031621217728  PSNR: 19.220535278320312
[TRAIN] Iter: 948000 Loss: 0.02485160529613495  PSNR: 19.59645652770996
[TRAIN] Iter: 948100 Loss: 0.029830629006028175  PSNR: 18.46902084350586
[TRAIN] Iter: 948200 Loss: 0.024159424006938934  PSNR: 19.62816047668457
[TRAIN] Iter: 948300 Loss: 0.0244960468262434  PSNR: 19.4859619140625
[TRAIN] Iter: 948400 Loss: 0.02387477271258831  PSNR: 19.419897079467773
[TRAIN] Iter: 948500 Loss: 0.028072956949472427  PSNR: 18.68619155883789
[TRAIN] Iter: 948600 Loss: 0.023659011349081993  PSNR: 19.79791831970215
[TRAIN] Iter: 948700 Loss: 0.02368573099374771  PSNR: 19.705989837646484
[TRAIN] Iter: 948800 Loss: 0.027271363884210587  PSNR: 18.7868709564209
[TRAIN] Iter: 948900 Loss: 0.024151166900992393  PSNR: 19.431066513061523
[TRAIN] Iter: 949000 Loss: 0.028683766722679138  PSNR: 18.678356170654297
[TRAIN] Iter: 949100 Loss: 0.027622679248452187  PSNR: 18.64592933654785
[TRAIN] Iter: 949200 Loss: 0.03062729351222515  PSNR: 18.370208740234375
[TRAIN] Iter: 949300 Loss: 0.02631670981645584  PSNR: 19.06633758544922
[TRAIN] Iter: 949400 Loss: 0.03239988163113594  PSNR: 18.059667587280273
[TRAIN] Iter: 949500 Loss: 0.025182701647281647  PSNR: 19.504613876342773
[TRAIN] Iter: 949600 Loss: 0.026328451931476593  PSNR: 19.092653274536133
[TRAIN] Iter: 949700 Loss: 0.022934766486287117  PSNR: 19.50361442565918
[TRAIN] Iter: 949800 Loss: 0.030994806438684464  PSNR: 18.262388229370117
[TRAIN] Iter: 949900 Loss: 0.02421812154352665  PSNR: 19.446578979492188
Saved checkpoints at ./logs/TUT-LAB-nerf/950000.tar
0 0.0004036426544189453
torch.Size([320, 640, 3]) torch.Size([320, 640])
1 17.689472198486328
2 15.755504131317139
3 13.7054603099823
4 17.330576419830322
5 15.202823638916016
6 17.463143825531006
7 13.725798606872559
8 15.899746417999268
9 13.290626287460327
10 15.941889762878418
11 13.327720403671265
12 13.688251256942749
13 15.697915315628052
14 13.435763597488403
15 15.934494972229004
16 13.287136554718018
17 15.999908208847046
18 13.24552035331726
19 13.80826210975647
20 15.7893545627594
21 13.139580726623535
22 16.0524263381958
23 13.535002946853638
24 13.563063144683838
25 15.671817302703857
26 13.65212631225586
27 15.667664289474487
28 13.554794311523438
29 15.652186632156372
30 13.61624550819397
31 13.588455438613892
32 15.65258526802063
33 13.577186822891235
34 15.673446416854858
35 13.59001612663269
36 15.673943519592285
37 13.585054397583008
38 13.581347942352295
39 15.708799839019775
40 13.598363161087036
41 15.705042362213135
42 13.593259334564209
43 13.580723524093628
44 15.640317678451538
45 13.579890489578247
46 15.678744316101074
47 13.588757276535034
48 15.653711795806885
49 13.571162223815918
50 13.567135095596313
51 15.750800848007202
52 13.614799499511719
53 15.678704738616943
54 13.540041446685791
55 15.69364070892334
56 13.43491530418396
57 13.576847791671753
58 15.818624019622803
59 13.568881034851074
60 15.638524532318115
61 13.428622961044312
62 13.659813165664673
63 15.635456562042236
64 13.539052486419678
65 15.697563409805298
66 13.541601657867432
67 15.707676887512207
68 13.573307275772095
69 13.602567672729492
70 15.706704378128052
71 13.566216945648193
72 15.829700946807861
73 13.493570327758789
74 15.735339164733887
75 13.467040777206421
76 13.675800085067749
77 15.900689601898193
78 13.417537927627563
79 15.823659420013428
80 13.305213689804077
81 15.725918292999268
82 13.719043731689453
83 13.324715852737427
84 16.1621356010437
85 13.178529977798462
86 16.05887722969055
87 13.382018566131592
88 13.44252324104309
89 15.993038415908813
90 13.323513507843018
91 15.866824626922607
92 13.425240278244019
93 15.893578052520752
94 13.571861743927002
95 13.420065879821777
96 15.76402473449707
97 13.449030637741089
98 15.970582008361816
99 13.348492622375488
100 15.86189603805542
101 13.589109420776367
102 13.47253680229187
103 15.883628129959106
104 13.331454038619995
105 16.072368383407593
106 13.209546327590942
107 13.765110969543457
108 15.533581495285034
109 13.572311878204346
110 15.873183488845825
111 13.274998903274536
112 16.10716199874878
113 13.220640897750854
114 13.825376272201538
115 15.589285373687744
116 13.613170385360718
117 15.834486961364746
118 13.018835067749023
119 16.22199773788452
Done, saving (120, 320, 640, 3) (120, 320, 640)
extras:{'raw': tensor([[[-6.7014e-01, -4.3546e-01,  7.2582e-01, -2.1917e+01],
         [-7.9853e-01, -8.8181e-01,  8.1127e-02, -2.6316e+01],
         [-8.0103e-01, -7.9791e-01, -1.3524e-01, -1.8250e+01],
         ...,
         [-1.6527e+00, -1.2047e+00,  2.1259e-01, -6.6489e-01],
         [-1.7348e+00, -1.1312e+00,  2.7905e-01, -4.4512e+00],
         [-2.1947e+00, -1.2012e+00,  6.9461e-01,  2.8801e+00]],

        [[-5.0236e-02, -2.0035e-01, -3.6743e-01,  1.7918e+01],
         [-4.2196e-02, -2.3689e-01, -4.8844e-01,  1.7813e+01],
         [ 4.0285e-02, -1.7218e-01, -4.4285e-01,  1.7903e+01],
         ...,
         [ 1.0067e+01,  3.9107e+00, -3.6648e+00,  2.4770e+02],
         [ 1.2546e+01,  6.2583e+00, -1.2271e+00,  2.4831e+02],
         [ 1.3171e+01,  6.2284e+00, -2.2228e+00,  2.5355e+02]],

        [[ 4.8822e-01,  2.9794e-01,  1.2530e-01,  3.8897e+00],
         [ 6.3603e-01,  4.8020e-01,  3.8031e-01, -1.2079e+01],
         [ 5.0381e-01,  4.0285e-01,  4.6909e-01, -1.3397e+01],
         ...,
         [ 2.2125e+01,  2.1721e+01,  2.6180e+01,  1.8745e+02],
         [ 2.2325e+01,  2.2109e+01,  2.6967e+01,  1.8909e+02],
         [ 2.3531e+01,  2.3168e+01,  2.8187e+01,  2.0119e+02]],

        ...,

        [[ 4.3143e-01,  2.4578e-01,  2.2400e-02,  8.1604e+00],
         [ 7.2832e-01,  5.8234e-01,  2.0732e-01, -1.8428e+01],
         [ 5.0708e-01,  3.1398e-01,  6.7573e-02, -1.8473e+01],
         ...,
         [ 4.2552e+00,  2.9356e+00,  1.0940e+00,  1.6698e+02],
         [ 4.3168e+00,  2.9620e+00,  1.0123e+00,  1.6917e+02],
         [ 4.2320e+00,  2.8757e+00,  8.6159e-01,  1.7169e+02]],

        [[ 1.8012e-01, -1.2204e-02, -2.1844e-01,  5.9604e+00],
         [ 2.0577e-01,  3.8541e-02, -1.9427e-01, -7.1265e-01],
         [ 1.8251e-01,  1.1604e-02, -1.6990e-01, -2.6609e+00],
         ...,
         [ 9.2362e-01,  6.4527e-01, -4.2910e+00,  4.4632e+02],
         [ 1.1502e+00,  1.3078e+00, -3.7746e+00,  4.6960e+02],
         [ 1.8896e+00,  1.4467e+00, -3.2731e+00,  4.6317e+02]],

        [[-6.0340e-01, -3.7338e-01,  4.4298e-01, -2.2916e+01],
         [-8.0572e-01, -8.9290e-01,  7.4525e-02, -2.5322e+01],
         [-2.0375e-01, -3.9326e-01,  2.6556e-01, -1.3453e+01],
         ...,
         [-1.2086e+00, -7.0260e-01,  5.1507e-01, -5.2132e+00],
         [-1.8490e+00, -1.2107e+00,  3.5146e-01, -1.2312e+01],
         [-7.9627e-01, -5.9377e-01,  3.8229e-01,  4.1781e+00]]],
       grad_fn=<ReshapeAliasBackward0>), 'rgb0': tensor([[0.2152, 0.3544, 0.7132],
        [0.4740, 0.4359, 0.4005],
        [0.6224, 0.5815, 0.5413],
        ...,
        [0.6062, 0.5569, 0.4933],
        [0.5400, 0.4954, 0.4481],
        [0.2174, 0.3575, 0.7029]], grad_fn=<ReshapeAliasBackward0>), 'disp0': tensor([  20.6300,  509.3695, 1142.5549,  ...,   29.0485,   20.2969,
          21.7824], grad_fn=<ReshapeAliasBackward0>), 'acc0': tensor([1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000],
       grad_fn=<ReshapeAliasBackward0>), 'z_std': tensor([0.0017, 0.3024, 0.2050,  ..., 0.0117, 0.0031, 0.0027])}
0 0.0004229545593261719
torch.Size([320, 640, 3]) torch.Size([320, 640])
1 13.99447226524353
2 16.094104051589966
3 14.700847148895264
4 14.249959230422974
5 13.114338397979736
6 15.959441423416138
7 13.533496141433716
8 13.725493669509888
9 15.31326150894165
10 13.47822093963623
11 15.974312543869019
12 13.25516152381897
13 16.084384202957153
14 13.320537805557251
15 13.889668703079224
16 15.487541437149048
17 13.540672302246094
18 15.80331015586853
19 13.456691980361938
20 15.966020822525024
21 13.745654106140137
22 13.717774152755737
23 15.551156282424927
24 13.712494134902954
25 15.514657497406006
26 13.666428804397583
27 15.512542486190796
28 13.694631338119507
29 13.703377962112427
30 15.539917707443237
31 13.689316511154175
32 15.485426902770996
33 13.656147480010986
34 13.596805810928345
35 15.563929080963135
36 13.687500238418579
37 15.494149923324585
38 13.655414342880249
39 15.509982347488403
40 13.6608567237854
41 13.721305131912231
42 15.548362493515015
43 13.741185665130615
44 15.534027338027954
45 13.649317502975464
46 15.524778127670288
47 13.663326025009155
48 13.657006740570068
49 15.442557573318481
50 13.443241357803345
51 15.786627054214478
52 13.680919647216797
53 15.598938465118408
54 13.844157695770264
55 13.585841417312622
56 15.633092403411865
57 13.24810242652893
58 16.30555534362793
59 13.29549789428711
60 13.538123846054077
61 15.682224750518799
62 13.438866138458252
63 15.92723798751831
64 13.237595558166504
65 15.961648225784302
66 13.182931900024414
67 13.71375560760498
68 15.716275453567505
69 13.589430809020996
70 15.611737966537476
71 13.449933052062988
72 15.770365476608276
73 13.788420915603638
74 13.615051984786987
75 15.41935682296753
76 13.583433866500854
77 15.652251243591309
78 13.668513059616089
79 15.555614471435547
80 13.631495237350464
81 13.649152994155884
82 15.603361368179321
83 13.645339250564575
84 15.532950639724731
85 13.652010202407837
86 13.60093379020691
87 15.550259828567505
88 13.631028652191162
89 15.569563388824463
90 13.560880422592163
91 15.54088282585144
92 13.64593505859375
93 13.579596519470215
94 15.517609357833862
95 13.554939985275269
96 15.562854051589966
97 13.571214199066162
98 15.561298608779907
99 13.60538625717163
100 13.733208656311035
101 15.636903047561646
102 13.467709064483643
103 15.741620302200317
104 13.580010414123535
105 13.680543661117554
106 15.674971580505371
107 13.6082923412323
108 15.613524436950684
109 13.641347169876099
110 15.63145112991333
111 13.62033724784851
112 13.639068841934204
113 15.599621534347534
114 13.597700595855713
115 15.59981393814087
116 13.610881090164185
117 15.671271085739136
118 13.646629333496094
119 13.60656189918518
test poses shape torch.Size([13, 3, 4])
0 0.0007288455963134766
torch.Size([320, 640, 3]) torch.Size([320, 640])
1 13.654016733169556
2 15.630217790603638
3 13.63119888305664
4 15.715775489807129
5 13.66928768157959
6 13.65693998336792
7 15.714580297470093
8 13.565231561660767
9 15.810023784637451
10 13.544699907302856
11 15.778861045837402
12 13.57387661933899
Saved test set
[TRAIN] Iter: 950000 Loss: 0.027720633894205093  PSNR: 18.77378273010254
[TRAIN] Iter: 950100 Loss: 0.02699495665729046  PSNR: 19.045480728149414
[TRAIN] Iter: 950200 Loss: 0.02839057520031929  PSNR: 18.72309112548828
[TRAIN] Iter: 950300 Loss: 0.026771634817123413  PSNR: 18.805843353271484
[TRAIN] Iter: 950400 Loss: 0.028044668957591057  PSNR: 18.748855590820312
[TRAIN] Iter: 950500 Loss: 0.020717576146125793  PSNR: 19.822065353393555
[TRAIN] Iter: 950600 Loss: 0.03250009939074516  PSNR: 18.284564971923828
[TRAIN] Iter: 950700 Loss: 0.026521947234869003  PSNR: 18.916196823120117
[TRAIN] Iter: 950800 Loss: 0.030860692262649536  PSNR: 18.318708419799805
[TRAIN] Iter: 950900 Loss: 0.03253018110990524  PSNR: 18.0860652923584
[TRAIN] Iter: 951000 Loss: 0.02544795535504818  PSNR: 19.05027961730957
[TRAIN] Iter: 951100 Loss: 0.02460222691297531  PSNR: 19.556591033935547
[TRAIN] Iter: 951200 Loss: 0.024135105311870575  PSNR: 19.35590934753418
[TRAIN] Iter: 951300 Loss: 0.022047406062483788  PSNR: 20.149370193481445
[TRAIN] Iter: 951400 Loss: 0.02738756500184536  PSNR: 18.94743537902832
[TRAIN] Iter: 951500 Loss: 0.023323457688093185  PSNR: 19.608299255371094
[TRAIN] Iter: 951600 Loss: 0.030055424198508263  PSNR: 18.448184967041016
[TRAIN] Iter: 951700 Loss: 0.023237474262714386  PSNR: 19.447843551635742
[TRAIN] Iter: 951800 Loss: 0.030032791197299957  PSNR: 18.509611129760742
[TRAIN] Iter: 951900 Loss: 0.023326072841882706  PSNR: 19.4462833404541
[TRAIN] Iter: 952000 Loss: 0.025347884744405746  PSNR: 19.11628532409668
[TRAIN] Iter: 952100 Loss: 0.027577238157391548  PSNR: 19.112600326538086
[TRAIN] Iter: 952200 Loss: 0.033633384853601456  PSNR: 17.919143676757812
[TRAIN] Iter: 952300 Loss: 0.025256706401705742  PSNR: 19.203350067138672
[TRAIN] Iter: 952400 Loss: 0.02682456001639366  PSNR: 18.867568969726562
[TRAIN] Iter: 952500 Loss: 0.024747665971517563  PSNR: 19.03553581237793
[TRAIN] Iter: 952600 Loss: 0.0287176426500082  PSNR: 18.642581939697266
[TRAIN] Iter: 952700 Loss: 0.02817342057824135  PSNR: 18.699657440185547
[TRAIN] Iter: 952800 Loss: 0.027181942015886307  PSNR: 18.71732521057129
[TRAIN] Iter: 952900 Loss: 0.02322126366198063  PSNR: 19.460168838500977
[TRAIN] Iter: 953000 Loss: 0.023130828514695168  PSNR: 19.395597457885742
[TRAIN] Iter: 953100 Loss: 0.030285997316241264  PSNR: 18.337783813476562
[TRAIN] Iter: 953200 Loss: 0.02594836615025997  PSNR: 19.128267288208008
[TRAIN] Iter: 953300 Loss: 0.026736311614513397  PSNR: 19.091886520385742
[TRAIN] Iter: 953400 Loss: 0.027973227202892303  PSNR: 18.848976135253906
[TRAIN] Iter: 953500 Loss: 0.02389490231871605  PSNR: 19.335018157958984
[TRAIN] Iter: 953600 Loss: 0.028249960392713547  PSNR: 18.73499870300293
[TRAIN] Iter: 953700 Loss: 0.02611558698117733  PSNR: 19.40584373474121
[TRAIN] Iter: 953800 Loss: 0.03272882103919983  PSNR: 18.067623138427734
[TRAIN] Iter: 953900 Loss: 0.03058624267578125  PSNR: 18.215436935424805
[TRAIN] Iter: 954000 Loss: 0.033633120357990265  PSNR: 17.90839385986328
[TRAIN] Iter: 954100 Loss: 0.025304574519395828  PSNR: 19.352142333984375
[TRAIN] Iter: 954200 Loss: 0.027736186981201172  PSNR: 18.796934127807617
[TRAIN] Iter: 954300 Loss: 0.02381831780076027  PSNR: 19.348398208618164
[TRAIN] Iter: 954400 Loss: 0.025005802512168884  PSNR: 19.08902359008789
[TRAIN] Iter: 954500 Loss: 0.029258720576763153  PSNR: 18.630290985107422
[TRAIN] Iter: 954600 Loss: 0.02376444637775421  PSNR: 19.50757598876953
[TRAIN] Iter: 954700 Loss: 0.03135017305612564  PSNR: 18.213804244995117
[TRAIN] Iter: 954800 Loss: 0.029627664014697075  PSNR: 18.413896560668945
[TRAIN] Iter: 954900 Loss: 0.025394544005393982  PSNR: 19.004703521728516
[TRAIN] Iter: 955000 Loss: 0.02741197496652603  PSNR: 18.699989318847656
[TRAIN] Iter: 955100 Loss: 0.02790822647511959  PSNR: 18.8030948638916
[TRAIN] Iter: 955200 Loss: 0.027761895209550858  PSNR: 18.850317001342773
[TRAIN] Iter: 955300 Loss: 0.026376552879810333  PSNR: 19.06156349182129
[TRAIN] Iter: 955400 Loss: 0.026156259700655937  PSNR: 19.21181297302246
[TRAIN] Iter: 955500 Loss: 0.02412213385105133  PSNR: 19.370615005493164
[TRAIN] Iter: 955600 Loss: 0.02856210060417652  PSNR: 18.649738311767578
[TRAIN] Iter: 955700 Loss: 0.022753695026040077  PSNR: 19.81529426574707
[TRAIN] Iter: 955800 Loss: 0.02189669758081436  PSNR: 19.790626525878906
[TRAIN] Iter: 955900 Loss: 0.029698040336370468  PSNR: 18.481773376464844
[TRAIN] Iter: 956000 Loss: 0.02701125107705593  PSNR: 18.86211395263672
[TRAIN] Iter: 956100 Loss: 0.03177739307284355  PSNR: 18.091800689697266
[TRAIN] Iter: 956200 Loss: 0.02564893290400505  PSNR: 19.02943992614746
[TRAIN] Iter: 956300 Loss: 0.026315977796912193  PSNR: 19.288785934448242
[TRAIN] Iter: 956400 Loss: 0.02541256695985794  PSNR: 19.21639060974121
[TRAIN] Iter: 956500 Loss: 0.028688719496130943  PSNR: 18.465518951416016
[TRAIN] Iter: 956600 Loss: 0.03090018406510353  PSNR: 18.24358367919922
[TRAIN] Iter: 956700 Loss: 0.02666543796658516  PSNR: 18.824283599853516
[TRAIN] Iter: 956800 Loss: 0.03275298699736595  PSNR: 18.099163055419922
[TRAIN] Iter: 956900 Loss: 0.02949627861380577  PSNR: 18.507110595703125
[TRAIN] Iter: 957000 Loss: 0.024640176445245743  PSNR: 19.057327270507812
[TRAIN] Iter: 957100 Loss: 0.019343551248311996  PSNR: 20.26494598388672
[TRAIN] Iter: 957200 Loss: 0.026863642036914825  PSNR: 18.910587310791016
[TRAIN] Iter: 957300 Loss: 0.027169378474354744  PSNR: 18.767135620117188
[TRAIN] Iter: 957400 Loss: 0.024722544476389885  PSNR: 19.166175842285156
[TRAIN] Iter: 957500 Loss: 0.035424988716840744  PSNR: 17.609683990478516
[TRAIN] Iter: 957600 Loss: 0.024260248988866806  PSNR: 19.316118240356445
[TRAIN] Iter: 957700 Loss: 0.022455131635069847  PSNR: 19.678356170654297
[TRAIN] Iter: 957800 Loss: 0.029897747561335564  PSNR: 18.51825714111328
[TRAIN] Iter: 957900 Loss: 0.029935287311673164  PSNR: 18.460216522216797
[TRAIN] Iter: 958000 Loss: 0.028344308957457542  PSNR: 18.625728607177734
[TRAIN] Iter: 958100 Loss: 0.023680847138166428  PSNR: 19.504106521606445
[TRAIN] Iter: 958200 Loss: 0.02455275133252144  PSNR: 19.078588485717773
[TRAIN] Iter: 958300 Loss: 0.027765411883592606  PSNR: 18.799345016479492
[TRAIN] Iter: 958400 Loss: 0.022741585969924927  PSNR: 19.557327270507812
[TRAIN] Iter: 958500 Loss: 0.022955717518925667  PSNR: 19.391565322875977
[TRAIN] Iter: 958600 Loss: 0.030086055397987366  PSNR: 18.401403427124023
[TRAIN] Iter: 958700 Loss: 0.0240318663418293  PSNR: 19.65940284729004
[TRAIN] Iter: 958800 Loss: 0.02755872532725334  PSNR: 18.817337036132812
[TRAIN] Iter: 958900 Loss: 0.031439825892448425  PSNR: 18.2626895904541
[TRAIN] Iter: 959000 Loss: 0.025355961173772812  PSNR: 19.24209213256836
[TRAIN] Iter: 959100 Loss: 0.02586713805794716  PSNR: 19.31991195678711
[TRAIN] Iter: 959200 Loss: 0.023915275931358337  PSNR: 18.934951782226562
[TRAIN] Iter: 959300 Loss: 0.02289574220776558  PSNR: 19.590267181396484
[TRAIN] Iter: 959400 Loss: 0.031085936352610588  PSNR: 18.292739868164062
[TRAIN] Iter: 959500 Loss: 0.02886703610420227  PSNR: 18.55691146850586
[TRAIN] Iter: 959600 Loss: 0.028724346309900284  PSNR: 18.616004943847656
[TRAIN] Iter: 959700 Loss: 0.02610434964299202  PSNR: 19.033784866333008
[TRAIN] Iter: 959800 Loss: 0.02311769314110279  PSNR: 19.533016204833984
[TRAIN] Iter: 959900 Loss: 0.024701863527297974  PSNR: 19.095348358154297
Saved checkpoints at ./logs/TUT-LAB-nerf/960000.tar
[TRAIN] Iter: 960000 Loss: 0.0290786512196064  PSNR: 18.844701766967773
[TRAIN] Iter: 960100 Loss: 0.02829318307340145  PSNR: 18.61139678955078
[TRAIN] Iter: 960200 Loss: 0.025994177907705307  PSNR: 19.046228408813477
[TRAIN] Iter: 960300 Loss: 0.025320222601294518  PSNR: 19.070697784423828
[TRAIN] Iter: 960400 Loss: 0.020688561722636223  PSNR: 20.152679443359375
[TRAIN] Iter: 960500 Loss: 0.029467791318893433  PSNR: 18.481935501098633
[TRAIN] Iter: 960600 Loss: 0.023473484441637993  PSNR: 19.56106948852539
[TRAIN] Iter: 960700 Loss: 0.028267456218600273  PSNR: 19.176870346069336
[TRAIN] Iter: 960800 Loss: 0.02896857261657715  PSNR: 18.44474220275879
[TRAIN] Iter: 960900 Loss: 0.030604878440499306  PSNR: 18.33696937561035
[TRAIN] Iter: 961000 Loss: 0.02533268742263317  PSNR: 19.341672897338867
[TRAIN] Iter: 961100 Loss: 0.023310275748372078  PSNR: 19.88555145263672
[TRAIN] Iter: 961200 Loss: 0.034410856664180756  PSNR: 17.81602668762207
[TRAIN] Iter: 961300 Loss: 0.030307024717330933  PSNR: 18.662389755249023
[TRAIN] Iter: 961400 Loss: 0.030042029917240143  PSNR: 18.335861206054688
[TRAIN] Iter: 961500 Loss: 0.023108800873160362  PSNR: 19.667879104614258
[TRAIN] Iter: 961600 Loss: 0.026605650782585144  PSNR: 18.735904693603516
[TRAIN] Iter: 961700 Loss: 0.028355728834867477  PSNR: 18.738075256347656
[TRAIN] Iter: 961800 Loss: 0.026517249643802643  PSNR: 18.91107177734375
[TRAIN] Iter: 961900 Loss: 0.033345118165016174  PSNR: 17.85561752319336
[TRAIN] Iter: 962000 Loss: 0.029861291870474815  PSNR: 18.564773559570312
[TRAIN] Iter: 962100 Loss: 0.024468835443258286  PSNR: 18.708147048950195
[TRAIN] Iter: 962200 Loss: 0.026857275515794754  PSNR: 18.757476806640625
[TRAIN] Iter: 962300 Loss: 0.029154572635889053  PSNR: 18.67466163635254
[TRAIN] Iter: 962400 Loss: 0.02859412133693695  PSNR: 18.59491539001465
[TRAIN] Iter: 962500 Loss: 0.02389802783727646  PSNR: 19.459941864013672
[TRAIN] Iter: 962600 Loss: 0.034295544028282166  PSNR: 17.741474151611328
[TRAIN] Iter: 962700 Loss: 0.032465849071741104  PSNR: 18.199079513549805
[TRAIN] Iter: 962800 Loss: 0.02673378959298134  PSNR: 18.974674224853516
[TRAIN] Iter: 962900 Loss: 0.03074301779270172  PSNR: 18.376434326171875
[TRAIN] Iter: 963000 Loss: 0.029919631779193878  PSNR: 18.478954315185547
[TRAIN] Iter: 963100 Loss: 0.019873887300491333  PSNR: 20.15509605407715
[TRAIN] Iter: 963200 Loss: 0.028876561671495438  PSNR: 18.526945114135742
[TRAIN] Iter: 963300 Loss: 0.024446872994303703  PSNR: 19.584123611450195
[TRAIN] Iter: 963400 Loss: 0.023363608866930008  PSNR: 19.295358657836914
[TRAIN] Iter: 963500 Loss: 0.02558206580579281  PSNR: 19.02150535583496
[TRAIN] Iter: 963600 Loss: 0.02936330810189247  PSNR: 18.463951110839844
[TRAIN] Iter: 963700 Loss: 0.028345178812742233  PSNR: 18.580780029296875
[TRAIN] Iter: 963800 Loss: 0.025029022246599197  PSNR: 19.30501937866211
[TRAIN] Iter: 963900 Loss: 0.026024937629699707  PSNR: 18.871604919433594
[TRAIN] Iter: 964000 Loss: 0.025300249457359314  PSNR: 19.031856536865234
[TRAIN] Iter: 964100 Loss: 0.0253236573189497  PSNR: 19.17795753479004
[TRAIN] Iter: 964200 Loss: 0.02467561885714531  PSNR: 19.30609703063965
[TRAIN] Iter: 964300 Loss: 0.031236473470926285  PSNR: 18.16352081298828
[TRAIN] Iter: 964400 Loss: 0.023093953728675842  PSNR: 19.623586654663086
[TRAIN] Iter: 964500 Loss: 0.019949812442064285  PSNR: 20.250158309936523
[TRAIN] Iter: 964600 Loss: 0.02513156086206436  PSNR: 19.30074691772461
[TRAIN] Iter: 964700 Loss: 0.02495761215686798  PSNR: 19.060617446899414
[TRAIN] Iter: 964800 Loss: 0.024008188396692276  PSNR: 19.57320213317871
[TRAIN] Iter: 964900 Loss: 0.02782963402569294  PSNR: 18.34061050415039
[TRAIN] Iter: 965000 Loss: 0.03061862662434578  PSNR: 18.35723304748535
[TRAIN] Iter: 965100 Loss: 0.02232244983315468  PSNR: 19.801044464111328
[TRAIN] Iter: 965200 Loss: 0.03043924830853939  PSNR: 18.352306365966797
[TRAIN] Iter: 965300 Loss: 0.021363120526075363  PSNR: 19.509363174438477
[TRAIN] Iter: 965400 Loss: 0.02461848594248295  PSNR: 19.245574951171875
[TRAIN] Iter: 965500 Loss: 0.030697934329509735  PSNR: 18.307565689086914
[TRAIN] Iter: 965600 Loss: 0.03350045531988144  PSNR: 17.933950424194336
[TRAIN] Iter: 965700 Loss: 0.028997179120779037  PSNR: 18.545490264892578
[TRAIN] Iter: 965800 Loss: 0.027976248413324356  PSNR: 18.647689819335938
[TRAIN] Iter: 965900 Loss: 0.03242356702685356  PSNR: 18.04242515563965
[TRAIN] Iter: 966000 Loss: 0.023103680461645126  PSNR: 19.588319778442383
[TRAIN] Iter: 966100 Loss: 0.025506790727376938  PSNR: 19.096355438232422
[TRAIN] Iter: 966200 Loss: 0.022022739052772522  PSNR: 19.599964141845703
[TRAIN] Iter: 966300 Loss: 0.019329920411109924  PSNR: 20.165714263916016
[TRAIN] Iter: 966400 Loss: 0.027898404747247696  PSNR: 18.763992309570312
[TRAIN] Iter: 966500 Loss: 0.0313311442732811  PSNR: 18.258724212646484
[TRAIN] Iter: 966600 Loss: 0.026223091408610344  PSNR: 19.04397201538086
[TRAIN] Iter: 966700 Loss: 0.027979396283626556  PSNR: 18.69569969177246
[TRAIN] Iter: 966800 Loss: 0.026165876537561417  PSNR: 19.16644859313965
[TRAIN] Iter: 966900 Loss: 0.02910158596932888  PSNR: 18.488866806030273
[TRAIN] Iter: 967000 Loss: 0.02742604725062847  PSNR: 18.73114013671875
[TRAIN] Iter: 967100 Loss: 0.029078984633088112  PSNR: 18.57036018371582
[TRAIN] Iter: 967200 Loss: 0.0270082950592041  PSNR: 18.71043586730957
[TRAIN] Iter: 967300 Loss: 0.02044612169265747  PSNR: 19.661157608032227
[TRAIN] Iter: 967400 Loss: 0.03100326471030712  PSNR: 18.417314529418945
[TRAIN] Iter: 967500 Loss: 0.027718301862478256  PSNR: 18.797340393066406
[TRAIN] Iter: 967600 Loss: 0.030096225440502167  PSNR: 18.339141845703125
[TRAIN] Iter: 967700 Loss: 0.033261530101299286  PSNR: 17.967018127441406
[TRAIN] Iter: 967800 Loss: 0.026634525507688522  PSNR: 18.883174896240234
[TRAIN] Iter: 967900 Loss: 0.0318148136138916  PSNR: 18.109167098999023
[TRAIN] Iter: 968000 Loss: 0.02467608079314232  PSNR: 19.086471557617188
[TRAIN] Iter: 968100 Loss: 0.027467897161841393  PSNR: 19.110754013061523
[TRAIN] Iter: 968200 Loss: 0.031119512394070625  PSNR: 18.244277954101562
[TRAIN] Iter: 968300 Loss: 0.022579681128263474  PSNR: 19.349990844726562
[TRAIN] Iter: 968400 Loss: 0.030420882627367973  PSNR: 18.367454528808594
[TRAIN] Iter: 968500 Loss: 0.027711238712072372  PSNR: 18.986570358276367
[TRAIN] Iter: 968600 Loss: 0.025363503023982048  PSNR: 19.10761833190918
[TRAIN] Iter: 968700 Loss: 0.029576119035482407  PSNR: 18.489255905151367
[TRAIN] Iter: 968800 Loss: 0.024098491296172142  PSNR: 19.37665557861328
[TRAIN] Iter: 968900 Loss: 0.028386466205120087  PSNR: 18.696165084838867
[TRAIN] Iter: 969000 Loss: 0.02577289007604122  PSNR: 19.366352081298828
[TRAIN] Iter: 969100 Loss: 0.02626599185168743  PSNR: 18.663650512695312
[TRAIN] Iter: 969200 Loss: 0.026856355369091034  PSNR: 18.82976531982422
[TRAIN] Iter: 969300 Loss: 0.031507425010204315  PSNR: 18.20937728881836
[TRAIN] Iter: 969400 Loss: 0.02791786752641201  PSNR: 18.961427688598633
[TRAIN] Iter: 969500 Loss: 0.028568968176841736  PSNR: 18.838422775268555
[TRAIN] Iter: 969600 Loss: 0.028166241943836212  PSNR: 18.51230239868164
[TRAIN] Iter: 969700 Loss: 0.025596745312213898  PSNR: 18.614843368530273
[TRAIN] Iter: 969800 Loss: 0.032094284892082214  PSNR: 18.167734146118164
[TRAIN] Iter: 969900 Loss: 0.02595432475209236  PSNR: 19.01934242248535
Saved checkpoints at ./logs/TUT-LAB-nerf/970000.tar
[TRAIN] Iter: 970000 Loss: 0.0242164209485054  PSNR: 19.474546432495117
[TRAIN] Iter: 970100 Loss: 0.026839636266231537  PSNR: 19.283710479736328
[TRAIN] Iter: 970200 Loss: 0.03127949684858322  PSNR: 18.26771354675293
[TRAIN] Iter: 970300 Loss: 0.0222695991396904  PSNR: 19.69804573059082
[TRAIN] Iter: 970400 Loss: 0.027212657034397125  PSNR: 18.927623748779297
[TRAIN] Iter: 970500 Loss: 0.02558092214167118  PSNR: 19.1226806640625
[TRAIN] Iter: 970600 Loss: 0.03206533566117287  PSNR: 18.04345703125
[TRAIN] Iter: 970700 Loss: 0.028867848217487335  PSNR: 18.680217742919922
[TRAIN] Iter: 970800 Loss: 0.022999083623290062  PSNR: 19.574962615966797
[TRAIN] Iter: 970900 Loss: 0.0262953769415617  PSNR: 19.242319107055664
[TRAIN] Iter: 971000 Loss: 0.024022361263632774  PSNR: 19.43201446533203
[TRAIN] Iter: 971100 Loss: 0.028291497379541397  PSNR: 18.793502807617188
[TRAIN] Iter: 971200 Loss: 0.029265668243169785  PSNR: 18.569002151489258
[TRAIN] Iter: 971300 Loss: 0.028399091213941574  PSNR: 18.703611373901367
[TRAIN] Iter: 971400 Loss: 0.022524401545524597  PSNR: 19.57077980041504
[TRAIN] Iter: 971500 Loss: 0.024391138926148415  PSNR: 19.452106475830078
[TRAIN] Iter: 971600 Loss: 0.02722599171102047  PSNR: 18.879220962524414
[TRAIN] Iter: 971700 Loss: 0.021921325474977493  PSNR: 19.7672176361084
[TRAIN] Iter: 971800 Loss: 0.0258803591132164  PSNR: 19.16064453125
[TRAIN] Iter: 971900 Loss: 0.025456134229898453  PSNR: 19.11163330078125
[TRAIN] Iter: 972000 Loss: 0.027987124398350716  PSNR: 18.728090286254883
[TRAIN] Iter: 972100 Loss: 0.026796938851475716  PSNR: 18.960399627685547
[TRAIN] Iter: 972200 Loss: 0.02286154218018055  PSNR: 19.593814849853516
[TRAIN] Iter: 972300 Loss: 0.02900773659348488  PSNR: 18.374250411987305
[TRAIN] Iter: 972400 Loss: 0.029101593419909477  PSNR: 18.53738784790039
[TRAIN] Iter: 972500 Loss: 0.02417740970849991  PSNR: 19.125337600708008
[TRAIN] Iter: 972600 Loss: 0.02840215340256691  PSNR: 18.22087860107422
[TRAIN] Iter: 972700 Loss: 0.02149055339396  PSNR: 19.952011108398438
[TRAIN] Iter: 972800 Loss: 0.027869030833244324  PSNR: 18.846424102783203
[TRAIN] Iter: 972900 Loss: 0.025087151676416397  PSNR: 19.31581687927246
[TRAIN] Iter: 973000 Loss: 0.021339163184165955  PSNR: 19.89937973022461
[TRAIN] Iter: 973100 Loss: 0.02630016766488552  PSNR: 18.758440017700195
[TRAIN] Iter: 973200 Loss: 0.022057896479964256  PSNR: 19.856645584106445
[TRAIN] Iter: 973300 Loss: 0.023076258599758148  PSNR: 19.663883209228516
[TRAIN] Iter: 973400 Loss: 0.022859230637550354  PSNR: 19.61925506591797
[TRAIN] Iter: 973500 Loss: 0.03085518814623356  PSNR: 18.26827049255371
[TRAIN] Iter: 973600 Loss: 0.025784824043512344  PSNR: 18.95331573486328
[TRAIN] Iter: 973700 Loss: 0.029210947453975677  PSNR: 18.541181564331055
[TRAIN] Iter: 973800 Loss: 0.028697917237877846  PSNR: 18.576839447021484
[TRAIN] Iter: 973900 Loss: 0.018695838749408722  PSNR: 20.453638076782227
[TRAIN] Iter: 974000 Loss: 0.028836477547883987  PSNR: 18.691007614135742
[TRAIN] Iter: 974100 Loss: 0.029336687177419662  PSNR: 18.470338821411133
[TRAIN] Iter: 974200 Loss: 0.031049299985170364  PSNR: 18.145071029663086
[TRAIN] Iter: 974300 Loss: 0.029814962297677994  PSNR: 18.465227127075195
[TRAIN] Iter: 974400 Loss: 0.030560215935111046  PSNR: 18.361492156982422
[TRAIN] Iter: 974500 Loss: 0.022216619923710823  PSNR: 19.93560791015625
[TRAIN] Iter: 974600 Loss: 0.026253357529640198  PSNR: 19.025279998779297
[TRAIN] Iter: 974700 Loss: 0.02268194407224655  PSNR: 19.764904022216797
[TRAIN] Iter: 974800 Loss: 0.024726837873458862  PSNR: 19.31661605834961
[TRAIN] Iter: 974900 Loss: 0.03325206786394119  PSNR: 17.924537658691406
[TRAIN] Iter: 975000 Loss: 0.021070696413517  PSNR: 19.874849319458008
[TRAIN] Iter: 975100 Loss: 0.024782614782452583  PSNR: 19.17890167236328
[TRAIN] Iter: 975200 Loss: 0.025981470942497253  PSNR: 19.367366790771484
[TRAIN] Iter: 975300 Loss: 0.025359027087688446  PSNR: 19.002883911132812
[TRAIN] Iter: 975400 Loss: 0.02146366983652115  PSNR: 19.86009979248047
[TRAIN] Iter: 975500 Loss: 0.03263356164097786  PSNR: 18.09955406188965
[TRAIN] Iter: 975600 Loss: 0.02210931107401848  PSNR: 19.540918350219727
[TRAIN] Iter: 975700 Loss: 0.029236655682325363  PSNR: 18.557353973388672
[TRAIN] Iter: 975800 Loss: 0.03074561059474945  PSNR: 18.21773338317871
[TRAIN] Iter: 975900 Loss: 0.022031007334589958  PSNR: 19.742589950561523
[TRAIN] Iter: 976000 Loss: 0.021391509100794792  PSNR: 19.708148956298828
[TRAIN] Iter: 976100 Loss: 0.028611425310373306  PSNR: 18.644554138183594
[TRAIN] Iter: 976200 Loss: 0.0284882839769125  PSNR: 18.7951602935791
[TRAIN] Iter: 976300 Loss: 0.021381067112088203  PSNR: 20.279705047607422
[TRAIN] Iter: 976400 Loss: 0.027855340391397476  PSNR: 18.60892105102539
[TRAIN] Iter: 976500 Loss: 0.030052371323108673  PSNR: 18.386749267578125
[TRAIN] Iter: 976600 Loss: 0.021268896758556366  PSNR: 19.981807708740234
[TRAIN] Iter: 976700 Loss: 0.02424435317516327  PSNR: 19.513721466064453
[TRAIN] Iter: 976800 Loss: 0.02936151996254921  PSNR: 18.588735580444336
[TRAIN] Iter: 976900 Loss: 0.02303408458828926  PSNR: 19.263185501098633
[TRAIN] Iter: 977000 Loss: 0.030288778245449066  PSNR: 18.422794342041016
[TRAIN] Iter: 977100 Loss: 0.022217124700546265  PSNR: 19.409372329711914
[TRAIN] Iter: 977200 Loss: 0.023867448791861534  PSNR: 19.43428611755371
[TRAIN] Iter: 977300 Loss: 0.02770402655005455  PSNR: 18.803890228271484
[TRAIN] Iter: 977400 Loss: 0.02572847157716751  PSNR: 19.239431381225586
[TRAIN] Iter: 977500 Loss: 0.031005851924419403  PSNR: 18.340499877929688
[TRAIN] Iter: 977600 Loss: 0.025143636390566826  PSNR: 19.159534454345703
[TRAIN] Iter: 977700 Loss: 0.029356863349676132  PSNR: 18.5765323638916
[TRAIN] Iter: 977800 Loss: 0.027200235053896904  PSNR: 18.864473342895508
[TRAIN] Iter: 977900 Loss: 0.025645211338996887  PSNR: 19.128337860107422
[TRAIN] Iter: 978000 Loss: 0.03041750192642212  PSNR: 18.276803970336914
[TRAIN] Iter: 978100 Loss: 0.028549708425998688  PSNR: 18.727540969848633
[TRAIN] Iter: 978200 Loss: 0.024600215256214142  PSNR: 18.95578384399414
[TRAIN] Iter: 978300 Loss: 0.021686071529984474  PSNR: 19.736379623413086
[TRAIN] Iter: 978400 Loss: 0.028925195336341858  PSNR: 18.67612648010254
[TRAIN] Iter: 978500 Loss: 0.029255639761686325  PSNR: 18.568510055541992
[TRAIN] Iter: 978600 Loss: 0.03218550235033035  PSNR: 18.02408790588379
[TRAIN] Iter: 978700 Loss: 0.030382225289940834  PSNR: 18.233041763305664
[TRAIN] Iter: 978800 Loss: 0.020334908738732338  PSNR: 20.049503326416016
[TRAIN] Iter: 978900 Loss: 0.02152690291404724  PSNR: 20.012388229370117
[TRAIN] Iter: 979000 Loss: 0.029135841876268387  PSNR: 18.477802276611328
[TRAIN] Iter: 979100 Loss: 0.027180463075637817  PSNR: 18.94057846069336
[TRAIN] Iter: 979200 Loss: 0.030184192582964897  PSNR: 18.263124465942383
[TRAIN] Iter: 979300 Loss: 0.021892890334129333  PSNR: 19.55667495727539
[TRAIN] Iter: 979400 Loss: 0.023863986134529114  PSNR: 19.43137550354004
[TRAIN] Iter: 979500 Loss: 0.022748202085494995  PSNR: 19.704845428466797
[TRAIN] Iter: 979600 Loss: 0.02769368328154087  PSNR: 18.79861068725586
[TRAIN] Iter: 979700 Loss: 0.027539528906345367  PSNR: 18.954021453857422
[TRAIN] Iter: 979800 Loss: 0.027511633932590485  PSNR: 19.118967056274414
[TRAIN] Iter: 979900 Loss: 0.02899119444191456  PSNR: 18.5532169342041
Saved checkpoints at ./logs/TUT-LAB-nerf/980000.tar
[TRAIN] Iter: 980000 Loss: 0.02490362897515297  PSNR: 19.31447982788086
[TRAIN] Iter: 980100 Loss: 0.02767302840948105  PSNR: 18.79391098022461
[TRAIN] Iter: 980200 Loss: 0.027696384117007256  PSNR: 18.819250106811523
[TRAIN] Iter: 980300 Loss: 0.02913019247353077  PSNR: 18.54876708984375
[TRAIN] Iter: 980400 Loss: 0.03189549595117569  PSNR: 18.06121826171875
[TRAIN] Iter: 980500 Loss: 0.02565668523311615  PSNR: 18.98828125
[TRAIN] Iter: 980600 Loss: 0.022511225193738937  PSNR: 19.628684997558594
[TRAIN] Iter: 980700 Loss: 0.02604687213897705  PSNR: 19.086894989013672
[TRAIN] Iter: 980800 Loss: 0.026924682781100273  PSNR: 18.979764938354492
[TRAIN] Iter: 980900 Loss: 0.022569024935364723  PSNR: 19.5424747467041
[TRAIN] Iter: 981000 Loss: 0.02572636306285858  PSNR: 19.248428344726562
[TRAIN] Iter: 981100 Loss: 0.033176109194755554  PSNR: 18.119977951049805
[TRAIN] Iter: 981200 Loss: 0.023624055087566376  PSNR: 19.479753494262695
[TRAIN] Iter: 981300 Loss: 0.019900765269994736  PSNR: 20.170093536376953
[TRAIN] Iter: 981400 Loss: 0.02670288272202015  PSNR: 18.996061325073242
[TRAIN] Iter: 981500 Loss: 0.02445930615067482  PSNR: 19.23609733581543
[TRAIN] Iter: 981600 Loss: 0.02452082186937332  PSNR: 19.443769454956055
[TRAIN] Iter: 981700 Loss: 0.02845601737499237  PSNR: 18.721656799316406
[TRAIN] Iter: 981800 Loss: 0.025174058973789215  PSNR: 19.169706344604492
[TRAIN] Iter: 981900 Loss: 0.02809293195605278  PSNR: 18.832324981689453
[TRAIN] Iter: 982000 Loss: 0.024890942499041557  PSNR: 19.228242874145508
[TRAIN] Iter: 982100 Loss: 0.02872176095843315  PSNR: 18.296920776367188
[TRAIN] Iter: 982200 Loss: 0.021774444729089737  PSNR: 19.69420623779297
[TRAIN] Iter: 982300 Loss: 0.02979274094104767  PSNR: 18.57501792907715
[TRAIN] Iter: 982400 Loss: 0.030307335779070854  PSNR: 18.457015991210938
[TRAIN] Iter: 982500 Loss: 0.023665424436330795  PSNR: 19.469614028930664
[TRAIN] Iter: 982600 Loss: 0.02944130077958107  PSNR: 18.516481399536133
[TRAIN] Iter: 982700 Loss: 0.035570137202739716  PSNR: 17.655620574951172
[TRAIN] Iter: 982800 Loss: 0.031079284846782684  PSNR: 18.266847610473633
[TRAIN] Iter: 982900 Loss: 0.022695094347000122  PSNR: 19.52007484436035
[TRAIN] Iter: 983000 Loss: 0.027957716956734657  PSNR: 18.900699615478516
[TRAIN] Iter: 983100 Loss: 0.025658831000328064  PSNR: 19.238649368286133
[TRAIN] Iter: 983200 Loss: 0.030968794599175453  PSNR: 18.183223724365234
[TRAIN] Iter: 983300 Loss: 0.017205974087119102  PSNR: 20.7617244720459
[TRAIN] Iter: 983400 Loss: 0.02812950871884823  PSNR: 18.53834342956543
[TRAIN] Iter: 983500 Loss: 0.02844526432454586  PSNR: 18.744144439697266
[TRAIN] Iter: 983600 Loss: 0.027006790041923523  PSNR: 18.891494750976562
[TRAIN] Iter: 983700 Loss: 0.026674043387174606  PSNR: 18.951478958129883
[TRAIN] Iter: 983800 Loss: 0.02635730803012848  PSNR: 18.89158058166504
[TRAIN] Iter: 983900 Loss: 0.023375604301691055  PSNR: 19.766101837158203
[TRAIN] Iter: 984000 Loss: 0.031180907040834427  PSNR: 18.282901763916016
[TRAIN] Iter: 984100 Loss: 0.025644809007644653  PSNR: 19.064533233642578
[TRAIN] Iter: 984200 Loss: 0.022555964067578316  PSNR: 19.625621795654297
[TRAIN] Iter: 984300 Loss: 0.02578883431851864  PSNR: 18.992412567138672
[TRAIN] Iter: 984400 Loss: 0.023595619946718216  PSNR: 19.439462661743164
[TRAIN] Iter: 984500 Loss: 0.029710160568356514  PSNR: 18.360973358154297
[TRAIN] Iter: 984600 Loss: 0.021789856255054474  PSNR: 19.736459732055664
[TRAIN] Iter: 984700 Loss: 0.028920680284500122  PSNR: 18.516925811767578
[TRAIN] Iter: 984800 Loss: 0.02302028052508831  PSNR: 19.808725357055664
[TRAIN] Iter: 984900 Loss: 0.025543758645653725  PSNR: 18.73793601989746
[TRAIN] Iter: 985000 Loss: 0.023524921387434006  PSNR: 19.641218185424805
[TRAIN] Iter: 985100 Loss: 0.0279654860496521  PSNR: 18.75257110595703
[TRAIN] Iter: 985200 Loss: 0.02589179202914238  PSNR: 19.052181243896484
[TRAIN] Iter: 985300 Loss: 0.02473512291908264  PSNR: 19.472623825073242
[TRAIN] Iter: 985400 Loss: 0.03093237429857254  PSNR: 18.22248649597168
[TRAIN] Iter: 985500 Loss: 0.023162372410297394  PSNR: 19.350656509399414
[TRAIN] Iter: 985600 Loss: 0.028149444609880447  PSNR: 18.80060386657715
[TRAIN] Iter: 985700 Loss: 0.02887326292693615  PSNR: 18.561569213867188
[TRAIN] Iter: 985800 Loss: 0.02212347835302353  PSNR: 19.739627838134766
[TRAIN] Iter: 985900 Loss: 0.023378316313028336  PSNR: 19.525663375854492
[TRAIN] Iter: 986000 Loss: 0.024024760350584984  PSNR: 19.46323013305664
[TRAIN] Iter: 986100 Loss: 0.026901021599769592  PSNR: 18.764036178588867
[TRAIN] Iter: 986200 Loss: 0.030260872095823288  PSNR: 18.372499465942383
[TRAIN] Iter: 986300 Loss: 0.029104186221957207  PSNR: 18.49683952331543
[TRAIN] Iter: 986400 Loss: 0.024878263473510742  PSNR: 19.238306045532227
[TRAIN] Iter: 986500 Loss: 0.030094699934124947  PSNR: 18.405263900756836
[TRAIN] Iter: 986600 Loss: 0.026712652295827866  PSNR: 18.962095260620117
[TRAIN] Iter: 986700 Loss: 0.025296881794929504  PSNR: 19.267770767211914
[TRAIN] Iter: 986800 Loss: 0.021282829344272614  PSNR: 19.946704864501953
[TRAIN] Iter: 986900 Loss: 0.025548085570335388  PSNR: 19.634401321411133
[TRAIN] Iter: 987000 Loss: 0.03082144632935524  PSNR: 18.282642364501953
[TRAIN] Iter: 987100 Loss: 0.028959080576896667  PSNR: 18.40877342224121
[TRAIN] Iter: 987200 Loss: 0.021233417093753815  PSNR: 19.997940063476562
[TRAIN] Iter: 987300 Loss: 0.02425246126949787  PSNR: 18.991968154907227
[TRAIN] Iter: 987400 Loss: 0.02019910141825676  PSNR: 20.057193756103516
[TRAIN] Iter: 987500 Loss: 0.02401949092745781  PSNR: 19.36703109741211
[TRAIN] Iter: 987600 Loss: 0.026100274175405502  PSNR: 18.812232971191406
[TRAIN] Iter: 987700 Loss: 0.03158213198184967  PSNR: 18.15717887878418
[TRAIN] Iter: 987800 Loss: 0.024063657969236374  PSNR: 19.141382217407227
[TRAIN] Iter: 987900 Loss: 0.02756926417350769  PSNR: 18.794971466064453
[TRAIN] Iter: 988000 Loss: 0.028475403785705566  PSNR: 18.576904296875
[TRAIN] Iter: 988100 Loss: 0.028547266498208046  PSNR: 18.490055084228516
[TRAIN] Iter: 988200 Loss: 0.01952403597533703  PSNR: 20.281028747558594
[TRAIN] Iter: 988300 Loss: 0.02404686063528061  PSNR: 19.31001853942871
[TRAIN] Iter: 988400 Loss: 0.03189236670732498  PSNR: 18.52669334411621
[TRAIN] Iter: 988500 Loss: 0.031233783811330795  PSNR: 18.11576271057129
[TRAIN] Iter: 988600 Loss: 0.02095809206366539  PSNR: 19.953033447265625
[TRAIN] Iter: 988700 Loss: 0.0310046449303627  PSNR: 18.241008758544922
[TRAIN] Iter: 988800 Loss: 0.031686410307884216  PSNR: 18.13627815246582
[TRAIN] Iter: 988900 Loss: 0.02500273659825325  PSNR: 19.07401466369629
[TRAIN] Iter: 989000 Loss: 0.030081525444984436  PSNR: 18.253442764282227
[TRAIN] Iter: 989100 Loss: 0.02662741206586361  PSNR: 18.79286766052246
[TRAIN] Iter: 989200 Loss: 0.029483119025826454  PSNR: 18.483715057373047
[TRAIN] Iter: 989300 Loss: 0.020540310069918633  PSNR: 20.005325317382812
[TRAIN] Iter: 989400 Loss: 0.027169806882739067  PSNR: 19.080907821655273
[TRAIN] Iter: 989500 Loss: 0.028467979282140732  PSNR: 18.409194946289062
[TRAIN] Iter: 989600 Loss: 0.024228299036622047  PSNR: 19.48697280883789
[TRAIN] Iter: 989700 Loss: 0.027579722926020622  PSNR: 18.8026123046875
[TRAIN] Iter: 989800 Loss: 0.024006739258766174  PSNR: 19.38421058654785
[TRAIN] Iter: 989900 Loss: 0.026000266894698143  PSNR: 18.80881690979004
Saved checkpoints at ./logs/TUT-LAB-nerf/990000.tar
[TRAIN] Iter: 990000 Loss: 0.02732308954000473  PSNR: 19.077743530273438
[TRAIN] Iter: 990100 Loss: 0.03326612338423729  PSNR: 17.945674896240234
[TRAIN] Iter: 990200 Loss: 0.02444060519337654  PSNR: 19.30095672607422
[TRAIN] Iter: 990300 Loss: 0.023083247244358063  PSNR: 19.641727447509766
[TRAIN] Iter: 990400 Loss: 0.025917014107108116  PSNR: 19.06139373779297
[TRAIN] Iter: 990500 Loss: 0.023694084957242012  PSNR: 19.53187370300293
[TRAIN] Iter: 990600 Loss: 0.027243269607424736  PSNR: 19.038841247558594
[TRAIN] Iter: 990700 Loss: 0.027970483526587486  PSNR: 18.59364891052246
[TRAIN] Iter: 990800 Loss: 0.02588306926190853  PSNR: 19.255863189697266
[TRAIN] Iter: 990900 Loss: 0.03180459514260292  PSNR: 18.139816284179688
[TRAIN] Iter: 991000 Loss: 0.02234480157494545  PSNR: 19.85320472717285
[TRAIN] Iter: 991100 Loss: 0.029504556208848953  PSNR: 18.41343879699707
[TRAIN] Iter: 991200 Loss: 0.0293741375207901  PSNR: 18.363296508789062
[TRAIN] Iter: 991300 Loss: 0.025307616218924522  PSNR: 19.12295913696289
[TRAIN] Iter: 991400 Loss: 0.028914671391248703  PSNR: 18.811941146850586
[TRAIN] Iter: 991500 Loss: 0.03054550290107727  PSNR: 18.361970901489258
[TRAIN] Iter: 991600 Loss: 0.03214089199900627  PSNR: 18.228479385375977
[TRAIN] Iter: 991700 Loss: 0.02757936716079712  PSNR: 18.797571182250977
[TRAIN] Iter: 991800 Loss: 0.02561260014772415  PSNR: 19.036226272583008
[TRAIN] Iter: 991900 Loss: 0.02164022997021675  PSNR: 19.344257354736328
[TRAIN] Iter: 992000 Loss: 0.021942630410194397  PSNR: 19.66448211669922
[TRAIN] Iter: 992100 Loss: 0.026379438117146492  PSNR: 19.20647621154785
[TRAIN] Iter: 992200 Loss: 0.024995187297463417  PSNR: 19.26158905029297
[TRAIN] Iter: 992300 Loss: 0.026428602635860443  PSNR: 18.91122055053711
[TRAIN] Iter: 992400 Loss: 0.023353269323706627  PSNR: 19.57172966003418
[TRAIN] Iter: 992500 Loss: 0.030547678470611572  PSNR: 18.460796356201172
[TRAIN] Iter: 992600 Loss: 0.02678067982196808  PSNR: 18.909467697143555
[TRAIN] Iter: 992700 Loss: 0.029310090467333794  PSNR: 18.508609771728516
[TRAIN] Iter: 992800 Loss: 0.026385635137557983  PSNR: 18.861385345458984
[TRAIN] Iter: 992900 Loss: 0.02677731029689312  PSNR: 18.70344352722168
[TRAIN] Iter: 993000 Loss: 0.029526829719543457  PSNR: 18.76800537109375
[TRAIN] Iter: 993100 Loss: 0.026806674897670746  PSNR: 18.873598098754883
[TRAIN] Iter: 993200 Loss: 0.024856524541974068  PSNR: 19.093772888183594
[TRAIN] Iter: 993300 Loss: 0.028751682490110397  PSNR: 18.61806297302246
[TRAIN] Iter: 993400 Loss: 0.02154185064136982  PSNR: 19.695173263549805
[TRAIN] Iter: 993500 Loss: 0.02743101865053177  PSNR: 18.82848358154297
[TRAIN] Iter: 993600 Loss: 0.024462847039103508  PSNR: 19.437503814697266
[TRAIN] Iter: 993700 Loss: 0.022889932617545128  PSNR: 19.775943756103516
[TRAIN] Iter: 993800 Loss: 0.02602153643965721  PSNR: 19.15814971923828
[TRAIN] Iter: 993900 Loss: 0.0286627858877182  PSNR: 18.509092330932617
[TRAIN] Iter: 994000 Loss: 0.02496621012687683  PSNR: 19.2895565032959
[TRAIN] Iter: 994100 Loss: 0.029738308861851692  PSNR: 18.369140625
[TRAIN] Iter: 994200 Loss: 0.02997656911611557  PSNR: 18.738433837890625
[TRAIN] Iter: 994300 Loss: 0.03366445377469063  PSNR: 17.861854553222656
[TRAIN] Iter: 994400 Loss: 0.019134528934955597  PSNR: 20.261659622192383
[TRAIN] Iter: 994500 Loss: 0.02206498570740223  PSNR: 19.764371871948242
[TRAIN] Iter: 994600 Loss: 0.025865774601697922  PSNR: 18.961519241333008
[TRAIN] Iter: 994700 Loss: 0.025870785117149353  PSNR: 19.36178207397461
[TRAIN] Iter: 994800 Loss: 0.023822765797376633  PSNR: 19.380931854248047
[TRAIN] Iter: 994900 Loss: 0.025202523916959763  PSNR: 19.238632202148438
[TRAIN] Iter: 995000 Loss: 0.019980015233159065  PSNR: 20.099273681640625
[TRAIN] Iter: 995100 Loss: 0.02886030822992325  PSNR: 18.547494888305664
[TRAIN] Iter: 995200 Loss: 0.02680203877389431  PSNR: 18.89023780822754
[TRAIN] Iter: 995300 Loss: 0.0314665250480175  PSNR: 18.167104721069336
[TRAIN] Iter: 995400 Loss: 0.021222881972789764  PSNR: 20.015377044677734
[TRAIN] Iter: 995500 Loss: 0.030223730951547623  PSNR: 18.19081687927246
[TRAIN] Iter: 995600 Loss: 0.030446765944361687  PSNR: 18.39761734008789
[TRAIN] Iter: 995700 Loss: 0.023396804928779602  PSNR: 19.576038360595703
[TRAIN] Iter: 995800 Loss: 0.021078385412693024  PSNR: 20.265432357788086
[TRAIN] Iter: 995900 Loss: 0.0273221917450428  PSNR: 19.047849655151367
[TRAIN] Iter: 996000 Loss: 0.02388543263077736  PSNR: 19.506553649902344
[TRAIN] Iter: 996100 Loss: 0.024544622749090195  PSNR: 18.9534912109375
[TRAIN] Iter: 996200 Loss: 0.023277055472135544  PSNR: 19.783082962036133
[TRAIN] Iter: 996300 Loss: 0.031247396022081375  PSNR: 18.270544052124023
[TRAIN] Iter: 996400 Loss: 0.023448113352060318  PSNR: 19.283084869384766
[TRAIN] Iter: 996500 Loss: 0.020963340997695923  PSNR: 19.912927627563477
[TRAIN] Iter: 996600 Loss: 0.034939929842948914  PSNR: 17.81158447265625
[TRAIN] Iter: 996700 Loss: 0.03234165906906128  PSNR: 18.014450073242188
[TRAIN] Iter: 996800 Loss: 0.023724764585494995  PSNR: 19.498855590820312
[TRAIN] Iter: 996900 Loss: 0.025756385177373886  PSNR: 19.02538299560547
[TRAIN] Iter: 997000 Loss: 0.025831207633018494  PSNR: 18.880258560180664
[TRAIN] Iter: 997100 Loss: 0.02396383136510849  PSNR: 19.227548599243164
[TRAIN] Iter: 997200 Loss: 0.022574210539460182  PSNR: 19.548864364624023
[TRAIN] Iter: 997300 Loss: 0.03375231474637985  PSNR: 17.885005950927734
[TRAIN] Iter: 997400 Loss: 0.02988128736615181  PSNR: 18.487167358398438
[TRAIN] Iter: 997500 Loss: 0.029330886900424957  PSNR: 18.7308349609375
[TRAIN] Iter: 997600 Loss: 0.020109307020902634  PSNR: 20.1065673828125
[TRAIN] Iter: 997700 Loss: 0.027802495285868645  PSNR: 18.75061798095703
[TRAIN] Iter: 997800 Loss: 0.029023990035057068  PSNR: 18.54237174987793
[TRAIN] Iter: 997900 Loss: 0.027969611808657646  PSNR: 18.672744750976562
[TRAIN] Iter: 998000 Loss: 0.024334773421287537  PSNR: 19.2949275970459
[TRAIN] Iter: 998100 Loss: 0.026660211384296417  PSNR: 18.61387062072754
[TRAIN] Iter: 998200 Loss: 0.030804067850112915  PSNR: 18.38521385192871
[TRAIN] Iter: 998300 Loss: 0.021566709503531456  PSNR: 19.74338150024414
[TRAIN] Iter: 998400 Loss: 0.022780366241931915  PSNR: 19.63844871520996
[TRAIN] Iter: 998500 Loss: 0.027813225984573364  PSNR: 18.206018447875977
[TRAIN] Iter: 998600 Loss: 0.029991397634148598  PSNR: 18.458402633666992
[TRAIN] Iter: 998700 Loss: 0.026135489344596863  PSNR: 18.60966682434082
[TRAIN] Iter: 998800 Loss: 0.024260565638542175  PSNR: 19.262950897216797
[TRAIN] Iter: 998900 Loss: 0.018219975754618645  PSNR: 20.582292556762695
[TRAIN] Iter: 999000 Loss: 0.02698981761932373  PSNR: 18.992759704589844
[TRAIN] Iter: 999100 Loss: 0.02661409229040146  PSNR: 18.94818115234375
[TRAIN] Iter: 999200 Loss: 0.023349735885858536  PSNR: 19.542264938354492
[TRAIN] Iter: 999300 Loss: 0.026140332221984863  PSNR: 18.9664249420166
[TRAIN] Iter: 999400 Loss: 0.022503595799207687  PSNR: 19.61371421813965
[TRAIN] Iter: 999500 Loss: 0.026936158537864685  PSNR: 18.868915557861328
[TRAIN] Iter: 999600 Loss: 0.024041127413511276  PSNR: 19.523393630981445
[TRAIN] Iter: 999700 Loss: 0.029743660241365433  PSNR: 18.40663719177246
[TRAIN] Iter: 999800 Loss: 0.03127344697713852  PSNR: 18.25065040588379
[TRAIN] Iter: 999900 Loss: 0.026717599481344223  PSNR: 19.12588119506836
Saved checkpoints at ./logs/TUT-LAB-nerf/1000000.tar
0 0.0004315376281738281
torch.Size([320, 640, 3]) torch.Size([320, 640])
1 7.673429250717163
2 5.3907310962677
3 5.403008222579956
4 5.3657872676849365
5 5.428807735443115
6 5.426980972290039
7 7.695241212844849
8 5.391324281692505
9 5.345061540603638
10 5.357115268707275
11 5.402908086776733
12 5.463951349258423
13 7.6954896450042725
14 5.3980231285095215
15 5.352151870727539
16 5.360670328140259
17 5.4096879959106445
18 5.418464183807373
19 7.723249197006226
20 5.401552438735962
21 5.3542563915252686
22 5.372546672821045
23 5.415939569473267
24 5.421634674072266
25 7.674100637435913
26 5.434366464614868
27 5.350367069244385
28 5.383850812911987
29 5.4138970375061035
30 5.426029920578003
31 7.687485218048096
32 5.437713623046875
33 5.346895217895508
34 5.388345956802368
35 5.423222303390503
36 5.444260120391846
37 7.678732633590698
38 5.399501323699951
39 5.332996368408203
40 5.3761749267578125
41 5.408484220504761
42 5.428303003311157
43 7.70891809463501
44 5.440242528915405
45 5.22735857963562
46 5.4366419315338135
47 5.429498910903931
48 5.445022344589233
49 7.73004674911499
50 5.383276462554932
51 5.26389479637146
52 5.434208631515503
53 5.433272838592529
54 5.450307130813599
55 7.735793828964233
56 5.388416051864624
57 5.186572313308716
58 5.464257717132568
59 5.439483404159546
60 5.4577014446258545
61 7.764237642288208
62 5.39350962638855
63 5.1875574588775635
64 5.3660454750061035
65 5.401867628097534
66 5.440648555755615
67 7.978064298629761
68 5.404343366622925
69 5.206446647644043
70 5.248763561248779
71 5.4304563999176025
72 5.4479146003723145
73 7.957812070846558
74 5.37844443321228
75 5.185925006866455
76 5.2318525314331055
77 5.3335301876068115
78 5.506154298782349
79 8.157928943634033
80 5.409682989120483
81 5.193104982376099
82 5.185240983963013
83 5.355262041091919
84 5.458927154541016
85 8.10685133934021
86 5.505079507827759
87 5.418726921081543
88 5.196889877319336
89 5.319040536880493
90 7.524479866027832
91 5.494406700134277
92 5.298222064971924
93 5.203328371047974
94 5.2517478466033936
95 5.397525310516357
96 8.068563222885132
97 5.643855571746826
98 5.274061918258667
99 5.220236301422119
100 5.255211353302002
101 5.430051803588867
102 7.894703388214111
103 5.587692499160767
104 5.251291036605835
105 5.205117702484131
106 5.236699342727661
107 5.41463828086853
108 7.9687042236328125
109 5.433946132659912
110 5.375871419906616
111 5.353466033935547
112 5.333832263946533
113 5.399451971054077
114 7.75282883644104
115 5.48472261428833
116 5.291722297668457
117 5.279649972915649
118 5.338552951812744
119 5.428670167922974
Done, saving (120, 320, 640, 3) (120, 320, 640)
extras:{'raw': tensor([[[-9.3658e-01, -8.6189e-01, -1.2422e+00,  3.3814e+00],
         [-1.1528e+00, -8.3222e-01, -1.2978e+00,  6.2402e+00],
         [-1.0878e+00, -7.7404e-01, -1.2346e+00,  6.2030e+00],
         ...,
         [ 1.6206e+00, -1.2953e+00, -2.6014e+00,  3.1507e+02],
         [-5.3677e-01, -3.1746e+00, -4.2772e+00,  3.3305e+02],
         [-2.2298e+00, -4.6574e+00, -6.2325e+00,  3.3209e+02]],

        [[ 2.2256e-01, -4.9332e-02, -1.7025e-01, -7.2352e-01],
         [ 1.0322e-01, -7.7080e-02, -6.8065e-02,  1.7695e+00],
         [ 5.3492e-02, -1.2087e-01, -1.1736e-01,  3.9344e+00],
         ...,
         [ 3.0022e+01,  2.5497e+01,  2.3399e+01,  3.2063e+02],
         [ 3.0072e+01,  2.5317e+01,  2.2696e+01,  3.0469e+02],
         [ 2.9550e+01,  2.4667e+01,  2.1838e+01,  3.0024e+02]],

        [[-1.6320e+00, -1.3052e+00,  1.1746e-01, -1.5277e+01],
         [-3.8786e-01, -4.3547e-01,  6.6783e-02, -1.6402e+01],
         [-3.3057e+00, -2.4509e+00, -8.1356e-01, -1.1794e+01],
         ...,
         [-2.1638e+00, -1.2678e+00, -7.5311e-03, -1.9247e+01],
         [-7.3504e+00, -4.6680e+00, -6.2012e-01, -1.0873e+01],
         [-1.1178e+01, -7.5117e+00, -2.2749e+00, -1.6158e+01]],

        ...,

        [[ 1.3431e-01, -1.1297e-01, -2.6312e-01, -4.9058e+00],
         [-1.7723e-01, -4.2601e-01, -6.8491e-01, -4.7701e+00],
         [ 2.2226e-02, -2.1685e-01, -4.5317e-01, -3.8076e+00],
         ...,
         [ 3.6420e+00,  2.4700e+00, -4.0364e-01,  3.3960e+02],
         [ 4.1507e+00,  2.9264e+00, -2.2987e-02,  3.2440e+02],
         [ 4.2558e+00,  2.9483e+00, -9.2228e-02,  3.2358e+02]],

        [[ 5.8891e-01,  3.4351e-01,  1.6960e-01, -8.8063e+00],
         [ 4.8625e-01,  3.0016e-01,  2.3349e-01, -5.3501e+00],
         [ 3.3161e-01,  6.2796e-02, -2.2163e-01,  4.7059e+00],
         ...,
         [ 2.3171e+01,  1.9133e+01,  1.8650e+01,  1.8481e+02],
         [ 2.8383e+01,  2.4512e+01,  2.5422e+01,  1.7164e+02],
         [ 2.7038e+01,  2.2718e+01,  2.2583e+01,  1.6535e+02]],

        [[-2.0410e-01, -4.3286e-01, -1.9375e-01, -1.3658e+01],
         [-1.9893e+00, -1.4076e+00, -1.6737e-01, -2.1451e+01],
         [-1.0380e+00, -7.6988e-01,  2.5927e-01, -2.3639e+01],
         ...,
         [-2.0423e+00, -1.5344e+00, -4.0191e-01,  1.0966e+01],
         [-1.1616e+00, -1.0000e+00, -3.8364e-02,  1.2394e+01],
         [-1.5174e+00, -1.5255e+00, -8.4357e-01,  5.6680e+00]]],
       grad_fn=<ReshapeAliasBackward0>), 'rgb0': tensor([[0.2374, 0.2786, 0.1686],
        [0.5586, 0.5038, 0.4826],
        [0.1684, 0.2512, 0.5000],
        ...,
        [0.5569, 0.5010, 0.4710],
        [0.5358, 0.4606, 0.3789],
        [0.1170, 0.2339, 0.6385]], grad_fn=<ReshapeAliasBackward0>), 'disp0': tensor([741.2204,  93.0935,  38.1571,  ..., 228.8897,  62.8105,  44.1558],
       grad_fn=<ReshapeAliasBackward0>), 'acc0': tensor([1., 1., 1.,  ..., 1., 1., 1.], grad_fn=<ReshapeAliasBackward0>), 'z_std': tensor([0.2477, 0.0024, 0.0029,  ..., 0.0112, 0.0024, 0.0222])}
0 0.00046133995056152344
torch.Size([320, 640, 3]) torch.Size([320, 640])
1 5.547932863235474
2 5.202432870864868
3 5.210435390472412
4 5.323070049285889
5 5.441838026046753
6 8.04819917678833
7 5.447619915008545
8 5.314724683761597
9 5.227752685546875
10 5.3235437870025635
11 5.447155952453613
12 7.922595739364624
13 5.436688661575317
14 5.26594352722168
15 5.255880832672119
16 5.353276014328003
17 5.449902534484863
18 7.943464517593384
19 5.366051912307739
20 5.210268974304199
21 5.401921987533569
22 5.449296236038208
23 5.437633037567139
24 7.782609224319458
25 5.399792194366455
26 5.279505252838135
27 5.413219213485718
28 5.44866418838501
29 5.403401613235474
30 7.68586540222168
31 5.419130325317383
32 5.417105197906494
33 5.388446092605591
34 5.395954370498657
35 5.40450644493103
36 7.6531243324279785
37 5.397086143493652
38 5.39984393119812
39 5.37566065788269
40 5.427157640457153
41 5.403499603271484
42 7.679713010787964
43 5.404207229614258
44 5.403339147567749
45 5.3782360553741455
46 5.3830885887146
47 5.433766603469849
48 7.671475887298584
49 5.407184839248657
50 5.404952526092529
51 5.3833394050598145
52 5.3866918087005615
53 5.386882781982422
54 7.698929309844971
55 5.412616491317749
56 5.402100324630737
57 5.383563280105591
58 5.38998007774353
59 5.384312629699707
60 7.709468841552734
61 5.420737028121948
62 5.409782648086548
63 5.394177436828613
64 5.404020071029663
65 5.405127286911011
66 7.658193588256836
67 5.405670642852783
68 5.395139932632446
69 5.377289772033691
70 5.395384073257446
71 5.401501178741455
72 7.698096990585327
73 5.412014722824097
74 5.399471998214722
75 5.381954193115234
76 5.418785572052002
77 5.430522680282593
78 7.626483917236328
79 5.517228126525879
80 5.439302444458008
81 5.440454006195068
82 5.419808864593506
83 5.419186592102051
84 7.47257924079895
85 5.400487422943115
86 5.444819450378418
87 5.399933099746704
88 5.410527229309082
89 5.42548394203186
90 7.58238959312439
91 5.395775318145752
92 5.419257640838623
93 5.3964760303497314
94 5.429936647415161
95 5.457164525985718
96 7.621405839920044
97 5.400618076324463
98 5.295670986175537
99 5.314483165740967
100 5.3961098194122314
101 5.441238880157471
102 7.8540472984313965
103 5.4385597705841064
104 5.257372856140137
105 5.256096601486206
106 5.335869312286377
107 5.495008707046509
108 7.944321155548096
109 5.461858034133911
110 5.347148895263672
111 5.430180072784424
112 5.439789295196533
113 5.42487907409668
114 7.500333070755005
115 5.405501127243042
116 5.363044500350952
117 5.37773871421814
118 5.442290782928467
119 5.429061412811279
test poses shape torch.Size([13, 3, 4])
0 0.0006537437438964844
torch.Size([320, 640, 3]) torch.Size([320, 640])
1 5.439008951187134
2 5.393847465515137
3 5.424932241439819
4 5.487244606018066
5 7.7319560050964355
6 5.4569408893585205
7 5.437296152114868
8 5.388032674789429
9 5.4632697105407715
10 5.475162982940674
11 7.714806079864502
12 5.448329925537109
Saved test set
[TRAIN] Iter: 1000000 Loss: 0.028668463230133057  PSNR: 18.72142791748047
