13
13
13
13
13
13
13
(7, 3, 4)
Loaded 360 (7, 320, 640, 3) (120, 3, 5) [-0.10562767  0.00531578 -0.21419485] ./data/indoor/KE101
[0 1 2]
DEFINING BOUNDS
NEAR FAR 0.0 1.0
hwf [-0.10562767  0.00531578 -0.21419485]
Found ckpts ['./logs/TUT-KE101-nerf/010000.tar', './logs/TUT-KE101-nerf/020000.tar']
Reloading from ./logs/TUT-KE101-nerf/020000.tar
Not ndc!
Begin
TRAIN views are [0 1 2]
TEST views are [3 4 5 6]
VAL views are [3 4 5 6]
Saved checkpoints at ./logs/TUT-KE101-nerf/020000.tar
[TRAIN] Iter: 20000 Loss: 0.016440344974398613  PSNR: 21.72865867614746
[TRAIN] Iter: 20100 Loss: 0.02088078483939171  PSNR: 20.598325729370117
[TRAIN] Iter: 20200 Loss: 0.018006226047873497  PSNR: 21.506664276123047
[TRAIN] Iter: 20300 Loss: 0.01716645061969757  PSNR: 21.518640518188477
[TRAIN] Iter: 20400 Loss: 0.01637513004243374  PSNR: 21.806371688842773
[TRAIN] Iter: 20500 Loss: 0.015269572846591473  PSNR: 22.176021575927734
[TRAIN] Iter: 20600 Loss: 0.01619908958673477  PSNR: 21.739957809448242
[TRAIN] Iter: 20700 Loss: 0.016820479184389114  PSNR: 21.439987182617188
[TRAIN] Iter: 20800 Loss: 0.01800525188446045  PSNR: 21.449649810791016
[TRAIN] Iter: 20900 Loss: 0.015173882246017456  PSNR: 22.146318435668945
[TRAIN] Iter: 21000 Loss: 0.011979008093476295  PSNR: 23.393558502197266
[TRAIN] Iter: 21100 Loss: 0.014617782086133957  PSNR: 22.129150390625
[TRAIN] Iter: 21200 Loss: 0.012530393898487091  PSNR: 23.098907470703125
[TRAIN] Iter: 21300 Loss: 0.01529272273182869  PSNR: 22.196632385253906
[TRAIN] Iter: 21400 Loss: 0.014135336503386497  PSNR: 22.54163360595703
[TRAIN] Iter: 21500 Loss: 0.015085913240909576  PSNR: 22.16036033630371
[TRAIN] Iter: 21600 Loss: 0.01564566045999527  PSNR: 22.144912719726562
[TRAIN] Iter: 21700 Loss: 0.014798473566770554  PSNR: 22.088001251220703
[TRAIN] Iter: 21800 Loss: 0.015038232319056988  PSNR: 22.03101921081543
[TRAIN] Iter: 21900 Loss: 0.015261019580066204  PSNR: 22.25468635559082
[TRAIN] Iter: 22000 Loss: 0.017608018592000008  PSNR: 21.265209197998047
[TRAIN] Iter: 22100 Loss: 0.014613992534577847  PSNR: 21.998750686645508
[TRAIN] Iter: 22200 Loss: 0.013078817166388035  PSNR: 22.935142517089844
[TRAIN] Iter: 22300 Loss: 0.019746899604797363  PSNR: 20.902902603149414
[TRAIN] Iter: 22400 Loss: 0.01691565103828907  PSNR: 21.49741554260254
[TRAIN] Iter: 22500 Loss: 0.013051353394985199  PSNR: 22.894420623779297
[TRAIN] Iter: 22600 Loss: 0.01624308153986931  PSNR: 21.5736026763916
[TRAIN] Iter: 22700 Loss: 0.013511128723621368  PSNR: 22.84207534790039
[TRAIN] Iter: 22800 Loss: 0.01084466278553009  PSNR: 23.617080688476562
[TRAIN] Iter: 22900 Loss: 0.01617855578660965  PSNR: 22.108264923095703
[TRAIN] Iter: 23000 Loss: 0.0143261244520545  PSNR: 22.577882766723633
[TRAIN] Iter: 23100 Loss: 0.011764737777411938  PSNR: 23.447174072265625
[TRAIN] Iter: 23200 Loss: 0.016040287911891937  PSNR: 22.15526008605957
[TRAIN] Iter: 23300 Loss: 0.012760598212480545  PSNR: 23.00018310546875
[TRAIN] Iter: 23400 Loss: 0.01588558405637741  PSNR: 22.059200286865234
[TRAIN] Iter: 23500 Loss: 0.012172447517514229  PSNR: 23.1328067779541
[TRAIN] Iter: 23600 Loss: 0.01450659055262804  PSNR: 22.250179290771484
[TRAIN] Iter: 23700 Loss: 0.015672456473112106  PSNR: 21.678953170776367
[TRAIN] Iter: 23800 Loss: 0.011153528466820717  PSNR: 23.552509307861328
[TRAIN] Iter: 23900 Loss: 0.013806733302772045  PSNR: 22.314075469970703
[TRAIN] Iter: 24000 Loss: 0.019762428477406502  PSNR: 21.020992279052734
[TRAIN] Iter: 24100 Loss: 0.013995610177516937  PSNR: 22.364032745361328
[TRAIN] Iter: 24200 Loss: 0.016804037615656853  PSNR: 21.297348022460938
[TRAIN] Iter: 24300 Loss: 0.01477486826479435  PSNR: 22.238880157470703
[TRAIN] Iter: 24400 Loss: 0.015995047986507416  PSNR: 21.93956756591797
[TRAIN] Iter: 24500 Loss: 0.01473955623805523  PSNR: 21.96419906616211
[TRAIN] Iter: 24600 Loss: 0.012405900284647942  PSNR: 23.45034408569336
[TRAIN] Iter: 24700 Loss: 0.01660006307065487  PSNR: 21.766321182250977
[TRAIN] Iter: 24800 Loss: 0.013103374280035496  PSNR: 23.0734920501709
[TRAIN] Iter: 24900 Loss: 0.011026940308511257  PSNR: 24.233829498291016
[TRAIN] Iter: 25000 Loss: 0.015235600993037224  PSNR: 21.84868049621582
[TRAIN] Iter: 25100 Loss: 0.01369292102754116  PSNR: 22.760866165161133
[TRAIN] Iter: 25200 Loss: 0.01852274313569069  PSNR: 21.514169692993164
[TRAIN] Iter: 25300 Loss: 0.014971485361456871  PSNR: 21.934419631958008
[TRAIN] Iter: 25400 Loss: 0.014772727154195309  PSNR: 22.237796783447266
[TRAIN] Iter: 25500 Loss: 0.015242848545312881  PSNR: 21.97128677368164
[TRAIN] Iter: 25600 Loss: 0.014307855628430843  PSNR: 22.55198860168457
[TRAIN] Iter: 25700 Loss: 0.013503273017704487  PSNR: 22.585308074951172
[TRAIN] Iter: 25800 Loss: 0.012307691387832165  PSNR: 22.914207458496094
[TRAIN] Iter: 25900 Loss: 0.01761489547789097  PSNR: 21.391101837158203
[TRAIN] Iter: 26000 Loss: 0.016508078202605247  PSNR: 22.032453536987305
[TRAIN] Iter: 26100 Loss: 0.013667444698512554  PSNR: 23.45090675354004
[TRAIN] Iter: 26200 Loss: 0.013161963783204556  PSNR: 22.716590881347656
[TRAIN] Iter: 26300 Loss: 0.012689342722296715  PSNR: 23.277637481689453
[TRAIN] Iter: 26400 Loss: 0.01263811718672514  PSNR: 22.914085388183594
[TRAIN] Iter: 26500 Loss: 0.01302999909967184  PSNR: 22.80470848083496
[TRAIN] Iter: 26600 Loss: 0.010606788098812103  PSNR: 23.430219650268555
[TRAIN] Iter: 26700 Loss: 0.010315001010894775  PSNR: 23.77459716796875
[TRAIN] Iter: 26800 Loss: 0.014164101332426071  PSNR: 22.327083587646484
[TRAIN] Iter: 26900 Loss: 0.013998606242239475  PSNR: 22.787628173828125
[TRAIN] Iter: 27000 Loss: 0.015258366242051125  PSNR: 22.03765869140625
[TRAIN] Iter: 27100 Loss: 0.012678127735853195  PSNR: 23.029958724975586
[TRAIN] Iter: 27200 Loss: 0.012395055964589119  PSNR: 23.416568756103516
[TRAIN] Iter: 27300 Loss: 0.014214267954230309  PSNR: 22.387889862060547
[TRAIN] Iter: 27400 Loss: 0.01749526709318161  PSNR: 21.52067756652832
[TRAIN] Iter: 27500 Loss: 0.013225064612925053  PSNR: 22.696277618408203
[TRAIN] Iter: 27600 Loss: 0.01222430169582367  PSNR: 23.969799041748047
[TRAIN] Iter: 27700 Loss: 0.013236463069915771  PSNR: 22.56676483154297
[TRAIN] Iter: 27800 Loss: 0.014666300266981125  PSNR: 22.12939453125
[TRAIN] Iter: 27900 Loss: 0.019365031272172928  PSNR: 21.01104736328125
[TRAIN] Iter: 28000 Loss: 0.0127406045794487  PSNR: 23.271055221557617
[TRAIN] Iter: 28100 Loss: 0.01294726599007845  PSNR: 22.90302848815918
[TRAIN] Iter: 28200 Loss: 0.01127673126757145  PSNR: 23.73325538635254
[TRAIN] Iter: 28300 Loss: 0.011326795443892479  PSNR: 23.604249954223633
[TRAIN] Iter: 28400 Loss: 0.011914504691958427  PSNR: 23.136802673339844
[TRAIN] Iter: 28500 Loss: 0.01470732782036066  PSNR: 22.134960174560547
[TRAIN] Iter: 28600 Loss: 0.014780954457819462  PSNR: 22.041255950927734
[TRAIN] Iter: 28700 Loss: 0.016848785802721977  PSNR: 21.52192497253418
[TRAIN] Iter: 28800 Loss: 0.01654217392206192  PSNR: 21.53656005859375
[TRAIN] Iter: 28900 Loss: 0.013685883954167366  PSNR: 22.86663818359375
[TRAIN] Iter: 29000 Loss: 0.018029149621725082  PSNR: 21.10051155090332
[TRAIN] Iter: 29100 Loss: 0.016348540782928467  PSNR: 21.615554809570312
[TRAIN] Iter: 29200 Loss: 0.012140986509621143  PSNR: 22.880939483642578
[TRAIN] Iter: 29300 Loss: 0.01505563035607338  PSNR: 22.26659393310547
[TRAIN] Iter: 29400 Loss: 0.011887593194842339  PSNR: 23.35474967956543
[TRAIN] Iter: 29500 Loss: 0.014172853901982307  PSNR: 22.67801284790039
[TRAIN] Iter: 29600 Loss: 0.011577248573303223  PSNR: 23.197158813476562
[TRAIN] Iter: 29700 Loss: 0.014575622975826263  PSNR: 22.207361221313477
[TRAIN] Iter: 29800 Loss: 0.01793190836906433  PSNR: 21.25984001159668
[TRAIN] Iter: 29900 Loss: 0.00964248925447464  PSNR: 24.39280128479004
Saved checkpoints at ./logs/TUT-KE101-nerf/030000.tar
[TRAIN] Iter: 30000 Loss: 0.015309443697333336  PSNR: 22.169851303100586
[TRAIN] Iter: 30100 Loss: 0.010697366669774055  PSNR: 23.5163516998291
[TRAIN] Iter: 30200 Loss: 0.013910441659390926  PSNR: 22.756547927856445
[TRAIN] Iter: 30300 Loss: 0.012129310518503189  PSNR: 22.8682861328125
[TRAIN] Iter: 30400 Loss: 0.011496253311634064  PSNR: 23.875558853149414
[TRAIN] Iter: 30500 Loss: 0.00951667595654726  PSNR: 25.00208282470703
[TRAIN] Iter: 30600 Loss: 0.013808025047183037  PSNR: 22.584087371826172
[TRAIN] Iter: 30700 Loss: 0.01604723557829857  PSNR: 21.721420288085938
[TRAIN] Iter: 30800 Loss: 0.01503419503569603  PSNR: 22.325477600097656
[TRAIN] Iter: 30900 Loss: 0.012864360585808754  PSNR: 22.65778350830078
[TRAIN] Iter: 31000 Loss: 0.013883840292692184  PSNR: 22.5833797454834
[TRAIN] Iter: 31100 Loss: 0.011047986336052418  PSNR: 24.504573822021484
[TRAIN] Iter: 31200 Loss: 0.010831535793840885  PSNR: 23.488920211791992
[TRAIN] Iter: 31300 Loss: 0.013045428320765495  PSNR: 22.872900009155273
[TRAIN] Iter: 31400 Loss: 0.012788396328687668  PSNR: 23.041423797607422
[TRAIN] Iter: 31500 Loss: 0.014011419378221035  PSNR: 22.662006378173828
[TRAIN] Iter: 31600 Loss: 0.012246683239936829  PSNR: 23.24737548828125
[TRAIN] Iter: 31700 Loss: 0.009821750223636627  PSNR: 24.67039680480957
[TRAIN] Iter: 31800 Loss: 0.015273066237568855  PSNR: 22.02839469909668
[TRAIN] Iter: 31900 Loss: 0.012440921738743782  PSNR: 22.85964012145996
[TRAIN] Iter: 32000 Loss: 0.014474895782768726  PSNR: 22.588638305664062
[TRAIN] Iter: 32100 Loss: 0.011524022556841373  PSNR: 23.590007781982422
[TRAIN] Iter: 32200 Loss: 0.01349475048482418  PSNR: 22.814088821411133
[TRAIN] Iter: 32300 Loss: 0.011333740316331387  PSNR: 23.61701774597168
[TRAIN] Iter: 32400 Loss: 0.013388333842158318  PSNR: 22.84107208251953
[TRAIN] Iter: 32500 Loss: 0.011787628754973412  PSNR: 23.371763229370117
[TRAIN] Iter: 32600 Loss: 0.012834187597036362  PSNR: 23.008411407470703
[TRAIN] Iter: 32700 Loss: 0.012415547855198383  PSNR: 23.561660766601562
[TRAIN] Iter: 32800 Loss: 0.010485292412340641  PSNR: 23.92720603942871
[TRAIN] Iter: 32900 Loss: 0.011846312321722507  PSNR: 23.419944763183594
[TRAIN] Iter: 33000 Loss: 0.014836340211331844  PSNR: 22.168834686279297
[TRAIN] Iter: 33100 Loss: 0.010119657963514328  PSNR: 24.304367065429688
[TRAIN] Iter: 33200 Loss: 0.012632200494408607  PSNR: 23.01972198486328
[TRAIN] Iter: 33300 Loss: 0.01571858860552311  PSNR: 21.91059684753418
[TRAIN] Iter: 33400 Loss: 0.014160742051899433  PSNR: 22.536941528320312
[TRAIN] Iter: 33500 Loss: 0.01271837018430233  PSNR: 23.1424560546875
[TRAIN] Iter: 33600 Loss: 0.01279900036752224  PSNR: 22.984886169433594
[TRAIN] Iter: 33700 Loss: 0.013854136690497398  PSNR: 22.532194137573242
[TRAIN] Iter: 33800 Loss: 0.012315034866333008  PSNR: 23.214061737060547
[TRAIN] Iter: 33900 Loss: 0.012569991871714592  PSNR: 23.278160095214844
[TRAIN] Iter: 34000 Loss: 0.010102680884301662  PSNR: 24.02253532409668
[TRAIN] Iter: 34100 Loss: 0.010639626532793045  PSNR: 24.03913688659668
[TRAIN] Iter: 34200 Loss: 0.008693069219589233  PSNR: 25.096694946289062
[TRAIN] Iter: 34300 Loss: 0.014885826036334038  PSNR: 22.458271026611328
[TRAIN] Iter: 34400 Loss: 0.009874578565359116  PSNR: 24.247661590576172
[TRAIN] Iter: 34500 Loss: 0.013106055557727814  PSNR: 22.705856323242188
[TRAIN] Iter: 34600 Loss: 0.014370033517479897  PSNR: 22.407012939453125
[TRAIN] Iter: 34700 Loss: 0.011806621216237545  PSNR: 23.183425903320312
[TRAIN] Iter: 34800 Loss: 0.009992807172238827  PSNR: 24.659343719482422
[TRAIN] Iter: 34900 Loss: 0.017370615154504776  PSNR: 21.504072189331055
[TRAIN] Iter: 35000 Loss: 0.009896012023091316  PSNR: 24.452613830566406
[TRAIN] Iter: 35100 Loss: 0.015859849750995636  PSNR: 22.061233520507812
[TRAIN] Iter: 35200 Loss: 0.011430448852479458  PSNR: 23.446332931518555
[TRAIN] Iter: 35300 Loss: 0.015880312770605087  PSNR: 21.643949508666992
[TRAIN] Iter: 35400 Loss: 0.014489743858575821  PSNR: 22.169620513916016
[TRAIN] Iter: 35500 Loss: 0.011013641953468323  PSNR: 23.508169174194336
[TRAIN] Iter: 35600 Loss: 0.010765555314719677  PSNR: 23.61151123046875
[TRAIN] Iter: 35700 Loss: 0.014143573120236397  PSNR: 22.195236206054688
[TRAIN] Iter: 35800 Loss: 0.012735899537801743  PSNR: 22.848804473876953
[TRAIN] Iter: 35900 Loss: 0.01282541360706091  PSNR: 23.141733169555664
[TRAIN] Iter: 36000 Loss: 0.009519681334495544  PSNR: 24.34202766418457
[TRAIN] Iter: 36100 Loss: 0.00826633907854557  PSNR: 25.08110809326172
[TRAIN] Iter: 36200 Loss: 0.011909563094377518  PSNR: 23.519250869750977
[TRAIN] Iter: 36300 Loss: 0.013260219246149063  PSNR: 22.50193977355957
[TRAIN] Iter: 36400 Loss: 0.015669094398617744  PSNR: 21.713722229003906
[TRAIN] Iter: 36500 Loss: 0.010497065261006355  PSNR: 24.191692352294922
[TRAIN] Iter: 36600 Loss: 0.011795837432146072  PSNR: 23.5654296875
[TRAIN] Iter: 36700 Loss: 0.012724054045975208  PSNR: 23.21427345275879
[TRAIN] Iter: 36800 Loss: 0.012639046646654606  PSNR: 23.261295318603516
[TRAIN] Iter: 36900 Loss: 0.012746583670377731  PSNR: 22.900421142578125
[TRAIN] Iter: 37000 Loss: 0.01266428828239441  PSNR: 23.47434425354004
[TRAIN] Iter: 37100 Loss: 0.015149403363466263  PSNR: 22.541885375976562
[TRAIN] Iter: 37200 Loss: 0.010337995365262032  PSNR: 24.53679084777832
[TRAIN] Iter: 37300 Loss: 0.010146456770598888  PSNR: 24.21322250366211
[TRAIN] Iter: 37400 Loss: 0.015334323979914188  PSNR: 22.000701904296875
[TRAIN] Iter: 37500 Loss: 0.011672765016555786  PSNR: 23.966445922851562
[TRAIN] Iter: 37600 Loss: 0.013819842599332333  PSNR: 22.578399658203125
[TRAIN] Iter: 37700 Loss: 0.01189829409122467  PSNR: 23.386560440063477
[TRAIN] Iter: 37800 Loss: 0.016281215474009514  PSNR: 21.785329818725586
[TRAIN] Iter: 37900 Loss: 0.010350204072892666  PSNR: 23.932283401489258
[TRAIN] Iter: 38000 Loss: 0.01375897228717804  PSNR: 22.32518768310547
[TRAIN] Iter: 38100 Loss: 0.012996390461921692  PSNR: 23.27667808532715
[TRAIN] Iter: 38200 Loss: 0.009215536527335644  PSNR: 24.2160587310791
[TRAIN] Iter: 38300 Loss: 0.015061285346746445  PSNR: 22.575571060180664
[TRAIN] Iter: 38400 Loss: 0.012449244037270546  PSNR: 23.191043853759766
[TRAIN] Iter: 38500 Loss: 0.011999652720987797  PSNR: 23.488935470581055
[TRAIN] Iter: 38600 Loss: 0.01262601651251316  PSNR: 23.002330780029297
[TRAIN] Iter: 38700 Loss: 0.01075261365622282  PSNR: 23.874486923217773
[TRAIN] Iter: 38800 Loss: 0.011556234210729599  PSNR: 23.56633758544922
[TRAIN] Iter: 38900 Loss: 0.010448766872286797  PSNR: 24.07697868347168
[TRAIN] Iter: 39000 Loss: 0.013954995200037956  PSNR: 22.631078720092773
[TRAIN] Iter: 39100 Loss: 0.010015107691287994  PSNR: 24.217992782592773
[TRAIN] Iter: 39200 Loss: 0.012148906476795673  PSNR: 23.452905654907227
[TRAIN] Iter: 39300 Loss: 0.011917159892618656  PSNR: 23.306638717651367
[TRAIN] Iter: 39400 Loss: 0.010566828772425652  PSNR: 23.921823501586914
[TRAIN] Iter: 39500 Loss: 0.015169726684689522  PSNR: 22.569499969482422
[TRAIN] Iter: 39600 Loss: 0.01349656842648983  PSNR: 22.476295471191406
[TRAIN] Iter: 39700 Loss: 0.011956140398979187  PSNR: 23.28654670715332
[TRAIN] Iter: 39800 Loss: 0.010468200780451298  PSNR: 24.524105072021484
[TRAIN] Iter: 39900 Loss: 0.012589017860591412  PSNR: 23.526973724365234
Saved checkpoints at ./logs/TUT-KE101-nerf/040000.tar
[TRAIN] Iter: 40000 Loss: 0.013156026601791382  PSNR: 23.01898956298828
[TRAIN] Iter: 40100 Loss: 0.011117774993181229  PSNR: 24.07467269897461
[TRAIN] Iter: 40200 Loss: 0.013806488364934921  PSNR: 22.880449295043945
[TRAIN] Iter: 40300 Loss: 0.013209158554673195  PSNR: 22.670059204101562
[TRAIN] Iter: 40400 Loss: 0.00919277686625719  PSNR: 24.70389175415039
[TRAIN] Iter: 40500 Loss: 0.013130772858858109  PSNR: 22.36072540283203
[TRAIN] Iter: 40600 Loss: 0.011562587693333626  PSNR: 23.525100708007812
[TRAIN] Iter: 40700 Loss: 0.011729306541383266  PSNR: 23.176740646362305
[TRAIN] Iter: 40800 Loss: 0.012484573759138584  PSNR: 23.703187942504883
[TRAIN] Iter: 40900 Loss: 0.010710661299526691  PSNR: 23.54604148864746
[TRAIN] Iter: 41000 Loss: 0.009927395731210709  PSNR: 24.08670997619629
[TRAIN] Iter: 41100 Loss: 0.013430275954306126  PSNR: 22.570358276367188
[TRAIN] Iter: 41200 Loss: 0.010696684941649437  PSNR: 23.887182235717773
[TRAIN] Iter: 41300 Loss: 0.012307804077863693  PSNR: 23.175785064697266
[TRAIN] Iter: 41400 Loss: 0.012525880709290504  PSNR: 22.86541175842285
[TRAIN] Iter: 41500 Loss: 0.010911313816905022  PSNR: 23.874025344848633
[TRAIN] Iter: 41600 Loss: 0.010420795530080795  PSNR: 24.29507827758789
[TRAIN] Iter: 41700 Loss: 0.009170111268758774  PSNR: 24.526294708251953
[TRAIN] Iter: 41800 Loss: 0.01186513900756836  PSNR: 23.725967407226562
[TRAIN] Iter: 41900 Loss: 0.00930510088801384  PSNR: 24.922035217285156
[TRAIN] Iter: 42000 Loss: 0.013721069321036339  PSNR: 23.004127502441406
[TRAIN] Iter: 42100 Loss: 0.009979939088225365  PSNR: 24.885019302368164
[TRAIN] Iter: 42200 Loss: 0.009544800035655499  PSNR: 24.641124725341797
[TRAIN] Iter: 42300 Loss: 0.010892003774642944  PSNR: 24.181407928466797
[TRAIN] Iter: 42400 Loss: 0.014293406158685684  PSNR: 22.37653923034668
[TRAIN] Iter: 42500 Loss: 0.01203605905175209  PSNR: 23.425748825073242
[TRAIN] Iter: 42600 Loss: 0.011896896176040173  PSNR: 23.56881332397461
[TRAIN] Iter: 42700 Loss: 0.012753838673233986  PSNR: 22.874454498291016
[TRAIN] Iter: 42800 Loss: 0.01120372861623764  PSNR: 23.725048065185547
[TRAIN] Iter: 42900 Loss: 0.012969925068318844  PSNR: 22.766231536865234
[TRAIN] Iter: 43000 Loss: 0.011004092171788216  PSNR: 23.773319244384766
[TRAIN] Iter: 43100 Loss: 0.011862235143780708  PSNR: 23.28537368774414
[TRAIN] Iter: 43200 Loss: 0.012007849290966988  PSNR: 23.30740737915039
[TRAIN] Iter: 43300 Loss: 0.012409646064043045  PSNR: 23.569915771484375
[TRAIN] Iter: 43400 Loss: 0.01377866230905056  PSNR: 23.085575103759766
[TRAIN] Iter: 43500 Loss: 0.009635362774133682  PSNR: 24.15471649169922
[TRAIN] Iter: 43600 Loss: 0.015000741928815842  PSNR: 22.1221923828125
[TRAIN] Iter: 43700 Loss: 0.013692029751837254  PSNR: 22.725046157836914
[TRAIN] Iter: 43800 Loss: 0.01437684427946806  PSNR: 22.244401931762695
[TRAIN] Iter: 43900 Loss: 0.009214488789439201  PSNR: 25.09952163696289
[TRAIN] Iter: 44000 Loss: 0.015175031498074532  PSNR: 22.23067855834961
[TRAIN] Iter: 44100 Loss: 0.013190088793635368  PSNR: 22.83384895324707
[TRAIN] Iter: 44200 Loss: 0.011213534511625767  PSNR: 23.6348934173584
[TRAIN] Iter: 44300 Loss: 0.013125808909535408  PSNR: 22.958574295043945
[TRAIN] Iter: 44400 Loss: 0.012479837983846664  PSNR: 23.035770416259766
[TRAIN] Iter: 44500 Loss: 0.013269832357764244  PSNR: 22.740022659301758
[TRAIN] Iter: 44600 Loss: 0.01192125678062439  PSNR: 22.8774471282959
[TRAIN] Iter: 44700 Loss: 0.013546476140618324  PSNR: 22.334199905395508
[TRAIN] Iter: 44800 Loss: 0.010309259407222271  PSNR: 24.20863914489746
[TRAIN] Iter: 44900 Loss: 0.008987490087747574  PSNR: 25.09584617614746
[TRAIN] Iter: 45000 Loss: 0.00999586284160614  PSNR: 24.577131271362305
[TRAIN] Iter: 45100 Loss: 0.01176021434366703  PSNR: 23.63503074645996
[TRAIN] Iter: 45200 Loss: 0.009675894863903522  PSNR: 24.602619171142578
[TRAIN] Iter: 45300 Loss: 0.008041080087423325  PSNR: 25.367115020751953
[TRAIN] Iter: 45400 Loss: 0.012669248506426811  PSNR: 23.016826629638672
[TRAIN] Iter: 45500 Loss: 0.012199969962239265  PSNR: 23.201372146606445
[TRAIN] Iter: 45600 Loss: 0.011080902069807053  PSNR: 23.948034286499023
[TRAIN] Iter: 45700 Loss: 0.011463303118944168  PSNR: 23.701791763305664
[TRAIN] Iter: 45800 Loss: 0.008059987798333168  PSNR: 25.481279373168945
[TRAIN] Iter: 45900 Loss: 0.009859982877969742  PSNR: 24.169849395751953
[TRAIN] Iter: 46000 Loss: 0.012670477852225304  PSNR: 22.962852478027344
[TRAIN] Iter: 46100 Loss: 0.011165786534547806  PSNR: 23.897510528564453
[TRAIN] Iter: 46200 Loss: 0.01474901381880045  PSNR: 22.21196937561035
[TRAIN] Iter: 46300 Loss: 0.011266184970736504  PSNR: 23.782041549682617
[TRAIN] Iter: 46400 Loss: 0.009913716465234756  PSNR: 24.983007431030273
[TRAIN] Iter: 46500 Loss: 0.009207004681229591  PSNR: 24.782936096191406
[TRAIN] Iter: 46600 Loss: 0.010105546563863754  PSNR: 24.105043411254883
[TRAIN] Iter: 46700 Loss: 0.008111667819321156  PSNR: 25.211509704589844
[TRAIN] Iter: 46800 Loss: 0.013989374041557312  PSNR: 22.424787521362305
[TRAIN] Iter: 46900 Loss: 0.01104644499719143  PSNR: 23.669654846191406
[TRAIN] Iter: 47000 Loss: 0.010712899267673492  PSNR: 24.5100040435791
[TRAIN] Iter: 47100 Loss: 0.009848903864622116  PSNR: 24.132091522216797
[TRAIN] Iter: 47200 Loss: 0.010604484006762505  PSNR: 24.36309242248535
[TRAIN] Iter: 47300 Loss: 0.008301584050059319  PSNR: 25.327247619628906
[TRAIN] Iter: 47400 Loss: 0.013822991400957108  PSNR: 22.967453002929688
[TRAIN] Iter: 47500 Loss: 0.011578772217035294  PSNR: 23.411699295043945
[TRAIN] Iter: 47600 Loss: 0.009821366518735886  PSNR: 24.700077056884766
[TRAIN] Iter: 47700 Loss: 0.012502284720540047  PSNR: 22.958261489868164
[TRAIN] Iter: 47800 Loss: 0.008360599167644978  PSNR: 25.47222137451172
[TRAIN] Iter: 47900 Loss: 0.011483956128358841  PSNR: 24.04094886779785
[TRAIN] Iter: 48000 Loss: 0.009127704426646233  PSNR: 24.943437576293945
[TRAIN] Iter: 48100 Loss: 0.010740653611719608  PSNR: 23.837512969970703
[TRAIN] Iter: 48200 Loss: 0.008618182502686977  PSNR: 24.926591873168945
[TRAIN] Iter: 48300 Loss: 0.007561301812529564  PSNR: 25.395004272460938
[TRAIN] Iter: 48400 Loss: 0.01100822351872921  PSNR: 23.752580642700195
[TRAIN] Iter: 48500 Loss: 0.012502707540988922  PSNR: 23.366939544677734
[TRAIN] Iter: 48600 Loss: 0.008753041736781597  PSNR: 25.531734466552734
[TRAIN] Iter: 48700 Loss: 0.013141615316271782  PSNR: 23.229217529296875
[TRAIN] Iter: 48800 Loss: 0.008261380717158318  PSNR: 25.651395797729492
[TRAIN] Iter: 48900 Loss: 0.010328712873160839  PSNR: 24.21943473815918
[TRAIN] Iter: 49000 Loss: 0.007811180781573057  PSNR: 25.464445114135742
[TRAIN] Iter: 49100 Loss: 0.012534696608781815  PSNR: 23.656654357910156
[TRAIN] Iter: 49200 Loss: 0.014219455420970917  PSNR: 22.50927734375
[TRAIN] Iter: 49300 Loss: 0.00817000400274992  PSNR: 25.409849166870117
[TRAIN] Iter: 49400 Loss: 0.012322375550866127  PSNR: 23.210533142089844
[TRAIN] Iter: 49500 Loss: 0.011103370226919651  PSNR: 23.553131103515625
[TRAIN] Iter: 49600 Loss: 0.011625576764345169  PSNR: 23.382577896118164
[TRAIN] Iter: 49700 Loss: 0.008038226515054703  PSNR: 25.569053649902344
[TRAIN] Iter: 49800 Loss: 0.009813354350626469  PSNR: 24.896251678466797
[TRAIN] Iter: 49900 Loss: 0.009632926434278488  PSNR: 24.747413635253906
Saved checkpoints at ./logs/TUT-KE101-nerf/050000.tar
0 0.00033926963806152344
torch.Size([320, 640, 3]) torch.Size([320, 640])
1 13.303060054779053
2 15.27603530883789
3 13.288093566894531
4 15.341954946517944
5 13.321160316467285
6 15.099225521087646
7 13.56038784980774
8 13.44679307937622
9 15.23560357093811
10 12.97329306602478
11 15.591392755508423
12 13.307845115661621
13 13.276991128921509
14 15.381466627120972
15 13.130305051803589
16 15.55026125907898
17 13.10984492301941
18 13.481300592422485
19 15.144999742507935
20 13.496777296066284
21 17.166186094284058
22 15.10834264755249
23 17.321691513061523
24 15.056748151779175
25 17.31258797645569
26 15.102511405944824
27 17.1930890083313
28 15.76740837097168
29 17.415499925613403
30 15.47850489616394
31 17.076472520828247
32 15.351198673248291
33 17.0666823387146
34 15.25534701347351
35 15.28468942642212
36 17.042157649993896
37 15.355093240737915
38 17.047413110733032
39 15.332844018936157
40 17.08298397064209
41 15.259598970413208
42 17.02423596382141
43 15.304175615310669
44 17.07484245300293
45 15.337058544158936
46 17.08009433746338
47 15.229683876037598
48 17.055933952331543
49 15.244086027145386
50 17.103941679000854
51 15.458431243896484
52 16.919217824935913
53 15.358974695205688
54 15.271306037902832
55 16.806896448135376
56 15.35377025604248
57 17.23741102218628
58 15.360511064529419
59 17.192067623138428
60 15.73711895942688
61 17.393235445022583
62 15.269962787628174
63 17.072343587875366
64 15.294764041900635
65 17.08714771270752
66 15.314433097839355
67 17.113819122314453
68 15.258699417114258
69 17.057743310928345
70 15.30260705947876
71 17.04141902923584
72 15.307453632354736
73 17.105700492858887
74 15.191443920135498
75 15.358260154724121
76 17.233500957489014
77 15.168762445449829
78 17.60096311569214
79 15.721060991287231
80 17.66262459754944
81 15.778615951538086
82 17.70320224761963
83 16.113603591918945
84 17.664416313171387
85 15.630935668945312
86 17.586389303207397
87 15.471864700317383
88 17.788233757019043
89 15.189375638961792
90 17.735466480255127
91 15.302144050598145
92 17.935184955596924
93 15.463149309158325
94 18.244702100753784
95 15.556217432022095
96 17.741788625717163
97 15.2953200340271
98 17.676902532577515
99 15.49684739112854
100 17.625225067138672
101 15.610342502593994
102 17.288612365722656
103 15.679236650466919
104 17.419967651367188
105 15.646581411361694
106 17.43800687789917
107 15.655050039291382
108 17.3640353679657
109 15.625051259994507
110 17.433361053466797
111 15.619940519332886
112 15.605521440505981
113 17.44847273826599
114 15.604038953781128
115 17.456596612930298
116 15.614612817764282
117 17.417340993881226
118 15.586961030960083
119 17.450743913650513
Done, saving (120, 320, 640, 3) (120, 320, 640)
extras:{'raw': tensor([[[-7.6634e-01, -3.0122e-01,  4.3759e-01, -3.7979e+01],
         [-1.4903e+00, -1.1057e+00, -4.3219e-01, -2.2137e+01],
         [-1.0463e+00, -1.0434e+00, -9.0663e-01, -8.1088e+00],
         ...,
         [-2.5400e+01, -2.2963e+01, -2.2689e+01, -1.4388e+02],
         [-2.5083e+01, -2.2606e+01, -2.2562e+01, -3.5480e+01],
         [-2.5633e+01, -2.2926e+01, -2.2163e+01, -1.0826e+02]],

        [[-5.0548e-01,  2.2382e-01,  1.2439e+00, -3.2484e+01],
         [ 6.3700e-01,  7.5253e-01,  1.3339e+00, -1.9188e+01],
         [-3.0888e-01, -2.4184e-01,  1.9253e-01, -6.7612e+00],
         ...,
         [ 7.6997e-01,  8.0508e-02, -1.3328e-01,  2.2266e+02],
         [ 8.5135e-01,  1.8490e-01,  6.8712e-02,  1.8652e+02],
         [ 1.5298e+00,  5.7674e-01,  3.9736e-03,  2.2877e+02]],

        [[-2.3350e+00, -1.4034e+00, -2.7810e-01, -4.1060e+01],
         [-3.6675e-02, -1.9585e-01, -5.4057e-01, -1.7482e+01],
         [-9.4547e-02, -1.9369e-01, -4.5510e-01,  8.2667e+00],
         ...,
         [ 2.0275e+00,  8.7435e-02, -5.4832e+00,  4.1141e+01],
         [ 1.7723e+00, -1.9690e-01, -5.8672e+00,  5.7260e+01],
         [ 1.9719e+00,  5.3878e-02, -5.5048e+00,  7.1102e+01]],

        ...,

        [[-9.7566e-01, -5.3342e-01, -1.2782e-01, -3.9920e+01],
         [ 2.1185e+00,  2.1321e+00,  2.4821e+00, -2.0313e+01],
         [ 2.9344e-01,  2.5329e-01,  1.2638e-01, -7.3463e+00],
         ...,
         [ 3.9387e-01, -1.0469e+00, -2.6162e+00,  3.7457e+02],
         [ 1.2245e+00, -4.2287e-01, -2.1374e+00,  3.6066e+02],
         [ 6.8142e-01, -8.2183e-01, -2.6381e+00,  3.8285e+02]],

        [[-1.9154e+00, -1.8661e+00, -1.9503e+00, -3.3960e+01],
         [-2.3348e-01, -1.4717e-01, -1.3177e-01, -1.2402e+01],
         [ 5.3868e-01,  5.0520e-01,  4.4143e-01,  1.9713e+00],
         ...,
         [ 1.2692e+00,  5.9446e-03, -3.6800e+00, -6.6644e+00],
         [ 1.0829e+00, -9.7275e-02, -3.7066e+00,  9.2446e+00],
         [ 1.5315e+00,  2.5440e-01, -3.4539e+00,  1.4179e+01]],

        [[-1.8167e+00, -1.3207e+00, -1.5228e+00, -4.4479e+01],
         [-1.9437e+00, -1.5860e+00, -1.3906e+00, -3.0830e+01],
         [-7.8369e-01, -1.5840e+00, -3.0097e+00, -1.0462e+01],
         ...,
         [ 1.3880e-01, -1.4213e+00, -4.2233e+00, -1.4193e+01],
         [-5.1601e-02, -1.6136e+00, -4.5201e+00, -2.5664e+00],
         [-7.4226e-01, -1.9265e+00, -4.3041e+00,  2.7336e+01]]],
       grad_fn=<ReshapeAliasBackward0>), 'rgb0': tensor([[0.3215, 0.3053, 0.3044],
        [0.6354, 0.6342, 0.6688],
        [0.4829, 0.4635, 0.3889],
        ...,
        [0.6400, 0.6282, 0.6059],
        [0.6172, 0.6117, 0.6286],
        [0.3494, 0.3029, 0.2388]], grad_fn=<ReshapeAliasBackward0>), 'disp0': tensor([44.2890, 50.4936, 53.2233,  ..., 65.9892, 63.3402, 81.6680],
       grad_fn=<ReshapeAliasBackward0>), 'acc0': tensor([1., 1., 1.,  ..., 1., 1., 1.], grad_fn=<ReshapeAliasBackward0>), 'z_std': tensor([0.0017, 0.0039, 0.0017,  ..., 0.0025, 0.0022, 0.0030])}
0 0.00042939186096191406
torch.Size([320, 640, 3]) torch.Size([320, 640])
1 17.4060161113739
2 15.62871503829956
3 17.39386296272278
4 15.549680948257446
5 17.561949491500854
6 15.508257865905762
7 17.53057861328125
8 15.598643779754639
9 17.39496350288391
10 15.612640619277954
11 17.446925401687622
12 15.585603952407837
13 17.445112943649292
14 15.589223384857178
15 17.46081042289734
16 15.600924491882324
17 17.428091526031494
18 15.587000131607056
19 17.39811062812805
20 15.280411720275879
21 17.142972469329834
22 15.302066326141357
23 14.744266510009766
24 15.310258150100708
25 15.245039939880371
26 17.253769397735596
27 15.145899295806885
28 17.298898220062256
29 15.107943534851074
30 17.226771354675293
31 15.090670347213745
32 17.28829336166382
33 15.152711391448975
34 17.234081745147705
35 15.154161214828491
36 17.180396556854248
37 15.144837617874146
38 17.137745141983032
39 15.063452959060669
40 15.257128477096558
41 17.23183012008667
42 15.17685317993164
43 17.24268865585327
44 15.049093008041382
45 17.265546321868896
46 15.125884532928467
47 17.34045958518982
48 15.02418065071106
49 17.429250240325928
50 14.983302116394043
51 17.392686367034912
52 14.869242668151855
53 17.59303331375122
54 15.295474767684937
55 17.929333686828613
56 15.110766172409058
57 17.53128719329834
58 14.786920547485352
59 15.67740774154663
60 16.669476747512817
61 15.517688751220703
62 16.81945276260376
63 15.481411695480347
64 17.050513744354248
65 15.401603698730469
66 17.007965326309204
67 15.295885801315308
68 17.095260620117188
69 15.10715937614441
70 17.395190715789795
71 15.483257532119751
72 16.775213718414307
73 15.171103954315186
74 17.337013959884644
75 15.332782506942749
76 17.062126636505127
77 15.222691297531128
78 15.269809246063232
79 17.081581830978394
80 15.261876106262207
81 17.104787349700928
82 15.284920930862427
83 17.155173301696777
84 15.225720405578613
85 17.073469400405884
86 15.277050971984863
87 17.096240043640137
88 15.295870065689087
89 17.113830089569092
90 15.19292140007019
91 17.104656457901
92 15.36550235748291
93 17.555867195129395
94 15.649891138076782
95 17.387770414352417
96 15.408604621887207
97 16.9322407245636
98 15.426645040512085
99 15.161321878433228
100 17.196245193481445
101 15.194424390792847
102 17.162986278533936
103 15.25307321548462
104 17.107834339141846
105 15.228391885757446
106 17.022897481918335
107 15.170168161392212
108 17.374861240386963
109 14.98455286026001
110 17.42067313194275
111 14.919347286224365
112 17.569972038269043
113 14.889749526977539
114 17.435473442077637
115 14.95537543296814
116 17.36578941345215
117 14.976081132888794
118 15.298370838165283
119 16.967833995819092
test poses shape torch.Size([4, 3, 4])
0 0.0008037090301513672
torch.Size([320, 640, 3]) torch.Size([320, 640])
1 17.09021782875061
2 15.51700472831726
3 17.132805109024048
Saved test set
[TRAIN] Iter: 50000 Loss: 0.013134997338056564  PSNR: 23.117385864257812
[TRAIN] Iter: 50100 Loss: 0.011805214919149876  PSNR: 23.09908103942871
[TRAIN] Iter: 50200 Loss: 0.009889563545584679  PSNR: 24.67479705810547
[TRAIN] Iter: 50300 Loss: 0.0073323482647538185  PSNR: 26.187362670898438
[TRAIN] Iter: 50400 Loss: 0.011515943333506584  PSNR: 23.892841339111328
[TRAIN] Iter: 50500 Loss: 0.012980196624994278  PSNR: 23.252952575683594
[TRAIN] Iter: 50600 Loss: 0.009214552119374275  PSNR: 24.463743209838867
[TRAIN] Iter: 50700 Loss: 0.014798734337091446  PSNR: 22.40260124206543
[TRAIN] Iter: 50800 Loss: 0.009889526292681694  PSNR: 24.145662307739258
[TRAIN] Iter: 50900 Loss: 0.010826705023646355  PSNR: 23.325836181640625
[TRAIN] Iter: 51000 Loss: 0.012076455168426037  PSNR: 23.29133415222168
[TRAIN] Iter: 51100 Loss: 0.011538851074874401  PSNR: 22.990354537963867
[TRAIN] Iter: 51200 Loss: 0.010797309689223766  PSNR: 24.10115623474121
[TRAIN] Iter: 51300 Loss: 0.011430072598159313  PSNR: 23.40437889099121
[TRAIN] Iter: 51400 Loss: 0.009521073661744595  PSNR: 24.648414611816406
[TRAIN] Iter: 51500 Loss: 0.009898388758301735  PSNR: 23.888051986694336
[TRAIN] Iter: 51600 Loss: 0.013188205659389496  PSNR: 22.94412612915039
[TRAIN] Iter: 51700 Loss: 0.012193919159471989  PSNR: 23.1192626953125
[TRAIN] Iter: 51800 Loss: 0.009040361270308495  PSNR: 24.711475372314453
[TRAIN] Iter: 51900 Loss: 0.009219000115990639  PSNR: 24.935945510864258
[TRAIN] Iter: 52000 Loss: 0.00854563619941473  PSNR: 25.498186111450195
[TRAIN] Iter: 52100 Loss: 0.010290650650858879  PSNR: 24.209001541137695
[TRAIN] Iter: 52200 Loss: 0.010729162953794003  PSNR: 23.945722579956055
[TRAIN] Iter: 52300 Loss: 0.008694738149642944  PSNR: 25.027700424194336
[TRAIN] Iter: 52400 Loss: 0.014679942280054092  PSNR: 22.761150360107422
[TRAIN] Iter: 52500 Loss: 0.012302225455641747  PSNR: 23.290620803833008
[TRAIN] Iter: 52600 Loss: 0.011802231892943382  PSNR: 23.516700744628906
[TRAIN] Iter: 52700 Loss: 0.010940629057586193  PSNR: 23.543752670288086
[TRAIN] Iter: 52800 Loss: 0.009211152791976929  PSNR: 24.34407615661621
[TRAIN] Iter: 52900 Loss: 0.009919598698616028  PSNR: 24.46337127685547
[TRAIN] Iter: 53000 Loss: 0.010833116248250008  PSNR: 23.933439254760742
[TRAIN] Iter: 53100 Loss: 0.012025396339595318  PSNR: 23.458187103271484
[TRAIN] Iter: 53200 Loss: 0.012433374300599098  PSNR: 23.22750473022461
[TRAIN] Iter: 53300 Loss: 0.013173275627195835  PSNR: 22.765024185180664
[TRAIN] Iter: 53400 Loss: 0.010812526568770409  PSNR: 23.536821365356445
[TRAIN] Iter: 53500 Loss: 0.011200916953384876  PSNR: 23.544666290283203
[TRAIN] Iter: 53600 Loss: 0.01422932744026184  PSNR: 22.496850967407227
[TRAIN] Iter: 53700 Loss: 0.010529323481023312  PSNR: 23.97230339050293
[TRAIN] Iter: 53800 Loss: 0.01077861525118351  PSNR: 23.979270935058594
[TRAIN] Iter: 53900 Loss: 0.012122073210775852  PSNR: 23.721439361572266
[TRAIN] Iter: 54000 Loss: 0.007882457226514816  PSNR: 25.209897994995117
[TRAIN] Iter: 54100 Loss: 0.008572014048695564  PSNR: 25.761669158935547
[TRAIN] Iter: 54200 Loss: 0.011336002498865128  PSNR: 23.854278564453125
[TRAIN] Iter: 54300 Loss: 0.008679873310029507  PSNR: 25.00609588623047
[TRAIN] Iter: 54400 Loss: 0.010435511358082294  PSNR: 23.881633758544922
[TRAIN] Iter: 54500 Loss: 0.009336457587778568  PSNR: 24.781126022338867
[TRAIN] Iter: 54600 Loss: 0.00977631751447916  PSNR: 24.590944290161133
[TRAIN] Iter: 54700 Loss: 0.012185510247945786  PSNR: 23.014970779418945
[TRAIN] Iter: 54800 Loss: 0.011474419385194778  PSNR: 22.93685531616211
[TRAIN] Iter: 54900 Loss: 0.010287951678037643  PSNR: 24.12257194519043
[TRAIN] Iter: 55000 Loss: 0.012585898861289024  PSNR: 23.322406768798828
[TRAIN] Iter: 55100 Loss: 0.009437344036996365  PSNR: 24.3140926361084
[TRAIN] Iter: 55200 Loss: 0.01266572903841734  PSNR: 23.177751541137695
[TRAIN] Iter: 55300 Loss: 0.013105940073728561  PSNR: 22.873634338378906
[TRAIN] Iter: 55400 Loss: 0.009760912507772446  PSNR: 24.49793243408203
[TRAIN] Iter: 55500 Loss: 0.010476138442754745  PSNR: 23.666032791137695
[TRAIN] Iter: 55600 Loss: 0.008228166028857231  PSNR: 25.27402114868164
[TRAIN] Iter: 55700 Loss: 0.010528188198804855  PSNR: 23.87684440612793
[TRAIN] Iter: 55800 Loss: 0.008734049275517464  PSNR: 24.721965789794922
[TRAIN] Iter: 55900 Loss: 0.009193729609251022  PSNR: 25.52533721923828
[TRAIN] Iter: 56000 Loss: 0.011555254459381104  PSNR: 23.661163330078125
[TRAIN] Iter: 56100 Loss: 0.008843157440423965  PSNR: 25.376733779907227
[TRAIN] Iter: 56200 Loss: 0.009005608037114143  PSNR: 25.315635681152344
[TRAIN] Iter: 56300 Loss: 0.009350663051009178  PSNR: 24.959375381469727
[TRAIN] Iter: 56400 Loss: 0.009528493508696556  PSNR: 25.19412612915039
[TRAIN] Iter: 56500 Loss: 0.011337959207594395  PSNR: 23.44332504272461
[TRAIN] Iter: 56600 Loss: 0.007809249218553305  PSNR: 25.343185424804688
[TRAIN] Iter: 56700 Loss: 0.008245119825005531  PSNR: 25.599685668945312
[TRAIN] Iter: 56800 Loss: 0.008299196138978004  PSNR: 25.484878540039062
[TRAIN] Iter: 56900 Loss: 0.009219560772180557  PSNR: 24.812776565551758
[TRAIN] Iter: 57000 Loss: 0.012184594757854939  PSNR: 23.032407760620117
[TRAIN] Iter: 57100 Loss: 0.010250601917505264  PSNR: 23.89993667602539
[TRAIN] Iter: 57200 Loss: 0.011288024485111237  PSNR: 23.923015594482422
[TRAIN] Iter: 57300 Loss: 0.011232210323214531  PSNR: 23.55344581604004
[TRAIN] Iter: 57400 Loss: 0.010314570739865303  PSNR: 23.976627349853516
[TRAIN] Iter: 57500 Loss: 0.008898735977709293  PSNR: 25.50172233581543
[TRAIN] Iter: 57600 Loss: 0.00898207537829876  PSNR: 24.749149322509766
[TRAIN] Iter: 57700 Loss: 0.01085712295025587  PSNR: 23.69408416748047
[TRAIN] Iter: 57800 Loss: 0.011044299229979515  PSNR: 23.733156204223633
[TRAIN] Iter: 57900 Loss: 0.01057974249124527  PSNR: 23.77237319946289
[TRAIN] Iter: 58000 Loss: 0.01155189611017704  PSNR: 23.307188034057617
[TRAIN] Iter: 58100 Loss: 0.00975414365530014  PSNR: 24.574962615966797
[TRAIN] Iter: 58200 Loss: 0.007610122673213482  PSNR: 26.011201858520508
[TRAIN] Iter: 58300 Loss: 0.008950358256697655  PSNR: 24.533140182495117
[TRAIN] Iter: 58400 Loss: 0.011170504614710808  PSNR: 23.525144577026367
[TRAIN] Iter: 58500 Loss: 0.012076287530362606  PSNR: 23.370630264282227
[TRAIN] Iter: 58600 Loss: 0.012038161978125572  PSNR: 23.7702579498291
[TRAIN] Iter: 58700 Loss: 0.007867054082453251  PSNR: 25.602951049804688
[TRAIN] Iter: 58800 Loss: 0.007678983733057976  PSNR: 25.562490463256836
[TRAIN] Iter: 58900 Loss: 0.011239521205425262  PSNR: 23.66043472290039
[TRAIN] Iter: 59000 Loss: 0.011328154243528843  PSNR: 24.1475772857666
[TRAIN] Iter: 59100 Loss: 0.00763116218149662  PSNR: 25.552169799804688
[TRAIN] Iter: 59200 Loss: 0.008590083569288254  PSNR: 24.454524993896484
[TRAIN] Iter: 59300 Loss: 0.007954766973853111  PSNR: 25.50636863708496
[TRAIN] Iter: 59400 Loss: 0.006512324325740337  PSNR: 26.46894645690918
[TRAIN] Iter: 59500 Loss: 0.009990107268095016  PSNR: 23.927440643310547
[TRAIN] Iter: 59600 Loss: 0.008764265105128288  PSNR: 24.659618377685547
[TRAIN] Iter: 59700 Loss: 0.009322123602032661  PSNR: 24.812124252319336
[TRAIN] Iter: 59800 Loss: 0.012837381102144718  PSNR: 23.215003967285156
[TRAIN] Iter: 59900 Loss: 0.00814110692590475  PSNR: 25.500871658325195
Saved checkpoints at ./logs/TUT-KE101-nerf/060000.tar
[TRAIN] Iter: 60000 Loss: 0.01089603640139103  PSNR: 24.29072380065918
[TRAIN] Iter: 60100 Loss: 0.01133742555975914  PSNR: 23.843517303466797
[TRAIN] Iter: 60200 Loss: 0.009013453498482704  PSNR: 24.76616096496582
[TRAIN] Iter: 60300 Loss: 0.009562005288898945  PSNR: 24.553926467895508
[TRAIN] Iter: 60400 Loss: 0.00827424880117178  PSNR: 25.63086700439453
[TRAIN] Iter: 60500 Loss: 0.008069810457527637  PSNR: 25.350379943847656
[TRAIN] Iter: 60600 Loss: 0.010729435831308365  PSNR: 24.044296264648438
[TRAIN] Iter: 60700 Loss: 0.008317612111568451  PSNR: 25.429353713989258
[TRAIN] Iter: 60800 Loss: 0.009542159736156464  PSNR: 24.28678321838379
[TRAIN] Iter: 60900 Loss: 0.009492896497249603  PSNR: 24.34003448486328
[TRAIN] Iter: 61000 Loss: 0.010234314948320389  PSNR: 24.073802947998047
[TRAIN] Iter: 61100 Loss: 0.008560710586607456  PSNR: 25.347993850708008
[TRAIN] Iter: 61200 Loss: 0.008983886800706387  PSNR: 24.681377410888672
[TRAIN] Iter: 61300 Loss: 0.009842930361628532  PSNR: 24.087753295898438
[TRAIN] Iter: 61400 Loss: 0.008670592680573463  PSNR: 26.033130645751953
[TRAIN] Iter: 61500 Loss: 0.007620496209710836  PSNR: 25.979595184326172
[TRAIN] Iter: 61600 Loss: 0.011583168059587479  PSNR: 23.557430267333984
[TRAIN] Iter: 61700 Loss: 0.0096544548869133  PSNR: 24.350502014160156
[TRAIN] Iter: 61800 Loss: 0.012316293083131313  PSNR: 23.095993041992188
[TRAIN] Iter: 61900 Loss: 0.008349554613232613  PSNR: 25.664945602416992
[TRAIN] Iter: 62000 Loss: 0.01106909941881895  PSNR: 23.32642936706543
[TRAIN] Iter: 62100 Loss: 0.010514792054891586  PSNR: 24.04624366760254
[TRAIN] Iter: 62200 Loss: 0.009026126936078072  PSNR: 24.75855827331543
[TRAIN] Iter: 62300 Loss: 0.011541396379470825  PSNR: 23.447383880615234
[TRAIN] Iter: 62400 Loss: 0.010593648999929428  PSNR: 23.795841217041016
[TRAIN] Iter: 62500 Loss: 0.009607305750250816  PSNR: 25.347076416015625
[TRAIN] Iter: 62600 Loss: 0.012146783992648125  PSNR: 23.080781936645508
[TRAIN] Iter: 62700 Loss: 0.010673054493963718  PSNR: 24.099185943603516
[TRAIN] Iter: 62800 Loss: 0.010759169235825539  PSNR: 23.939638137817383
[TRAIN] Iter: 62900 Loss: 0.008899198845028877  PSNR: 24.429128646850586
[TRAIN] Iter: 63000 Loss: 0.009973486885428429  PSNR: 24.61418342590332
[TRAIN] Iter: 63100 Loss: 0.00899660773575306  PSNR: 25.366376876831055
[TRAIN] Iter: 63200 Loss: 0.008654266595840454  PSNR: 24.809486389160156
[TRAIN] Iter: 63300 Loss: 0.009676259942352772  PSNR: 24.549610137939453
[TRAIN] Iter: 63400 Loss: 0.01165375579148531  PSNR: 23.31048583984375
[TRAIN] Iter: 63500 Loss: 0.010796989314258099  PSNR: 23.664621353149414
[TRAIN] Iter: 63600 Loss: 0.009557588957250118  PSNR: 25.258766174316406
[TRAIN] Iter: 63700 Loss: 0.00824909470975399  PSNR: 25.463748931884766
[TRAIN] Iter: 63800 Loss: 0.007695946376770735  PSNR: 26.148481369018555
[TRAIN] Iter: 63900 Loss: 0.009374182671308517  PSNR: 24.663589477539062
[TRAIN] Iter: 64000 Loss: 0.008147213608026505  PSNR: 25.68247413635254
[TRAIN] Iter: 64100 Loss: 0.006966483313590288  PSNR: 25.786251068115234
[TRAIN] Iter: 64200 Loss: 0.011128826066851616  PSNR: 23.875898361206055
[TRAIN] Iter: 64300 Loss: 0.008153578266501427  PSNR: 25.90439796447754
[TRAIN] Iter: 64400 Loss: 0.009771758690476418  PSNR: 23.948833465576172
[TRAIN] Iter: 64500 Loss: 0.007092570886015892  PSNR: 26.274995803833008
[TRAIN] Iter: 64600 Loss: 0.012350748293101788  PSNR: 23.278057098388672
[TRAIN] Iter: 64700 Loss: 0.012023896910250187  PSNR: 23.6054744720459
[TRAIN] Iter: 64800 Loss: 0.010632550343871117  PSNR: 23.676902770996094
[TRAIN] Iter: 64900 Loss: 0.007716854102909565  PSNR: 26.37674331665039
[TRAIN] Iter: 65000 Loss: 0.01055496372282505  PSNR: 24.180767059326172
[TRAIN] Iter: 65100 Loss: 0.01174420490860939  PSNR: 23.35893440246582
[TRAIN] Iter: 65200 Loss: 0.012632541358470917  PSNR: 23.1841983795166
[TRAIN] Iter: 65300 Loss: 0.00925296451896429  PSNR: 24.735612869262695
[TRAIN] Iter: 65400 Loss: 0.011562916450202465  PSNR: 23.739999771118164
[TRAIN] Iter: 65500 Loss: 0.008984686806797981  PSNR: 24.552587509155273
[TRAIN] Iter: 65600 Loss: 0.007649076636880636  PSNR: 25.64271354675293
[TRAIN] Iter: 65700 Loss: 0.010666432790458202  PSNR: 23.791852951049805
[TRAIN] Iter: 65800 Loss: 0.010850710794329643  PSNR: 23.959123611450195
[TRAIN] Iter: 65900 Loss: 0.009732787497341633  PSNR: 24.065401077270508
[TRAIN] Iter: 66000 Loss: 0.009515644051134586  PSNR: 24.500652313232422
[TRAIN] Iter: 66100 Loss: 0.01058476697653532  PSNR: 23.90471076965332
[TRAIN] Iter: 66200 Loss: 0.011600930243730545  PSNR: 23.54448699951172
[TRAIN] Iter: 66300 Loss: 0.00978793203830719  PSNR: 24.400970458984375
[TRAIN] Iter: 66400 Loss: 0.008613907732069492  PSNR: 25.515247344970703
[TRAIN] Iter: 66500 Loss: 0.007686099037528038  PSNR: 25.97359275817871
[TRAIN] Iter: 66600 Loss: 0.009607095271348953  PSNR: 24.44751739501953
[TRAIN] Iter: 66700 Loss: 0.007460562977939844  PSNR: 25.71377944946289
[TRAIN] Iter: 66800 Loss: 0.009283123537898064  PSNR: 24.248746871948242
[TRAIN] Iter: 66900 Loss: 0.009792573750019073  PSNR: 24.1114444732666
[TRAIN] Iter: 67000 Loss: 0.010494377464056015  PSNR: 23.988872528076172
[TRAIN] Iter: 67100 Loss: 0.011776847764849663  PSNR: 23.365751266479492
[TRAIN] Iter: 67200 Loss: 0.006541507318615913  PSNR: 26.534944534301758
[TRAIN] Iter: 67300 Loss: 0.01037317980080843  PSNR: 24.04068374633789
[TRAIN] Iter: 67400 Loss: 0.010197682306170464  PSNR: 24.470632553100586
[TRAIN] Iter: 67500 Loss: 0.01185240875929594  PSNR: 23.54774284362793
[TRAIN] Iter: 67600 Loss: 0.008388725109398365  PSNR: 25.53734016418457
[TRAIN] Iter: 67700 Loss: 0.006839326582849026  PSNR: 26.049381256103516
[TRAIN] Iter: 67800 Loss: 0.009989520534873009  PSNR: 24.485118865966797
[TRAIN] Iter: 67900 Loss: 0.009881221689283848  PSNR: 24.110797882080078
[TRAIN] Iter: 68000 Loss: 0.011563453823328018  PSNR: 23.515962600708008
[TRAIN] Iter: 68100 Loss: 0.007931898348033428  PSNR: 25.792348861694336
[TRAIN] Iter: 68200 Loss: 0.011151686310768127  PSNR: 23.96649932861328
[TRAIN] Iter: 68300 Loss: 0.01049555093050003  PSNR: 24.245773315429688
[TRAIN] Iter: 68400 Loss: 0.009608985856175423  PSNR: 24.450178146362305
[TRAIN] Iter: 68500 Loss: 0.006740894168615341  PSNR: 26.668190002441406
[TRAIN] Iter: 68600 Loss: 0.009931051172316074  PSNR: 24.378982543945312
[TRAIN] Iter: 68700 Loss: 0.008776189759373665  PSNR: 25.495159149169922
[TRAIN] Iter: 68800 Loss: 0.010512781329452991  PSNR: 24.193199157714844
[TRAIN] Iter: 68900 Loss: 0.008221467956900597  PSNR: 24.997177124023438
[TRAIN] Iter: 69000 Loss: 0.007965466938912868  PSNR: 25.386728286743164
[TRAIN] Iter: 69100 Loss: 0.010052768513560295  PSNR: 24.070951461791992
[TRAIN] Iter: 69200 Loss: 0.009893987327814102  PSNR: 24.328758239746094
[TRAIN] Iter: 69300 Loss: 0.007502707652747631  PSNR: 25.946008682250977
[TRAIN] Iter: 69400 Loss: 0.010293222963809967  PSNR: 24.761516571044922
[TRAIN] Iter: 69500 Loss: 0.01244744285941124  PSNR: 22.934600830078125
[TRAIN] Iter: 69600 Loss: 0.007674971595406532  PSNR: 25.550283432006836
[TRAIN] Iter: 69700 Loss: 0.010901120491325855  PSNR: 24.220701217651367
[TRAIN] Iter: 69800 Loss: 0.00784490630030632  PSNR: 26.81456756591797
[TRAIN] Iter: 69900 Loss: 0.006895335391163826  PSNR: 26.53961944580078
Saved checkpoints at ./logs/TUT-KE101-nerf/070000.tar
[TRAIN] Iter: 70000 Loss: 0.008464733138680458  PSNR: 25.33066177368164
[TRAIN] Iter: 70100 Loss: 0.011075524613261223  PSNR: 23.91068458557129
[TRAIN] Iter: 70200 Loss: 0.009078385308384895  PSNR: 25.80008316040039
[TRAIN] Iter: 70300 Loss: 0.011717136949300766  PSNR: 23.586849212646484
[TRAIN] Iter: 70400 Loss: 0.00890254508703947  PSNR: 24.918455123901367
[TRAIN] Iter: 70500 Loss: 0.010072351433336735  PSNR: 24.292245864868164
[TRAIN] Iter: 70600 Loss: 0.010866064578294754  PSNR: 23.784273147583008
[TRAIN] Iter: 70700 Loss: 0.006615287624299526  PSNR: 26.97812843322754
[TRAIN] Iter: 70800 Loss: 0.007976762019097805  PSNR: 26.098674774169922
[TRAIN] Iter: 70900 Loss: 0.009176808409392834  PSNR: 24.67333221435547
[TRAIN] Iter: 71000 Loss: 0.008680714294314384  PSNR: 24.885465621948242
[TRAIN] Iter: 71100 Loss: 0.007909986190497875  PSNR: 25.609521865844727
[TRAIN] Iter: 71200 Loss: 0.007623463869094849  PSNR: 25.707677841186523
[TRAIN] Iter: 71300 Loss: 0.010237932205200195  PSNR: 23.9248104095459
[TRAIN] Iter: 71400 Loss: 0.008197028189897537  PSNR: 25.758033752441406
[TRAIN] Iter: 71500 Loss: 0.009566672146320343  PSNR: 25.21613121032715
[TRAIN] Iter: 71600 Loss: 0.010716955177485943  PSNR: 24.121192932128906
[TRAIN] Iter: 71700 Loss: 0.008661756291985512  PSNR: 25.870628356933594
[TRAIN] Iter: 71800 Loss: 0.006827440578490496  PSNR: 25.9990234375
[TRAIN] Iter: 71900 Loss: 0.009623400866985321  PSNR: 24.47324562072754
[TRAIN] Iter: 72000 Loss: 0.00793741550296545  PSNR: 24.807321548461914
[TRAIN] Iter: 72100 Loss: 0.008464054204523563  PSNR: 25.50640106201172
[TRAIN] Iter: 72200 Loss: 0.009046653285622597  PSNR: 24.358259201049805
[TRAIN] Iter: 72300 Loss: 0.010298826731741428  PSNR: 24.344947814941406
[TRAIN] Iter: 72400 Loss: 0.00757400318980217  PSNR: 25.719715118408203
[TRAIN] Iter: 72500 Loss: 0.008565608412027359  PSNR: 24.70489501953125
[TRAIN] Iter: 72600 Loss: 0.012234993278980255  PSNR: 23.312896728515625
[TRAIN] Iter: 72700 Loss: 0.012685468420386314  PSNR: 23.344585418701172
[TRAIN] Iter: 72800 Loss: 0.010417901910841465  PSNR: 24.134235382080078
[TRAIN] Iter: 72900 Loss: 0.012218701653182507  PSNR: 23.41140365600586
[TRAIN] Iter: 73000 Loss: 0.006891514174640179  PSNR: 26.52747344970703
[TRAIN] Iter: 73100 Loss: 0.00929657556116581  PSNR: 25.024320602416992
[TRAIN] Iter: 73200 Loss: 0.008144514635205269  PSNR: 25.50123405456543
[TRAIN] Iter: 73300 Loss: 0.009667669422924519  PSNR: 24.707134246826172
[TRAIN] Iter: 73400 Loss: 0.010632839053869247  PSNR: 24.26272964477539
[TRAIN] Iter: 73500 Loss: 0.0090784365311265  PSNR: 24.791990280151367
[TRAIN] Iter: 73600 Loss: 0.007631647400557995  PSNR: 26.361034393310547
[TRAIN] Iter: 73700 Loss: 0.006977529730647802  PSNR: 26.78547477722168
[TRAIN] Iter: 73800 Loss: 0.011454611085355282  PSNR: 23.760515213012695
[TRAIN] Iter: 73900 Loss: 0.008043047972023487  PSNR: 25.203006744384766
[TRAIN] Iter: 74000 Loss: 0.007239812053740025  PSNR: 26.59613609313965
[TRAIN] Iter: 74100 Loss: 0.006701589561998844  PSNR: 26.271583557128906
[TRAIN] Iter: 74200 Loss: 0.01071230135858059  PSNR: 23.92442512512207
[TRAIN] Iter: 74300 Loss: 0.010207303799688816  PSNR: 24.091768264770508
[TRAIN] Iter: 74400 Loss: 0.007829258218407631  PSNR: 26.081270217895508
[TRAIN] Iter: 74500 Loss: 0.009075958281755447  PSNR: 25.396825790405273
[TRAIN] Iter: 74600 Loss: 0.011581980623304844  PSNR: 23.413135528564453
[TRAIN] Iter: 74700 Loss: 0.008955676108598709  PSNR: 25.221046447753906
[TRAIN] Iter: 74800 Loss: 0.01059296727180481  PSNR: 24.159833908081055
[TRAIN] Iter: 74900 Loss: 0.008237635716795921  PSNR: 26.302465438842773
[TRAIN] Iter: 75000 Loss: 0.012332038022577763  PSNR: 23.513063430786133
[TRAIN] Iter: 75100 Loss: 0.008850311860442162  PSNR: 24.99473762512207
[TRAIN] Iter: 75200 Loss: 0.008301087655127048  PSNR: 25.853622436523438
[TRAIN] Iter: 75300 Loss: 0.009717618115246296  PSNR: 24.653324127197266
[TRAIN] Iter: 75400 Loss: 0.009658625349402428  PSNR: 24.531307220458984
[TRAIN] Iter: 75500 Loss: 0.00574349332600832  PSNR: 27.332029342651367
[TRAIN] Iter: 75600 Loss: 0.007874011062085629  PSNR: 25.70369529724121
[TRAIN] Iter: 75700 Loss: 0.010104775428771973  PSNR: 24.411802291870117
[TRAIN] Iter: 75800 Loss: 0.010436823591589928  PSNR: 23.856590270996094
[TRAIN] Iter: 75900 Loss: 0.008588019758462906  PSNR: 24.59173583984375
[TRAIN] Iter: 76000 Loss: 0.009083614684641361  PSNR: 25.444515228271484
[TRAIN] Iter: 76100 Loss: 0.008144832216203213  PSNR: 25.78023910522461
[TRAIN] Iter: 76200 Loss: 0.009478773921728134  PSNR: 25.157930374145508
[TRAIN] Iter: 76300 Loss: 0.00730533991008997  PSNR: 26.25855255126953
[TRAIN] Iter: 76400 Loss: 0.007392049767076969  PSNR: 26.140047073364258
[TRAIN] Iter: 76500 Loss: 0.009297864511609077  PSNR: 24.86371612548828
[TRAIN] Iter: 76600 Loss: 0.007718619890511036  PSNR: 25.812774658203125
[TRAIN] Iter: 76700 Loss: 0.010781629011034966  PSNR: 23.808496475219727
[TRAIN] Iter: 76800 Loss: 0.00961170345544815  PSNR: 24.291080474853516
[TRAIN] Iter: 76900 Loss: 0.007475659716874361  PSNR: 25.735843658447266
[TRAIN] Iter: 77000 Loss: 0.00837461557239294  PSNR: 25.666406631469727
[TRAIN] Iter: 77100 Loss: 0.00831560604274273  PSNR: 25.002925872802734
[TRAIN] Iter: 77200 Loss: 0.010183043777942657  PSNR: 24.1416072845459
[TRAIN] Iter: 77300 Loss: 0.009218145161867142  PSNR: 24.65467071533203
[TRAIN] Iter: 77400 Loss: 0.00982721708714962  PSNR: 24.004627227783203
[TRAIN] Iter: 77500 Loss: 0.00849865935742855  PSNR: 25.230926513671875
[TRAIN] Iter: 77600 Loss: 0.010233037173748016  PSNR: 23.920074462890625
[TRAIN] Iter: 77700 Loss: 0.010778455063700676  PSNR: 23.73007583618164
[TRAIN] Iter: 77800 Loss: 0.009886907413601875  PSNR: 23.847497940063477
[TRAIN] Iter: 77900 Loss: 0.00965291727334261  PSNR: 24.6191349029541
[TRAIN] Iter: 78000 Loss: 0.010532889515161514  PSNR: 23.75156021118164
[TRAIN] Iter: 78100 Loss: 0.010054490529000759  PSNR: 24.554821014404297
[TRAIN] Iter: 78200 Loss: 0.009591899812221527  PSNR: 24.474349975585938
[TRAIN] Iter: 78300 Loss: 0.01233804039657116  PSNR: 23.663124084472656
[TRAIN] Iter: 78400 Loss: 0.009579853154718876  PSNR: 24.439590454101562
[TRAIN] Iter: 78500 Loss: 0.009955689311027527  PSNR: 24.256162643432617
[TRAIN] Iter: 78600 Loss: 0.008052511140704155  PSNR: 26.142786026000977
[TRAIN] Iter: 78700 Loss: 0.009969053789973259  PSNR: 24.576305389404297
[TRAIN] Iter: 78800 Loss: 0.010822431184351444  PSNR: 23.88030433654785
[TRAIN] Iter: 78900 Loss: 0.008241133764386177  PSNR: 25.74576187133789
[TRAIN] Iter: 79000 Loss: 0.01010570116341114  PSNR: 24.29957389831543
[TRAIN] Iter: 79100 Loss: 0.011488585732877254  PSNR: 23.78619384765625
[TRAIN] Iter: 79200 Loss: 0.008039981126785278  PSNR: 25.210893630981445
[TRAIN] Iter: 79300 Loss: 0.011047637090086937  PSNR: 23.84951400756836
[TRAIN] Iter: 79400 Loss: 0.007817905396223068  PSNR: 26.068618774414062
[TRAIN] Iter: 79500 Loss: 0.009771250188350677  PSNR: 24.6182804107666
[TRAIN] Iter: 79600 Loss: 0.008867273107171059  PSNR: 24.88971710205078
[TRAIN] Iter: 79700 Loss: 0.007488884497433901  PSNR: 25.97121810913086
[TRAIN] Iter: 79800 Loss: 0.009795802645385265  PSNR: 24.681842803955078
[TRAIN] Iter: 79900 Loss: 0.007479761727154255  PSNR: 26.143709182739258
Saved checkpoints at ./logs/TUT-KE101-nerf/080000.tar
[TRAIN] Iter: 80000 Loss: 0.007628851104527712  PSNR: 25.503305435180664
[TRAIN] Iter: 80100 Loss: 0.009852360934019089  PSNR: 24.02910041809082
[TRAIN] Iter: 80200 Loss: 0.0071634091436862946  PSNR: 26.560365676879883
[TRAIN] Iter: 80300 Loss: 0.010849187150597572  PSNR: 23.887531280517578
[TRAIN] Iter: 80400 Loss: 0.010520193725824356  PSNR: 23.97627830505371
[TRAIN] Iter: 80500 Loss: 0.00924106128513813  PSNR: 24.451641082763672
[TRAIN] Iter: 80600 Loss: 0.00857785064727068  PSNR: 25.52651596069336
[TRAIN] Iter: 80700 Loss: 0.0094035929068923  PSNR: 24.420921325683594
[TRAIN] Iter: 80800 Loss: 0.007743960712105036  PSNR: 25.313650131225586
[TRAIN] Iter: 80900 Loss: 0.009007565677165985  PSNR: 24.86026954650879
[TRAIN] Iter: 81000 Loss: 0.008797340095043182  PSNR: 24.604324340820312
[TRAIN] Iter: 81100 Loss: 0.007778886239975691  PSNR: 25.303686141967773
[TRAIN] Iter: 81200 Loss: 0.00824566651135683  PSNR: 25.374984741210938
[TRAIN] Iter: 81300 Loss: 0.006854206323623657  PSNR: 26.196380615234375
[TRAIN] Iter: 81400 Loss: 0.010295039974153042  PSNR: 24.148693084716797
[TRAIN] Iter: 81500 Loss: 0.010082794353365898  PSNR: 24.323043823242188
[TRAIN] Iter: 81600 Loss: 0.00934060849249363  PSNR: 24.58896827697754
[TRAIN] Iter: 81700 Loss: 0.010171689093112946  PSNR: 24.381540298461914
[TRAIN] Iter: 81800 Loss: 0.008096284233033657  PSNR: 24.99091911315918
[TRAIN] Iter: 81900 Loss: 0.006922970525920391  PSNR: 25.90326690673828
[TRAIN] Iter: 82000 Loss: 0.010326149873435497  PSNR: 23.84822654724121
[TRAIN] Iter: 82100 Loss: 0.006116589531302452  PSNR: 26.800491333007812
[TRAIN] Iter: 82200 Loss: 0.008602732792496681  PSNR: 24.95366859436035
[TRAIN] Iter: 82300 Loss: 0.008229824714362621  PSNR: 26.07044792175293
[TRAIN] Iter: 82400 Loss: 0.010804742574691772  PSNR: 23.759275436401367
[TRAIN] Iter: 82500 Loss: 0.006651757284998894  PSNR: 25.858484268188477
[TRAIN] Iter: 82600 Loss: 0.007561652921140194  PSNR: 25.82671546936035
[TRAIN] Iter: 82700 Loss: 0.009774242527782917  PSNR: 24.674518585205078
[TRAIN] Iter: 82800 Loss: 0.007720705587416887  PSNR: 25.318256378173828
[TRAIN] Iter: 82900 Loss: 0.009471136145293713  PSNR: 24.42188835144043
[TRAIN] Iter: 83000 Loss: 0.011138327419757843  PSNR: 23.315690994262695
[TRAIN] Iter: 83100 Loss: 0.008788982406258583  PSNR: 24.86772346496582
[TRAIN] Iter: 83200 Loss: 0.011394571512937546  PSNR: 24.075284957885742
[TRAIN] Iter: 83300 Loss: 0.008941460400819778  PSNR: 24.859642028808594
[TRAIN] Iter: 83400 Loss: 0.00936206802725792  PSNR: 24.646745681762695
[TRAIN] Iter: 83500 Loss: 0.009451981633901596  PSNR: 24.685836791992188
[TRAIN] Iter: 83600 Loss: 0.010772200301289558  PSNR: 24.08927345275879
[TRAIN] Iter: 83700 Loss: 0.009159701876342297  PSNR: 24.95615005493164
[TRAIN] Iter: 83800 Loss: 0.008668052032589912  PSNR: 24.61164665222168
[TRAIN] Iter: 83900 Loss: 0.007268617395311594  PSNR: 26.197473526000977
[TRAIN] Iter: 84000 Loss: 0.006022986955940723  PSNR: 26.85282325744629
[TRAIN] Iter: 84100 Loss: 0.007831243798136711  PSNR: 25.791276931762695
[TRAIN] Iter: 84200 Loss: 0.008938943967223167  PSNR: 25.3966007232666
[TRAIN] Iter: 84300 Loss: 0.008816014975309372  PSNR: 24.62555694580078
[TRAIN] Iter: 84400 Loss: 0.009977342560887337  PSNR: 24.419198989868164
[TRAIN] Iter: 84500 Loss: 0.007817092351615429  PSNR: 25.50720977783203
[TRAIN] Iter: 84600 Loss: 0.007313402369618416  PSNR: 26.359649658203125
[TRAIN] Iter: 84700 Loss: 0.007888033986091614  PSNR: 24.867982864379883
[TRAIN] Iter: 84800 Loss: 0.011833807453513145  PSNR: 23.414216995239258
[TRAIN] Iter: 84900 Loss: 0.007184246554970741  PSNR: 26.601980209350586
[TRAIN] Iter: 85000 Loss: 0.006799123249948025  PSNR: 26.250690460205078
[TRAIN] Iter: 85100 Loss: 0.011683802120387554  PSNR: 23.37236213684082
[TRAIN] Iter: 85200 Loss: 0.00937788188457489  PSNR: 24.629316329956055
[TRAIN] Iter: 85300 Loss: 0.007224168162792921  PSNR: 26.36240577697754
[TRAIN] Iter: 85400 Loss: 0.006679222919046879  PSNR: 26.411718368530273
[TRAIN] Iter: 85500 Loss: 0.007197323255240917  PSNR: 25.984750747680664
[TRAIN] Iter: 85600 Loss: 0.006994790397584438  PSNR: 26.311824798583984
[TRAIN] Iter: 85700 Loss: 0.009700929746031761  PSNR: 24.304119110107422
[TRAIN] Iter: 85800 Loss: 0.008587944321334362  PSNR: 24.58400535583496
[TRAIN] Iter: 85900 Loss: 0.008837005123496056  PSNR: 24.660900115966797
[TRAIN] Iter: 86000 Loss: 0.007536476477980614  PSNR: 25.926498413085938
[TRAIN] Iter: 86100 Loss: 0.011292334645986557  PSNR: 23.71101951599121
[TRAIN] Iter: 86200 Loss: 0.00888154562562704  PSNR: 24.709115982055664
[TRAIN] Iter: 86300 Loss: 0.005966628901660442  PSNR: 27.419458389282227
[TRAIN] Iter: 86400 Loss: 0.011485423892736435  PSNR: 23.45807456970215
[TRAIN] Iter: 86500 Loss: 0.008444594219326973  PSNR: 25.452251434326172
[TRAIN] Iter: 86600 Loss: 0.009705506265163422  PSNR: 24.201671600341797
[TRAIN] Iter: 86700 Loss: 0.009618219919502735  PSNR: 25.020732879638672
[TRAIN] Iter: 86800 Loss: 0.00874305609613657  PSNR: 24.90239143371582
[TRAIN] Iter: 86900 Loss: 0.010665894486010075  PSNR: 24.204206466674805
[TRAIN] Iter: 87000 Loss: 0.006618984043598175  PSNR: 26.4219970703125
[TRAIN] Iter: 87100 Loss: 0.008837909437716007  PSNR: 24.953983306884766
[TRAIN] Iter: 87200 Loss: 0.009775425307452679  PSNR: 24.336851119995117
[TRAIN] Iter: 87300 Loss: 0.010408077389001846  PSNR: 24.24954605102539
[TRAIN] Iter: 87400 Loss: 0.006659294478595257  PSNR: 26.360416412353516
[TRAIN] Iter: 87500 Loss: 0.00972071010619402  PSNR: 24.464305877685547
[TRAIN] Iter: 87600 Loss: 0.006535184569656849  PSNR: 26.75132942199707
[TRAIN] Iter: 87700 Loss: 0.008932431228458881  PSNR: 24.75250244140625
[TRAIN] Iter: 87800 Loss: 0.008825751021504402  PSNR: 24.859710693359375
[TRAIN] Iter: 87900 Loss: 0.007626397535204887  PSNR: 25.281518936157227
[TRAIN] Iter: 88000 Loss: 0.008705893531441689  PSNR: 24.426847457885742
[TRAIN] Iter: 88100 Loss: 0.009233081713318825  PSNR: 24.59513282775879
[TRAIN] Iter: 88200 Loss: 0.008670260198414326  PSNR: 25.151243209838867
[TRAIN] Iter: 88300 Loss: 0.010949358344078064  PSNR: 23.803125381469727
[TRAIN] Iter: 88400 Loss: 0.00701942341402173  PSNR: 26.04541778564453
[TRAIN] Iter: 88500 Loss: 0.010408636182546616  PSNR: 24.381040573120117
[TRAIN] Iter: 88600 Loss: 0.006554393097758293  PSNR: 26.871294021606445
[TRAIN] Iter: 88700 Loss: 0.00747019425034523  PSNR: 25.927749633789062
[TRAIN] Iter: 88800 Loss: 0.010325849056243896  PSNR: 24.05879783630371
[TRAIN] Iter: 88900 Loss: 0.008257969282567501  PSNR: 25.25172996520996
[TRAIN] Iter: 89000 Loss: 0.008411760441958904  PSNR: 24.87626075744629
[TRAIN] Iter: 89100 Loss: 0.00795800518244505  PSNR: 25.35333824157715
[TRAIN] Iter: 89200 Loss: 0.008548544719815254  PSNR: 24.83554458618164
[TRAIN] Iter: 89300 Loss: 0.008813861757516861  PSNR: 24.128883361816406
[TRAIN] Iter: 89400 Loss: 0.008255496621131897  PSNR: 25.76400375366211
[TRAIN] Iter: 89500 Loss: 0.008323964662849903  PSNR: 25.70542335510254
[TRAIN] Iter: 89600 Loss: 0.006379994563758373  PSNR: 27.070344924926758
[TRAIN] Iter: 89700 Loss: 0.00625345716252923  PSNR: 26.647871017456055
[TRAIN] Iter: 89800 Loss: 0.006887123454362154  PSNR: 25.82985496520996
[TRAIN] Iter: 89900 Loss: 0.007366143632680178  PSNR: 25.74934196472168
Saved checkpoints at ./logs/TUT-KE101-nerf/090000.tar
[TRAIN] Iter: 90000 Loss: 0.008878405205905437  PSNR: 24.659311294555664
[TRAIN] Iter: 90100 Loss: 0.010273247957229614  PSNR: 24.389089584350586
[TRAIN] Iter: 90200 Loss: 0.01093090046197176  PSNR: 23.698726654052734
[TRAIN] Iter: 90300 Loss: 0.008021444082260132  PSNR: 25.22065544128418
[TRAIN] Iter: 90400 Loss: 0.00845107901841402  PSNR: 25.201040267944336
[TRAIN] Iter: 90500 Loss: 0.008450358174741268  PSNR: 25.016582489013672
[TRAIN] Iter: 90600 Loss: 0.009086593985557556  PSNR: 24.505189895629883
[TRAIN] Iter: 90700 Loss: 0.006900353357195854  PSNR: 26.21344757080078
[TRAIN] Iter: 90800 Loss: 0.008111930452287197  PSNR: 25.023122787475586
[TRAIN] Iter: 90900 Loss: 0.006320439279079437  PSNR: 26.722797393798828
[TRAIN] Iter: 91000 Loss: 0.011722107417881489  PSNR: 23.772748947143555
[TRAIN] Iter: 91100 Loss: 0.00633610226213932  PSNR: 27.009843826293945
[TRAIN] Iter: 91200 Loss: 0.00926794670522213  PSNR: 24.544567108154297
[TRAIN] Iter: 91300 Loss: 0.007667685393244028  PSNR: 25.59755516052246
[TRAIN] Iter: 91400 Loss: 0.011227968148887157  PSNR: 24.08561134338379
[TRAIN] Iter: 91500 Loss: 0.009898471646010876  PSNR: 24.24268913269043
[TRAIN] Iter: 91600 Loss: 0.005934674292802811  PSNR: 27.016740798950195
[TRAIN] Iter: 91700 Loss: 0.009652704000473022  PSNR: 24.544591903686523
[TRAIN] Iter: 91800 Loss: 0.00751947145909071  PSNR: 25.90407371520996
[TRAIN] Iter: 91900 Loss: 0.008533862419426441  PSNR: 25.134065628051758
[TRAIN] Iter: 92000 Loss: 0.005315507296472788  PSNR: 27.626087188720703
[TRAIN] Iter: 92100 Loss: 0.009066332131624222  PSNR: 24.807985305786133
[TRAIN] Iter: 92200 Loss: 0.009381194598972797  PSNR: 24.942102432250977
[TRAIN] Iter: 92300 Loss: 0.007001547142863274  PSNR: 26.729660034179688
[TRAIN] Iter: 92400 Loss: 0.008876490406692028  PSNR: 24.933115005493164
[TRAIN] Iter: 92500 Loss: 0.006065707188099623  PSNR: 26.451921463012695
[TRAIN] Iter: 92600 Loss: 0.008518312126398087  PSNR: 24.80780029296875
[TRAIN] Iter: 92700 Loss: 0.009154191240668297  PSNR: 25.641067504882812
[TRAIN] Iter: 92800 Loss: 0.007896766997873783  PSNR: 25.4591121673584
[TRAIN] Iter: 92900 Loss: 0.010320743545889854  PSNR: 24.02686309814453
[TRAIN] Iter: 93000 Loss: 0.005698224995285273  PSNR: 27.438322067260742
[TRAIN] Iter: 93100 Loss: 0.007883102633059025  PSNR: 25.459304809570312
[TRAIN] Iter: 93200 Loss: 0.0069162314757704735  PSNR: 26.853450775146484
[TRAIN] Iter: 93300 Loss: 0.006714268587529659  PSNR: 26.636327743530273
[TRAIN] Iter: 93400 Loss: 0.008253025822341442  PSNR: 25.166189193725586
[TRAIN] Iter: 93500 Loss: 0.00981329195201397  PSNR: 24.649137496948242
[TRAIN] Iter: 93600 Loss: 0.008030872792005539  PSNR: 25.840097427368164
[TRAIN] Iter: 93700 Loss: 0.007913729175925255  PSNR: 26.23324966430664
[TRAIN] Iter: 93800 Loss: 0.009536397643387318  PSNR: 24.815959930419922
[TRAIN] Iter: 93900 Loss: 0.00955960527062416  PSNR: 24.683984756469727
[TRAIN] Iter: 94000 Loss: 0.006895103026181459  PSNR: 26.920692443847656
[TRAIN] Iter: 94100 Loss: 0.0106289591640234  PSNR: 24.294178009033203
[TRAIN] Iter: 94200 Loss: 0.009021731093525887  PSNR: 24.64726448059082
[TRAIN] Iter: 94300 Loss: 0.01015832182019949  PSNR: 24.35526466369629
[TRAIN] Iter: 94400 Loss: 0.005330492742359638  PSNR: 28.280872344970703
[TRAIN] Iter: 94500 Loss: 0.006894959602504969  PSNR: 27.03500747680664
[TRAIN] Iter: 94600 Loss: 0.005618089810013771  PSNR: 27.39439582824707
[TRAIN] Iter: 94700 Loss: 0.009802618995308876  PSNR: 24.00344467163086
[TRAIN] Iter: 94800 Loss: 0.00749884732067585  PSNR: 26.40445327758789
[TRAIN] Iter: 94900 Loss: 0.006659284699708223  PSNR: 27.02059555053711
[TRAIN] Iter: 95000 Loss: 0.008156337775290012  PSNR: 25.266584396362305
[TRAIN] Iter: 95100 Loss: 0.006308374460786581  PSNR: 26.91657829284668
[TRAIN] Iter: 95200 Loss: 0.008020063862204552  PSNR: 24.88083267211914
[TRAIN] Iter: 95300 Loss: 0.008865926414728165  PSNR: 24.80146598815918
[TRAIN] Iter: 95400 Loss: 0.0075024631805717945  PSNR: 25.948637008666992
[TRAIN] Iter: 95500 Loss: 0.00844581238925457  PSNR: 24.825767517089844
[TRAIN] Iter: 95600 Loss: 0.007454829290509224  PSNR: 26.498779296875
[TRAIN] Iter: 95700 Loss: 0.009121198207139969  PSNR: 24.631959915161133
[TRAIN] Iter: 95800 Loss: 0.006141307298094034  PSNR: 26.746522903442383
[TRAIN] Iter: 95900 Loss: 0.008671834133565426  PSNR: 25.346967697143555
[TRAIN] Iter: 96000 Loss: 0.007749938406050205  PSNR: 26.293380737304688
[TRAIN] Iter: 96100 Loss: 0.007711063604801893  PSNR: 26.352306365966797
[TRAIN] Iter: 96200 Loss: 0.01001514121890068  PSNR: 24.48316192626953
[TRAIN] Iter: 96300 Loss: 0.008934671059250832  PSNR: 24.660024642944336
[TRAIN] Iter: 96400 Loss: 0.006954836659133434  PSNR: 26.4133358001709
[TRAIN] Iter: 96500 Loss: 0.006284141913056374  PSNR: 26.830570220947266
[TRAIN] Iter: 96600 Loss: 0.00899968110024929  PSNR: 24.95733642578125
[TRAIN] Iter: 96700 Loss: 0.0077138920314610004  PSNR: 25.8604679107666
[TRAIN] Iter: 96800 Loss: 0.008557824417948723  PSNR: 25.40311622619629
[TRAIN] Iter: 96900 Loss: 0.007147400174289942  PSNR: 25.395458221435547
[TRAIN] Iter: 97000 Loss: 0.008453577756881714  PSNR: 25.20722770690918
[TRAIN] Iter: 97100 Loss: 0.008951700292527676  PSNR: 24.81841278076172
[TRAIN] Iter: 97200 Loss: 0.006897451356053352  PSNR: 26.564762115478516
[TRAIN] Iter: 97300 Loss: 0.006441114004701376  PSNR: 26.979354858398438
[TRAIN] Iter: 97400 Loss: 0.0072946129366755486  PSNR: 25.68561553955078
[TRAIN] Iter: 97500 Loss: 0.006501053459942341  PSNR: 27.029699325561523
[TRAIN] Iter: 97600 Loss: 0.007485776208341122  PSNR: 26.06928253173828
[TRAIN] Iter: 97700 Loss: 0.006951327435672283  PSNR: 25.870534896850586
[TRAIN] Iter: 97800 Loss: 0.0063758911564946175  PSNR: 26.448040008544922
[TRAIN] Iter: 97900 Loss: 0.007921582087874413  PSNR: 25.13943099975586
[TRAIN] Iter: 98000 Loss: 0.006030149757862091  PSNR: 27.184804916381836
[TRAIN] Iter: 98100 Loss: 0.007986421696841717  PSNR: 25.4995174407959
[TRAIN] Iter: 98200 Loss: 0.006571554113179445  PSNR: 27.01791763305664
[TRAIN] Iter: 98300 Loss: 0.009678944945335388  PSNR: 24.68046760559082
[TRAIN] Iter: 98400 Loss: 0.007499313913285732  PSNR: 26.389354705810547
[TRAIN] Iter: 98500 Loss: 0.009156898595392704  PSNR: 25.166536331176758
[TRAIN] Iter: 98600 Loss: 0.006142142694443464  PSNR: 27.16839599609375
[TRAIN] Iter: 98700 Loss: 0.00794963538646698  PSNR: 25.154251098632812
[TRAIN] Iter: 98800 Loss: 0.0071996357291936874  PSNR: 26.438032150268555
[TRAIN] Iter: 98900 Loss: 0.008322973735630512  PSNR: 25.555456161499023
[TRAIN] Iter: 99000 Loss: 0.007573320530354977  PSNR: 25.586889266967773
[TRAIN] Iter: 99100 Loss: 0.005224909167736769  PSNR: 27.850116729736328
[TRAIN] Iter: 99200 Loss: 0.006881432607769966  PSNR: 26.643033981323242
[TRAIN] Iter: 99300 Loss: 0.009033408015966415  PSNR: 24.852869033813477
[TRAIN] Iter: 99400 Loss: 0.007634203415364027  PSNR: 25.60341453552246
[TRAIN] Iter: 99500 Loss: 0.00672348216176033  PSNR: 26.545188903808594
[TRAIN] Iter: 99600 Loss: 0.00666792131960392  PSNR: 27.089120864868164
[TRAIN] Iter: 99700 Loss: 0.007142260670661926  PSNR: 27.813236236572266
[TRAIN] Iter: 99800 Loss: 0.006617741193622351  PSNR: 27.25617790222168
[TRAIN] Iter: 99900 Loss: 0.006837698630988598  PSNR: 26.66375732421875
Saved checkpoints at ./logs/TUT-KE101-nerf/100000.tar
0 0.00035572052001953125
torch.Size([320, 640, 3]) torch.Size([320, 640])
1 13.38489556312561
2 15.483305215835571
3 13.04317307472229
4 15.512071132659912
5 13.096792936325073
6 13.339233636856079
7 15.484870195388794
8 12.919960498809814
9 15.961586475372314
10 12.975353956222534
11 13.173078060150146
12 15.54736852645874
13 12.980615615844727
14 15.746647834777832
15 12.871145009994507
16 15.511081457138062
17 13.124858617782593
18 13.241288900375366
19 15.639489650726318
20 12.793497323989868
21 15.821799278259277
22 13.039330959320068
23 13.145749807357788
24 15.591103553771973
25 12.867956638336182
26 15.674893140792847
27 13.057022333145142
28 13.371013402938843
29 15.566158771514893
30 13.385401487350464
31 14.979393243789673
32 13.331322193145752
33 15.249147891998291
34 13.344327449798584
35 13.375778198242188
36 15.225792407989502
37 13.597424507141113
38 17.161392211914062
39 15.186543226242065
40 17.08141040802002
41 15.24050784111023
42 17.119373559951782
43 15.243070125579834
44 17.116639852523804
45 15.235186100006104
46 15.288633584976196
47 17.145310640335083
48 15.465516090393066
49 17.5852952003479
50 15.69913363456726
51 17.235576629638672
52 15.226303577423096
53 17.120760917663574
54 15.279499530792236
55 17.143614768981934
56 15.281256198883057
57 17.163289546966553
58 15.065682172775269
59 17.283289194107056
60 15.222938537597656
61 17.122241020202637
62 15.202927112579346
63 17.158342599868774
64 15.244384765625
65 15.1915762424469
66 17.123079538345337
67 15.17260193824768
68 17.21374201774597
69 15.190441370010376
70 17.20051646232605
71 14.976401805877686
72 17.331425189971924
73 15.193876028060913
74 17.18465757369995
75 15.157477855682373
76 17.240182161331177
77 15.142147302627563
78 17.271660566329956
79 15.084027528762817
80 17.321993589401245
81 15.097348928451538
82 15.045713663101196
83 17.593381643295288
84 14.723007917404175
85 17.849826097488403
86 14.582887172698975
87 17.754072904586792
88 14.774264097213745
89 18.351074934005737
90 14.946925163269043
91 17.74326753616333
92 14.726232290267944
93 17.75832223892212
94 14.77964186668396
95 17.49906635284424
96 14.870362997055054
97 15.181608200073242
98 17.24983501434326
99 15.01902437210083
100 17.391956329345703
101 14.93706727027893
102 17.525413751602173
103 14.876587867736816
104 17.586930751800537
105 14.637170553207397
106 17.729642868041992
107 15.111891746520996
108 17.238115549087524
109 15.122071266174316
110 17.273126125335693
111 15.156369924545288
112 15.107763290405273
113 17.30948281288147
114 15.107991218566895
115 17.340282678604126
116 15.061861991882324
117 17.306381940841675
118 15.103076219558716
119 17.287200927734375
Done, saving (120, 320, 640, 3) (120, 320, 640)
extras:{'raw': tensor([[[ 3.3524e+00,  3.6960e+00,  5.1571e+00, -2.6379e+01],
         [-1.8301e-01, -4.3643e-01, -8.8142e-01, -1.5061e+01],
         [-1.6618e-01, -4.1999e-01, -8.7149e-01, -1.4272e+01],
         ...,
         [ 6.5797e+00,  3.5692e+00,  5.7182e+00,  5.2838e+02],
         [ 6.4121e+00,  3.6861e+00,  5.4567e+00,  5.2466e+02],
         [ 7.1213e+00,  4.0236e+00,  6.4336e+00,  4.9915e+02]],

        [[-1.0390e+00, -5.9839e-01, -2.6282e-01, -3.6153e+01],
         [-1.2173e+00, -8.2634e-01, -6.0752e-01, -4.5337e+01],
         [-9.0272e-02, -1.1954e-01, -5.1099e-02, -2.3743e+01],
         ...,
         [-2.2658e+00, -4.7451e+00, -1.0861e+01, -2.0042e+02],
         [-2.8973e+00, -5.6365e+00, -1.2394e+01, -2.0463e+02],
         [-2.8316e+00, -5.5992e+00, -1.2384e+01, -2.0126e+02]],

        [[-3.8037e+00, -3.5974e+00, -2.9579e+00, -3.0026e+01],
         [-3.9004e+00, -3.2866e+00, -1.9401e+00, -3.2720e+01],
         [-1.4765e+00, -1.1939e+00, -6.5329e-01, -2.5335e+01],
         ...,
         [-1.5639e+01, -1.7578e+01, -2.4398e+01, -2.4913e+02],
         [-1.6460e+01, -1.7849e+01, -2.4215e+01, -2.3237e+02],
         [-1.8967e+01, -1.9301e+01, -2.4802e+01, -1.7125e+02]],

        ...,

        [[-3.3361e+00, -2.6954e+00, -3.5637e-01, -3.1378e+01],
         [-2.3213e+00, -2.1658e+00, -1.4404e+00, -2.5884e+01],
         [-2.8189e-01, -1.6524e-01, -2.2685e-02, -1.0153e+01],
         ...,
         [-2.0193e+01, -2.0178e+01, -2.2214e+01, -2.5020e+02],
         [-2.1322e+01, -2.2613e+01, -2.6513e+01, -3.2585e+02],
         [-2.1881e+01, -2.1228e+01, -2.3653e+01, -2.2132e+02]],

        [[ 2.9118e-01,  6.0057e-01,  1.8874e+00, -2.6415e+01],
         [-1.9162e+00, -1.9058e+00, -1.9881e+00, -3.4744e+01],
         [-3.0155e-01, -3.5687e-01, -4.2858e-01, -1.8914e+01],
         ...,
         [-2.4215e+01, -2.1099e+01, -2.0174e+01, -1.0893e+02],
         [-2.3010e+01, -2.1434e+01, -2.2703e+01, -1.8553e+02],
         [-2.4838e+01, -2.1062e+01, -1.9514e+01, -9.6799e+01]],

        [[-2.9803e-02,  2.8813e-01,  1.5416e+00, -2.5810e+01],
         [-1.2046e+00, -6.2851e-01,  2.7911e-01, -4.1558e+01],
         [-1.4260e+00, -9.0698e-01, -3.7855e-01, -4.2049e+01],
         ...,
         [ 1.7999e+00, -7.0806e-01, -6.9057e+00, -1.7683e+02],
         [ 1.4476e+00, -1.4289e+00, -8.5682e+00, -1.6947e+02],
         [ 1.4661e+00, -1.4471e+00, -8.7422e+00, -1.4778e+02]]],
       grad_fn=<ReshapeAliasBackward0>), 'rgb0': tensor([[0.4600, 0.4070, 0.3398],
        [0.1073, 0.1182, 0.1187],
        [0.2996, 0.2761, 0.2315],
        ...,
        [0.4177, 0.3998, 0.3691],
        [0.4839, 0.4092, 0.2799],
        [0.3602, 0.3398, 0.3106]], grad_fn=<ReshapeAliasBackward0>), 'disp0': tensor([88.3177, 70.4754, 23.6451,  ..., 37.4828, 48.9454, 94.8724],
       grad_fn=<ReshapeAliasBackward0>), 'acc0': tensor([1., 1., 1.,  ..., 1., 1., 1.], grad_fn=<ReshapeAliasBackward0>), 'z_std': tensor([0.0030, 0.0026, 0.0181,  ..., 0.0020, 0.0688, 0.0028])}
0 0.00040841102600097656
torch.Size([320, 640, 3]) torch.Size([320, 640])
1 17.35918664932251
2 15.144128561019897
3 17.239815711975098
4 15.119093418121338
5 17.288440465927124
6 15.114424228668213
7 17.290400743484497
8 15.129213809967041
9 17.34286618232727
10 15.07223105430603
11 15.096002101898193
12 17.209139823913574
13 14.94820761680603
14 17.657856702804565
15 15.191413879394531
16 16.997767448425293
17 15.076070785522461
18 17.348538398742676
19 15.041859149932861
20 17.335951566696167
21 15.047132015228271
22 17.642817735671997
23 15.508576393127441
24 17.67685842514038
25 15.077146768569946
26 17.367899656295776
27 15.035327196121216
28 15.235564470291138
29 17.191617250442505
30 15.120936393737793
31 17.355379819869995
32 15.015908479690552
33 17.5290789604187
34 14.788711547851562
35 17.69018864631653
36 14.904269933700562
37 17.402857542037964
38 14.708085298538208
39 17.7535719871521
40 12.4135582447052
41 14.738217830657959
42 17.466326236724854
43 14.736512660980225
44 18.021497011184692
45 14.744900941848755
46 17.394850969314575
47 14.586113929748535
48 18.01273798942566
49 14.543958187103271
50 17.686985969543457
51 14.862907409667969
52 18.097062349319458
53 15.157668352127075
54 17.694798231124878
55 15.16895866394043
56 15.13224196434021
57 17.255162715911865
58 15.052388906478882
59 17.376631259918213
60 15.12011456489563
61 17.228339195251465
62 15.139206171035767
63 17.320366859436035
64 15.066888332366943
65 17.25026798248291
66 15.148926258087158
67 17.315581798553467
68 15.133239030838013
69 17.30686926841736
70 15.034884691238403
71 17.243056058883667
72 15.093180656433105
73 15.08568811416626
74 17.33888840675354
75 15.10562252998352
76 17.28137445449829
77 15.040066480636597
78 17.374504804611206
79 15.038139820098877
80 17.356293439865112
81 15.074993371963501
82 17.3114013671875
83 15.144373178482056
84 17.200230360031128
85 15.178932189941406
86 17.260236501693726
87 15.101635932922363
88 15.150301933288574
89 17.242398500442505
90 15.074187755584717
91 17.578928232192993
92 15.542025804519653
93 17.827245235443115
94 15.251999855041504
95 17.412806272506714
96 15.084364891052246
97 17.296823740005493
98 15.041916370391846
99 17.35962438583374
100 15.046207666397095
101 17.370070695877075
102 15.039652824401855
103 17.286298274993896
104 15.005420684814453
105 15.184978246688843
106 17.232962369918823
107 15.15489912033081
108 17.344018697738647
109 15.043738603591919
110 17.473845958709717
111 14.774266481399536
112 17.774183750152588
113 14.940415143966675
114 17.467746019363403
115 14.594404458999634
116 17.852861404418945
117 14.937602519989014
118 17.277273178100586
119 15.005948066711426
test poses shape torch.Size([4, 3, 4])
0 0.0006313323974609375
torch.Size([320, 640, 3]) torch.Size([320, 640])
1 15.538576126098633
2 14.760523557662964
3 17.33876371383667
Saved test set
[TRAIN] Iter: 100000 Loss: 0.007784530520439148  PSNR: 26.264249801635742
[TRAIN] Iter: 100100 Loss: 0.00620099063962698  PSNR: 26.70367431640625
[TRAIN] Iter: 100200 Loss: 0.006532752420753241  PSNR: 26.58928680419922
[TRAIN] Iter: 100300 Loss: 0.01038934476673603  PSNR: 23.901212692260742
[TRAIN] Iter: 100400 Loss: 0.0067632137797772884  PSNR: 26.8140926361084
[TRAIN] Iter: 100500 Loss: 0.008453627116978168  PSNR: 24.96332359313965
[TRAIN] Iter: 100600 Loss: 0.010189302265644073  PSNR: 24.07602882385254
[TRAIN] Iter: 100700 Loss: 0.008998999372124672  PSNR: 24.55607032775879
[TRAIN] Iter: 100800 Loss: 0.009656894020736217  PSNR: 25.062679290771484
[TRAIN] Iter: 100900 Loss: 0.010295319370925426  PSNR: 24.438488006591797
[TRAIN] Iter: 101000 Loss: 0.010887949727475643  PSNR: 23.820724487304688
[TRAIN] Iter: 101100 Loss: 0.008708644658327103  PSNR: 24.94623374938965
[TRAIN] Iter: 101200 Loss: 0.009481477551162243  PSNR: 24.4896297454834
[TRAIN] Iter: 101300 Loss: 0.008630866184830666  PSNR: 24.93430519104004
[TRAIN] Iter: 101400 Loss: 0.009875146672129631  PSNR: 23.778583526611328
[TRAIN] Iter: 101500 Loss: 0.008013603277504444  PSNR: 25.536218643188477
[TRAIN] Iter: 101600 Loss: 0.009083567187190056  PSNR: 24.88288688659668
[TRAIN] Iter: 101700 Loss: 0.0077974568121135235  PSNR: 25.38173484802246
[TRAIN] Iter: 101800 Loss: 0.006383719854056835  PSNR: 27.065446853637695
[TRAIN] Iter: 101900 Loss: 0.0063657257705926895  PSNR: 26.423608779907227
[TRAIN] Iter: 102000 Loss: 0.008842391893267632  PSNR: 25.082674026489258
[TRAIN] Iter: 102100 Loss: 0.005226370878517628  PSNR: 27.266511917114258
[TRAIN] Iter: 102200 Loss: 0.007460510358214378  PSNR: 26.46723747253418
[TRAIN] Iter: 102300 Loss: 0.008919160813093185  PSNR: 24.9985408782959
[TRAIN] Iter: 102400 Loss: 0.00664729718118906  PSNR: 26.251842498779297
[TRAIN] Iter: 102500 Loss: 0.009317231364548206  PSNR: 24.677738189697266
[TRAIN] Iter: 102600 Loss: 0.008578497916460037  PSNR: 25.438913345336914
[TRAIN] Iter: 102700 Loss: 0.007461960427463055  PSNR: 25.29159164428711
[TRAIN] Iter: 102800 Loss: 0.009030953049659729  PSNR: 25.172203063964844
[TRAIN] Iter: 102900 Loss: 0.008176978677511215  PSNR: 25.776004791259766
[TRAIN] Iter: 103000 Loss: 0.008225143887102604  PSNR: 25.26068687438965
[TRAIN] Iter: 103100 Loss: 0.006695548538118601  PSNR: 27.02825164794922
[TRAIN] Iter: 103200 Loss: 0.0055650207214057446  PSNR: 27.13028335571289
[TRAIN] Iter: 103300 Loss: 0.009257053956389427  PSNR: 24.95826530456543
[TRAIN] Iter: 103400 Loss: 0.005634879227727652  PSNR: 27.614688873291016
[TRAIN] Iter: 103500 Loss: 0.009217234328389168  PSNR: 24.555145263671875
[TRAIN] Iter: 103600 Loss: 0.008718432858586311  PSNR: 25.127452850341797
[TRAIN] Iter: 103700 Loss: 0.005971946287900209  PSNR: 26.705307006835938
[TRAIN] Iter: 103800 Loss: 0.006431386340409517  PSNR: 26.887351989746094
[TRAIN] Iter: 103900 Loss: 0.005478270351886749  PSNR: 27.425527572631836
[TRAIN] Iter: 104000 Loss: 0.006639461498707533  PSNR: 26.518234252929688
[TRAIN] Iter: 104100 Loss: 0.0056195007637143135  PSNR: 27.92096519470215
[TRAIN] Iter: 104200 Loss: 0.009530926123261452  PSNR: 24.58623504638672
[TRAIN] Iter: 104300 Loss: 0.009891694411635399  PSNR: 24.696928024291992
[TRAIN] Iter: 104400 Loss: 0.009688761085271835  PSNR: 24.599010467529297
[TRAIN] Iter: 104500 Loss: 0.006625867914408445  PSNR: 25.619441986083984
[TRAIN] Iter: 104600 Loss: 0.005652629304677248  PSNR: 27.483909606933594
[TRAIN] Iter: 104700 Loss: 0.0077176401391625404  PSNR: 26.766502380371094
[TRAIN] Iter: 104800 Loss: 0.0077401259914040565  PSNR: 25.24954605102539
[TRAIN] Iter: 104900 Loss: 0.006568153854459524  PSNR: 27.176469802856445
[TRAIN] Iter: 105000 Loss: 0.00921531394124031  PSNR: 24.758214950561523
[TRAIN] Iter: 105100 Loss: 0.008358348160982132  PSNR: 25.360530853271484
[TRAIN] Iter: 105200 Loss: 0.005388827063143253  PSNR: 28.259428024291992
[TRAIN] Iter: 105300 Loss: 0.007838410325348377  PSNR: 25.166784286499023
[TRAIN] Iter: 105400 Loss: 0.007477493956685066  PSNR: 25.894699096679688
[TRAIN] Iter: 105500 Loss: 0.009767768904566765  PSNR: 24.487457275390625
[TRAIN] Iter: 105600 Loss: 0.006133785471320152  PSNR: 27.004968643188477
[TRAIN] Iter: 105700 Loss: 0.009261281229555607  PSNR: 25.01198959350586
[TRAIN] Iter: 105800 Loss: 0.006085656583309174  PSNR: 27.4384708404541
[TRAIN] Iter: 105900 Loss: 0.007434522733092308  PSNR: 26.36223602294922
[TRAIN] Iter: 106000 Loss: 0.008630378171801567  PSNR: 24.906558990478516
[TRAIN] Iter: 106100 Loss: 0.007388529367744923  PSNR: 26.2833251953125
[TRAIN] Iter: 106200 Loss: 0.00635168282315135  PSNR: 27.37186050415039
[TRAIN] Iter: 106300 Loss: 0.008743764832615852  PSNR: 24.9664249420166
[TRAIN] Iter: 106400 Loss: 0.007118924520909786  PSNR: 26.18688201904297
[TRAIN] Iter: 106500 Loss: 0.006377874873578548  PSNR: 27.043153762817383
[TRAIN] Iter: 106600 Loss: 0.0072613866068422794  PSNR: 26.278989791870117
[TRAIN] Iter: 106700 Loss: 0.008336641825735569  PSNR: 24.798080444335938
[TRAIN] Iter: 106800 Loss: 0.008091316558420658  PSNR: 25.078317642211914
[TRAIN] Iter: 106900 Loss: 0.008947044610977173  PSNR: 24.89927864074707
[TRAIN] Iter: 107000 Loss: 0.007894827052950859  PSNR: 25.095142364501953
[TRAIN] Iter: 107100 Loss: 0.008647989481687546  PSNR: 25.054847717285156
[TRAIN] Iter: 107200 Loss: 0.007983598858118057  PSNR: 25.08816909790039
[TRAIN] Iter: 107300 Loss: 0.006905371323227882  PSNR: 25.937824249267578
[TRAIN] Iter: 107400 Loss: 0.00824147928506136  PSNR: 24.838802337646484
[TRAIN] Iter: 107500 Loss: 0.005843678489327431  PSNR: 27.47787857055664
[TRAIN] Iter: 107600 Loss: 0.00735894963145256  PSNR: 25.67368507385254
[TRAIN] Iter: 107700 Loss: 0.006066964939236641  PSNR: 27.391279220581055
[TRAIN] Iter: 107800 Loss: 0.008561788126826286  PSNR: 25.26741600036621
[TRAIN] Iter: 107900 Loss: 0.008468475192785263  PSNR: 25.251577377319336
[TRAIN] Iter: 108000 Loss: 0.006095247343182564  PSNR: 27.620378494262695
[TRAIN] Iter: 108100 Loss: 0.00690938625484705  PSNR: 26.951385498046875
[TRAIN] Iter: 108200 Loss: 0.00790490210056305  PSNR: 25.286325454711914
[TRAIN] Iter: 108300 Loss: 0.007139880210161209  PSNR: 26.3669376373291
[TRAIN] Iter: 108400 Loss: 0.008535994216799736  PSNR: 25.223852157592773
[TRAIN] Iter: 108500 Loss: 0.0063357106409966946  PSNR: 27.42407989501953
[TRAIN] Iter: 108600 Loss: 0.007051141932606697  PSNR: 26.23426055908203
[TRAIN] Iter: 108700 Loss: 0.010041040368378162  PSNR: 24.34664535522461
[TRAIN] Iter: 108800 Loss: 0.005994655657559633  PSNR: 27.22327423095703
[TRAIN] Iter: 108900 Loss: 0.00782298855483532  PSNR: 25.443254470825195
[TRAIN] Iter: 109000 Loss: 0.008122771978378296  PSNR: 25.171722412109375
[TRAIN] Iter: 109100 Loss: 0.006030431482940912  PSNR: 26.838193893432617
[TRAIN] Iter: 109200 Loss: 0.008942432701587677  PSNR: 25.065980911254883
[TRAIN] Iter: 109300 Loss: 0.006748140789568424  PSNR: 26.594772338867188
[TRAIN] Iter: 109400 Loss: 0.007674701511859894  PSNR: 25.769386291503906
[TRAIN] Iter: 109500 Loss: 0.007396018132567406  PSNR: 25.750581741333008
[TRAIN] Iter: 109600 Loss: 0.006071415729820728  PSNR: 26.904029846191406
[TRAIN] Iter: 109700 Loss: 0.00823189690709114  PSNR: 25.107481002807617
[TRAIN] Iter: 109800 Loss: 0.008264990523457527  PSNR: 24.981428146362305
[TRAIN] Iter: 109900 Loss: 0.007481342181563377  PSNR: 26.004169464111328
Saved checkpoints at ./logs/TUT-KE101-nerf/110000.tar
[TRAIN] Iter: 110000 Loss: 0.007056333590298891  PSNR: 26.979337692260742
[TRAIN] Iter: 110100 Loss: 0.010094660334289074  PSNR: 24.41257095336914
[TRAIN] Iter: 110200 Loss: 0.009246823377907276  PSNR: 24.728065490722656
[TRAIN] Iter: 110300 Loss: 0.008607150055468082  PSNR: 25.014402389526367
[TRAIN] Iter: 110400 Loss: 0.00918481033295393  PSNR: 24.60641098022461
[TRAIN] Iter: 110500 Loss: 0.00715147890150547  PSNR: 26.991222381591797
[TRAIN] Iter: 110600 Loss: 0.0065590813755989075  PSNR: 26.997045516967773
[TRAIN] Iter: 110700 Loss: 0.00682905875146389  PSNR: 26.43345069885254
[TRAIN] Iter: 110800 Loss: 0.007962968200445175  PSNR: 25.455541610717773
[TRAIN] Iter: 110900 Loss: 0.007823629304766655  PSNR: 25.96619415283203
[TRAIN] Iter: 111000 Loss: 0.007468440104275942  PSNR: 25.752622604370117
[TRAIN] Iter: 111100 Loss: 0.006627380847930908  PSNR: 26.568693161010742
[TRAIN] Iter: 111200 Loss: 0.0055221859365701675  PSNR: 27.182584762573242
[TRAIN] Iter: 111300 Loss: 0.006498158909380436  PSNR: 26.02756690979004
[TRAIN] Iter: 111400 Loss: 0.00694570317864418  PSNR: 26.961584091186523
[TRAIN] Iter: 111500 Loss: 0.0071829394437372684  PSNR: 26.42713737487793
[TRAIN] Iter: 111600 Loss: 0.007661578711122274  PSNR: 25.735702514648438
[TRAIN] Iter: 111700 Loss: 0.008204849436879158  PSNR: 25.153575897216797
[TRAIN] Iter: 111800 Loss: 0.007959529757499695  PSNR: 25.543657302856445
[TRAIN] Iter: 111900 Loss: 0.0063361357897520065  PSNR: 26.920795440673828
[TRAIN] Iter: 112000 Loss: 0.006701729726046324  PSNR: 26.16681480407715
[TRAIN] Iter: 112100 Loss: 0.007812146097421646  PSNR: 25.921524047851562
[TRAIN] Iter: 112200 Loss: 0.007932370528578758  PSNR: 25.63563346862793
[TRAIN] Iter: 112300 Loss: 0.009117748588323593  PSNR: 24.77239990234375
[TRAIN] Iter: 112400 Loss: 0.007702126167714596  PSNR: 25.998762130737305
[TRAIN] Iter: 112500 Loss: 0.00628798408433795  PSNR: 27.08733558654785
[TRAIN] Iter: 112600 Loss: 0.008428841829299927  PSNR: 25.661592483520508
[TRAIN] Iter: 112700 Loss: 0.0085534006357193  PSNR: 25.664875030517578
[TRAIN] Iter: 112800 Loss: 0.008372754789888859  PSNR: 25.108278274536133
[TRAIN] Iter: 112900 Loss: 0.010163335129618645  PSNR: 24.16738510131836
[TRAIN] Iter: 113000 Loss: 0.0066664256155490875  PSNR: 26.5824031829834
[TRAIN] Iter: 113100 Loss: 0.006898615509271622  PSNR: 26.56666374206543
[TRAIN] Iter: 113200 Loss: 0.007662915159016848  PSNR: 25.716320037841797
[TRAIN] Iter: 113300 Loss: 0.008419964462518692  PSNR: 25.279705047607422
[TRAIN] Iter: 113400 Loss: 0.006158030591905117  PSNR: 27.243118286132812
[TRAIN] Iter: 113500 Loss: 0.008299799636006355  PSNR: 25.641878128051758
[TRAIN] Iter: 113600 Loss: 0.006495016627013683  PSNR: 26.617650985717773
[TRAIN] Iter: 113700 Loss: 0.008519340306520462  PSNR: 25.0114803314209
[TRAIN] Iter: 113800 Loss: 0.010113080032169819  PSNR: 24.88360595703125
[TRAIN] Iter: 113900 Loss: 0.006477877032011747  PSNR: 26.936906814575195
[TRAIN] Iter: 114000 Loss: 0.008501144126057625  PSNR: 25.3955078125
[TRAIN] Iter: 114100 Loss: 0.008485903032124043  PSNR: 25.153432846069336
[TRAIN] Iter: 114200 Loss: 0.00665414659306407  PSNR: 26.9521541595459
[TRAIN] Iter: 114300 Loss: 0.007220186293125153  PSNR: 26.204084396362305
[TRAIN] Iter: 114400 Loss: 0.008960735984146595  PSNR: 24.318544387817383
[TRAIN] Iter: 114500 Loss: 0.010186622850596905  PSNR: 25.044660568237305
[TRAIN] Iter: 114600 Loss: 0.005878722760826349  PSNR: 27.104755401611328
[TRAIN] Iter: 114700 Loss: 0.006407109554857016  PSNR: 26.77840232849121
[TRAIN] Iter: 114800 Loss: 0.009447470307350159  PSNR: 24.59406089782715
[TRAIN] Iter: 114900 Loss: 0.008804171346127987  PSNR: 25.514320373535156
[TRAIN] Iter: 115000 Loss: 0.008380584418773651  PSNR: 25.267629623413086
[TRAIN] Iter: 115100 Loss: 0.007822316139936447  PSNR: 25.376832962036133
[TRAIN] Iter: 115200 Loss: 0.005737733095884323  PSNR: 27.65365219116211
[TRAIN] Iter: 115300 Loss: 0.007249568589031696  PSNR: 25.58081817626953
[TRAIN] Iter: 115400 Loss: 0.006985917221754789  PSNR: 26.163333892822266
[TRAIN] Iter: 115500 Loss: 0.007895192131400108  PSNR: 25.852479934692383
[TRAIN] Iter: 115600 Loss: 0.008067484013736248  PSNR: 25.371986389160156
[TRAIN] Iter: 115700 Loss: 0.0072154700756073  PSNR: 26.624330520629883
[TRAIN] Iter: 115800 Loss: 0.006341254338622093  PSNR: 26.897491455078125
[TRAIN] Iter: 115900 Loss: 0.007662195712327957  PSNR: 25.754070281982422
[TRAIN] Iter: 116000 Loss: 0.00852307677268982  PSNR: 25.324872970581055
[TRAIN] Iter: 116100 Loss: 0.009904216974973679  PSNR: 24.78919219970703
[TRAIN] Iter: 116200 Loss: 0.007195420563220978  PSNR: 25.656776428222656
[TRAIN] Iter: 116300 Loss: 0.00838669203221798  PSNR: 25.548328399658203
[TRAIN] Iter: 116400 Loss: 0.008557752706110477  PSNR: 24.93790626525879
[TRAIN] Iter: 116500 Loss: 0.0062877279706299305  PSNR: 27.27046012878418
[TRAIN] Iter: 116600 Loss: 0.008526897989213467  PSNR: 25.367717742919922
[TRAIN] Iter: 116700 Loss: 0.00689702806994319  PSNR: 26.537275314331055
[TRAIN] Iter: 116800 Loss: 0.006046333350241184  PSNR: 26.802738189697266
[TRAIN] Iter: 116900 Loss: 0.005887757055461407  PSNR: 27.540802001953125
[TRAIN] Iter: 117000 Loss: 0.005465956404805183  PSNR: 27.294048309326172
[TRAIN] Iter: 117100 Loss: 0.008807431906461716  PSNR: 25.50227928161621
[TRAIN] Iter: 117200 Loss: 0.007163260597735643  PSNR: 26.1300106048584
[TRAIN] Iter: 117300 Loss: 0.00556164188310504  PSNR: 27.919692993164062
[TRAIN] Iter: 117400 Loss: 0.006100303027778864  PSNR: 27.321731567382812
[TRAIN] Iter: 117500 Loss: 0.007736758328974247  PSNR: 25.72279930114746
[TRAIN] Iter: 117600 Loss: 0.005392746534198523  PSNR: 28.073183059692383
[TRAIN] Iter: 117700 Loss: 0.007906366139650345  PSNR: 25.148401260375977
[TRAIN] Iter: 117800 Loss: 0.008006870746612549  PSNR: 25.301742553710938
[TRAIN] Iter: 117900 Loss: 0.006430245470255613  PSNR: 26.893932342529297
[TRAIN] Iter: 118000 Loss: 0.007023208308964968  PSNR: 26.0079402923584
[TRAIN] Iter: 118100 Loss: 0.005123523995280266  PSNR: 27.548751831054688
[TRAIN] Iter: 118200 Loss: 0.006905009038746357  PSNR: 25.824541091918945
[TRAIN] Iter: 118300 Loss: 0.005403121933341026  PSNR: 27.019123077392578
[TRAIN] Iter: 118400 Loss: 0.005153508856892586  PSNR: 27.509550094604492
[TRAIN] Iter: 118500 Loss: 0.006315121427178383  PSNR: 26.81029510498047
[TRAIN] Iter: 118600 Loss: 0.009433821775019169  PSNR: 24.52048683166504
[TRAIN] Iter: 118700 Loss: 0.006040181498974562  PSNR: 27.465818405151367
[TRAIN] Iter: 118800 Loss: 0.008642671629786491  PSNR: 24.606586456298828
[TRAIN] Iter: 118900 Loss: 0.007701859809458256  PSNR: 25.4544734954834
[TRAIN] Iter: 119000 Loss: 0.007731257006525993  PSNR: 25.944408416748047
[TRAIN] Iter: 119100 Loss: 0.008090700954198837  PSNR: 25.55072021484375
[TRAIN] Iter: 119200 Loss: 0.00849396176636219  PSNR: 25.360322952270508
[TRAIN] Iter: 119300 Loss: 0.006099453195929527  PSNR: 26.76609992980957
[TRAIN] Iter: 119400 Loss: 0.009186387993395329  PSNR: 25.051904678344727
[TRAIN] Iter: 119500 Loss: 0.006752962712198496  PSNR: 26.244401931762695
[TRAIN] Iter: 119600 Loss: 0.0077568478882312775  PSNR: 25.67156410217285
[TRAIN] Iter: 119700 Loss: 0.008248724043369293  PSNR: 25.27392578125
[TRAIN] Iter: 119800 Loss: 0.006984161213040352  PSNR: 26.219749450683594
[TRAIN] Iter: 119900 Loss: 0.008396536111831665  PSNR: 25.15290641784668
Saved checkpoints at ./logs/TUT-KE101-nerf/120000.tar
[TRAIN] Iter: 120000 Loss: 0.00840805098414421  PSNR: 25.173095703125
[TRAIN] Iter: 120100 Loss: 0.008654714561998844  PSNR: 25.496822357177734
[TRAIN] Iter: 120200 Loss: 0.006916571408510208  PSNR: 25.578046798706055
[TRAIN] Iter: 120300 Loss: 0.007845459505915642  PSNR: 25.580005645751953
[TRAIN] Iter: 120400 Loss: 0.0073231118731200695  PSNR: 26.235994338989258
[TRAIN] Iter: 120500 Loss: 0.007020911201834679  PSNR: 26.213937759399414
[TRAIN] Iter: 120600 Loss: 0.008402833715081215  PSNR: 25.149948120117188
[TRAIN] Iter: 120700 Loss: 0.010504938662052155  PSNR: 24.16168212890625
[TRAIN] Iter: 120800 Loss: 0.005302405916154385  PSNR: 27.354551315307617
[TRAIN] Iter: 120900 Loss: 0.008850554004311562  PSNR: 24.881834030151367
[TRAIN] Iter: 121000 Loss: 0.008696421980857849  PSNR: 25.223718643188477
[TRAIN] Iter: 121100 Loss: 0.008380988612771034  PSNR: 25.516637802124023
[TRAIN] Iter: 121200 Loss: 0.010538211092352867  PSNR: 25.019554138183594
[TRAIN] Iter: 121300 Loss: 0.006757427006959915  PSNR: 26.298877716064453
[TRAIN] Iter: 121400 Loss: 0.0074919406324625015  PSNR: 25.535757064819336
[TRAIN] Iter: 121500 Loss: 0.008160384371876717  PSNR: 24.937911987304688
[TRAIN] Iter: 121600 Loss: 0.00793287344276905  PSNR: 25.386287689208984
[TRAIN] Iter: 121700 Loss: 0.00564934778958559  PSNR: 26.79563331604004
[TRAIN] Iter: 121800 Loss: 0.007437193766236305  PSNR: 25.96343421936035
[TRAIN] Iter: 121900 Loss: 0.006961583159863949  PSNR: 25.831247329711914
[TRAIN] Iter: 122000 Loss: 0.006497872993350029  PSNR: 27.156749725341797
[TRAIN] Iter: 122100 Loss: 0.007834610529243946  PSNR: 25.239151000976562
[TRAIN] Iter: 122200 Loss: 0.00925954058766365  PSNR: 24.640087127685547
[TRAIN] Iter: 122300 Loss: 0.0065818182192742825  PSNR: 26.215412139892578
[TRAIN] Iter: 122400 Loss: 0.007595372386276722  PSNR: 25.171091079711914
[TRAIN] Iter: 122500 Loss: 0.00886987242847681  PSNR: 25.248750686645508
[TRAIN] Iter: 122600 Loss: 0.0062147388234734535  PSNR: 27.675752639770508
[TRAIN] Iter: 122700 Loss: 0.006586058996617794  PSNR: 26.647113800048828
[TRAIN] Iter: 122800 Loss: 0.006175565533339977  PSNR: 26.522924423217773
[TRAIN] Iter: 122900 Loss: 0.007962243631482124  PSNR: 25.10175132751465
[TRAIN] Iter: 123000 Loss: 0.006001437548547983  PSNR: 27.554683685302734
[TRAIN] Iter: 123100 Loss: 0.00714360736310482  PSNR: 25.97296142578125
[TRAIN] Iter: 123200 Loss: 0.008831629529595375  PSNR: 25.516469955444336
[TRAIN] Iter: 123300 Loss: 0.006961798295378685  PSNR: 25.9778995513916
[TRAIN] Iter: 123400 Loss: 0.007912453263998032  PSNR: 25.8227481842041
[TRAIN] Iter: 123500 Loss: 0.009812043979763985  PSNR: 24.554275512695312
[TRAIN] Iter: 123600 Loss: 0.010046467185020447  PSNR: 24.110271453857422
[TRAIN] Iter: 123700 Loss: 0.007072439417243004  PSNR: 26.60589599609375
[TRAIN] Iter: 123800 Loss: 0.010564538650214672  PSNR: 24.28954315185547
[TRAIN] Iter: 123900 Loss: 0.005543487146496773  PSNR: 27.030696868896484
[TRAIN] Iter: 124000 Loss: 0.006890902295708656  PSNR: 26.636667251586914
[TRAIN] Iter: 124100 Loss: 0.007800610736012459  PSNR: 25.874670028686523
[TRAIN] Iter: 124200 Loss: 0.007636282593011856  PSNR: 25.942852020263672
[TRAIN] Iter: 124300 Loss: 0.0075662750750780106  PSNR: 25.91228675842285
[TRAIN] Iter: 124400 Loss: 0.00845297146588564  PSNR: 25.629560470581055
[TRAIN] Iter: 124500 Loss: 0.0061508649960160255  PSNR: 26.221120834350586
[TRAIN] Iter: 124600 Loss: 0.007603643462061882  PSNR: 25.767192840576172
[TRAIN] Iter: 124700 Loss: 0.007826386019587517  PSNR: 25.418926239013672
[TRAIN] Iter: 124800 Loss: 0.008336368948221207  PSNR: 25.539012908935547
[TRAIN] Iter: 124900 Loss: 0.006912680808454752  PSNR: 26.28068733215332
[TRAIN] Iter: 125000 Loss: 0.006680415011942387  PSNR: 26.41322135925293
[TRAIN] Iter: 125100 Loss: 0.007030521519482136  PSNR: 26.342853546142578
[TRAIN] Iter: 125200 Loss: 0.00800219178199768  PSNR: 25.37118911743164
[TRAIN] Iter: 125300 Loss: 0.005986037664115429  PSNR: 26.61526107788086
[TRAIN] Iter: 125400 Loss: 0.0064786337316036224  PSNR: 26.37252426147461
[TRAIN] Iter: 125500 Loss: 0.00787439662963152  PSNR: 25.464080810546875
[TRAIN] Iter: 125600 Loss: 0.005781134124845266  PSNR: 27.310218811035156
[TRAIN] Iter: 125700 Loss: 0.00763071421533823  PSNR: 26.030216217041016
[TRAIN] Iter: 125800 Loss: 0.009045872837305069  PSNR: 25.211549758911133
[TRAIN] Iter: 125900 Loss: 0.0082481000572443  PSNR: 25.63650894165039
[TRAIN] Iter: 126000 Loss: 0.007704991847276688  PSNR: 24.980165481567383
[TRAIN] Iter: 126100 Loss: 0.006978207267820835  PSNR: 26.352310180664062
[TRAIN] Iter: 126200 Loss: 0.007778743281960487  PSNR: 26.609107971191406
[TRAIN] Iter: 126300 Loss: 0.007993731647729874  PSNR: 25.730636596679688
[TRAIN] Iter: 126400 Loss: 0.006078535225242376  PSNR: 27.604402542114258
[TRAIN] Iter: 126500 Loss: 0.006392568349838257  PSNR: 26.8330135345459
[TRAIN] Iter: 126600 Loss: 0.00540489237755537  PSNR: 27.245319366455078
[TRAIN] Iter: 126700 Loss: 0.007622502278536558  PSNR: 26.316024780273438
[TRAIN] Iter: 126800 Loss: 0.00934087298810482  PSNR: 24.50475311279297
[TRAIN] Iter: 126900 Loss: 0.008417313918471336  PSNR: 24.9650936126709
[TRAIN] Iter: 127000 Loss: 0.008331134915351868  PSNR: 25.770410537719727
[TRAIN] Iter: 127100 Loss: 0.007203557528555393  PSNR: 26.788616180419922
[TRAIN] Iter: 127200 Loss: 0.007675927132368088  PSNR: 25.750499725341797
[TRAIN] Iter: 127300 Loss: 0.006972793489694595  PSNR: 25.921064376831055
[TRAIN] Iter: 127400 Loss: 0.006930414587259293  PSNR: 25.61219024658203
[TRAIN] Iter: 127500 Loss: 0.0070351469330489635  PSNR: 26.482547760009766
[TRAIN] Iter: 127600 Loss: 0.006372230593115091  PSNR: 27.39682960510254
[TRAIN] Iter: 127700 Loss: 0.007567891385406256  PSNR: 25.735729217529297
[TRAIN] Iter: 127800 Loss: 0.0059809936210513115  PSNR: 27.460092544555664
[TRAIN] Iter: 127900 Loss: 0.007681145332753658  PSNR: 25.59403419494629
[TRAIN] Iter: 128000 Loss: 0.007215474732220173  PSNR: 25.902204513549805
[TRAIN] Iter: 128100 Loss: 0.008231114596128464  PSNR: 25.578411102294922
[TRAIN] Iter: 128200 Loss: 0.005935566499829292  PSNR: 27.256122589111328
[TRAIN] Iter: 128300 Loss: 0.007791605778038502  PSNR: 25.365352630615234
[TRAIN] Iter: 128400 Loss: 0.008075255900621414  PSNR: 25.317384719848633
[TRAIN] Iter: 128500 Loss: 0.0063425591215491295  PSNR: 26.91119384765625
[TRAIN] Iter: 128600 Loss: 0.007432164624333382  PSNR: 25.70401954650879
[TRAIN] Iter: 128700 Loss: 0.008427435532212257  PSNR: 25.62206268310547
[TRAIN] Iter: 128800 Loss: 0.0069723427295684814  PSNR: 25.94080352783203
[TRAIN] Iter: 128900 Loss: 0.00849334429949522  PSNR: 25.457225799560547
[TRAIN] Iter: 129000 Loss: 0.006585780531167984  PSNR: 26.800832748413086
[TRAIN] Iter: 129100 Loss: 0.00639950530603528  PSNR: 26.504703521728516
[TRAIN] Iter: 129200 Loss: 0.007338503375649452  PSNR: 25.94603729248047
[TRAIN] Iter: 129300 Loss: 0.007183053996413946  PSNR: 25.995088577270508
[TRAIN] Iter: 129400 Loss: 0.005703641567379236  PSNR: 27.578346252441406
[TRAIN] Iter: 129500 Loss: 0.005953484680503607  PSNR: 27.81875228881836
[TRAIN] Iter: 129600 Loss: 0.009404603391885757  PSNR: 24.699506759643555
[TRAIN] Iter: 129700 Loss: 0.007650243118405342  PSNR: 25.853578567504883
[TRAIN] Iter: 129800 Loss: 0.006885232403874397  PSNR: 26.15899085998535
[TRAIN] Iter: 129900 Loss: 0.005741733126342297  PSNR: 27.654760360717773
Saved checkpoints at ./logs/TUT-KE101-nerf/130000.tar
[TRAIN] Iter: 130000 Loss: 0.008704915642738342  PSNR: 24.813018798828125
[TRAIN] Iter: 130100 Loss: 0.008189787156879902  PSNR: 25.349924087524414
[TRAIN] Iter: 130200 Loss: 0.005920863710343838  PSNR: 27.79044532775879
[TRAIN] Iter: 130300 Loss: 0.007914219051599503  PSNR: 25.76123809814453
[TRAIN] Iter: 130400 Loss: 0.0092682596296072  PSNR: 24.592302322387695
[TRAIN] Iter: 130500 Loss: 0.0076324609108269215  PSNR: 26.11907386779785
[TRAIN] Iter: 130600 Loss: 0.007380826864391565  PSNR: 25.97688102722168
[TRAIN] Iter: 130700 Loss: 0.005750168114900589  PSNR: 27.6444149017334
[TRAIN] Iter: 130800 Loss: 0.00993293896317482  PSNR: 24.705387115478516
[TRAIN] Iter: 130900 Loss: 0.0071401819586753845  PSNR: 26.20633316040039
[TRAIN] Iter: 131000 Loss: 0.006797994486987591  PSNR: 26.994264602661133
[TRAIN] Iter: 131100 Loss: 0.00534429308027029  PSNR: 27.75547981262207
[TRAIN] Iter: 131200 Loss: 0.0052916500717401505  PSNR: 27.442663192749023
[TRAIN] Iter: 131300 Loss: 0.006088314577937126  PSNR: 27.0458927154541
[TRAIN] Iter: 131400 Loss: 0.005104171112179756  PSNR: 27.4945068359375
[TRAIN] Iter: 131500 Loss: 0.005080623552203178  PSNR: 27.78973960876465
[TRAIN] Iter: 131600 Loss: 0.00876984279602766  PSNR: 24.98853302001953
[TRAIN] Iter: 131700 Loss: 0.008986713364720345  PSNR: 25.070985794067383
[TRAIN] Iter: 131800 Loss: 0.0063846902921795845  PSNR: 26.78985023498535
[TRAIN] Iter: 131900 Loss: 0.009123027324676514  PSNR: 25.146696090698242
[TRAIN] Iter: 132000 Loss: 0.007738422602415085  PSNR: 25.43002700805664
[TRAIN] Iter: 132100 Loss: 0.010225243866443634  PSNR: 24.334035873413086
[TRAIN] Iter: 132200 Loss: 0.007734209764748812  PSNR: 25.43751335144043
[TRAIN] Iter: 132300 Loss: 0.005774557590484619  PSNR: 26.533241271972656
[TRAIN] Iter: 132400 Loss: 0.005815798882395029  PSNR: 27.614953994750977
[TRAIN] Iter: 132500 Loss: 0.0060217902064323425  PSNR: 27.0990047454834
[TRAIN] Iter: 132600 Loss: 0.006755257956683636  PSNR: 26.139102935791016
[TRAIN] Iter: 132700 Loss: 0.0054258182644844055  PSNR: 27.771318435668945
[TRAIN] Iter: 132800 Loss: 0.006227462086826563  PSNR: 26.990251541137695
[TRAIN] Iter: 132900 Loss: 0.005735836923122406  PSNR: 27.25177001953125
[TRAIN] Iter: 133000 Loss: 0.006473112851381302  PSNR: 26.144216537475586
[TRAIN] Iter: 133100 Loss: 0.007188926450908184  PSNR: 26.007801055908203
[TRAIN] Iter: 133200 Loss: 0.006680828053504229  PSNR: 26.990835189819336
[TRAIN] Iter: 133300 Loss: 0.007761938497424126  PSNR: 25.771507263183594
[TRAIN] Iter: 133400 Loss: 0.006457258015871048  PSNR: 26.331817626953125
[TRAIN] Iter: 133500 Loss: 0.0066427262499928474  PSNR: 25.937641143798828
[TRAIN] Iter: 133600 Loss: 0.005633972119539976  PSNR: 27.604488372802734
[TRAIN] Iter: 133700 Loss: 0.006910964846611023  PSNR: 26.01529884338379
[TRAIN] Iter: 133800 Loss: 0.008306358009576797  PSNR: 25.15995979309082
[TRAIN] Iter: 133900 Loss: 0.006139897741377354  PSNR: 27.29608726501465
[TRAIN] Iter: 134000 Loss: 0.006964538246393204  PSNR: 26.89444923400879
[TRAIN] Iter: 134100 Loss: 0.006090240087360144  PSNR: 26.939598083496094
[TRAIN] Iter: 134200 Loss: 0.0075898366048932076  PSNR: 25.564409255981445
[TRAIN] Iter: 134300 Loss: 0.007185637019574642  PSNR: 25.55765151977539
[TRAIN] Iter: 134400 Loss: 0.006936756428331137  PSNR: 26.670324325561523
[TRAIN] Iter: 134500 Loss: 0.00920850783586502  PSNR: 24.955577850341797
[TRAIN] Iter: 134600 Loss: 0.007159527391195297  PSNR: 25.719196319580078
[TRAIN] Iter: 134700 Loss: 0.0069902874529361725  PSNR: 25.692901611328125
[TRAIN] Iter: 134800 Loss: 0.008825923316180706  PSNR: 25.090715408325195
[TRAIN] Iter: 134900 Loss: 0.006501973606646061  PSNR: 26.893163681030273
[TRAIN] Iter: 135000 Loss: 0.007017677649855614  PSNR: 26.73099136352539
[TRAIN] Iter: 135100 Loss: 0.006128096021711826  PSNR: 27.381500244140625
[TRAIN] Iter: 135200 Loss: 0.007448920048773289  PSNR: 25.899330139160156
[TRAIN] Iter: 135300 Loss: 0.007816450670361519  PSNR: 25.240867614746094
[TRAIN] Iter: 135400 Loss: 0.00555486511439085  PSNR: 27.700519561767578
[TRAIN] Iter: 135500 Loss: 0.007835893891751766  PSNR: 25.663822174072266
[TRAIN] Iter: 135600 Loss: 0.006123220548033714  PSNR: 27.477210998535156
[TRAIN] Iter: 135700 Loss: 0.007256226614117622  PSNR: 26.423757553100586
[TRAIN] Iter: 135800 Loss: 0.006748517509549856  PSNR: 26.44902229309082
[TRAIN] Iter: 135900 Loss: 0.006134365685284138  PSNR: 27.485937118530273
[TRAIN] Iter: 136000 Loss: 0.005735613405704498  PSNR: 27.187524795532227
[TRAIN] Iter: 136100 Loss: 0.007835944183170795  PSNR: 25.55813217163086
[TRAIN] Iter: 136200 Loss: 0.005389056168496609  PSNR: 27.785053253173828
[TRAIN] Iter: 136300 Loss: 0.006585344206541777  PSNR: 26.227571487426758
[TRAIN] Iter: 136400 Loss: 0.006408949848264456  PSNR: 26.683076858520508
[TRAIN] Iter: 136500 Loss: 0.0049541788175702095  PSNR: 28.819026947021484
[TRAIN] Iter: 136600 Loss: 0.007192209362983704  PSNR: 26.198726654052734
[TRAIN] Iter: 136700 Loss: 0.008011936210095882  PSNR: 25.92735481262207
[TRAIN] Iter: 136800 Loss: 0.007632327266037464  PSNR: 25.785892486572266
[TRAIN] Iter: 136900 Loss: 0.0068013970740139484  PSNR: 27.616592407226562
[TRAIN] Iter: 137000 Loss: 0.005701552610844374  PSNR: 27.447439193725586
[TRAIN] Iter: 137100 Loss: 0.006700351368635893  PSNR: 26.094249725341797
[TRAIN] Iter: 137200 Loss: 0.008105503395199776  PSNR: 25.46436882019043
[TRAIN] Iter: 137300 Loss: 0.005978384055197239  PSNR: 26.64283561706543
[TRAIN] Iter: 137400 Loss: 0.005157118197530508  PSNR: 28.10456657409668
[TRAIN] Iter: 137500 Loss: 0.0051461318507790565  PSNR: 27.678699493408203
[TRAIN] Iter: 137600 Loss: 0.005068029277026653  PSNR: 27.93634605407715
[TRAIN] Iter: 137700 Loss: 0.006594477221369743  PSNR: 26.5264892578125
[TRAIN] Iter: 137800 Loss: 0.005909798201173544  PSNR: 27.2615966796875
[TRAIN] Iter: 137900 Loss: 0.0077555240131914616  PSNR: 25.421186447143555
[TRAIN] Iter: 138000 Loss: 0.006163365673273802  PSNR: 27.179248809814453
[TRAIN] Iter: 138100 Loss: 0.008288346230983734  PSNR: 26.148523330688477
[TRAIN] Iter: 138200 Loss: 0.00546894920989871  PSNR: 27.592748641967773
[TRAIN] Iter: 138300 Loss: 0.007414320018142462  PSNR: 26.377607345581055
[TRAIN] Iter: 138400 Loss: 0.007595047354698181  PSNR: 25.474031448364258
[TRAIN] Iter: 138500 Loss: 0.008365833200514317  PSNR: 25.805978775024414
[TRAIN] Iter: 138600 Loss: 0.007811998948454857  PSNR: 26.14881134033203
[TRAIN] Iter: 138700 Loss: 0.00688129011541605  PSNR: 25.69120979309082
[TRAIN] Iter: 138800 Loss: 0.006329129915684462  PSNR: 26.425336837768555
[TRAIN] Iter: 138900 Loss: 0.007700219284743071  PSNR: 25.866106033325195
[TRAIN] Iter: 139000 Loss: 0.006942685693502426  PSNR: 25.721431732177734
[TRAIN] Iter: 139100 Loss: 0.005895532667636871  PSNR: 27.619747161865234
[TRAIN] Iter: 139200 Loss: 0.005782850086688995  PSNR: 26.298202514648438
[TRAIN] Iter: 139300 Loss: 0.006290550343692303  PSNR: 27.145078659057617
[TRAIN] Iter: 139400 Loss: 0.007164145819842815  PSNR: 25.706697463989258
[TRAIN] Iter: 139500 Loss: 0.006972773000597954  PSNR: 26.45365333557129
[TRAIN] Iter: 139600 Loss: 0.008157676085829735  PSNR: 25.340774536132812
[TRAIN] Iter: 139700 Loss: 0.005065138451755047  PSNR: 28.020631790161133
[TRAIN] Iter: 139800 Loss: 0.008581262081861496  PSNR: 25.323930740356445
[TRAIN] Iter: 139900 Loss: 0.006265171803534031  PSNR: 27.436969757080078
Saved checkpoints at ./logs/TUT-KE101-nerf/140000.tar
[TRAIN] Iter: 140000 Loss: 0.00822022370994091  PSNR: 25.960205078125
[TRAIN] Iter: 140100 Loss: 0.005712364800274372  PSNR: 26.9573974609375
[TRAIN] Iter: 140200 Loss: 0.009507185779511929  PSNR: 24.356204986572266
[TRAIN] Iter: 140300 Loss: 0.005679625552147627  PSNR: 27.104528427124023
[TRAIN] Iter: 140400 Loss: 0.008211405947804451  PSNR: 25.578168869018555
[TRAIN] Iter: 140500 Loss: 0.007840266451239586  PSNR: 25.410730361938477
[TRAIN] Iter: 140600 Loss: 0.004994906019419432  PSNR: 27.691997528076172
[TRAIN] Iter: 140700 Loss: 0.007535914424806833  PSNR: 25.53078842163086
[TRAIN] Iter: 140800 Loss: 0.006441432051360607  PSNR: 27.1474552154541
[TRAIN] Iter: 140900 Loss: 0.007472740486264229  PSNR: 25.571685791015625
[TRAIN] Iter: 141000 Loss: 0.008606232702732086  PSNR: 25.283781051635742
[TRAIN] Iter: 141100 Loss: 0.007680141367018223  PSNR: 25.656938552856445
[TRAIN] Iter: 141200 Loss: 0.00635115522891283  PSNR: 26.659597396850586
[TRAIN] Iter: 141300 Loss: 0.006769198924303055  PSNR: 26.58188819885254
[TRAIN] Iter: 141400 Loss: 0.009017180651426315  PSNR: 24.732524871826172
[TRAIN] Iter: 141500 Loss: 0.006678038276731968  PSNR: 26.42620277404785
[TRAIN] Iter: 141600 Loss: 0.0075989230535924435  PSNR: 25.710886001586914
[TRAIN] Iter: 141700 Loss: 0.005519106052815914  PSNR: 27.953088760375977
[TRAIN] Iter: 141800 Loss: 0.00815966259688139  PSNR: 25.65831184387207
[TRAIN] Iter: 141900 Loss: 0.008108649402856827  PSNR: 25.236427307128906
[TRAIN] Iter: 142000 Loss: 0.00810110755264759  PSNR: 25.20058250427246
[TRAIN] Iter: 142100 Loss: 0.008451132103800774  PSNR: 25.249073028564453
[TRAIN] Iter: 142200 Loss: 0.005030527710914612  PSNR: 27.777183532714844
[TRAIN] Iter: 142300 Loss: 0.007311626337468624  PSNR: 25.90936279296875
[TRAIN] Iter: 142400 Loss: 0.007817234843969345  PSNR: 24.870243072509766
[TRAIN] Iter: 142500 Loss: 0.006257431581616402  PSNR: 26.86892318725586
[TRAIN] Iter: 142600 Loss: 0.0063335951417684555  PSNR: 26.568687438964844
[TRAIN] Iter: 142700 Loss: 0.007204923778772354  PSNR: 25.969127655029297
[TRAIN] Iter: 142800 Loss: 0.006905427202582359  PSNR: 26.268266677856445
[TRAIN] Iter: 142900 Loss: 0.007242708001285791  PSNR: 25.445457458496094
[TRAIN] Iter: 143000 Loss: 0.008432441391050816  PSNR: 25.79509925842285
[TRAIN] Iter: 143100 Loss: 0.005291550885885954  PSNR: 27.90793228149414
[TRAIN] Iter: 143200 Loss: 0.0074137067422270775  PSNR: 25.6484317779541
[TRAIN] Iter: 143300 Loss: 0.006399349309504032  PSNR: 26.76422691345215
[TRAIN] Iter: 143400 Loss: 0.008880561217665672  PSNR: 25.19623565673828
[TRAIN] Iter: 143500 Loss: 0.007456118240952492  PSNR: 25.740493774414062
[TRAIN] Iter: 143600 Loss: 0.007805690169334412  PSNR: 25.71930503845215
[TRAIN] Iter: 143700 Loss: 0.007060004398226738  PSNR: 25.79389190673828
[TRAIN] Iter: 143800 Loss: 0.007445549592375755  PSNR: 25.533191680908203
[TRAIN] Iter: 143900 Loss: 0.006530760787427425  PSNR: 26.92317008972168
[TRAIN] Iter: 144000 Loss: 0.004769556224346161  PSNR: 28.178909301757812
[TRAIN] Iter: 144100 Loss: 0.006239702459424734  PSNR: 26.860361099243164
[TRAIN] Iter: 144200 Loss: 0.005121992900967598  PSNR: 27.949575424194336
[TRAIN] Iter: 144300 Loss: 0.005790777504444122  PSNR: 27.18577003479004
[TRAIN] Iter: 144400 Loss: 0.007110258564352989  PSNR: 26.18831443786621
[TRAIN] Iter: 144500 Loss: 0.007476647384464741  PSNR: 25.54193115234375
[TRAIN] Iter: 144600 Loss: 0.008922219276428223  PSNR: 24.95577621459961
[TRAIN] Iter: 144700 Loss: 0.0069379620254039764  PSNR: 26.70779800415039
[TRAIN] Iter: 144800 Loss: 0.0056894319131970406  PSNR: 26.912445068359375
[TRAIN] Iter: 144900 Loss: 0.005222473293542862  PSNR: 27.4327392578125
[TRAIN] Iter: 145000 Loss: 0.005774995312094688  PSNR: 27.250843048095703
[TRAIN] Iter: 145100 Loss: 0.0061555830761790276  PSNR: 27.347808837890625
[TRAIN] Iter: 145200 Loss: 0.007032693363726139  PSNR: 26.679353713989258
[TRAIN] Iter: 145300 Loss: 0.007802522741258144  PSNR: 25.51803970336914
[TRAIN] Iter: 145400 Loss: 0.006710359826683998  PSNR: 26.69239044189453
[TRAIN] Iter: 145500 Loss: 0.00696597620844841  PSNR: 26.504955291748047
[TRAIN] Iter: 145600 Loss: 0.007921796292066574  PSNR: 25.944499969482422
[TRAIN] Iter: 145700 Loss: 0.005282696336507797  PSNR: 28.00433349609375
[TRAIN] Iter: 145800 Loss: 0.004848049953579903  PSNR: 27.653724670410156
[TRAIN] Iter: 145900 Loss: 0.008019516244530678  PSNR: 26.046401977539062
[TRAIN] Iter: 146000 Loss: 0.006156559567898512  PSNR: 26.815921783447266
[TRAIN] Iter: 146100 Loss: 0.0047782594338059425  PSNR: 28.327579498291016
[TRAIN] Iter: 146200 Loss: 0.005634375382214785  PSNR: 27.413084030151367
[TRAIN] Iter: 146300 Loss: 0.00784902460873127  PSNR: 25.83186149597168
[TRAIN] Iter: 146400 Loss: 0.0055527775548398495  PSNR: 26.986953735351562
[TRAIN] Iter: 146500 Loss: 0.006684258580207825  PSNR: 26.683639526367188
[TRAIN] Iter: 146600 Loss: 0.005221232771873474  PSNR: 27.4981632232666
[TRAIN] Iter: 146700 Loss: 0.008036328479647636  PSNR: 25.483564376831055
[TRAIN] Iter: 146800 Loss: 0.005518658552318811  PSNR: 27.795549392700195
[TRAIN] Iter: 146900 Loss: 0.0068687801249325275  PSNR: 26.561010360717773
[TRAIN] Iter: 147000 Loss: 0.005109047517180443  PSNR: 28.137256622314453
[TRAIN] Iter: 147100 Loss: 0.005762441549450159  PSNR: 26.851072311401367
[TRAIN] Iter: 147200 Loss: 0.007230257615447044  PSNR: 25.405841827392578
[TRAIN] Iter: 147300 Loss: 0.0068701100535690784  PSNR: 26.58013343811035
[TRAIN] Iter: 147400 Loss: 0.00606054300442338  PSNR: 26.92723846435547
[TRAIN] Iter: 147500 Loss: 0.006531887222081423  PSNR: 26.54636001586914
[TRAIN] Iter: 147600 Loss: 0.006147487089037895  PSNR: 26.110567092895508
[TRAIN] Iter: 147700 Loss: 0.005863645114004612  PSNR: 26.682048797607422
[TRAIN] Iter: 147800 Loss: 0.008723301813006401  PSNR: 25.487707138061523
[TRAIN] Iter: 147900 Loss: 0.0071968273259699345  PSNR: 26.147377014160156
[TRAIN] Iter: 148000 Loss: 0.007056673057377338  PSNR: 26.14596176147461
[TRAIN] Iter: 148100 Loss: 0.005353213753551245  PSNR: 28.161752700805664
[TRAIN] Iter: 148200 Loss: 0.0064868563786149025  PSNR: 26.727140426635742
[TRAIN] Iter: 148300 Loss: 0.0049935393035411835  PSNR: 28.24223518371582
[TRAIN] Iter: 148400 Loss: 0.007380171678960323  PSNR: 26.113353729248047
[TRAIN] Iter: 148500 Loss: 0.005382353439927101  PSNR: 27.729536056518555
[TRAIN] Iter: 148600 Loss: 0.005615983158349991  PSNR: 27.160293579101562
[TRAIN] Iter: 148700 Loss: 0.007345668040215969  PSNR: 26.123703002929688
[TRAIN] Iter: 148800 Loss: 0.007933286018669605  PSNR: 25.25149154663086
[TRAIN] Iter: 148900 Loss: 0.007705181837081909  PSNR: 25.53327178955078
[TRAIN] Iter: 149000 Loss: 0.0080169178545475  PSNR: 25.461875915527344
[TRAIN] Iter: 149100 Loss: 0.006315820850431919  PSNR: 26.77266502380371
[TRAIN] Iter: 149200 Loss: 0.006929949391633272  PSNR: 26.559484481811523
[TRAIN] Iter: 149300 Loss: 0.007404180243611336  PSNR: 25.909793853759766
[TRAIN] Iter: 149400 Loss: 0.006367525085806847  PSNR: 26.57650375366211
[TRAIN] Iter: 149500 Loss: 0.005267594940960407  PSNR: 28.00555992126465
[TRAIN] Iter: 149600 Loss: 0.007331849541515112  PSNR: 25.730281829833984
[TRAIN] Iter: 149700 Loss: 0.0069689759984612465  PSNR: 26.592388153076172
[TRAIN] Iter: 149800 Loss: 0.00690076407045126  PSNR: 26.61532974243164
[TRAIN] Iter: 149900 Loss: 0.004548424389213324  PSNR: 28.110361099243164
Saved checkpoints at ./logs/TUT-KE101-nerf/150000.tar
0 0.00034499168395996094
torch.Size([320, 640, 3]) torch.Size([320, 640])
1 16.025832891464233
2 12.945882081985474
3 15.57960033416748
4 13.160759925842285
5 12.931249141693115
6 15.85254693031311
7 12.747162580490112
8 15.842675924301147
9 13.044311046600342
10 13.127479553222656
11 15.684839487075806
12 12.78381633758545
13 15.919776439666748
14 12.788830757141113
15 12.969107627868652
16 16.004876613616943
17 12.667096376419067
18 16.06871747970581
19 12.76177716255188
20 12.947282075881958
21 15.921688795089722
22 12.75399923324585
23 16.13877558708191
24 12.756136894226074
25 13.024292230606079
26 15.692526578903198
27 12.768309354782104
28 16.169283866882324
29 12.729312896728516
30 15.806775331497192
31 13.03402590751648
32 12.847039461135864
33 16.041887521743774
34 12.676323890686035
35 15.928519248962402
36 12.959864139556885
37 12.832831621170044
38 16.071120977401733
39 12.728989362716675
40 15.998029470443726
41 12.840623378753662
42 12.913096904754639
43 16.378538131713867
44 13.156378269195557
45 15.177955627441406
46 12.89240312576294
47 13.247358560562134
48 15.7265043258667
49 12.826379537582397
50 15.853541135787964
51 12.810861110687256
52 13.122050523757935
53 15.651567935943604
54 14.613190650939941
55 17.74881148338318
56 14.609646558761597
57 17.89076852798462
58 14.599349021911621
59 17.958070278167725
60 14.521057367324829
61 17.955510139465332
62 14.575161457061768
63 15.184789180755615
64 17.367095947265625
65 14.821319341659546
66 17.71738886833191
67 14.691202402114868
68 17.822250366210938
69 14.650542736053467
70 17.856217622756958
71 14.642839431762695
72 17.920073986053467
73 14.583377122879028
74 17.821215867996216
75 14.915570497512817
76 14.91978120803833
77 17.538533449172974
78 14.963983297348022
79 17.51240348815918
80 14.959552526473999
81 17.511436939239502
82 14.80831003189087
83 17.751288652420044
84 14.608241558074951
85 17.83921718597412
86 14.802809476852417
87 17.552756547927856
88 15.199668407440186
89 15.064144611358643
90 17.522890329360962
91 15.40248703956604
92 17.70492911338806
93 14.897242069244385
94 17.455010175704956
95 14.882665872573853
96 17.55843234062195
97 14.731451272964478
98 17.806350469589233
99 14.630025625228882
100 17.755430698394775
101 14.6108717918396
102 17.83766508102417
103 14.837209463119507
104 14.880459547042847
105 17.685967206954956
106 14.944162845611572
107 17.48913025856018
108 14.94541597366333
109 17.525888681411743
110 14.955881118774414
111 17.5145263671875
112 14.873701095581055
113 17.485870361328125
114 14.929208040237427
115 17.50696849822998
116 14.930790185928345
117 14.941176176071167
118 17.516728401184082
119 14.928263902664185
Done, saving (120, 320, 640, 3) (120, 320, 640)
extras:{'raw': tensor([[[-8.3003e-01, -6.5698e-01,  2.7010e-02, -4.8472e+01],
         [-1.0735e+00, -8.7715e-01, -2.8346e-01, -4.8441e+01],
         [-9.8538e-01, -7.2351e-01, -1.1675e-01, -4.7382e+01],
         ...,
         [-6.3527e+00, -5.9117e+00, -6.4014e+00, -4.7602e+01],
         [-1.0621e+01, -1.0778e+01, -1.2900e+01, -6.1843e+01],
         [-3.4126e+00, -2.9640e+00, -2.9940e+00, -4.0303e+01]],

        [[ 1.1172e+01,  1.2633e+01,  1.7390e+01, -5.0490e+01],
         [-9.9261e-02,  2.9015e-01,  1.3702e+00, -4.0972e+01],
         [-8.9585e-02,  2.9214e-01,  1.3418e+00, -4.1503e+01],
         ...,
         [ 1.7337e+01,  1.7163e+01,  2.4137e+01,  2.0417e+02],
         [ 1.5063e+01,  1.4745e+01,  2.1761e+01,  2.3385e+02],
         [ 1.3276e+01,  1.3597e+01,  2.1193e+01,  2.2040e+02]],

        [[-9.3493e-01, -1.0839e+00, -1.3250e+00, -4.4277e+01],
         [-4.5329e-02, -3.9631e-01, -1.3606e+00, -1.3462e+01],
         [-2.7084e-01, -6.0843e-01, -1.5749e+00, -1.3225e+01],
         ...,
         [ 4.9859e+00, -1.3709e+00, -1.4735e+01, -2.4778e+02],
         [ 5.1729e+00, -3.5093e-01, -1.2172e+01, -2.0750e+02],
         [ 4.1798e+00, -1.3595e+00, -1.3815e+01, -1.5018e+02]],

        ...,

        [[-1.0363e+00, -3.8006e-01,  7.9014e-01, -6.4224e+01],
         [-6.8091e-02, -1.7997e-01, -3.1504e-01,  3.3601e+01],
         [-1.9009e-02, -1.5790e-01, -2.8668e-01,  1.5590e+00],
         ...,
         [ 1.3871e+00,  1.3690e+00,  1.9419e+00,  2.3845e+02],
         [ 1.3653e+00,  1.1411e+00,  1.8004e+00,  2.2990e+02],
         [ 1.2072e+00,  9.0585e-01,  1.2303e+00,  2.7943e+02]],

        [[-3.2742e-01,  1.9951e-01,  1.6304e+00, -5.9675e+01],
         [-4.8243e-01, -1.2960e-01,  6.4922e-01, -4.4714e+01],
         [ 4.5367e+00,  3.4551e+00,  1.3997e+00, -1.8422e+01],
         ...,
         [-5.5204e+00, -6.8190e+00, -9.7258e+00, -1.8082e+02],
         [-5.8004e+00, -7.0308e+00, -9.8702e+00, -1.7629e+02],
         [-3.5568e+00, -4.9663e+00, -7.9567e+00, -1.1929e+02]],

        [[-3.3147e-01, -1.9567e-01,  3.1291e-01, -3.3021e+01],
         [ 1.2545e-01,  4.5292e-02,  1.2128e-01,  1.0480e+01],
         [ 1.5922e-01,  8.0449e-02,  1.6272e-01,  1.1756e+01],
         ...,
         [ 7.4753e+00,  7.6325e+00,  1.1077e+01,  4.4424e+02],
         [ 9.2072e+00,  9.0801e+00,  1.4096e+01,  4.0832e+02],
         [ 8.0980e+00,  8.1282e+00,  1.2196e+01,  4.2504e+02]]],
       grad_fn=<ReshapeAliasBackward0>), 'rgb0': tensor([[0.4939, 0.4023, 0.2992],
        [0.4900, 0.4675, 0.4500],
        [0.5312, 0.5155, 0.4309],
        ...,
        [0.4660, 0.4335, 0.3976],
        [0.2571, 0.2241, 0.1882],
        [0.5381, 0.5072, 0.5059]], grad_fn=<ReshapeAliasBackward0>), 'disp0': tensor([ 93.9236, 209.8942,  99.2502,  ..., 137.6789,  65.9109, 263.8213],
       grad_fn=<ReshapeAliasBackward0>), 'acc0': tensor([1., 1., 1.,  ..., 1., 1., 1.], grad_fn=<ReshapeAliasBackward0>), 'z_std': tensor([0.0035, 0.0028, 0.0021,  ..., 0.0019, 0.0025, 0.1754])}
0 0.0004131793975830078
torch.Size([320, 640, 3]) torch.Size([320, 640])
1 14.895458698272705
2 17.606067895889282
3 14.85411787033081
4 17.506792068481445
5 14.933097124099731
6 17.5328106880188
7 14.943244934082031
8 17.547248601913452
9 14.862008571624756
10 17.625776052474976
11 14.752888679504395
12 14.888488531112671
13 17.69777822494507
14 14.92362904548645
15 17.428119659423828
16 14.959195375442505
17 17.53083562850952
18 14.922407150268555
19 17.557565212249756
20 14.922926664352417
21 17.5625102519989
22 14.845255613327026
23 17.804006814956665
24 15.372773170471191
25 17.829317569732666
26 14.902642250061035
27 14.9111909866333
28 17.546135187149048
29 14.92906403541565
30 17.60116696357727
31 14.770305871963501
32 17.774489641189575
33 14.656217336654663
34 17.830711126327515
35 14.834407567977905
36 17.52715539932251
37 14.931463241577148
38 17.441428422927856
39 15.03856897354126
40 14.732726097106934
41 18.060171127319336
42 14.529218912124634
43 17.714132070541382
44 14.557719945907593
45 17.922941207885742
46 14.534392833709717
47 17.887274026870728
48 14.544541597366333
49 18.02086114883423
50 14.580268383026123
51 17.622215032577515
52 14.767294883728027
53 14.744879722595215
54 17.80936026573181
55 14.67677927017212
56 16.553576469421387
57 13.994814157485962
58 18.023487091064453
59 14.533712148666382
60 17.8746337890625
61 14.605997800827026
62 17.843188047409058
63 14.68690037727356
64 14.862048864364624
65 17.749602794647217
66 14.704841375350952
67 17.723509073257446
68 14.631862878799438
69 18.293668031692505
70 15.05229926109314
71 16.93963098526001
72 14.527161121368408
73 18.302091121673584
74 15.069047212600708
75 17.03117871284485
76 14.970996856689453
77 17.473487377166748
78 14.96599531173706
79 14.850788593292236
80 17.639435529708862
81 14.954490661621094
82 17.553218364715576
83 14.970194816589355
84 17.449506282806396
85 14.937562465667725
86 17.51114559173584
87 14.959776639938354
88 17.519588232040405
89 14.960168600082397
90 17.485400676727295
91 15.03007435798645
92 15.37751579284668
93 17.953214168548584
94 15.170128583908081
95 17.511081218719482
96 14.95618462562561
97 17.531678676605225
98 14.864295482635498
99 17.467623949050903
100 14.97348928451538
101 17.469005823135376
102 14.962080478668213
103 17.485579013824463
104 14.945438861846924
105 17.468539714813232
106 14.932509660720825
107 14.988159894943237
108 17.569245100021362
109 14.89599895477295
110 17.593916416168213
111 14.62105941772461
112 18.074898719787598
113 14.76105523109436
114 17.605672359466553
115 14.834764242172241
116 17.702108144760132
117 14.729805707931519
118 17.540184497833252
119 15.024020671844482
test poses shape torch.Size([4, 3, 4])
0 0.0006699562072753906
torch.Size([320, 640, 3]) torch.Size([320, 640])
1 15.173229694366455
2 14.667683839797974
3 18.029114723205566
Saved test set
[TRAIN] Iter: 150000 Loss: 0.006448525004088879  PSNR: 26.948192596435547
[TRAIN] Iter: 150100 Loss: 0.006399915087968111  PSNR: 26.787456512451172
[TRAIN] Iter: 150200 Loss: 0.007658602669835091  PSNR: 25.896926879882812
[TRAIN] Iter: 150300 Loss: 0.007403295021504164  PSNR: 25.992671966552734
[TRAIN] Iter: 150400 Loss: 0.007336173206567764  PSNR: 26.08877944946289
[TRAIN] Iter: 150500 Loss: 0.005169325973838568  PSNR: 27.745441436767578
[TRAIN] Iter: 150600 Loss: 0.008869725279510021  PSNR: 24.9920654296875
[TRAIN] Iter: 150700 Loss: 0.006234338041394949  PSNR: 26.7652587890625
[TRAIN] Iter: 150800 Loss: 0.005490725859999657  PSNR: 27.414958953857422
[TRAIN] Iter: 150900 Loss: 0.00545975286513567  PSNR: 27.708955764770508
[TRAIN] Iter: 151000 Loss: 0.007766881491988897  PSNR: 25.70874786376953
[TRAIN] Iter: 151100 Loss: 0.006767789833247662  PSNR: 26.280431747436523
[TRAIN] Iter: 151200 Loss: 0.005947668105363846  PSNR: 28.185728073120117
[TRAIN] Iter: 151300 Loss: 0.007522157393395901  PSNR: 25.832380294799805
[TRAIN] Iter: 151400 Loss: 0.004834015388041735  PSNR: 28.277698516845703
[TRAIN] Iter: 151500 Loss: 0.007523492444306612  PSNR: 26.031766891479492
[TRAIN] Iter: 151600 Loss: 0.0048073832876980305  PSNR: 28.65593719482422
[TRAIN] Iter: 151700 Loss: 0.005890128668397665  PSNR: 27.052860260009766
[TRAIN] Iter: 151800 Loss: 0.008021034300327301  PSNR: 25.27835464477539
[TRAIN] Iter: 151900 Loss: 0.006221082527190447  PSNR: 26.654687881469727
[TRAIN] Iter: 152000 Loss: 0.009580518119037151  PSNR: 24.78752899169922
[TRAIN] Iter: 152100 Loss: 0.005882301367819309  PSNR: 27.089874267578125
[TRAIN] Iter: 152200 Loss: 0.005797714926302433  PSNR: 27.568159103393555
[TRAIN] Iter: 152300 Loss: 0.00483294390141964  PSNR: 27.98387908935547
[TRAIN] Iter: 152400 Loss: 0.005496739409863949  PSNR: 28.468399047851562
[TRAIN] Iter: 152500 Loss: 0.006217621266841888  PSNR: 27.293304443359375
[TRAIN] Iter: 152600 Loss: 0.004693462513387203  PSNR: 28.701519012451172
[TRAIN] Iter: 152700 Loss: 0.007761185988783836  PSNR: 26.09598159790039
[TRAIN] Iter: 152800 Loss: 0.007093312684446573  PSNR: 26.454622268676758
[TRAIN] Iter: 152900 Loss: 0.007631836924701929  PSNR: 26.546785354614258
[TRAIN] Iter: 153000 Loss: 0.007244683336466551  PSNR: 26.490144729614258
[TRAIN] Iter: 153100 Loss: 0.007360091432929039  PSNR: 26.325075149536133
[TRAIN] Iter: 153200 Loss: 0.006116967648267746  PSNR: 27.4727725982666
[TRAIN] Iter: 153300 Loss: 0.004942667670547962  PSNR: 28.654661178588867
[TRAIN] Iter: 153400 Loss: 0.0065043312497437  PSNR: 26.312095642089844
[TRAIN] Iter: 153500 Loss: 0.006399981677532196  PSNR: 26.132186889648438
[TRAIN] Iter: 153600 Loss: 0.007999623194336891  PSNR: 25.0688419342041
[TRAIN] Iter: 153700 Loss: 0.008059351705014706  PSNR: 24.96822738647461
[TRAIN] Iter: 153800 Loss: 0.007486855611205101  PSNR: 26.034791946411133
[TRAIN] Iter: 153900 Loss: 0.005705367773771286  PSNR: 28.207027435302734
[TRAIN] Iter: 154000 Loss: 0.006958159618079662  PSNR: 26.132484436035156
[TRAIN] Iter: 154100 Loss: 0.005199199076741934  PSNR: 28.71751594543457
[TRAIN] Iter: 154200 Loss: 0.006963073741644621  PSNR: 26.167217254638672
[TRAIN] Iter: 154300 Loss: 0.008045271970331669  PSNR: 25.76319694519043
[TRAIN] Iter: 154400 Loss: 0.006447851192206144  PSNR: 26.874475479125977
[TRAIN] Iter: 154500 Loss: 0.008100006729364395  PSNR: 25.389545440673828
[TRAIN] Iter: 154600 Loss: 0.007518692873418331  PSNR: 25.704851150512695
[TRAIN] Iter: 154700 Loss: 0.006987274158746004  PSNR: 25.922042846679688
[TRAIN] Iter: 154800 Loss: 0.005884632468223572  PSNR: 26.91389274597168
[TRAIN] Iter: 154900 Loss: 0.006416673306375742  PSNR: 25.942188262939453
[TRAIN] Iter: 155000 Loss: 0.006963565945625305  PSNR: 26.63694190979004
[TRAIN] Iter: 155100 Loss: 0.007455368060618639  PSNR: 25.775917053222656
[TRAIN] Iter: 155200 Loss: 0.007293437607586384  PSNR: 25.657867431640625
[TRAIN] Iter: 155300 Loss: 0.0041611213237047195  PSNR: 29.106653213500977
[TRAIN] Iter: 155400 Loss: 0.005994322244077921  PSNR: 26.83798599243164
[TRAIN] Iter: 155500 Loss: 0.007049053907394409  PSNR: 26.678491592407227
[TRAIN] Iter: 155600 Loss: 0.0056298417039215565  PSNR: 27.810949325561523
[TRAIN] Iter: 155700 Loss: 0.006779885850846767  PSNR: 25.916425704956055
[TRAIN] Iter: 155800 Loss: 0.006430674344301224  PSNR: 26.66257667541504
[TRAIN] Iter: 155900 Loss: 0.0063885170966386795  PSNR: 26.642026901245117
[TRAIN] Iter: 156000 Loss: 0.005269908346235752  PSNR: 28.330228805541992
[TRAIN] Iter: 156100 Loss: 0.007566002197563648  PSNR: 26.270782470703125
[TRAIN] Iter: 156200 Loss: 0.007088576443493366  PSNR: 26.577774047851562
[TRAIN] Iter: 156300 Loss: 0.007392743602395058  PSNR: 26.079345703125
[TRAIN] Iter: 156400 Loss: 0.0065453797578811646  PSNR: 26.49161148071289
[TRAIN] Iter: 156500 Loss: 0.006058793049305677  PSNR: 26.43541717529297
[TRAIN] Iter: 156600 Loss: 0.007708348333835602  PSNR: 25.395906448364258
[TRAIN] Iter: 156700 Loss: 0.0060772765427827835  PSNR: 26.418739318847656
[TRAIN] Iter: 156800 Loss: 0.005136573687195778  PSNR: 27.791847229003906
[TRAIN] Iter: 156900 Loss: 0.006260343827307224  PSNR: 26.0344181060791
[TRAIN] Iter: 157000 Loss: 0.006181969307363033  PSNR: 26.81180763244629
[TRAIN] Iter: 157100 Loss: 0.008349399082362652  PSNR: 25.354860305786133
[TRAIN] Iter: 157200 Loss: 0.0084514319896698  PSNR: 25.31813621520996
[TRAIN] Iter: 157300 Loss: 0.00702986866235733  PSNR: 26.7690372467041
[TRAIN] Iter: 157400 Loss: 0.006108068395406008  PSNR: 27.05093002319336
[TRAIN] Iter: 157500 Loss: 0.007674070540815592  PSNR: 25.599573135375977
[TRAIN] Iter: 157600 Loss: 0.008200554177165031  PSNR: 25.4176025390625
[TRAIN] Iter: 157700 Loss: 0.005388636142015457  PSNR: 27.070011138916016
[TRAIN] Iter: 157800 Loss: 0.0053404150530695915  PSNR: 27.170698165893555
[TRAIN] Iter: 157900 Loss: 0.006491085514426231  PSNR: 26.424962997436523
[TRAIN] Iter: 158000 Loss: 0.00868790503591299  PSNR: 25.31989860534668
[TRAIN] Iter: 158100 Loss: 0.007163388188928366  PSNR: 26.086544036865234
[TRAIN] Iter: 158200 Loss: 0.007890067063272  PSNR: 25.399274826049805
[TRAIN] Iter: 158300 Loss: 0.006115877069532871  PSNR: 26.63585662841797
[TRAIN] Iter: 158400 Loss: 0.006131098605692387  PSNR: 27.172021865844727
[TRAIN] Iter: 158500 Loss: 0.007558820769190788  PSNR: 25.528915405273438
[TRAIN] Iter: 158600 Loss: 0.00630608107894659  PSNR: 26.608789443969727
[TRAIN] Iter: 158700 Loss: 0.006342822685837746  PSNR: 26.371070861816406
[TRAIN] Iter: 158800 Loss: 0.0066083138808608055  PSNR: 26.600997924804688
[TRAIN] Iter: 158900 Loss: 0.007588803302496672  PSNR: 26.005077362060547
[TRAIN] Iter: 159000 Loss: 0.005238574929535389  PSNR: 28.380924224853516
[TRAIN] Iter: 159100 Loss: 0.0060852463357150555  PSNR: 27.468820571899414
[TRAIN] Iter: 159200 Loss: 0.007631650194525719  PSNR: 25.958316802978516
[TRAIN] Iter: 159300 Loss: 0.00931217335164547  PSNR: 24.581758499145508
[TRAIN] Iter: 159400 Loss: 0.005650875624269247  PSNR: 27.76171112060547
[TRAIN] Iter: 159500 Loss: 0.004760760813951492  PSNR: 28.181543350219727
[TRAIN] Iter: 159600 Loss: 0.008181048557162285  PSNR: 25.167640686035156
[TRAIN] Iter: 159700 Loss: 0.006171748042106628  PSNR: 26.975252151489258
[TRAIN] Iter: 159800 Loss: 0.008051393553614616  PSNR: 25.285350799560547
[TRAIN] Iter: 159900 Loss: 0.007619461044669151  PSNR: 25.909805297851562
Saved checkpoints at ./logs/TUT-KE101-nerf/160000.tar
[TRAIN] Iter: 160000 Loss: 0.006674179341644049  PSNR: 26.60978889465332
[TRAIN] Iter: 160100 Loss: 0.005716684274375439  PSNR: 27.496652603149414
[TRAIN] Iter: 160200 Loss: 0.006574506871402264  PSNR: 26.496347427368164
[TRAIN] Iter: 160300 Loss: 0.0045194728299975395  PSNR: 29.097673416137695
[TRAIN] Iter: 160400 Loss: 0.006343327462673187  PSNR: 26.890037536621094
[TRAIN] Iter: 160500 Loss: 0.00782155804336071  PSNR: 25.893863677978516
[TRAIN] Iter: 160600 Loss: 0.0055021559819579124  PSNR: 28.04536247253418
[TRAIN] Iter: 160700 Loss: 0.005881795194000006  PSNR: 27.341188430786133
[TRAIN] Iter: 160800 Loss: 0.0055543528869748116  PSNR: 27.933691024780273
[TRAIN] Iter: 160900 Loss: 0.005202185362577438  PSNR: 28.338726043701172
[TRAIN] Iter: 161000 Loss: 0.004678763449192047  PSNR: 28.511648178100586
[TRAIN] Iter: 161100 Loss: 0.0067240577191114426  PSNR: 26.015647888183594
[TRAIN] Iter: 161200 Loss: 0.005479313898831606  PSNR: 27.51363182067871
[TRAIN] Iter: 161300 Loss: 0.0056952787563204765  PSNR: 27.923240661621094
[TRAIN] Iter: 161400 Loss: 0.00511332368478179  PSNR: 28.026376724243164
[TRAIN] Iter: 161500 Loss: 0.007422364316880703  PSNR: 26.463083267211914
[TRAIN] Iter: 161600 Loss: 0.007179901469498873  PSNR: 25.90329933166504
[TRAIN] Iter: 161700 Loss: 0.007046014070510864  PSNR: 25.77139663696289
[TRAIN] Iter: 161800 Loss: 0.005563699174672365  PSNR: 27.138633728027344
[TRAIN] Iter: 161900 Loss: 0.006745439488440752  PSNR: 25.858436584472656
[TRAIN] Iter: 162000 Loss: 0.007555023767054081  PSNR: 25.785676956176758
[TRAIN] Iter: 162100 Loss: 0.006287755910307169  PSNR: 26.611799240112305
[TRAIN] Iter: 162200 Loss: 0.00557178258895874  PSNR: 27.21560287475586
[TRAIN] Iter: 162300 Loss: 0.005968833342194557  PSNR: 27.41550064086914
[TRAIN] Iter: 162400 Loss: 0.008413136005401611  PSNR: 25.087421417236328
[TRAIN] Iter: 162500 Loss: 0.004664958920329809  PSNR: 27.868139266967773
[TRAIN] Iter: 162600 Loss: 0.005268081557005644  PSNR: 28.402076721191406
[TRAIN] Iter: 162700 Loss: 0.00758439302444458  PSNR: 25.8586483001709
[TRAIN] Iter: 162800 Loss: 0.005484716966748238  PSNR: 28.447463989257812
[TRAIN] Iter: 162900 Loss: 0.008753213100135326  PSNR: 25.11091423034668
[TRAIN] Iter: 163000 Loss: 0.0073291240260005  PSNR: 25.301128387451172
[TRAIN] Iter: 163100 Loss: 0.007403274066746235  PSNR: 26.342885971069336
[TRAIN] Iter: 163200 Loss: 0.00672263465821743  PSNR: 26.256591796875
[TRAIN] Iter: 163300 Loss: 0.005581112112849951  PSNR: 27.248811721801758
[TRAIN] Iter: 163400 Loss: 0.005937020294368267  PSNR: 26.782840728759766
[TRAIN] Iter: 163500 Loss: 0.006313249934464693  PSNR: 27.1673526763916
[TRAIN] Iter: 163600 Loss: 0.007065141573548317  PSNR: 25.966642379760742
[TRAIN] Iter: 163700 Loss: 0.004426927305757999  PSNR: 28.729501724243164
[TRAIN] Iter: 163800 Loss: 0.00649125874042511  PSNR: 26.654298782348633
[TRAIN] Iter: 163900 Loss: 0.00712825171649456  PSNR: 26.164405822753906
[TRAIN] Iter: 164000 Loss: 0.006887623108923435  PSNR: 26.082433700561523
[TRAIN] Iter: 164100 Loss: 0.005992988124489784  PSNR: 26.793010711669922
[TRAIN] Iter: 164200 Loss: 0.006855488754808903  PSNR: 25.823375701904297
[TRAIN] Iter: 164300 Loss: 0.005222143139690161  PSNR: 28.073678970336914
[TRAIN] Iter: 164400 Loss: 0.00829582754522562  PSNR: 24.979158401489258
[TRAIN] Iter: 164500 Loss: 0.005976961925625801  PSNR: 26.68899154663086
[TRAIN] Iter: 164600 Loss: 0.00594678521156311  PSNR: 27.73673439025879
[TRAIN] Iter: 164700 Loss: 0.006312863435596228  PSNR: 27.10413932800293
[TRAIN] Iter: 164800 Loss: 0.006443158723413944  PSNR: 26.185266494750977
[TRAIN] Iter: 164900 Loss: 0.008043207228183746  PSNR: 25.28109359741211
[TRAIN] Iter: 165000 Loss: 0.007277306169271469  PSNR: 25.410005569458008
[TRAIN] Iter: 165100 Loss: 0.00636772345751524  PSNR: 26.495676040649414
[TRAIN] Iter: 165200 Loss: 0.00857781432569027  PSNR: 25.17755889892578
[TRAIN] Iter: 165300 Loss: 0.007614855654537678  PSNR: 25.855329513549805
[TRAIN] Iter: 165400 Loss: 0.00738426111638546  PSNR: 26.15984535217285
[TRAIN] Iter: 165500 Loss: 0.005520651116967201  PSNR: 27.766923904418945
[TRAIN] Iter: 165600 Loss: 0.0081016905605793  PSNR: 25.465007781982422
[TRAIN] Iter: 165700 Loss: 0.004757737275213003  PSNR: 28.662229537963867
[TRAIN] Iter: 165800 Loss: 0.007351794745773077  PSNR: 25.848094940185547
[TRAIN] Iter: 165900 Loss: 0.004805807955563068  PSNR: 28.151763916015625
[TRAIN] Iter: 166000 Loss: 0.006284941919147968  PSNR: 26.478994369506836
[TRAIN] Iter: 166100 Loss: 0.007005076855421066  PSNR: 26.34952735900879
[TRAIN] Iter: 166200 Loss: 0.005947573576122522  PSNR: 26.900226593017578
[TRAIN] Iter: 166300 Loss: 0.00709263002499938  PSNR: 26.207571029663086
[TRAIN] Iter: 166400 Loss: 0.006212548352777958  PSNR: 26.600509643554688
[TRAIN] Iter: 166500 Loss: 0.007435258943587542  PSNR: 25.552576065063477
[TRAIN] Iter: 166600 Loss: 0.005403084680438042  PSNR: 28.198888778686523
[TRAIN] Iter: 166700 Loss: 0.008547497913241386  PSNR: 25.581192016601562
[TRAIN] Iter: 166800 Loss: 0.005263547878712416  PSNR: 27.65804100036621
[TRAIN] Iter: 166900 Loss: 0.007272124290466309  PSNR: 25.89250946044922
[TRAIN] Iter: 167000 Loss: 0.0070046233013272285  PSNR: 26.251434326171875
[TRAIN] Iter: 167100 Loss: 0.005311544984579086  PSNR: 27.30228614807129
[TRAIN] Iter: 167200 Loss: 0.006344478577375412  PSNR: 26.875442504882812
[TRAIN] Iter: 167300 Loss: 0.004969022236764431  PSNR: 27.59527015686035
[TRAIN] Iter: 167400 Loss: 0.005257061682641506  PSNR: 28.49518394470215
[TRAIN] Iter: 167500 Loss: 0.004210973158478737  PSNR: 28.8443546295166
[TRAIN] Iter: 167600 Loss: 0.00539666973054409  PSNR: 27.388389587402344
[TRAIN] Iter: 167700 Loss: 0.006125325337052345  PSNR: 26.76799964904785
[TRAIN] Iter: 167800 Loss: 0.006967639550566673  PSNR: 25.933523178100586
[TRAIN] Iter: 167900 Loss: 0.005411325488239527  PSNR: 28.06755256652832
[TRAIN] Iter: 168000 Loss: 0.00772141246125102  PSNR: 25.186107635498047
[TRAIN] Iter: 168100 Loss: 0.00511858519166708  PSNR: 28.237834930419922
[TRAIN] Iter: 168200 Loss: 0.008107941597700119  PSNR: 25.50347900390625
[TRAIN] Iter: 168300 Loss: 0.006933179683983326  PSNR: 26.317625045776367
[TRAIN] Iter: 168400 Loss: 0.0071809738874435425  PSNR: 26.65205955505371
[TRAIN] Iter: 168500 Loss: 0.0071166870184242725  PSNR: 26.2298526763916
[TRAIN] Iter: 168600 Loss: 0.006772191263735294  PSNR: 26.04213523864746
[TRAIN] Iter: 168700 Loss: 0.007568280678242445  PSNR: 26.023427963256836
[TRAIN] Iter: 168800 Loss: 0.005710264667868614  PSNR: 27.352487564086914
[TRAIN] Iter: 168900 Loss: 0.0066710119135677814  PSNR: 26.05671501159668
[TRAIN] Iter: 169000 Loss: 0.004879379644989967  PSNR: 28.251811981201172
[TRAIN] Iter: 169100 Loss: 0.005444813519716263  PSNR: 27.31611442565918
[TRAIN] Iter: 169200 Loss: 0.004290296696126461  PSNR: 28.019853591918945
[TRAIN] Iter: 169300 Loss: 0.004967131651937962  PSNR: 28.250896453857422
[TRAIN] Iter: 169400 Loss: 0.006232582498341799  PSNR: 26.3226375579834
[TRAIN] Iter: 169500 Loss: 0.005376126617193222  PSNR: 27.92449378967285
[TRAIN] Iter: 169600 Loss: 0.005550792440772057  PSNR: 27.97257423400879
[TRAIN] Iter: 169700 Loss: 0.006609375588595867  PSNR: 26.134540557861328
[TRAIN] Iter: 169800 Loss: 0.004995447583496571  PSNR: 27.937475204467773
[TRAIN] Iter: 169900 Loss: 0.007976768538355827  PSNR: 25.373184204101562
Saved checkpoints at ./logs/TUT-KE101-nerf/170000.tar
[TRAIN] Iter: 170000 Loss: 0.004754433874040842  PSNR: 28.472637176513672
[TRAIN] Iter: 170100 Loss: 0.006246962118893862  PSNR: 26.585590362548828
[TRAIN] Iter: 170200 Loss: 0.004990109242498875  PSNR: 28.01593017578125
[TRAIN] Iter: 170300 Loss: 0.0074316589161753654  PSNR: 25.77500343322754
[TRAIN] Iter: 170400 Loss: 0.005033371038734913  PSNR: 27.834623336791992
[TRAIN] Iter: 170500 Loss: 0.004675468895584345  PSNR: 28.951396942138672
[TRAIN] Iter: 170600 Loss: 0.005743773188441992  PSNR: 27.125688552856445
[TRAIN] Iter: 170700 Loss: 0.006301873829215765  PSNR: 27.05272102355957
[TRAIN] Iter: 170800 Loss: 0.004600629210472107  PSNR: 28.266443252563477
[TRAIN] Iter: 170900 Loss: 0.006419641897082329  PSNR: 26.579206466674805
[TRAIN] Iter: 171000 Loss: 0.007143271621316671  PSNR: 26.633037567138672
[TRAIN] Iter: 171100 Loss: 0.0084703853353858  PSNR: 24.94426727294922
[TRAIN] Iter: 171200 Loss: 0.0061075109988451  PSNR: 26.68656349182129
[TRAIN] Iter: 171300 Loss: 0.007774783298373222  PSNR: 25.917617797851562
[TRAIN] Iter: 171400 Loss: 0.005118660628795624  PSNR: 27.620655059814453
[TRAIN] Iter: 171500 Loss: 0.007963525131344795  PSNR: 25.62734031677246
[TRAIN] Iter: 171600 Loss: 0.004672101698815823  PSNR: 28.555126190185547
[TRAIN] Iter: 171700 Loss: 0.004758228547871113  PSNR: 28.56306266784668
[TRAIN] Iter: 171800 Loss: 0.004903572611510754  PSNR: 28.373884201049805
[TRAIN] Iter: 171900 Loss: 0.007508410606533289  PSNR: 25.87025260925293
[TRAIN] Iter: 172000 Loss: 0.005347603000700474  PSNR: 27.506633758544922
[TRAIN] Iter: 172100 Loss: 0.00439651170745492  PSNR: 29.124969482421875
[TRAIN] Iter: 172200 Loss: 0.006882732734084129  PSNR: 25.780393600463867
[TRAIN] Iter: 172300 Loss: 0.007366325240582228  PSNR: 26.46098518371582
[TRAIN] Iter: 172400 Loss: 0.004098328296095133  PSNR: 28.839834213256836
[TRAIN] Iter: 172500 Loss: 0.00565712945535779  PSNR: 28.025869369506836
[TRAIN] Iter: 172600 Loss: 0.00732297170907259  PSNR: 25.984399795532227
[TRAIN] Iter: 172700 Loss: 0.00476475153118372  PSNR: 28.03440284729004
[TRAIN] Iter: 172800 Loss: 0.007582249119877815  PSNR: 25.511091232299805
[TRAIN] Iter: 172900 Loss: 0.004706290550529957  PSNR: 28.247753143310547
[TRAIN] Iter: 173000 Loss: 0.00621524453163147  PSNR: 26.859683990478516
[TRAIN] Iter: 173100 Loss: 0.005009772256016731  PSNR: 28.281208038330078
[TRAIN] Iter: 173200 Loss: 0.004700597375631332  PSNR: 28.37023162841797
[TRAIN] Iter: 173300 Loss: 0.006278710439801216  PSNR: 26.338848114013672
[TRAIN] Iter: 173400 Loss: 0.006242287345230579  PSNR: 27.022315979003906
[TRAIN] Iter: 173500 Loss: 0.0058170463889837265  PSNR: 27.111289978027344
[TRAIN] Iter: 173600 Loss: 0.005159187130630016  PSNR: 28.458202362060547
[TRAIN] Iter: 173700 Loss: 0.00717325322329998  PSNR: 26.30624771118164
[TRAIN] Iter: 173800 Loss: 0.004121267702430487  PSNR: 29.0421142578125
[TRAIN] Iter: 173900 Loss: 0.007507017347961664  PSNR: 25.588335037231445
[TRAIN] Iter: 174000 Loss: 0.006517151836305857  PSNR: 26.445112228393555
[TRAIN] Iter: 174100 Loss: 0.005646455567330122  PSNR: 27.88585662841797
[TRAIN] Iter: 174200 Loss: 0.005099662579596043  PSNR: 28.211772918701172
[TRAIN] Iter: 174300 Loss: 0.005149927455931902  PSNR: 28.193363189697266
[TRAIN] Iter: 174400 Loss: 0.0062264334410429  PSNR: 26.277387619018555
[TRAIN] Iter: 174500 Loss: 0.006981872487813234  PSNR: 26.621015548706055
[TRAIN] Iter: 174600 Loss: 0.008157305419445038  PSNR: 25.318025588989258
[TRAIN] Iter: 174700 Loss: 0.005591633729636669  PSNR: 27.440284729003906
[TRAIN] Iter: 174800 Loss: 0.005091839469969273  PSNR: 27.7601318359375
[TRAIN] Iter: 174900 Loss: 0.005595969036221504  PSNR: 26.94499397277832
[TRAIN] Iter: 175000 Loss: 0.004951009061187506  PSNR: 28.97071075439453
[TRAIN] Iter: 175100 Loss: 0.006511223502457142  PSNR: 26.403911590576172
[TRAIN] Iter: 175200 Loss: 0.005822745617479086  PSNR: 26.890512466430664
[TRAIN] Iter: 175300 Loss: 0.007180978078395128  PSNR: 26.46600341796875
[TRAIN] Iter: 175400 Loss: 0.008608602918684483  PSNR: 25.547897338867188
[TRAIN] Iter: 175500 Loss: 0.004333579912781715  PSNR: 29.163616180419922
[TRAIN] Iter: 175600 Loss: 0.006909684278070927  PSNR: 26.373451232910156
[TRAIN] Iter: 175700 Loss: 0.004863746929913759  PSNR: 28.404970169067383
[TRAIN] Iter: 175800 Loss: 0.006928956601768732  PSNR: 25.874752044677734
[TRAIN] Iter: 175900 Loss: 0.005888293497264385  PSNR: 27.2133846282959
[TRAIN] Iter: 176000 Loss: 0.004259414505213499  PSNR: 28.84779930114746
[TRAIN] Iter: 176100 Loss: 0.006797297857701778  PSNR: 26.472810745239258
[TRAIN] Iter: 176200 Loss: 0.007602559868246317  PSNR: 25.449092864990234
[TRAIN] Iter: 176300 Loss: 0.007360137067735195  PSNR: 25.812294006347656
[TRAIN] Iter: 176400 Loss: 0.007058005779981613  PSNR: 25.93886947631836
[TRAIN] Iter: 176500 Loss: 0.006325208581984043  PSNR: 27.668624877929688
[TRAIN] Iter: 176600 Loss: 0.007928306236863136  PSNR: 25.56386375427246
[TRAIN] Iter: 176700 Loss: 0.007564804516732693  PSNR: 25.875391006469727
[TRAIN] Iter: 176800 Loss: 0.007999365217983723  PSNR: 25.99152946472168
[TRAIN] Iter: 176900 Loss: 0.007992981001734734  PSNR: 24.82096290588379
[TRAIN] Iter: 177000 Loss: 0.006725262384861708  PSNR: 26.51200294494629
[TRAIN] Iter: 177100 Loss: 0.006615021266043186  PSNR: 26.156652450561523
[TRAIN] Iter: 177200 Loss: 0.004554846789687872  PSNR: 28.92953872680664
[TRAIN] Iter: 177300 Loss: 0.007704383693635464  PSNR: 25.641162872314453
[TRAIN] Iter: 177400 Loss: 0.005688618868589401  PSNR: 26.89629364013672
[TRAIN] Iter: 177500 Loss: 0.004782593343406916  PSNR: 27.960996627807617
[TRAIN] Iter: 177600 Loss: 0.005693467333912849  PSNR: 26.95061683654785
[TRAIN] Iter: 177700 Loss: 0.005337835289537907  PSNR: 27.11906623840332
[TRAIN] Iter: 177800 Loss: 0.006105102598667145  PSNR: 27.159212112426758
[TRAIN] Iter: 177900 Loss: 0.00756494328379631  PSNR: 26.26891326904297
[TRAIN] Iter: 178000 Loss: 0.00475973729044199  PSNR: 29.10537338256836
[TRAIN] Iter: 178100 Loss: 0.0049211448058485985  PSNR: 28.22833824157715
[TRAIN] Iter: 178200 Loss: 0.007100783754140139  PSNR: 25.79220199584961
[TRAIN] Iter: 178300 Loss: 0.005775913130491972  PSNR: 27.020160675048828
[TRAIN] Iter: 178400 Loss: 0.005242985673248768  PSNR: 27.967620849609375
[TRAIN] Iter: 178500 Loss: 0.004969886504113674  PSNR: 28.107343673706055
[TRAIN] Iter: 178600 Loss: 0.0051670437678694725  PSNR: 27.361682891845703
[TRAIN] Iter: 178700 Loss: 0.005395341664552689  PSNR: 27.71327018737793
[TRAIN] Iter: 178800 Loss: 0.007679678965359926  PSNR: 25.8249454498291
[TRAIN] Iter: 178900 Loss: 0.004767782520502806  PSNR: 28.182395935058594
[TRAIN] Iter: 179000 Loss: 0.00730739114806056  PSNR: 26.33991050720215
[TRAIN] Iter: 179100 Loss: 0.006676102057099342  PSNR: 26.461811065673828
[TRAIN] Iter: 179200 Loss: 0.006866169162094593  PSNR: 26.6058349609375
[TRAIN] Iter: 179300 Loss: 0.00455893948674202  PSNR: 28.744709014892578
[TRAIN] Iter: 179400 Loss: 0.00508976262062788  PSNR: 28.504722595214844
[TRAIN] Iter: 179500 Loss: 0.005498523823916912  PSNR: 27.3792667388916
[TRAIN] Iter: 179600 Loss: 0.006767702288925648  PSNR: 26.243928909301758
[TRAIN] Iter: 179700 Loss: 0.00571850873529911  PSNR: 26.777320861816406
[TRAIN] Iter: 179800 Loss: 0.006542243529111147  PSNR: 26.537574768066406
[TRAIN] Iter: 179900 Loss: 0.0053193336352705956  PSNR: 28.428041458129883
Saved checkpoints at ./logs/TUT-KE101-nerf/180000.tar
[TRAIN] Iter: 180000 Loss: 0.005217354744672775  PSNR: 28.884750366210938
[TRAIN] Iter: 180100 Loss: 0.005812104791402817  PSNR: 27.35294532775879
[TRAIN] Iter: 180200 Loss: 0.008512316271662712  PSNR: 24.49456787109375
[TRAIN] Iter: 180300 Loss: 0.007731623016297817  PSNR: 26.423019409179688
[TRAIN] Iter: 180400 Loss: 0.005297532305121422  PSNR: 27.27886962890625
[TRAIN] Iter: 180500 Loss: 0.007444035727530718  PSNR: 25.65181541442871
[TRAIN] Iter: 180600 Loss: 0.007496057078242302  PSNR: 25.96644401550293
[TRAIN] Iter: 180700 Loss: 0.007077774032950401  PSNR: 26.020822525024414
[TRAIN] Iter: 180800 Loss: 0.004936094395816326  PSNR: 28.103641510009766
[TRAIN] Iter: 180900 Loss: 0.004826358053833246  PSNR: 27.88321876525879
[TRAIN] Iter: 181000 Loss: 0.00708911195397377  PSNR: 25.808433532714844
[TRAIN] Iter: 181100 Loss: 0.005859374068677425  PSNR: 26.991098403930664
[TRAIN] Iter: 181200 Loss: 0.006200904957950115  PSNR: 26.757923126220703
[TRAIN] Iter: 181300 Loss: 0.006956262513995171  PSNR: 26.0726261138916
[TRAIN] Iter: 181400 Loss: 0.005724248010665178  PSNR: 27.6340389251709
[TRAIN] Iter: 181500 Loss: 0.007612859830260277  PSNR: 25.449665069580078
[TRAIN] Iter: 181600 Loss: 0.007169734686613083  PSNR: 26.30221176147461
[TRAIN] Iter: 181700 Loss: 0.005732422694563866  PSNR: 28.04431915283203
[TRAIN] Iter: 181800 Loss: 0.006425213068723679  PSNR: 26.365779876708984
[TRAIN] Iter: 181900 Loss: 0.00534010911360383  PSNR: 27.863162994384766
[TRAIN] Iter: 182000 Loss: 0.007595451548695564  PSNR: 25.779176712036133
[TRAIN] Iter: 182100 Loss: 0.007440425921231508  PSNR: 26.44507598876953
[TRAIN] Iter: 182200 Loss: 0.005822360515594482  PSNR: 26.690338134765625
[TRAIN] Iter: 182300 Loss: 0.0071142129600048065  PSNR: 26.033811569213867
[TRAIN] Iter: 182400 Loss: 0.007003429811447859  PSNR: 26.13936996459961
[TRAIN] Iter: 182500 Loss: 0.006931106559932232  PSNR: 26.010257720947266
[TRAIN] Iter: 182600 Loss: 0.0037610577419400215  PSNR: 29.54367446899414
[TRAIN] Iter: 182700 Loss: 0.007054983638226986  PSNR: 25.72266960144043
[TRAIN] Iter: 182800 Loss: 0.005220378283411264  PSNR: 27.451234817504883
[TRAIN] Iter: 182900 Loss: 0.005675499327480793  PSNR: 27.378524780273438
[TRAIN] Iter: 183000 Loss: 0.006619769148528576  PSNR: 26.909011840820312
[TRAIN] Iter: 183100 Loss: 0.005090320482850075  PSNR: 28.718103408813477
[TRAIN] Iter: 183200 Loss: 0.004724967759102583  PSNR: 28.43083381652832
[TRAIN] Iter: 183300 Loss: 0.004999144934117794  PSNR: 28.02998924255371
[TRAIN] Iter: 183400 Loss: 0.004747888073325157  PSNR: 28.741506576538086
[TRAIN] Iter: 183500 Loss: 0.006707167252898216  PSNR: 26.505374908447266
[TRAIN] Iter: 183600 Loss: 0.00497853197157383  PSNR: 28.46289825439453
[TRAIN] Iter: 183700 Loss: 0.005271616391837597  PSNR: 27.814319610595703
[TRAIN] Iter: 183800 Loss: 0.007128075696527958  PSNR: 26.49941062927246
[TRAIN] Iter: 183900 Loss: 0.005479424726217985  PSNR: 26.81819725036621
[TRAIN] Iter: 184000 Loss: 0.007351236883550882  PSNR: 26.087177276611328
[TRAIN] Iter: 184100 Loss: 0.006579954642802477  PSNR: 26.8079891204834
[TRAIN] Iter: 184200 Loss: 0.004988539032638073  PSNR: 28.48993492126465
[TRAIN] Iter: 184300 Loss: 0.005258932709693909  PSNR: 28.63861083984375
[TRAIN] Iter: 184400 Loss: 0.006591743789613247  PSNR: 26.44036865234375
[TRAIN] Iter: 184500 Loss: 0.004609736613929272  PSNR: 28.620267868041992
[TRAIN] Iter: 184600 Loss: 0.004854366183280945  PSNR: 28.527841567993164
[TRAIN] Iter: 184700 Loss: 0.0049263909459114075  PSNR: 27.318740844726562
[TRAIN] Iter: 184800 Loss: 0.005400213412940502  PSNR: 28.352903366088867
[TRAIN] Iter: 184900 Loss: 0.0073189400136470795  PSNR: 26.477476119995117
[TRAIN] Iter: 185000 Loss: 0.0069274092093110085  PSNR: 26.202781677246094
[TRAIN] Iter: 185100 Loss: 0.003908450249582529  PSNR: 29.126996994018555
[TRAIN] Iter: 185200 Loss: 0.0050500258803367615  PSNR: 28.425798416137695
[TRAIN] Iter: 185300 Loss: 0.00700894370675087  PSNR: 25.85298728942871
[TRAIN] Iter: 185400 Loss: 0.005953403655439615  PSNR: 27.143142700195312
[TRAIN] Iter: 185500 Loss: 0.00477154366672039  PSNR: 28.31783676147461
[TRAIN] Iter: 185600 Loss: 0.004997621290385723  PSNR: 28.701065063476562
[TRAIN] Iter: 185700 Loss: 0.005421921610832214  PSNR: 27.937137603759766
[TRAIN] Iter: 185800 Loss: 0.0066382987424731255  PSNR: 27.133373260498047
[TRAIN] Iter: 185900 Loss: 0.005973051302134991  PSNR: 26.714540481567383
[TRAIN] Iter: 186000 Loss: 0.005707624834030867  PSNR: 26.800443649291992
[TRAIN] Iter: 186100 Loss: 0.005753255914896727  PSNR: 26.46466064453125
[TRAIN] Iter: 186200 Loss: 0.004869477823376656  PSNR: 27.6072998046875
[TRAIN] Iter: 186300 Loss: 0.00465714605525136  PSNR: 28.344417572021484
[TRAIN] Iter: 186400 Loss: 0.005704755894839764  PSNR: 27.267532348632812
[TRAIN] Iter: 186500 Loss: 0.007255446631461382  PSNR: 26.79408836364746
[TRAIN] Iter: 186600 Loss: 0.0060402583330869675  PSNR: 26.742429733276367
[TRAIN] Iter: 186700 Loss: 0.004568836651742458  PSNR: 29.25446128845215
[TRAIN] Iter: 186800 Loss: 0.006407946813851595  PSNR: 26.764354705810547
[TRAIN] Iter: 186900 Loss: 0.005219161044806242  PSNR: 27.702754974365234
[TRAIN] Iter: 187000 Loss: 0.005057596601545811  PSNR: 28.15142822265625
[TRAIN] Iter: 187100 Loss: 0.006370715796947479  PSNR: 26.80088996887207
[TRAIN] Iter: 187200 Loss: 0.006560679990798235  PSNR: 27.12398910522461
[TRAIN] Iter: 187300 Loss: 0.006043208297342062  PSNR: 26.651844024658203
[TRAIN] Iter: 187400 Loss: 0.004845523275434971  PSNR: 28.523788452148438
[TRAIN] Iter: 187500 Loss: 0.0066640423610806465  PSNR: 25.951040267944336
[TRAIN] Iter: 187600 Loss: 0.005054968409240246  PSNR: 28.52039337158203
[TRAIN] Iter: 187700 Loss: 0.006074939854443073  PSNR: 26.67169189453125
[TRAIN] Iter: 187800 Loss: 0.0058548892848193645  PSNR: 27.662519454956055
[TRAIN] Iter: 187900 Loss: 0.006990734953433275  PSNR: 25.991695404052734
[TRAIN] Iter: 188000 Loss: 0.004985977429896593  PSNR: 27.97749137878418
[TRAIN] Iter: 188100 Loss: 0.007283029146492481  PSNR: 25.770509719848633
[TRAIN] Iter: 188200 Loss: 0.004792105406522751  PSNR: 28.10647964477539
[TRAIN] Iter: 188300 Loss: 0.004978623241186142  PSNR: 28.330856323242188
[TRAIN] Iter: 188400 Loss: 0.004447960760444403  PSNR: 28.952016830444336
[TRAIN] Iter: 188500 Loss: 0.0052821580320596695  PSNR: 28.253034591674805
[TRAIN] Iter: 188600 Loss: 0.004664476960897446  PSNR: 28.579580307006836
[TRAIN] Iter: 188700 Loss: 0.0066062891855835915  PSNR: 26.097984313964844
[TRAIN] Iter: 188800 Loss: 0.00607210723683238  PSNR: 26.654712677001953
[TRAIN] Iter: 188900 Loss: 0.006551383063197136  PSNR: 27.166501998901367
[TRAIN] Iter: 189000 Loss: 0.006490760948508978  PSNR: 27.509201049804688
[TRAIN] Iter: 189100 Loss: 0.004253189545124769  PSNR: 28.86525535583496
[TRAIN] Iter: 189200 Loss: 0.004392078146338463  PSNR: 28.648744583129883
[TRAIN] Iter: 189300 Loss: 0.006382449995726347  PSNR: 26.87907600402832
[TRAIN] Iter: 189400 Loss: 0.004555034451186657  PSNR: 28.18172264099121
[TRAIN] Iter: 189500 Loss: 0.005129889585077763  PSNR: 28.05790138244629
[TRAIN] Iter: 189600 Loss: 0.0075430735014379025  PSNR: 25.36004066467285
[TRAIN] Iter: 189700 Loss: 0.0056857094168663025  PSNR: 26.550148010253906
[TRAIN] Iter: 189800 Loss: 0.006198378745466471  PSNR: 27.30243492126465
[TRAIN] Iter: 189900 Loss: 0.005127870012074709  PSNR: 27.662879943847656
Saved checkpoints at ./logs/TUT-KE101-nerf/190000.tar
[TRAIN] Iter: 190000 Loss: 0.006638483610004187  PSNR: 26.381155014038086
[TRAIN] Iter: 190100 Loss: 0.006348935421556234  PSNR: 26.465137481689453
[TRAIN] Iter: 190200 Loss: 0.004795115441083908  PSNR: 28.17991065979004
[TRAIN] Iter: 190300 Loss: 0.006079582031816244  PSNR: 26.985008239746094
[TRAIN] Iter: 190400 Loss: 0.008643039502203465  PSNR: 25.168184280395508
[TRAIN] Iter: 190500 Loss: 0.005868467502295971  PSNR: 27.7589168548584
[TRAIN] Iter: 190600 Loss: 0.006336873862892389  PSNR: 26.108386993408203
[TRAIN] Iter: 190700 Loss: 0.005997679196298122  PSNR: 26.978410720825195
[TRAIN] Iter: 190800 Loss: 0.00845652911812067  PSNR: 25.64996910095215
[TRAIN] Iter: 190900 Loss: 0.005036204122006893  PSNR: 28.079004287719727
[TRAIN] Iter: 191000 Loss: 0.004801773466169834  PSNR: 28.48919105529785
[TRAIN] Iter: 191100 Loss: 0.005998627282679081  PSNR: 26.697866439819336
[TRAIN] Iter: 191200 Loss: 0.004904350731521845  PSNR: 28.399465560913086
[TRAIN] Iter: 191300 Loss: 0.007800656836479902  PSNR: 25.480995178222656
[TRAIN] Iter: 191400 Loss: 0.006920051760971546  PSNR: 25.933883666992188
[TRAIN] Iter: 191500 Loss: 0.006708238273859024  PSNR: 26.43281364440918
[TRAIN] Iter: 191600 Loss: 0.005188558716326952  PSNR: 27.690195083618164
[TRAIN] Iter: 191700 Loss: 0.007001273334026337  PSNR: 26.22117805480957
[TRAIN] Iter: 191800 Loss: 0.005637275520712137  PSNR: 27.93057632446289
[TRAIN] Iter: 191900 Loss: 0.00442547257989645  PSNR: 29.256017684936523
[TRAIN] Iter: 192000 Loss: 0.004452554043382406  PSNR: 29.010692596435547
[TRAIN] Iter: 192100 Loss: 0.004536003805696964  PSNR: 29.158626556396484
[TRAIN] Iter: 192200 Loss: 0.005048670340329409  PSNR: 28.647411346435547
[TRAIN] Iter: 192300 Loss: 0.006702787708491087  PSNR: 26.533985137939453
[TRAIN] Iter: 192400 Loss: 0.005591479130089283  PSNR: 27.413436889648438
[TRAIN] Iter: 192500 Loss: 0.006490248255431652  PSNR: 26.504230499267578
[TRAIN] Iter: 192600 Loss: 0.006823382340371609  PSNR: 26.23801040649414
[TRAIN] Iter: 192700 Loss: 0.007247431203722954  PSNR: 25.735122680664062
[TRAIN] Iter: 192800 Loss: 0.006651806645095348  PSNR: 26.427600860595703
[TRAIN] Iter: 192900 Loss: 0.007659846916794777  PSNR: 26.28123664855957
[TRAIN] Iter: 193000 Loss: 0.006219936069101095  PSNR: 26.9652156829834
[TRAIN] Iter: 193100 Loss: 0.005821529775857925  PSNR: 26.990978240966797
[TRAIN] Iter: 193200 Loss: 0.006648083683103323  PSNR: 26.066612243652344
[TRAIN] Iter: 193300 Loss: 0.005985889118164778  PSNR: 26.78367042541504
[TRAIN] Iter: 193400 Loss: 0.00753876194357872  PSNR: 25.57155990600586
[TRAIN] Iter: 193500 Loss: 0.004850239958614111  PSNR: 27.91682243347168
[TRAIN] Iter: 193600 Loss: 0.007209747564047575  PSNR: 26.38897705078125
[TRAIN] Iter: 193700 Loss: 0.005918411072343588  PSNR: 27.143918991088867
[TRAIN] Iter: 193800 Loss: 0.005219493992626667  PSNR: 27.318490982055664
[TRAIN] Iter: 193900 Loss: 0.007222624495625496  PSNR: 25.786266326904297
[TRAIN] Iter: 194000 Loss: 0.005433752667158842  PSNR: 28.732769012451172
[TRAIN] Iter: 194100 Loss: 0.005561593919992447  PSNR: 27.465587615966797
[TRAIN] Iter: 194200 Loss: 0.007777663879096508  PSNR: 25.438201904296875
[TRAIN] Iter: 194300 Loss: 0.005097965244203806  PSNR: 27.70730209350586
[TRAIN] Iter: 194400 Loss: 0.00618869811296463  PSNR: 26.659439086914062
[TRAIN] Iter: 194500 Loss: 0.004530772101134062  PSNR: 28.286724090576172
[TRAIN] Iter: 194600 Loss: 0.005245009437203407  PSNR: 28.20846176147461
[TRAIN] Iter: 194700 Loss: 0.0064596631564199924  PSNR: 26.87397575378418
[TRAIN] Iter: 194800 Loss: 0.0073157548904418945  PSNR: 26.1868953704834
[TRAIN] Iter: 194900 Loss: 0.007672290317714214  PSNR: 25.673084259033203
[TRAIN] Iter: 195000 Loss: 0.005588766187429428  PSNR: 27.249921798706055
[TRAIN] Iter: 195100 Loss: 0.005763988941907883  PSNR: 27.152633666992188
[TRAIN] Iter: 195200 Loss: 0.006115974858403206  PSNR: 26.885940551757812
[TRAIN] Iter: 195300 Loss: 0.006263460498303175  PSNR: 26.615461349487305
[TRAIN] Iter: 195400 Loss: 0.006576350424438715  PSNR: 26.804141998291016
[TRAIN] Iter: 195500 Loss: 0.006529742851853371  PSNR: 26.453594207763672
[TRAIN] Iter: 195600 Loss: 0.006028881762176752  PSNR: 27.091440200805664
[TRAIN] Iter: 195700 Loss: 0.005940509960055351  PSNR: 27.40325927734375
[TRAIN] Iter: 195800 Loss: 0.005835243035107851  PSNR: 27.116416931152344
[TRAIN] Iter: 195900 Loss: 0.006666503846645355  PSNR: 26.06220054626465
[TRAIN] Iter: 196000 Loss: 0.005221057217568159  PSNR: 28.62042808532715
[TRAIN] Iter: 196100 Loss: 0.007256891578435898  PSNR: 26.272104263305664
[TRAIN] Iter: 196200 Loss: 0.004313270095735788  PSNR: 29.034725189208984
[TRAIN] Iter: 196300 Loss: 0.00596892274916172  PSNR: 27.74724006652832
[TRAIN] Iter: 196400 Loss: 0.006437437143176794  PSNR: 26.318321228027344
[TRAIN] Iter: 196500 Loss: 0.004634975455701351  PSNR: 28.916847229003906
[TRAIN] Iter: 196600 Loss: 0.006391401868313551  PSNR: 27.154909133911133
[TRAIN] Iter: 196700 Loss: 0.007376819849014282  PSNR: 26.80486488342285
[TRAIN] Iter: 196800 Loss: 0.0062305559404194355  PSNR: 27.211584091186523
[TRAIN] Iter: 196900 Loss: 0.004333615768700838  PSNR: 28.59954261779785
[TRAIN] Iter: 197000 Loss: 0.006096678785979748  PSNR: 26.067873001098633
[TRAIN] Iter: 197100 Loss: 0.007681515999138355  PSNR: 25.576231002807617
[TRAIN] Iter: 197200 Loss: 0.007192481309175491  PSNR: 26.06441307067871
[TRAIN] Iter: 197300 Loss: 0.004714809358119965  PSNR: 28.51389503479004
[TRAIN] Iter: 197400 Loss: 0.006362512242048979  PSNR: 26.529787063598633
[TRAIN] Iter: 197500 Loss: 0.0070821866393089294  PSNR: 25.82781982421875
[TRAIN] Iter: 197600 Loss: 0.005434626247733831  PSNR: 28.27972984313965
[TRAIN] Iter: 197700 Loss: 0.004231974482536316  PSNR: 29.079612731933594
[TRAIN] Iter: 197800 Loss: 0.007106814533472061  PSNR: 25.886865615844727
[TRAIN] Iter: 197900 Loss: 0.006284486968070269  PSNR: 26.311988830566406
[TRAIN] Iter: 198000 Loss: 0.006612539291381836  PSNR: 26.273149490356445
[TRAIN] Iter: 198100 Loss: 0.00484124943614006  PSNR: 28.668292999267578
[TRAIN] Iter: 198200 Loss: 0.007570408284664154  PSNR: 25.67271614074707
[TRAIN] Iter: 198300 Loss: 0.004662282764911652  PSNR: 28.384967803955078
[TRAIN] Iter: 198400 Loss: 0.006979038938879967  PSNR: 26.221385955810547
[TRAIN] Iter: 198500 Loss: 0.0047354139387607574  PSNR: 28.171232223510742
[TRAIN] Iter: 198600 Loss: 0.0038129433523863554  PSNR: 29.097278594970703
[TRAIN] Iter: 198700 Loss: 0.00643910001963377  PSNR: 26.7701473236084
[TRAIN] Iter: 198800 Loss: 0.004495351575314999  PSNR: 28.67781639099121
[TRAIN] Iter: 198900 Loss: 0.004870831035077572  PSNR: 28.34503746032715
[TRAIN] Iter: 199000 Loss: 0.007164783775806427  PSNR: 27.00448989868164
[TRAIN] Iter: 199100 Loss: 0.004693956580013037  PSNR: 28.90667152404785
[TRAIN] Iter: 199200 Loss: 0.0048478953540325165  PSNR: 28.660062789916992
[TRAIN] Iter: 199300 Loss: 0.005783165339380503  PSNR: 27.7132511138916
[TRAIN] Iter: 199400 Loss: 0.006228179205209017  PSNR: 27.22016716003418
[TRAIN] Iter: 199500 Loss: 0.006045375019311905  PSNR: 26.400390625
[TRAIN] Iter: 199600 Loss: 0.005785027518868446  PSNR: 26.676578521728516
[TRAIN] Iter: 199700 Loss: 0.005090806633234024  PSNR: 28.007600784301758
[TRAIN] Iter: 199800 Loss: 0.0059164175763726234  PSNR: 26.943126678466797
[TRAIN] Iter: 199900 Loss: 0.0038807354867458344  PSNR: 30.080081939697266
Saved checkpoints at ./logs/TUT-KE101-nerf/200000.tar
0 0.00034332275390625
torch.Size([320, 640, 3]) torch.Size([320, 640])
1 13.255064964294434
2 15.473551511764526
3 12.975257158279419
4 15.908499240875244
5 12.997616529464722
6 13.146973848342896
7 15.841295719146729
8 12.966744661331177
9 15.937156438827515
10 12.984936475753784
11 13.08723258972168
12 15.844808101654053
13 12.94812798500061
14 15.931122303009033
15 12.912407159805298
16 13.285346269607544
17 15.605107545852661
18 13.242966413497925
19 15.760048389434814
20 13.184036254882812
21 15.494604587554932
22 13.089597225189209
23 13.276253938674927
24 15.551851749420166
25 13.0510094165802
26 15.867903232574463
27 12.954766988754272
28 13.199342250823975
29 15.806458234786987
30 13.007030725479126
31 15.898980140686035
32 13.039414644241333
33 13.126606702804565
34 15.67426061630249
35 13.1616792678833
36 15.72944688796997
37 13.067980289459229
38 13.156010150909424
39 15.652573585510254
40 13.196376323699951
41 15.676873922348022
42 13.114214181900024
43 15.837075471878052
44 12.980094194412231
45 13.193702697753906
46 15.693367004394531
47 12.98482871055603
48 15.848592519760132
49 13.37054991722107
50 13.040756940841675
51 15.845287561416626
52 12.99971890449524
53 15.632552862167358
54 13.320158958435059
55 13.047720670700073
56 15.963819026947021
57 12.957890510559082
58 15.698949098587036
59 13.146413803100586
60 13.126405954360962
61 15.813369035720825
62 13.032901048660278
63 15.927573204040527
64 12.952224254608154
65 15.547049522399902
66 13.401224374771118
67 12.991969347000122
68 15.827677726745605
69 12.967347145080566
70 15.911340713500977
71 15.078570127487183
72 14.912123680114746
73 18.078838348388672
74 15.349896430969238
75 16.274465560913086
76 14.749424695968628
77 17.847326278686523
78 14.732235431671143
79 18.05046248435974
80 15.171360492706299
81 18.16784644126892
82 14.67466425895691
83 17.70678162574768
84 14.7242271900177
85 17.641040802001953
86 14.82660698890686
87 17.446542024612427
88 15.124380350112915
89 14.735260725021362
90 17.613742351531982
91 14.770896196365356
92 17.71289324760437
93 14.745731830596924
94 17.775164127349854
95 14.920207262039185
96 17.53548574447632
97 14.961444616317749
98 17.51898455619812
99 14.96327018737793
100 17.48145866394043
101 14.909919738769531
102 17.311588764190674
103 15.058879613876343
104 15.006828546524048
105 17.422364711761475
106 15.05936598777771
107 17.2925443649292
108 14.740518569946289
109 17.7332022190094
110 14.83922553062439
111 18.173665046691895
112 15.148988485336304
113 18.44817304611206
114 15.506638288497925
115 16.165332317352295
116 14.751088619232178
117 17.79341149330139
118 15.142364025115967
119 15.233107805252075
Done, saving (120, 320, 640, 3) (120, 320, 640)
extras:{'raw': tensor([[[-6.1446e-01, -6.7181e-01, -6.2065e-01, -4.6610e+01],
         [-1.0745e-01, -2.7002e-01, -5.3272e-01,  8.9987e+00],
         [-1.0895e-01, -2.7066e-01, -5.2880e-01,  9.3463e+00],
         ...,
         [ 6.7527e+00,  5.8583e+00,  5.1199e+00,  9.5782e+02],
         [ 6.8761e+00,  6.6931e+00,  7.0015e+00,  9.4150e+02],
         [ 6.4986e+00,  4.9063e+00,  3.4369e+00,  1.0578e+03]],

        [[-1.6453e+00, -8.7821e-01,  4.2008e-01, -7.6194e+01],
         [-9.9717e-01, -1.2177e+00, -1.7315e+00, -3.0145e+00],
         [-1.2139e+00, -1.4727e+00, -2.0343e+00, -2.9807e+00],
         ...,
         [-1.0367e+01, -9.6262e+00, -8.4650e+00, -1.6718e+02],
         [-1.3000e+01, -1.2034e+01, -1.1007e+01, -1.5998e+02],
         [-8.2574e+00, -7.6481e+00, -6.3020e+00, -1.1853e+02]],

        [[-1.2130e+00, -1.1190e+00, -1.0476e+00, -7.6716e+01],
         [ 7.1707e-01,  7.8411e-01,  1.1025e+00, -9.2129e+00],
         [ 4.1904e+00,  4.2947e+00,  4.8761e+00, -1.1569e+01],
         ...,
         [ 1.4905e+00,  5.3223e-01, -6.7185e-01,  3.2580e+02],
         [ 1.1637e+00,  1.0530e+00,  5.8831e-01,  2.6167e+02],
         [ 1.1014e+00,  4.6718e-01, -5.9578e-01,  3.5740e+02]],

        ...,

        [[-1.3329e+00, -4.2378e-01,  1.3425e+00, -6.4685e+01],
         [-3.7076e-01, -9.0303e-01, -1.9434e+00, -1.1719e+01],
         [-7.0686e-01, -5.7282e-01, -2.7699e-01, -3.7606e+00],
         ...,
         [-1.7747e+01, -1.7437e+01, -1.7437e+01, -3.1319e+02],
         [-1.7991e+01, -1.7625e+01, -1.7376e+01, -1.6102e+02],
         [-1.7642e+01, -1.7165e+01, -1.6569e+01, -2.2934e+02]],

        [[ 1.0550e+00,  2.3346e+00,  6.0404e+00, -4.6311e+01],
         [-8.1516e-01, -1.0218e+00, -1.5098e+00, -6.7887e+00],
         [-1.1644e+00, -1.1536e+00, -1.0609e+00,  2.3954e+00],
         ...,
         [-1.7298e+01, -1.8777e+01, -2.2664e+01, -2.2348e+02],
         [-1.7963e+01, -1.9323e+01, -2.2986e+01, -7.8281e+01],
         [-1.7262e+01, -1.8629e+01, -2.1969e+01, -1.3533e+02]],

        [[-1.1022e+00, -5.2699e-01,  3.8914e-01, -5.6841e+01],
         [-1.6521e-01, -3.4719e-01, -6.4403e-01, -1.9768e+00],
         [-1.4663e-01, -3.2926e-01, -6.2825e-01, -2.4497e+00],
         ...,
         [ 2.3751e+00,  2.5057e+00,  2.4729e+00,  3.9108e+02],
         [ 2.3753e+00,  2.6525e+00,  2.9269e+00,  3.5002e+02],
         [ 1.4633e+00,  1.4346e+00,  1.0273e+00,  4.6833e+02]]],
       grad_fn=<ReshapeAliasBackward0>), 'rgb0': tensor([[0.4712, 0.4349, 0.3652],
        [0.2969, 0.2052, 0.1018],
        [0.4700, 0.4328, 0.3718],
        ...,
        [0.3785, 0.3607, 0.3360],
        [0.2209, 0.2057, 0.1852],
        [0.4519, 0.4056, 0.3375]], grad_fn=<ReshapeAliasBackward0>), 'disp0': tensor([ 87.6212,  94.7682, 275.2620,  ...,  72.2541,  64.1118, 321.5129],
       grad_fn=<ReshapeAliasBackward0>), 'acc0': tensor([1., 1., 1.,  ..., 1., 1., 1.], grad_fn=<ReshapeAliasBackward0>), 'z_std': tensor([0.0021, 0.0018, 0.0731,  ..., 0.0021, 0.0013, 0.1695])}
0 0.0004687309265136719
torch.Size([320, 640, 3]) torch.Size([320, 640])
1 15.177157163619995
2 17.405996561050415
3 15.445712804794312
4 17.729572057724
5 14.86231017112732
6 17.771722316741943
7 14.868171691894531
8 18.01761746406555
9 15.632362842559814
10 18.039660930633545
11 15.745343923568726
12 18.250818967819214
13 15.348669528961182
14 18.411198377609253
15 15.172338485717773
16 18.092382431030273
17 15.415333986282349
18 18.215579748153687
19 15.483937740325928
20 18.114808082580566
21 15.380659818649292
22 18.027723789215088
23 15.563024520874023
24 17.93445873260498
25 15.569969177246094
26 17.919386386871338
27 15.544952154159546
28 17.997400760650635
29 15.946966409683228
30 17.516038417816162
31 15.850964546203613
32 15.784734964370728
33 18.066469192504883
34 16.299142837524414
35 17.994413375854492
36 15.916903495788574
37 17.868858337402344
38 15.791438102722168
39 17.896764755249023
40 15.782198429107666
41 17.92001986503601
42 15.622689723968506
43 17.801105499267578
44 15.479868650436401
45 17.670701503753662
46 15.49290657043457
47 17.666391134262085
48 15.360713005065918
49 17.628508806228638
50 15.433476686477661
51 17.62514615058899
52 15.447846174240112
53 17.60109806060791
54 15.454773426055908
55 17.84836745262146
56 15.254390239715576
57 17.517541646957397
58 15.494073629379272
59 17.813860177993774
60 15.453466892242432
61 17.617244005203247
62 15.517012119293213
63 17.763784885406494
64 15.681846380233765
65 18.339224576950073
66 15.55536150932312
67 15.373252391815186
68 17.554531812667847
69 15.270557641983032
70 17.835436820983887
71 15.23612380027771
72 17.910101413726807
73 14.949296712875366
74 15.328911542892456
75 14.998407363891602
76 18.144388675689697
77 15.138201713562012
78 18.004937887191772
79 15.237730503082275
80 17.883744716644287
81 15.297138214111328
82 15.016395330429077
83 18.32490062713623
84 14.894437313079834
85 18.41548991203308
86 15.184855699539185
87 18.680152416229248
88 15.10037899017334
89 18.581679344177246
90 14.848121881484985
91 17.99132013320923
92 14.826659440994263
93 18.45542550086975
94 14.999369859695435
95 18.15493679046631
96 15.057222604751587
97 17.962350845336914
98 14.87554144859314
99 14.516003131866455
100 18.409780025482178
101 15.159334897994995
102 17.19178056716919
103 14.520660400390625
104 17.271631002426147
105 15.533937931060791
106 17.204805850982666
107 14.699923753738403
108 17.79569172859192
109 14.868626832962036
110 17.538342475891113
111 14.874015092849731
112 14.852113485336304
113 17.778424978256226
114 14.667053461074829
115 17.733020305633545
116 14.686697244644165
117 17.933363914489746
118 15.12992811203003
119 18.060970783233643
test poses shape torch.Size([4, 3, 4])
0 0.0005567073822021484
torch.Size([320, 640, 3]) torch.Size([320, 640])
1 17.781597137451172
2 14.725309371948242
3 17.833377599716187
Saved test set
[TRAIN] Iter: 200000 Loss: 0.006546993739902973  PSNR: 26.968612670898438
[TRAIN] Iter: 200100 Loss: 0.0062598129734396935  PSNR: 26.835161209106445
[TRAIN] Iter: 200200 Loss: 0.006137568969279528  PSNR: 27.164430618286133
[TRAIN] Iter: 200300 Loss: 0.007326240185648203  PSNR: 26.321922302246094
[TRAIN] Iter: 200400 Loss: 0.005788756534457207  PSNR: 27.233627319335938
[TRAIN] Iter: 200500 Loss: 0.004870504140853882  PSNR: 28.345657348632812
[TRAIN] Iter: 200600 Loss: 0.005744848866015673  PSNR: 27.19841957092285
[TRAIN] Iter: 200700 Loss: 0.007495377212762833  PSNR: 26.48220443725586
[TRAIN] Iter: 200800 Loss: 0.006740720476955175  PSNR: 26.303436279296875
[TRAIN] Iter: 200900 Loss: 0.00602082721889019  PSNR: 26.571687698364258
[TRAIN] Iter: 201000 Loss: 0.0065263183787465096  PSNR: 27.30191421508789
[TRAIN] Iter: 201100 Loss: 0.0062200529500842094  PSNR: 27.0960636138916
[TRAIN] Iter: 201200 Loss: 0.005671251565217972  PSNR: 28.251089096069336
[TRAIN] Iter: 201300 Loss: 0.0063400063663721085  PSNR: 26.557056427001953
[TRAIN] Iter: 201400 Loss: 0.007403861731290817  PSNR: 26.009244918823242
[TRAIN] Iter: 201500 Loss: 0.004808372817933559  PSNR: 28.53303337097168
[TRAIN] Iter: 201600 Loss: 0.007182789966464043  PSNR: 26.060924530029297
[TRAIN] Iter: 201700 Loss: 0.006282247137278318  PSNR: 26.790040969848633
[TRAIN] Iter: 201800 Loss: 0.005598676856607199  PSNR: 27.067855834960938
[TRAIN] Iter: 201900 Loss: 0.005050128325819969  PSNR: 28.96348762512207
[TRAIN] Iter: 202000 Loss: 0.006824897602200508  PSNR: 26.983762741088867
[TRAIN] Iter: 202100 Loss: 0.005552186630666256  PSNR: 27.198301315307617
[TRAIN] Iter: 202200 Loss: 0.006294759921729565  PSNR: 26.89923095703125
[TRAIN] Iter: 202300 Loss: 0.006712907459586859  PSNR: 25.958696365356445
[TRAIN] Iter: 202400 Loss: 0.004461810924112797  PSNR: 29.327037811279297
[TRAIN] Iter: 202500 Loss: 0.0045311846770346165  PSNR: 29.296985626220703
[TRAIN] Iter: 202600 Loss: 0.004702839069068432  PSNR: 28.67717933654785
[TRAIN] Iter: 202700 Loss: 0.006468610838055611  PSNR: 27.485876083374023
[TRAIN] Iter: 202800 Loss: 0.005230066366493702  PSNR: 27.566370010375977
[TRAIN] Iter: 202900 Loss: 0.005822338629513979  PSNR: 27.210817337036133
[TRAIN] Iter: 203000 Loss: 0.005872334353625774  PSNR: 26.49846076965332
[TRAIN] Iter: 203100 Loss: 0.005108329467475414  PSNR: 28.175748825073242
[TRAIN] Iter: 203200 Loss: 0.004515193402767181  PSNR: 28.307016372680664
[TRAIN] Iter: 203300 Loss: 0.004941863473504782  PSNR: 27.986520767211914
[TRAIN] Iter: 203400 Loss: 0.004536369815468788  PSNR: 28.66346549987793
[TRAIN] Iter: 203500 Loss: 0.006308181211352348  PSNR: 27.0200138092041
[TRAIN] Iter: 203600 Loss: 0.004821440204977989  PSNR: 28.460763931274414
[TRAIN] Iter: 203700 Loss: 0.005698413122445345  PSNR: 27.59116554260254
[TRAIN] Iter: 203800 Loss: 0.006980108562856913  PSNR: 26.464717864990234
[TRAIN] Iter: 203900 Loss: 0.005464029498398304  PSNR: 27.457569122314453
[TRAIN] Iter: 204000 Loss: 0.006163443438708782  PSNR: 26.427095413208008
[TRAIN] Iter: 204100 Loss: 0.005230157170444727  PSNR: 27.857263565063477
[TRAIN] Iter: 204200 Loss: 0.007049111183732748  PSNR: 26.179874420166016
[TRAIN] Iter: 204300 Loss: 0.006900980602949858  PSNR: 26.817121505737305
[TRAIN] Iter: 204400 Loss: 0.005073170643299818  PSNR: 27.49402618408203
[TRAIN] Iter: 204500 Loss: 0.005731274373829365  PSNR: 27.068506240844727
[TRAIN] Iter: 204600 Loss: 0.0045136744156479836  PSNR: 28.85787010192871
[TRAIN] Iter: 204700 Loss: 0.00393393961712718  PSNR: 28.602779388427734
[TRAIN] Iter: 204800 Loss: 0.004271745681762695  PSNR: 28.292076110839844
[TRAIN] Iter: 204900 Loss: 0.006055012810975313  PSNR: 26.769264221191406
[TRAIN] Iter: 205000 Loss: 0.0063034649938344955  PSNR: 26.60661506652832
[TRAIN] Iter: 205100 Loss: 0.006702135782688856  PSNR: 26.9769287109375
[TRAIN] Iter: 205200 Loss: 0.005377395078539848  PSNR: 27.833539962768555
[TRAIN] Iter: 205300 Loss: 0.006379245314747095  PSNR: 27.058176040649414
[TRAIN] Iter: 205400 Loss: 0.004817483015358448  PSNR: 27.678810119628906
[TRAIN] Iter: 205500 Loss: 0.006606211885809898  PSNR: 26.539339065551758
[TRAIN] Iter: 205600 Loss: 0.007154939696192741  PSNR: 25.855785369873047
[TRAIN] Iter: 205700 Loss: 0.005968588404357433  PSNR: 27.60272216796875
[TRAIN] Iter: 205800 Loss: 0.006982999853789806  PSNR: 26.13637924194336
[TRAIN] Iter: 205900 Loss: 0.005553817376494408  PSNR: 27.403995513916016
[TRAIN] Iter: 206000 Loss: 0.005744068417698145  PSNR: 27.254772186279297
[TRAIN] Iter: 206100 Loss: 0.004733400419354439  PSNR: 28.85382652282715
[TRAIN] Iter: 206200 Loss: 0.007143115159124136  PSNR: 26.12527084350586
[TRAIN] Iter: 206300 Loss: 0.004768865182995796  PSNR: 29.177202224731445
[TRAIN] Iter: 206400 Loss: 0.004435759969055653  PSNR: 28.814451217651367
[TRAIN] Iter: 206500 Loss: 0.005257077980786562  PSNR: 27.6480655670166
[TRAIN] Iter: 206600 Loss: 0.006010696291923523  PSNR: 27.39054298400879
[TRAIN] Iter: 206700 Loss: 0.00566531578078866  PSNR: 27.202177047729492
[TRAIN] Iter: 206800 Loss: 0.005374430678784847  PSNR: 28.24574851989746
[TRAIN] Iter: 206900 Loss: 0.004673170857131481  PSNR: 27.55976104736328
[TRAIN] Iter: 207000 Loss: 0.006069588474929333  PSNR: 26.85227394104004
[TRAIN] Iter: 207100 Loss: 0.005595695693045855  PSNR: 27.292110443115234
[TRAIN] Iter: 207200 Loss: 0.006035894621163607  PSNR: 26.642799377441406
[TRAIN] Iter: 207300 Loss: 0.004754791036248207  PSNR: 28.847515106201172
[TRAIN] Iter: 207400 Loss: 0.006941480562090874  PSNR: 26.300506591796875
[TRAIN] Iter: 207500 Loss: 0.005008605308830738  PSNR: 28.106491088867188
[TRAIN] Iter: 207600 Loss: 0.006694102194160223  PSNR: 26.3719539642334
[TRAIN] Iter: 207700 Loss: 0.0042228298261761665  PSNR: 28.63620948791504
[TRAIN] Iter: 207800 Loss: 0.007398011162877083  PSNR: 25.705602645874023
[TRAIN] Iter: 207900 Loss: 0.005244290456175804  PSNR: 27.737396240234375
[TRAIN] Iter: 208000 Loss: 0.005427198018878698  PSNR: 28.252483367919922
[TRAIN] Iter: 208100 Loss: 0.005000957287847996  PSNR: 28.254762649536133
[TRAIN] Iter: 208200 Loss: 0.005443543195724487  PSNR: 28.205860137939453
[TRAIN] Iter: 208300 Loss: 0.00447868462651968  PSNR: 28.42422103881836
[TRAIN] Iter: 208400 Loss: 0.004946442320942879  PSNR: 28.25597381591797
[TRAIN] Iter: 208500 Loss: 0.0054207853972911835  PSNR: 27.502567291259766
[TRAIN] Iter: 208600 Loss: 0.004240759648382664  PSNR: 29.514076232910156
[TRAIN] Iter: 208700 Loss: 0.007536629680544138  PSNR: 25.262516021728516
[TRAIN] Iter: 208800 Loss: 0.00507674366235733  PSNR: 28.48422622680664
[TRAIN] Iter: 208900 Loss: 0.004805844277143478  PSNR: 28.081823348999023
[TRAIN] Iter: 209000 Loss: 0.004707599990069866  PSNR: 29.157875061035156
[TRAIN] Iter: 209100 Loss: 0.006128585897386074  PSNR: 27.2652645111084
[TRAIN] Iter: 209200 Loss: 0.007470420561730862  PSNR: 26.012636184692383
[TRAIN] Iter: 209300 Loss: 0.0042170751839876175  PSNR: 29.47991371154785
[TRAIN] Iter: 209400 Loss: 0.004449143074452877  PSNR: 29.20955467224121
[TRAIN] Iter: 209500 Loss: 0.006167456042021513  PSNR: 26.94443702697754
[TRAIN] Iter: 209600 Loss: 0.00480459863319993  PSNR: 28.02623176574707
[TRAIN] Iter: 209700 Loss: 0.005280798301100731  PSNR: 27.024538040161133
[TRAIN] Iter: 209800 Loss: 0.006008131429553032  PSNR: 26.48819351196289
[TRAIN] Iter: 209900 Loss: 0.005975848995149136  PSNR: 26.825260162353516
Saved checkpoints at ./logs/TUT-KE101-nerf/210000.tar
[TRAIN] Iter: 210000 Loss: 0.0061758290976285934  PSNR: 26.898935317993164
[TRAIN] Iter: 210100 Loss: 0.006327653303742409  PSNR: 26.377588272094727
[TRAIN] Iter: 210200 Loss: 0.004192142281681299  PSNR: 29.039264678955078
[TRAIN] Iter: 210300 Loss: 0.0052786823362112045  PSNR: 27.35121726989746
[TRAIN] Iter: 210400 Loss: 0.005434942897409201  PSNR: 27.569839477539062
[TRAIN] Iter: 210500 Loss: 0.006152867339551449  PSNR: 26.64352035522461
[TRAIN] Iter: 210600 Loss: 0.004116414114832878  PSNR: 29.174209594726562
[TRAIN] Iter: 210700 Loss: 0.0066315908916294575  PSNR: 26.84255599975586
[TRAIN] Iter: 210800 Loss: 0.005173505283892155  PSNR: 27.341659545898438
[TRAIN] Iter: 210900 Loss: 0.005201430059969425  PSNR: 27.869043350219727
[TRAIN] Iter: 211000 Loss: 0.004288609139621258  PSNR: 28.698322296142578
[TRAIN] Iter: 211100 Loss: 0.005551540292799473  PSNR: 27.310117721557617
[TRAIN] Iter: 211200 Loss: 0.005765403620898724  PSNR: 26.89508819580078
[TRAIN] Iter: 211300 Loss: 0.005267767235636711  PSNR: 27.545467376708984
[TRAIN] Iter: 211400 Loss: 0.005709268618375063  PSNR: 26.73846435546875
[TRAIN] Iter: 211500 Loss: 0.004510541446506977  PSNR: 28.958158493041992
[TRAIN] Iter: 211600 Loss: 0.005261512473225594  PSNR: 28.171958923339844
[TRAIN] Iter: 211700 Loss: 0.004116801545023918  PSNR: 30.06268310546875
[TRAIN] Iter: 211800 Loss: 0.005929853767156601  PSNR: 27.11734390258789
[TRAIN] Iter: 211900 Loss: 0.004263326525688171  PSNR: 29.010839462280273
[TRAIN] Iter: 212000 Loss: 0.006197879556566477  PSNR: 26.592084884643555
[TRAIN] Iter: 212100 Loss: 0.004545355215668678  PSNR: 29.426029205322266
[TRAIN] Iter: 212200 Loss: 0.006876408122479916  PSNR: 25.702529907226562
[TRAIN] Iter: 212300 Loss: 0.0051542362198233604  PSNR: 28.267486572265625
[TRAIN] Iter: 212400 Loss: 0.006239541806280613  PSNR: 26.628751754760742
[TRAIN] Iter: 212500 Loss: 0.004765189252793789  PSNR: 29.052724838256836
[TRAIN] Iter: 212600 Loss: 0.004806575831025839  PSNR: 28.58465003967285
[TRAIN] Iter: 212700 Loss: 0.0062065646052360535  PSNR: 26.88469696044922
[TRAIN] Iter: 212800 Loss: 0.004425097722560167  PSNR: 28.771020889282227
[TRAIN] Iter: 212900 Loss: 0.0057083964347839355  PSNR: 26.95410919189453
[TRAIN] Iter: 213000 Loss: 0.005464945919811726  PSNR: 27.368093490600586
[TRAIN] Iter: 213100 Loss: 0.00638585863634944  PSNR: 26.51852798461914
[TRAIN] Iter: 213200 Loss: 0.0073298243805766106  PSNR: 25.99873924255371
[TRAIN] Iter: 213300 Loss: 0.005184277892112732  PSNR: 27.661975860595703
[TRAIN] Iter: 213400 Loss: 0.006529506761580706  PSNR: 26.754310607910156
[TRAIN] Iter: 213500 Loss: 0.005399996414780617  PSNR: 27.44437026977539
[TRAIN] Iter: 213600 Loss: 0.005675917491316795  PSNR: 27.02490997314453
[TRAIN] Iter: 213700 Loss: 0.006365383043885231  PSNR: 27.133066177368164
[TRAIN] Iter: 213800 Loss: 0.004988241009414196  PSNR: 27.20688819885254
[TRAIN] Iter: 213900 Loss: 0.0042513273656368256  PSNR: 28.715429306030273
[TRAIN] Iter: 214000 Loss: 0.0051876879297196865  PSNR: 28.486671447753906
[TRAIN] Iter: 214100 Loss: 0.004426703322678804  PSNR: 28.50959014892578
[TRAIN] Iter: 214200 Loss: 0.0056573981419205666  PSNR: 27.093050003051758
[TRAIN] Iter: 214300 Loss: 0.005742629524320364  PSNR: 26.74466896057129
[TRAIN] Iter: 214400 Loss: 0.004669028799980879  PSNR: 28.313579559326172
[TRAIN] Iter: 214500 Loss: 0.004847399890422821  PSNR: 28.984851837158203
[TRAIN] Iter: 214600 Loss: 0.004397368989884853  PSNR: 28.911357879638672
[TRAIN] Iter: 214700 Loss: 0.00530722551047802  PSNR: 27.49677848815918
[TRAIN] Iter: 214800 Loss: 0.005952895153313875  PSNR: 27.080055236816406
[TRAIN] Iter: 214900 Loss: 0.005101403221487999  PSNR: 27.406221389770508
[TRAIN] Iter: 215000 Loss: 0.004538519773632288  PSNR: 28.93677520751953
[TRAIN] Iter: 215100 Loss: 0.004675556905567646  PSNR: 28.565216064453125
[TRAIN] Iter: 215200 Loss: 0.005112593062222004  PSNR: 28.32244300842285
[TRAIN] Iter: 215300 Loss: 0.005341864190995693  PSNR: 28.75211524963379
[TRAIN] Iter: 215400 Loss: 0.00476800138130784  PSNR: 27.60097312927246
[TRAIN] Iter: 215500 Loss: 0.00520120095461607  PSNR: 28.468149185180664
[TRAIN] Iter: 215600 Loss: 0.006698055192828178  PSNR: 26.688995361328125
[TRAIN] Iter: 215700 Loss: 0.0070920283906161785  PSNR: 26.03196907043457
[TRAIN] Iter: 215800 Loss: 0.004689651075750589  PSNR: 28.832799911499023
[TRAIN] Iter: 215900 Loss: 0.005596858914941549  PSNR: 27.017742156982422
[TRAIN] Iter: 216000 Loss: 0.00802022498100996  PSNR: 25.38427734375
[TRAIN] Iter: 216100 Loss: 0.004104466177523136  PSNR: 29.28129005432129
[TRAIN] Iter: 216200 Loss: 0.0064562042243778706  PSNR: 26.74432373046875
[TRAIN] Iter: 216300 Loss: 0.005895966198295355  PSNR: 26.462589263916016
[TRAIN] Iter: 216400 Loss: 0.008243086747825146  PSNR: 26.057279586791992
[TRAIN] Iter: 216500 Loss: 0.005903702229261398  PSNR: 26.678054809570312
[TRAIN] Iter: 216600 Loss: 0.004702501464635134  PSNR: 29.472942352294922
[TRAIN] Iter: 216700 Loss: 0.0041781081818044186  PSNR: 29.52273941040039
[TRAIN] Iter: 216800 Loss: 0.005571461748331785  PSNR: 27.130016326904297
[TRAIN] Iter: 216900 Loss: 0.0058031813241541386  PSNR: 27.253368377685547
[TRAIN] Iter: 217000 Loss: 0.006166109815239906  PSNR: 26.48941993713379
[TRAIN] Iter: 217100 Loss: 0.005081173963844776  PSNR: 27.86846351623535
[TRAIN] Iter: 217200 Loss: 0.0035569097381085157  PSNR: 29.692575454711914
[TRAIN] Iter: 217300 Loss: 0.005118521861732006  PSNR: 27.927289962768555
[TRAIN] Iter: 217400 Loss: 0.004542759153991938  PSNR: 28.50020980834961
[TRAIN] Iter: 217500 Loss: 0.004640035796910524  PSNR: 28.616662979125977
[TRAIN] Iter: 217600 Loss: 0.005099722184240818  PSNR: 28.730783462524414
[TRAIN] Iter: 217700 Loss: 0.004783227574080229  PSNR: 28.047658920288086
[TRAIN] Iter: 217800 Loss: 0.005060064140707254  PSNR: 28.555856704711914
[TRAIN] Iter: 217900 Loss: 0.004868479445576668  PSNR: 28.978557586669922
[TRAIN] Iter: 218000 Loss: 0.005877538584172726  PSNR: 27.012557983398438
[TRAIN] Iter: 218100 Loss: 0.0042053088545799255  PSNR: 29.046518325805664
[TRAIN] Iter: 218200 Loss: 0.006508161313831806  PSNR: 26.61656379699707
[TRAIN] Iter: 218300 Loss: 0.007143296301364899  PSNR: 26.076082229614258
[TRAIN] Iter: 218400 Loss: 0.0044637215323746204  PSNR: 29.108776092529297
[TRAIN] Iter: 218500 Loss: 0.005345053039491177  PSNR: 27.561399459838867
[TRAIN] Iter: 218600 Loss: 0.005100841633975506  PSNR: 28.255674362182617
[TRAIN] Iter: 218700 Loss: 0.007515807636082172  PSNR: 26.017457962036133
[TRAIN] Iter: 218800 Loss: 0.005984343588352203  PSNR: 27.104496002197266
[TRAIN] Iter: 218900 Loss: 0.005992611404508352  PSNR: 26.91265869140625
[TRAIN] Iter: 219000 Loss: 0.0071271550841629505  PSNR: 26.693248748779297
[TRAIN] Iter: 219100 Loss: 0.007763766683638096  PSNR: 25.203479766845703
[TRAIN] Iter: 219200 Loss: 0.005840837024152279  PSNR: 27.43045425415039
[TRAIN] Iter: 219300 Loss: 0.006388192530721426  PSNR: 26.7274169921875
[TRAIN] Iter: 219400 Loss: 0.0046223234385252  PSNR: 29.121212005615234
[TRAIN] Iter: 219500 Loss: 0.004896207712590694  PSNR: 27.69114112854004
[TRAIN] Iter: 219600 Loss: 0.006495196372270584  PSNR: 27.749267578125
[TRAIN] Iter: 219700 Loss: 0.0044959913939237595  PSNR: 28.6289005279541
[TRAIN] Iter: 219800 Loss: 0.004519089125096798  PSNR: 28.039077758789062
[TRAIN] Iter: 219900 Loss: 0.004867108538746834  PSNR: 29.03986167907715
Saved checkpoints at ./logs/TUT-KE101-nerf/220000.tar
[TRAIN] Iter: 220000 Loss: 0.005575683433562517  PSNR: 27.062257766723633
[TRAIN] Iter: 220100 Loss: 0.006117269396781921  PSNR: 26.95146942138672
[TRAIN] Iter: 220200 Loss: 0.004499394912272692  PSNR: 28.630338668823242
[TRAIN] Iter: 220300 Loss: 0.005627690814435482  PSNR: 27.01762580871582
[TRAIN] Iter: 220400 Loss: 0.00689241848886013  PSNR: 26.48110580444336
[TRAIN] Iter: 220500 Loss: 0.0061258310452103615  PSNR: 27.017902374267578
[TRAIN] Iter: 220600 Loss: 0.005581455305218697  PSNR: 27.803455352783203
[TRAIN] Iter: 220700 Loss: 0.006048494018614292  PSNR: 26.938133239746094
[TRAIN] Iter: 220800 Loss: 0.005844742059707642  PSNR: 26.68691635131836
[TRAIN] Iter: 220900 Loss: 0.004947757348418236  PSNR: 27.68238639831543
[TRAIN] Iter: 221000 Loss: 0.004971873946487904  PSNR: 27.45948600769043
[TRAIN] Iter: 221100 Loss: 0.006655975244939327  PSNR: 26.78013038635254
[TRAIN] Iter: 221200 Loss: 0.004620336927473545  PSNR: 28.86642837524414
[TRAIN] Iter: 221300 Loss: 0.006903873756527901  PSNR: 26.654279708862305
[TRAIN] Iter: 221400 Loss: 0.004202340263873339  PSNR: 29.83876609802246
[TRAIN] Iter: 221500 Loss: 0.0064149703830480576  PSNR: 26.56650161743164
[TRAIN] Iter: 221600 Loss: 0.004135014023631811  PSNR: 29.391613006591797
[TRAIN] Iter: 221700 Loss: 0.005957459099590778  PSNR: 27.048641204833984
[TRAIN] Iter: 221800 Loss: 0.004767685662955046  PSNR: 29.19956398010254
[TRAIN] Iter: 221900 Loss: 0.003736834041774273  PSNR: 29.128263473510742
[TRAIN] Iter: 222000 Loss: 0.004831136669963598  PSNR: 28.429187774658203
[TRAIN] Iter: 222100 Loss: 0.004629931412637234  PSNR: 29.482362747192383
[TRAIN] Iter: 222200 Loss: 0.005770605057477951  PSNR: 27.294288635253906
[TRAIN] Iter: 222300 Loss: 0.006383436731994152  PSNR: 26.2135066986084
[TRAIN] Iter: 222400 Loss: 0.004264685790985823  PSNR: 29.447582244873047
[TRAIN] Iter: 222500 Loss: 0.0069499546661973  PSNR: 25.797401428222656
[TRAIN] Iter: 222600 Loss: 0.005478870589286089  PSNR: 27.46053695678711
[TRAIN] Iter: 222700 Loss: 0.004923045635223389  PSNR: 29.14818572998047
[TRAIN] Iter: 222800 Loss: 0.005295435898005962  PSNR: 27.445810317993164
[TRAIN] Iter: 222900 Loss: 0.003592560300603509  PSNR: 30.041522979736328
[TRAIN] Iter: 223000 Loss: 0.005176550708711147  PSNR: 27.913923263549805
[TRAIN] Iter: 223100 Loss: 0.006264498457312584  PSNR: 26.965776443481445
[TRAIN] Iter: 223200 Loss: 0.004656700883060694  PSNR: 28.20608901977539
[TRAIN] Iter: 223300 Loss: 0.007145489566028118  PSNR: 26.154340744018555
[TRAIN] Iter: 223400 Loss: 0.006623528897762299  PSNR: 26.405366897583008
[TRAIN] Iter: 223500 Loss: 0.004509261343628168  PSNR: 29.43006706237793
[TRAIN] Iter: 223600 Loss: 0.0038848519325256348  PSNR: 29.618452072143555
[TRAIN] Iter: 223700 Loss: 0.005735248792916536  PSNR: 27.42737579345703
[TRAIN] Iter: 223800 Loss: 0.004715845920145512  PSNR: 28.95891571044922
[TRAIN] Iter: 223900 Loss: 0.005952451843768358  PSNR: 26.693769454956055
[TRAIN] Iter: 224000 Loss: 0.005706719122827053  PSNR: 27.3056697845459
[TRAIN] Iter: 224100 Loss: 0.005932436790317297  PSNR: 26.768556594848633
[TRAIN] Iter: 224200 Loss: 0.004684434738010168  PSNR: 27.722183227539062
[TRAIN] Iter: 224300 Loss: 0.0056492178700864315  PSNR: 27.046005249023438
[TRAIN] Iter: 224400 Loss: 0.004124898463487625  PSNR: 29.53874397277832
[TRAIN] Iter: 224500 Loss: 0.0061341626569628716  PSNR: 26.76201629638672
[TRAIN] Iter: 224600 Loss: 0.004753672983497381  PSNR: 28.03818702697754
[TRAIN] Iter: 224700 Loss: 0.006491759791970253  PSNR: 26.313724517822266
[TRAIN] Iter: 224800 Loss: 0.006440677680075169  PSNR: 26.393606185913086
[TRAIN] Iter: 224900 Loss: 0.004709054250270128  PSNR: 27.94075584411621
[TRAIN] Iter: 225000 Loss: 0.006743925623595715  PSNR: 26.610675811767578
[TRAIN] Iter: 225100 Loss: 0.004470569547265768  PSNR: 29.390853881835938
[TRAIN] Iter: 225200 Loss: 0.005223205778747797  PSNR: 28.25115203857422
[TRAIN] Iter: 225300 Loss: 0.006989412009716034  PSNR: 26.09917640686035
[TRAIN] Iter: 225400 Loss: 0.004369788803160191  PSNR: 29.038599014282227
[TRAIN] Iter: 225500 Loss: 0.004011413548141718  PSNR: 29.047170639038086
[TRAIN] Iter: 225600 Loss: 0.004759843461215496  PSNR: 28.968292236328125
[TRAIN] Iter: 225700 Loss: 0.006536333821713924  PSNR: 26.533737182617188
[TRAIN] Iter: 225800 Loss: 0.0066623929888010025  PSNR: 27.434831619262695
[TRAIN] Iter: 225900 Loss: 0.004550150129944086  PSNR: 29.236608505249023
[TRAIN] Iter: 226000 Loss: 0.006389136426150799  PSNR: 26.111766815185547
[TRAIN] Iter: 226100 Loss: 0.006522367708384991  PSNR: 26.600557327270508
[TRAIN] Iter: 226200 Loss: 0.00429187435656786  PSNR: 29.130863189697266
[TRAIN] Iter: 226300 Loss: 0.005576912313699722  PSNR: 28.683929443359375
[TRAIN] Iter: 226400 Loss: 0.005726995877921581  PSNR: 26.941186904907227
[TRAIN] Iter: 226500 Loss: 0.004377923905849457  PSNR: 29.036788940429688
[TRAIN] Iter: 226600 Loss: 0.007108170539140701  PSNR: 25.744359970092773
[TRAIN] Iter: 226700 Loss: 0.004454270005226135  PSNR: 28.973413467407227
[TRAIN] Iter: 226800 Loss: 0.005439260974526405  PSNR: 27.400407791137695
[TRAIN] Iter: 226900 Loss: 0.006873396225273609  PSNR: 26.383922576904297
[TRAIN] Iter: 227000 Loss: 0.005086232908070087  PSNR: 27.3605899810791
[TRAIN] Iter: 227100 Loss: 0.006783940829336643  PSNR: 25.95174789428711
[TRAIN] Iter: 227200 Loss: 0.004326934926211834  PSNR: 29.057697296142578
[TRAIN] Iter: 227300 Loss: 0.005583235993981361  PSNR: 27.265777587890625
[TRAIN] Iter: 227400 Loss: 0.00634574331343174  PSNR: 27.193178176879883
[TRAIN] Iter: 227500 Loss: 0.006874684244394302  PSNR: 26.137123107910156
[TRAIN] Iter: 227600 Loss: 0.006631253752857447  PSNR: 26.451940536499023
[TRAIN] Iter: 227700 Loss: 0.004468957427889109  PSNR: 28.729516983032227
[TRAIN] Iter: 227800 Loss: 0.004107304383069277  PSNR: 29.071123123168945
[TRAIN] Iter: 227900 Loss: 0.004906748421490192  PSNR: 28.683269500732422
[TRAIN] Iter: 228000 Loss: 0.006903615314513445  PSNR: 25.76618003845215
[TRAIN] Iter: 228100 Loss: 0.00619366392493248  PSNR: 26.770809173583984
[TRAIN] Iter: 228200 Loss: 0.004933585412800312  PSNR: 27.731548309326172
[TRAIN] Iter: 228300 Loss: 0.0039002257399260998  PSNR: 28.716880798339844
[TRAIN] Iter: 228400 Loss: 0.005965704098343849  PSNR: 26.918533325195312
[TRAIN] Iter: 228500 Loss: 0.0056685651652514935  PSNR: 27.124467849731445
[TRAIN] Iter: 228600 Loss: 0.00595477782189846  PSNR: 27.02813148498535
[TRAIN] Iter: 228700 Loss: 0.005141543690115213  PSNR: 28.142776489257812
[TRAIN] Iter: 228800 Loss: 0.00417175330221653  PSNR: 29.38148307800293
[TRAIN] Iter: 228900 Loss: 0.006467022001743317  PSNR: 26.473896026611328
[TRAIN] Iter: 229000 Loss: 0.0053193457424640656  PSNR: 27.911670684814453
[TRAIN] Iter: 229100 Loss: 0.004355681128799915  PSNR: 29.051008224487305
[TRAIN] Iter: 229200 Loss: 0.004743244498968124  PSNR: 29.22588539123535
[TRAIN] Iter: 229300 Loss: 0.0043229288421571255  PSNR: 28.706409454345703
[TRAIN] Iter: 229400 Loss: 0.0043098172172904015  PSNR: 29.0082950592041
[TRAIN] Iter: 229500 Loss: 0.005971057340502739  PSNR: 26.952369689941406
[TRAIN] Iter: 229600 Loss: 0.004280294291675091  PSNR: 29.22394371032715
[TRAIN] Iter: 229700 Loss: 0.00443305866792798  PSNR: 28.631742477416992
[TRAIN] Iter: 229800 Loss: 0.005911091808229685  PSNR: 26.889249801635742
[TRAIN] Iter: 229900 Loss: 0.006378726568073034  PSNR: 27.107662200927734
Saved checkpoints at ./logs/TUT-KE101-nerf/230000.tar
[TRAIN] Iter: 230000 Loss: 0.004085075575858355  PSNR: 28.850622177124023
[TRAIN] Iter: 230100 Loss: 0.004292775876820087  PSNR: 28.770090103149414
[TRAIN] Iter: 230200 Loss: 0.004958181641995907  PSNR: 27.823740005493164
[TRAIN] Iter: 230300 Loss: 0.005568234715610743  PSNR: 27.625965118408203
[TRAIN] Iter: 230400 Loss: 0.004772307351231575  PSNR: 27.252033233642578
[TRAIN] Iter: 230500 Loss: 0.006254272535443306  PSNR: 26.97779083251953
[TRAIN] Iter: 230600 Loss: 0.005682006943970919  PSNR: 27.796390533447266
[TRAIN] Iter: 230700 Loss: 0.005154073238372803  PSNR: 29.07683563232422
[TRAIN] Iter: 230800 Loss: 0.00461195595562458  PSNR: 29.17664909362793
[TRAIN] Iter: 230900 Loss: 0.006630159914493561  PSNR: 26.602689743041992
[TRAIN] Iter: 231000 Loss: 0.006429382599890232  PSNR: 26.86848258972168
[TRAIN] Iter: 231100 Loss: 0.00584146985784173  PSNR: 26.894367218017578
[TRAIN] Iter: 231200 Loss: 0.004871036391705275  PSNR: 28.17778968811035
[TRAIN] Iter: 231300 Loss: 0.007176281418651342  PSNR: 26.17439842224121
[TRAIN] Iter: 231400 Loss: 0.006550875958055258  PSNR: 26.128145217895508
[TRAIN] Iter: 231500 Loss: 0.006305420771241188  PSNR: 26.76841926574707
[TRAIN] Iter: 231600 Loss: 0.005240004044026136  PSNR: 27.413724899291992
[TRAIN] Iter: 231700 Loss: 0.0076802256517112255  PSNR: 25.78424072265625
[TRAIN] Iter: 231800 Loss: 0.0054804058745503426  PSNR: 28.011436462402344
[TRAIN] Iter: 231900 Loss: 0.005153492093086243  PSNR: 28.84578514099121
[TRAIN] Iter: 232000 Loss: 0.006089226808398962  PSNR: 26.156600952148438
[TRAIN] Iter: 232100 Loss: 0.004031736869364977  PSNR: 29.839336395263672
[TRAIN] Iter: 232200 Loss: 0.004796680063009262  PSNR: 28.559457778930664
[TRAIN] Iter: 232300 Loss: 0.006245815195143223  PSNR: 25.38768768310547
[TRAIN] Iter: 232400 Loss: 0.00419523473829031  PSNR: 29.285497665405273
[TRAIN] Iter: 232500 Loss: 0.005212113261222839  PSNR: 27.756166458129883
[TRAIN] Iter: 232600 Loss: 0.004761413671076298  PSNR: 29.304662704467773
[TRAIN] Iter: 232700 Loss: 0.005248318426311016  PSNR: 28.009628295898438
[TRAIN] Iter: 232800 Loss: 0.004370402544736862  PSNR: 28.904207229614258
[TRAIN] Iter: 232900 Loss: 0.0048240432515740395  PSNR: 28.392919540405273
[TRAIN] Iter: 233000 Loss: 0.00533749395981431  PSNR: 27.751609802246094
[TRAIN] Iter: 233100 Loss: 0.004782087169587612  PSNR: 28.599699020385742
[TRAIN] Iter: 233200 Loss: 0.00486775953322649  PSNR: 27.52412223815918
[TRAIN] Iter: 233300 Loss: 0.00513310544192791  PSNR: 27.911256790161133
[TRAIN] Iter: 233400 Loss: 0.0049680424854159355  PSNR: 28.477861404418945
[TRAIN] Iter: 233500 Loss: 0.005434619262814522  PSNR: 27.460798263549805
[TRAIN] Iter: 233600 Loss: 0.006107969209551811  PSNR: 27.05880355834961
[TRAIN] Iter: 233700 Loss: 0.005156688392162323  PSNR: 27.604732513427734
[TRAIN] Iter: 233800 Loss: 0.006022830959409475  PSNR: 27.264741897583008
[TRAIN] Iter: 233900 Loss: 0.0056988829746842384  PSNR: 27.664653778076172
[TRAIN] Iter: 234000 Loss: 0.006903000641614199  PSNR: 26.522249221801758
[TRAIN] Iter: 234100 Loss: 0.004465658217668533  PSNR: 28.142127990722656
[TRAIN] Iter: 234200 Loss: 0.005875607952475548  PSNR: 27.126989364624023
[TRAIN] Iter: 234300 Loss: 0.006841057445853949  PSNR: 26.047527313232422
[TRAIN] Iter: 234400 Loss: 0.005723464768379927  PSNR: 26.627275466918945
[TRAIN] Iter: 234500 Loss: 0.005985199939459562  PSNR: 25.903362274169922
[TRAIN] Iter: 234600 Loss: 0.0048380703665316105  PSNR: 29.02482795715332
[TRAIN] Iter: 234700 Loss: 0.0055237459018826485  PSNR: 27.150875091552734
[TRAIN] Iter: 234800 Loss: 0.004165391903370619  PSNR: 28.94864273071289
[TRAIN] Iter: 234900 Loss: 0.005766566842794418  PSNR: 27.3382511138916
[TRAIN] Iter: 235000 Loss: 0.006789508741348982  PSNR: 25.791542053222656
[TRAIN] Iter: 235100 Loss: 0.005818784236907959  PSNR: 26.76324462890625
[TRAIN] Iter: 235200 Loss: 0.006135218311101198  PSNR: 27.03294563293457
[TRAIN] Iter: 235300 Loss: 0.006087840534746647  PSNR: 27.633289337158203
[TRAIN] Iter: 235400 Loss: 0.006435565650463104  PSNR: 26.84912109375
[TRAIN] Iter: 235500 Loss: 0.005859714932739735  PSNR: 27.827152252197266
[TRAIN] Iter: 235600 Loss: 0.004939098376780748  PSNR: 28.418310165405273
[TRAIN] Iter: 235700 Loss: 0.004873679019510746  PSNR: 29.296720504760742
[TRAIN] Iter: 235800 Loss: 0.005404878873378038  PSNR: 27.147754669189453
[TRAIN] Iter: 235900 Loss: 0.0061317142099142075  PSNR: 27.077613830566406
[TRAIN] Iter: 236000 Loss: 0.005972161889076233  PSNR: 26.703386306762695
[TRAIN] Iter: 236100 Loss: 0.006384158041328192  PSNR: 26.994977951049805
[TRAIN] Iter: 236200 Loss: 0.005288158543407917  PSNR: 27.728349685668945
[TRAIN] Iter: 236300 Loss: 0.0038651630748063326  PSNR: 29.401132583618164
[TRAIN] Iter: 236400 Loss: 0.005243330262601376  PSNR: 28.120656967163086
[TRAIN] Iter: 236500 Loss: 0.004646605812013149  PSNR: 28.48978042602539
[TRAIN] Iter: 236600 Loss: 0.00552414869889617  PSNR: 27.312604904174805
[TRAIN] Iter: 236700 Loss: 0.004749860614538193  PSNR: 29.273664474487305
[TRAIN] Iter: 236800 Loss: 0.005194046068936586  PSNR: 27.841962814331055
[TRAIN] Iter: 236900 Loss: 0.004568423144519329  PSNR: 29.06878662109375
[TRAIN] Iter: 237000 Loss: 0.005197849590331316  PSNR: 27.590391159057617
[TRAIN] Iter: 237100 Loss: 0.006026750896126032  PSNR: 27.032625198364258
[TRAIN] Iter: 237200 Loss: 0.006354008801281452  PSNR: 26.340349197387695
[TRAIN] Iter: 237300 Loss: 0.0064064618200063705  PSNR: 26.286705017089844
[TRAIN] Iter: 237400 Loss: 0.004882341716438532  PSNR: 28.845752716064453
[TRAIN] Iter: 237500 Loss: 0.0038731719832867384  PSNR: 29.3399658203125
[TRAIN] Iter: 237600 Loss: 0.005091409664601088  PSNR: 27.61685562133789
[TRAIN] Iter: 237700 Loss: 0.0059029594995081425  PSNR: 27.161771774291992
[TRAIN] Iter: 237800 Loss: 0.005437043029814959  PSNR: 26.967775344848633
[TRAIN] Iter: 237900 Loss: 0.004865473136305809  PSNR: 27.794906616210938
[TRAIN] Iter: 238000 Loss: 0.0045366487465798855  PSNR: 28.733295440673828
[TRAIN] Iter: 238100 Loss: 0.005943149793893099  PSNR: 27.633602142333984
[TRAIN] Iter: 238200 Loss: 0.006289032753556967  PSNR: 26.47594451904297
[TRAIN] Iter: 238300 Loss: 0.005792564712464809  PSNR: 26.891305923461914
[TRAIN] Iter: 238400 Loss: 0.006871017627418041  PSNR: 26.158044815063477
[TRAIN] Iter: 238500 Loss: 0.005794096272438765  PSNR: 26.912755966186523
[TRAIN] Iter: 238600 Loss: 0.005319610238075256  PSNR: 27.5540771484375
[TRAIN] Iter: 238700 Loss: 0.0066783856600522995  PSNR: 27.354747772216797
[TRAIN] Iter: 238800 Loss: 0.005369570106267929  PSNR: 28.751399993896484
[TRAIN] Iter: 238900 Loss: 0.005234852898865938  PSNR: 27.31551170349121
[TRAIN] Iter: 239000 Loss: 0.004291837103664875  PSNR: 29.07938003540039
[TRAIN] Iter: 239100 Loss: 0.003511610208079219  PSNR: 29.18522071838379
[TRAIN] Iter: 239200 Loss: 0.0042579397559165955  PSNR: 29.07366180419922
[TRAIN] Iter: 239300 Loss: 0.00391298858448863  PSNR: 29.817201614379883
[TRAIN] Iter: 239400 Loss: 0.006030838470906019  PSNR: 27.636220932006836
[TRAIN] Iter: 239500 Loss: 0.0060606240294873714  PSNR: 26.617849349975586
[TRAIN] Iter: 239600 Loss: 0.004138113930821419  PSNR: 29.109399795532227
[TRAIN] Iter: 239700 Loss: 0.006036144681274891  PSNR: 26.31098747253418
[TRAIN] Iter: 239800 Loss: 0.006755535025149584  PSNR: 27.052289962768555
[TRAIN] Iter: 239900 Loss: 0.006050310097634792  PSNR: 27.326642990112305
Saved checkpoints at ./logs/TUT-KE101-nerf/240000.tar
[TRAIN] Iter: 240000 Loss: 0.004318355116993189  PSNR: 29.48054313659668
[TRAIN] Iter: 240100 Loss: 0.006708352826535702  PSNR: 26.898054122924805
[TRAIN] Iter: 240200 Loss: 0.005562998820096254  PSNR: 27.77065658569336
[TRAIN] Iter: 240300 Loss: 0.005805010907351971  PSNR: 26.321273803710938
[TRAIN] Iter: 240400 Loss: 0.005635528825223446  PSNR: 27.39713478088379
[TRAIN] Iter: 240500 Loss: 0.005654738284647465  PSNR: 27.206771850585938
[TRAIN] Iter: 240600 Loss: 0.00458862166851759  PSNR: 28.21744728088379
[TRAIN] Iter: 240700 Loss: 0.004237069748342037  PSNR: 29.321914672851562
[TRAIN] Iter: 240800 Loss: 0.004595845937728882  PSNR: 28.352291107177734
[TRAIN] Iter: 240900 Loss: 0.005413689650595188  PSNR: 28.280128479003906
[TRAIN] Iter: 241000 Loss: 0.0060383933596313  PSNR: 26.709911346435547
[TRAIN] Iter: 241100 Loss: 0.006888588424772024  PSNR: 26.012277603149414
[TRAIN] Iter: 241200 Loss: 0.005822783801704645  PSNR: 27.252031326293945
[TRAIN] Iter: 241300 Loss: 0.004659217782318592  PSNR: 29.116046905517578
[TRAIN] Iter: 241400 Loss: 0.00645070057362318  PSNR: 26.159101486206055
[TRAIN] Iter: 241500 Loss: 0.0061988504603505135  PSNR: 26.943042755126953
[TRAIN] Iter: 241600 Loss: 0.005928356666117907  PSNR: 26.518295288085938
[TRAIN] Iter: 241700 Loss: 0.005872446112334728  PSNR: 27.881147384643555
[TRAIN] Iter: 241800 Loss: 0.004241897724568844  PSNR: 29.28378677368164
[TRAIN] Iter: 241900 Loss: 0.004385077394545078  PSNR: 29.371307373046875
[TRAIN] Iter: 242000 Loss: 0.005275459494441748  PSNR: 27.655517578125
[TRAIN] Iter: 242100 Loss: 0.00387269607745111  PSNR: 29.66541862487793
[TRAIN] Iter: 242200 Loss: 0.006618755403906107  PSNR: 26.373334884643555
[TRAIN] Iter: 242300 Loss: 0.00623136293143034  PSNR: 26.45608139038086
[TRAIN] Iter: 242400 Loss: 0.006110624875873327  PSNR: 26.611791610717773
[TRAIN] Iter: 242500 Loss: 0.00482640415430069  PSNR: 28.704057693481445
[TRAIN] Iter: 242600 Loss: 0.005378936417400837  PSNR: 27.921964645385742
[TRAIN] Iter: 242700 Loss: 0.0043082269839942455  PSNR: 29.12010955810547
[TRAIN] Iter: 242800 Loss: 0.0048677572049200535  PSNR: 27.84526252746582
[TRAIN] Iter: 242900 Loss: 0.005097203887999058  PSNR: 27.750686645507812
[TRAIN] Iter: 243000 Loss: 0.00586441345512867  PSNR: 26.936067581176758
[TRAIN] Iter: 243100 Loss: 0.00557636097073555  PSNR: 27.341495513916016
[TRAIN] Iter: 243200 Loss: 0.004878833889961243  PSNR: 27.6326847076416
[TRAIN] Iter: 243300 Loss: 0.005225067492574453  PSNR: 27.686033248901367
[TRAIN] Iter: 243400 Loss: 0.005458531901240349  PSNR: 27.22499656677246
[TRAIN] Iter: 243500 Loss: 0.004923518281430006  PSNR: 28.276138305664062
[TRAIN] Iter: 243600 Loss: 0.005232241004705429  PSNR: 27.7242431640625
[TRAIN] Iter: 243700 Loss: 0.005922260694205761  PSNR: 27.227291107177734
[TRAIN] Iter: 243800 Loss: 0.0046265400014817715  PSNR: 28.5172119140625
[TRAIN] Iter: 243900 Loss: 0.0041489554569125175  PSNR: 28.9296932220459
[TRAIN] Iter: 244000 Loss: 0.00496943574398756  PSNR: 27.49469566345215
[TRAIN] Iter: 244100 Loss: 0.004924362059682608  PSNR: 29.51816749572754
[TRAIN] Iter: 244200 Loss: 0.00430348701775074  PSNR: 29.459308624267578
[TRAIN] Iter: 244300 Loss: 0.004019822459667921  PSNR: 29.170019149780273
[TRAIN] Iter: 244400 Loss: 0.005271579138934612  PSNR: 27.402130126953125
[TRAIN] Iter: 244500 Loss: 0.00519589614123106  PSNR: 27.11853790283203
[TRAIN] Iter: 244600 Loss: 0.005717877764254808  PSNR: 27.0067081451416
[TRAIN] Iter: 244700 Loss: 0.006034593563526869  PSNR: 26.60488510131836
[TRAIN] Iter: 244800 Loss: 0.006267634220421314  PSNR: 26.703510284423828
[TRAIN] Iter: 244900 Loss: 0.004220930393785238  PSNR: 29.37936019897461
[TRAIN] Iter: 245000 Loss: 0.006300556473433971  PSNR: 26.81789779663086
[TRAIN] Iter: 245100 Loss: 0.0038139279931783676  PSNR: 29.170671463012695
[TRAIN] Iter: 245200 Loss: 0.006232817191630602  PSNR: 26.679723739624023
[TRAIN] Iter: 245300 Loss: 0.0055795516818761826  PSNR: 27.697412490844727
[TRAIN] Iter: 245400 Loss: 0.00577379297465086  PSNR: 27.369611740112305
[TRAIN] Iter: 245500 Loss: 0.003982430789619684  PSNR: 29.190053939819336
[TRAIN] Iter: 245600 Loss: 0.006325841881334782  PSNR: 26.20627784729004
[TRAIN] Iter: 245700 Loss: 0.005669726990163326  PSNR: 27.308740615844727
[TRAIN] Iter: 245800 Loss: 0.004372604191303253  PSNR: 28.076879501342773
[TRAIN] Iter: 245900 Loss: 0.004572688601911068  PSNR: 28.515968322753906
[TRAIN] Iter: 246000 Loss: 0.0059867980889976025  PSNR: 26.802274703979492
[TRAIN] Iter: 246100 Loss: 0.0067420536652207375  PSNR: 26.428325653076172
[TRAIN] Iter: 246200 Loss: 0.005665375851094723  PSNR: 27.452363967895508
[TRAIN] Iter: 246300 Loss: 0.005729357246309519  PSNR: 27.421201705932617
[TRAIN] Iter: 246400 Loss: 0.004059134982526302  PSNR: 28.639392852783203
[TRAIN] Iter: 246500 Loss: 0.005360850133001804  PSNR: 27.669273376464844
[TRAIN] Iter: 246600 Loss: 0.0038953819312155247  PSNR: 29.71051025390625
[TRAIN] Iter: 246700 Loss: 0.003820937592536211  PSNR: 29.37115478515625
[TRAIN] Iter: 246800 Loss: 0.004003726877272129  PSNR: 29.33952522277832
[TRAIN] Iter: 246900 Loss: 0.006456186529248953  PSNR: 26.350788116455078
[TRAIN] Iter: 247000 Loss: 0.0056549496948719025  PSNR: 27.09100341796875
[TRAIN] Iter: 247100 Loss: 0.005918062292039394  PSNR: 26.38656997680664
[TRAIN] Iter: 247200 Loss: 0.006221922114491463  PSNR: 26.718015670776367
[TRAIN] Iter: 247300 Loss: 0.005493165925145149  PSNR: 26.98771858215332
[TRAIN] Iter: 247400 Loss: 0.0052315592765808105  PSNR: 27.252349853515625
[TRAIN] Iter: 247500 Loss: 0.004577375017106533  PSNR: 28.442644119262695
[TRAIN] Iter: 247600 Loss: 0.004342629108577967  PSNR: 29.37208366394043
[TRAIN] Iter: 247700 Loss: 0.003923317883163691  PSNR: 29.44097137451172
[TRAIN] Iter: 247800 Loss: 0.0057280766777694225  PSNR: 27.0542049407959
[TRAIN] Iter: 247900 Loss: 0.005365919321775436  PSNR: 27.896726608276367
[TRAIN] Iter: 248000 Loss: 0.004425339866429567  PSNR: 29.14271354675293
[TRAIN] Iter: 248100 Loss: 0.0049771955236792564  PSNR: 27.365110397338867
[TRAIN] Iter: 248200 Loss: 0.004626864101737738  PSNR: 27.63153648376465
[TRAIN] Iter: 248300 Loss: 0.0046967859379947186  PSNR: 28.1052188873291
[TRAIN] Iter: 248400 Loss: 0.005747153423726559  PSNR: 26.62598419189453
[TRAIN] Iter: 248500 Loss: 0.006261521019041538  PSNR: 26.758363723754883
[TRAIN] Iter: 248600 Loss: 0.004334454424679279  PSNR: 29.959623336791992
[TRAIN] Iter: 248700 Loss: 0.006000898778438568  PSNR: 26.712146759033203
[TRAIN] Iter: 248800 Loss: 0.005442652851343155  PSNR: 27.9910888671875
[TRAIN] Iter: 248900 Loss: 0.005815937649458647  PSNR: 27.427030563354492
[TRAIN] Iter: 249000 Loss: 0.004531312268227339  PSNR: 28.991273880004883
[TRAIN] Iter: 249100 Loss: 0.005513674113899469  PSNR: 27.401477813720703
[TRAIN] Iter: 249200 Loss: 0.0067633818835020065  PSNR: 26.256336212158203
[TRAIN] Iter: 249300 Loss: 0.004545568022876978  PSNR: 28.67569923400879
[TRAIN] Iter: 249400 Loss: 0.005353299900889397  PSNR: 27.171504974365234
[TRAIN] Iter: 249500 Loss: 0.004999819677323103  PSNR: 28.72413444519043
[TRAIN] Iter: 249600 Loss: 0.005779217928647995  PSNR: 27.270902633666992
[TRAIN] Iter: 249700 Loss: 0.006788663100451231  PSNR: 25.700517654418945
[TRAIN] Iter: 249800 Loss: 0.006848283112049103  PSNR: 26.697216033935547
[TRAIN] Iter: 249900 Loss: 0.005063945427536964  PSNR: 27.941478729248047
Saved checkpoints at ./logs/TUT-KE101-nerf/250000.tar
0 0.00035953521728515625
torch.Size([320, 640, 3]) torch.Size([320, 640])
1 13.506038427352905
2 13.191679000854492
3 16.917449235916138
4 13.459707021713257
5 15.852045059204102
6 13.229672908782959
7 13.19808840751648
8 17.129651069641113
9 13.23368239402771
10 16.27335286140442
11 13.188650131225586
12 13.220805406570435
13 16.982879161834717
14 13.577670097351074
15 15.771575689315796
16 13.500391483306885
17 16.10005235671997
18 13.620225191116333
19 13.525637865066528
20 16.13345718383789
21 13.63596796989441
22 16.19472908973694
23 13.560299158096313
24 16.18032956123352
25 13.60994005203247
26 13.577993631362915
27 16.15192985534668
28 13.52802848815918
29 16.200652360916138
30 13.572900295257568
31 13.579782724380493
32 16.20542573928833
33 13.561285495758057
34 16.24351978302002
35 13.540087461471558
36 16.12190079689026
37 13.591963529586792
38 13.455982446670532
39 16.226114749908447
40 13.562636137008667
41 16.252769947052002
42 13.613728046417236
43 16.250824213027954
44 13.66478967666626
45 13.27584195137024
46 16.23052740097046
47 13.499728202819824
48 16.709293842315674
49 13.564085721969604
50 13.73215365409851
51 16.14132595062256
52 13.357909917831421
53 16.547438383102417
54 13.502429246902466
55 16.250226259231567
56 13.450316905975342
57 13.31416630744934
58 16.71410584449768
59 13.423890829086304
60 16.383355379104614
61 13.384077072143555
62 13.426450729370117
63 16.591311931610107
64 13.330477476119995
65 16.887490034103394
66 13.452741861343384
67 16.77289843559265
68 14.104906797409058
69 13.502129793167114
70 16.831563472747803
71 13.481483697891235
72 17.411163568496704
73 13.868310928344727
74 15.551785230636597
75 13.404069185256958
76 13.51937747001648
77 17.728779554367065
78 13.56589126586914
79 15.88317608833313
80 13.303131103515625
81 16.989697217941284
82 13.747061491012573
83 13.344012260437012
84 16.854652881622314
85 13.64720869064331
86 16.230196952819824
87 13.426900148391724
88 13.561279773712158
89 17.341577768325806
90 15.150881052017212
91 18.507565021514893
92 14.960708856582642
93 18.658788204193115
94 15.188371658325195
95 18.388723134994507
96 15.561403036117554
97 18.242387771606445
98 15.370941162109375
99 18.1645929813385
100 15.409204721450806
101 18.21269917488098
102 15.537460565567017
103 18.233989238739014
104 15.493733644485474
105 18.184484720230103
106 15.282957315444946
107 18.092955589294434
108 15.486686944961548
109 15.772087335586548
110 18.708919048309326
111 15.489631175994873
112 18.079339027404785
113 15.439604043960571
114 18.17633032798767
115 15.439674615859985
116 18.179722785949707
117 15.523862361907959
118 18.014678478240967
119 15.380362510681152
Done, saving (120, 320, 640, 3) (120, 320, 640)
extras:{'raw': tensor([[[-3.6534e-01,  1.0634e+00,  4.1955e+00, -7.0461e+01],
         [ 9.4604e-01,  1.2493e+00,  1.9519e+00, -1.7333e+01],
         [ 1.4036e+00,  1.7242e+00,  2.4899e+00, -1.5318e+01],
         ...,
         [ 6.7468e+00,  8.9162e+00,  1.5037e+01,  4.9418e+02],
         [ 7.4641e+00,  9.8876e+00,  1.6505e+01,  4.3711e+02],
         [ 7.3964e+00,  9.8298e+00,  1.6593e+01,  4.9383e+02]],

        [[-1.4592e+00, -6.4014e-01,  9.2633e-01, -6.3099e+01],
         [-1.0124e-01, -1.0123e-01, -8.9365e-02, -2.5404e+01],
         [-1.0111e-01, -1.0607e-01, -1.0292e-01, -2.4717e+01],
         ...,
         [-1.5680e+00, -3.0992e+00, -6.2259e+00,  1.2018e+02],
         [-1.4988e-01, -1.9206e+00, -4.9912e+00,  1.0231e+02],
         [-1.6130e+00, -3.1602e+00, -6.5148e+00,  1.3725e+02]],

        [[-1.0317e+00, -4.1318e-01,  9.0438e-01, -4.5587e+01],
         [-1.4505e-01,  1.2809e-01,  6.3612e-01, -2.8557e+01],
         [-1.3690e-01,  1.0985e-01,  5.6131e-01, -2.6947e+01],
         ...,
         [-1.1573e+01, -1.1699e+01, -1.1776e+01, -1.4500e+02],
         [-1.3637e+01, -1.3685e+01, -1.3607e+01, -1.3721e+02],
         [-1.0243e+01, -1.0366e+01, -1.0350e+01, -1.4628e+02]],

        ...,

        [[-1.3919e+00, -5.3431e-01,  1.7478e+00, -6.6530e+01],
         [-1.0792e-01,  5.0393e-02,  4.1425e-01, -5.0090e+01],
         [-7.9244e-02,  7.5812e-02,  4.4386e-01, -4.9122e+01],
         ...,
         [-2.1792e+00, -1.7567e+00, -6.6520e+00,  3.8389e+02],
         [-1.9404e+00, -1.7020e+00, -6.8489e+00,  4.1392e+02],
         [-2.3330e+00, -1.7028e+00, -6.8604e+00,  5.0749e+02]],

        [[ 2.1263e-01,  4.4212e-02, -2.4497e-01, -6.7696e+01],
         [ 1.8567e-02, -3.6154e-01, -1.4438e+00, -1.5007e+01],
         [ 6.1789e-02, -6.9113e-01, -2.3533e+00, -1.6096e+01],
         ...,
         [ 6.7438e+00, -7.7205e-01, -1.7090e+01, -3.3006e+02],
         [ 1.7942e+00, -4.8512e+00, -1.8622e+01, -2.5534e+02],
         [-4.2404e-01, -6.5490e+00, -1.8912e+01, -2.3110e+02]],

        [[-1.7476e+00, -1.3562e+00, -7.5006e-01, -6.3458e+01],
         [-3.3280e-01, -7.3392e-01, -2.0315e+00, -1.4728e+01],
         [-8.9895e-02, -4.9736e-01, -1.5586e+00, -3.8210e+00],
         ...,
         [ 5.9694e+00, -1.9645e+00, -1.9069e+01, -3.3996e+02],
         [-2.2898e+00, -8.7864e+00, -2.1909e+01, -2.0990e+02],
         [-2.8887e+00, -9.6796e+00, -2.3031e+01, -2.1759e+02]]],
       grad_fn=<ReshapeAliasBackward0>), 'rgb0': tensor([[0.4775, 0.4246, 0.3556],
        [0.4499, 0.2856, 0.0955],
        [0.6096, 0.6383, 0.7106],
        ...,
        [0.4338, 0.3899, 0.3222],
        [0.4185, 0.3377, 0.1845],
        [0.4282, 0.3841, 0.2765]], grad_fn=<ReshapeAliasBackward0>), 'disp0': tensor([338.2243, 105.7865, 214.7337,  ..., 143.0605,  73.3352,  63.1690],
       grad_fn=<ReshapeAliasBackward0>), 'acc0': tensor([1., 1., 1.,  ..., 1., 1., 1.], grad_fn=<ReshapeAliasBackward0>), 'z_std': tensor([0.0869, 0.0021, 0.0214,  ..., 0.0017, 0.0022, 0.0024])}
0 0.00045418739318847656
torch.Size([320, 640, 3]) torch.Size([320, 640])
1 15.413946151733398
2 18.20002794265747
3 15.334441423416138
4 18.240176677703857
5 15.25169587135315
6 18.48481297492981
7 15.887499332427979
8 18.697754621505737
9 15.558337450027466
10 18.24557137489319
11 15.381498336791992
12 18.18255114555359
13 15.339258670806885
14 18.2274329662323
15 15.30067229270935
16 15.336933851242065
17 18.210793018341064
18 15.320972919464111
19 18.176624536514282
20 15.116966247558594
21 18.714895725250244
22 15.053037643432617
23 18.690154552459717
24 15.549048900604248
25 17.5955228805542
26 15.111148357391357
27 19.227786779403687
28 15.746645450592041
29 17.049490213394165
30 14.984128475189209
31 19.088139295578003
32 15.766708612442017
33 17.63442873954773
34 15.267469644546509
35 18.925025463104248
36 15.291832685470581
37 15.061686277389526
38 18.529013872146606
39 15.362820863723755
40 18.808823108673096
41 15.363040924072266
42 18.687262296676636
43 15.39450740814209
44 19.007676601409912
45 15.408620119094849
46 18.371151208877563
47 15.181274175643921
48 18.032951831817627
49 15.832053184509277
50 17.798731088638306
51 15.03592300415039
52 18.54089117050171
53 15.311285495758057
54 18.214939832687378
55 15.211373090744019
56 17.982964754104614
57 15.11483645439148
58 18.050289154052734
59 15.093406200408936
60 15.008954524993896
61 18.15734624862671
62 14.830002307891846
63 18.324257135391235
64 14.842408657073975
65 18.261622667312622
66 14.930761575698853
67 18.239152908325195
68 14.956563711166382
69 18.454874277114868
70 15.407543420791626
71 18.564040422439575
72 15.028778553009033
73 18.204192876815796
74 15.029619932174683
75 15.036274433135986
76 18.298630952835083
77 15.24242353439331
78 18.452274560928345
79 15.17705774307251
80 18.425028800964355
81 15.15567135810852
82 18.380491256713867
83 15.088390827178955
84 18.362655878067017
85 15.185271501541138
86 18.415642976760864
87 15.222277879714966
88 18.395220518112183
89 15.225732326507568
90 18.552682876586914
91 15.258544683456421
92 10.791215181350708
93 18.900468349456787
94 15.566666603088379
95 18.885595560073853
96 15.068784713745117
97 18.719926118850708
98 15.45772099494934
99 17.940247774124146
100 15.13123869895935
101 19.09603476524353
102 15.880010604858398
103 16.94610333442688
104 15.134639263153076
105 18.989530324935913
106 15.82234001159668
107 17.292030334472656
108 14.885836839675903
109 15.126137018203735
110 18.757402658462524
111 14.955551385879517
112 18.50002908706665
113 14.924980401992798
114 18.482841968536377
115 14.866075992584229
116 18.007498025894165
117 14.766165256500244
118 18.485567569732666
119 15.048310995101929
test poses shape torch.Size([4, 3, 4])
0 0.0009915828704833984
torch.Size([320, 640, 3]) torch.Size([320, 640])
1 15.366764068603516
2 17.846644401550293
3 15.2248375415802
Saved test set
[TRAIN] Iter: 250000 Loss: 0.004866188392043114  PSNR: 27.51181983947754
[TRAIN] Iter: 250100 Loss: 0.007144080009311438  PSNR: 26.05040168762207
[TRAIN] Iter: 250200 Loss: 0.0050841765478253365  PSNR: 28.234203338623047
[TRAIN] Iter: 250300 Loss: 0.004144066479057074  PSNR: 29.419710159301758
[TRAIN] Iter: 250400 Loss: 0.003828412853181362  PSNR: 29.56842613220215
[TRAIN] Iter: 250500 Loss: 0.0041064126417040825  PSNR: 29.1151123046875
[TRAIN] Iter: 250600 Loss: 0.00546250818297267  PSNR: 27.209592819213867
[TRAIN] Iter: 250700 Loss: 0.0048982747830450535  PSNR: 29.262014389038086
[TRAIN] Iter: 250800 Loss: 0.004084814339876175  PSNR: 29.198606491088867
[TRAIN] Iter: 250900 Loss: 0.004783512093126774  PSNR: 27.361888885498047
[TRAIN] Iter: 251000 Loss: 0.004261935595422983  PSNR: 29.061845779418945
[TRAIN] Iter: 251100 Loss: 0.005431883968412876  PSNR: 27.511272430419922
[TRAIN] Iter: 251200 Loss: 0.006681129802018404  PSNR: 26.456605911254883
[TRAIN] Iter: 251300 Loss: 0.005885545630007982  PSNR: 27.023080825805664
[TRAIN] Iter: 251400 Loss: 0.00368439219892025  PSNR: 29.794511795043945
[TRAIN] Iter: 251500 Loss: 0.00467626703903079  PSNR: 28.285343170166016
[TRAIN] Iter: 251600 Loss: 0.005745388567447662  PSNR: 26.691558837890625
[TRAIN] Iter: 251700 Loss: 0.004466827493160963  PSNR: 30.4857120513916
[TRAIN] Iter: 251800 Loss: 0.004975263029336929  PSNR: 28.145408630371094
[TRAIN] Iter: 251900 Loss: 0.007583690341562033  PSNR: 25.539642333984375
[TRAIN] Iter: 252000 Loss: 0.006048599258065224  PSNR: 26.69357681274414
[TRAIN] Iter: 252100 Loss: 0.004690984264016151  PSNR: 28.484375
[TRAIN] Iter: 252200 Loss: 0.0052780634723603725  PSNR: 27.711729049682617
[TRAIN] Iter: 252300 Loss: 0.0064241038635373116  PSNR: 26.797780990600586
[TRAIN] Iter: 252400 Loss: 0.0059379693120718  PSNR: 26.806882858276367
[TRAIN] Iter: 252500 Loss: 0.005896793212741613  PSNR: 26.750263214111328
[TRAIN] Iter: 252600 Loss: 0.00780996959656477  PSNR: 26.47306251525879
[TRAIN] Iter: 252700 Loss: 0.004502059426158667  PSNR: 28.195470809936523
[TRAIN] Iter: 252800 Loss: 0.004583635367453098  PSNR: 28.058040618896484
[TRAIN] Iter: 252900 Loss: 0.00563344731926918  PSNR: 28.171775817871094
[TRAIN] Iter: 253000 Loss: 0.004468895494937897  PSNR: 28.33576202392578
[TRAIN] Iter: 253100 Loss: 0.0042510367929935455  PSNR: 28.914247512817383
[TRAIN] Iter: 253200 Loss: 0.005175473168492317  PSNR: 27.20181655883789
[TRAIN] Iter: 253300 Loss: 0.005620288662612438  PSNR: 27.13957405090332
[TRAIN] Iter: 253400 Loss: 0.004172004759311676  PSNR: 29.92521858215332
[TRAIN] Iter: 253500 Loss: 0.006383015774190426  PSNR: 26.686996459960938
[TRAIN] Iter: 253600 Loss: 0.0058020129799842834  PSNR: 27.482131958007812
[TRAIN] Iter: 253700 Loss: 0.005231821909546852  PSNR: 27.415714263916016
[TRAIN] Iter: 253800 Loss: 0.004049363546073437  PSNR: 29.27936363220215
[TRAIN] Iter: 253900 Loss: 0.003739268984645605  PSNR: 29.56402587890625
[TRAIN] Iter: 254000 Loss: 0.00563560426235199  PSNR: 26.24490737915039
[TRAIN] Iter: 254100 Loss: 0.004729241132736206  PSNR: 28.623952865600586
[TRAIN] Iter: 254200 Loss: 0.006276294123381376  PSNR: 26.92066764831543
[TRAIN] Iter: 254300 Loss: 0.004367179237306118  PSNR: 28.770919799804688
[TRAIN] Iter: 254400 Loss: 0.005073945503681898  PSNR: 27.932031631469727
[TRAIN] Iter: 254500 Loss: 0.003978922963142395  PSNR: 29.640050888061523
[TRAIN] Iter: 254600 Loss: 0.005466453731060028  PSNR: 27.449317932128906
[TRAIN] Iter: 254700 Loss: 0.005965898744761944  PSNR: 26.812597274780273
[TRAIN] Iter: 254800 Loss: 0.005203305743634701  PSNR: 28.583576202392578
[TRAIN] Iter: 254900 Loss: 0.005035677924752235  PSNR: 28.64444923400879
[TRAIN] Iter: 255000 Loss: 0.006883629597723484  PSNR: 25.64632225036621
[TRAIN] Iter: 255100 Loss: 0.006034116260707378  PSNR: 26.109111785888672
[TRAIN] Iter: 255200 Loss: 0.006494241300970316  PSNR: 26.409069061279297
[TRAIN] Iter: 255300 Loss: 0.004475730936974287  PSNR: 28.983638763427734
[TRAIN] Iter: 255400 Loss: 0.006796086672693491  PSNR: 26.44007110595703
[TRAIN] Iter: 255500 Loss: 0.0050260527059435844  PSNR: 27.63817596435547
[TRAIN] Iter: 255600 Loss: 0.004202548414468765  PSNR: 29.650890350341797
[TRAIN] Iter: 255700 Loss: 0.007493746932595968  PSNR: 25.605266571044922
[TRAIN] Iter: 255800 Loss: 0.00532239954918623  PSNR: 27.68675422668457
[TRAIN] Iter: 255900 Loss: 0.0037420843727886677  PSNR: 29.894620895385742
[TRAIN] Iter: 256000 Loss: 0.00697793485596776  PSNR: 26.37584114074707
[TRAIN] Iter: 256100 Loss: 0.004749452695250511  PSNR: 27.857114791870117
[TRAIN] Iter: 256200 Loss: 0.005869689863175154  PSNR: 26.62161636352539
[TRAIN] Iter: 256300 Loss: 0.0046575977467000484  PSNR: 28.865692138671875
[TRAIN] Iter: 256400 Loss: 0.004393208771944046  PSNR: 29.379972457885742
[TRAIN] Iter: 256500 Loss: 0.004391967318952084  PSNR: 29.222436904907227
[TRAIN] Iter: 256600 Loss: 0.006372874602675438  PSNR: 26.360082626342773
[TRAIN] Iter: 256700 Loss: 0.0051645333878695965  PSNR: 27.106542587280273
[TRAIN] Iter: 256800 Loss: 0.004279489628970623  PSNR: 28.792770385742188
[TRAIN] Iter: 256900 Loss: 0.006053774617612362  PSNR: 27.67451286315918
[TRAIN] Iter: 257000 Loss: 0.0055162617936730385  PSNR: 28.000288009643555
[TRAIN] Iter: 257100 Loss: 0.00531767588108778  PSNR: 28.14173698425293
[TRAIN] Iter: 257200 Loss: 0.006151812616735697  PSNR: 26.961929321289062
[TRAIN] Iter: 257300 Loss: 0.004980295896530151  PSNR: 28.276493072509766
[TRAIN] Iter: 257400 Loss: 0.004203932359814644  PSNR: 29.959260940551758
[TRAIN] Iter: 257500 Loss: 0.006529517471790314  PSNR: 26.516401290893555
[TRAIN] Iter: 257600 Loss: 0.004885473288595676  PSNR: 28.898412704467773
[TRAIN] Iter: 257700 Loss: 0.0038166996091604233  PSNR: 29.388713836669922
[TRAIN] Iter: 257800 Loss: 0.005774139426648617  PSNR: 27.01400375366211
[TRAIN] Iter: 257900 Loss: 0.005118652246892452  PSNR: 27.93888282775879
[TRAIN] Iter: 258000 Loss: 0.005194265395402908  PSNR: 26.89691162109375
[TRAIN] Iter: 258100 Loss: 0.006055395118892193  PSNR: 26.846397399902344
[TRAIN] Iter: 258200 Loss: 0.004390262067317963  PSNR: 28.997934341430664
[TRAIN] Iter: 258300 Loss: 0.005587243475019932  PSNR: 27.710063934326172
[TRAIN] Iter: 258400 Loss: 0.0042118411511182785  PSNR: 28.779205322265625
[TRAIN] Iter: 258500 Loss: 0.003331652842462063  PSNR: 30.277257919311523
[TRAIN] Iter: 258600 Loss: 0.0040447223000228405  PSNR: 28.929800033569336
[TRAIN] Iter: 258700 Loss: 0.0054725054651498795  PSNR: 27.21148109436035
[TRAIN] Iter: 258800 Loss: 0.005156661383807659  PSNR: 27.749723434448242
[TRAIN] Iter: 258900 Loss: 0.005572260823100805  PSNR: 27.50543785095215
[TRAIN] Iter: 259000 Loss: 0.004452155902981758  PSNR: 29.91206932067871
[TRAIN] Iter: 259100 Loss: 0.0051786950789391994  PSNR: 27.10077667236328
[TRAIN] Iter: 259200 Loss: 0.005688514094799757  PSNR: 26.67906379699707
[TRAIN] Iter: 259300 Loss: 0.004754452966153622  PSNR: 27.89651107788086
[TRAIN] Iter: 259400 Loss: 0.005240206606686115  PSNR: 27.431621551513672
[TRAIN] Iter: 259500 Loss: 0.0033085886389017105  PSNR: 30.293825149536133
[TRAIN] Iter: 259600 Loss: 0.005877201911062002  PSNR: 27.140949249267578
[TRAIN] Iter: 259700 Loss: 0.005062332842499018  PSNR: 28.657955169677734
[TRAIN] Iter: 259800 Loss: 0.006545147858560085  PSNR: 25.748903274536133
[TRAIN] Iter: 259900 Loss: 0.0043278904631733894  PSNR: 28.795074462890625
Saved checkpoints at ./logs/TUT-KE101-nerf/260000.tar
[TRAIN] Iter: 260000 Loss: 0.004718798212707043  PSNR: 29.258777618408203
[TRAIN] Iter: 260100 Loss: 0.005120153073221445  PSNR: 27.746601104736328
[TRAIN] Iter: 260200 Loss: 0.005935492925345898  PSNR: 26.703317642211914
[TRAIN] Iter: 260300 Loss: 0.0038228128105401993  PSNR: 30.001768112182617
[TRAIN] Iter: 260400 Loss: 0.00510293198749423  PSNR: 28.44447898864746
[TRAIN] Iter: 260500 Loss: 0.003935740329325199  PSNR: 30.016883850097656
[TRAIN] Iter: 260600 Loss: 0.0049792127683758736  PSNR: 28.049596786499023
[TRAIN] Iter: 260700 Loss: 0.004063631407916546  PSNR: 29.47826385498047
[TRAIN] Iter: 260800 Loss: 0.005713838618248701  PSNR: 27.58678436279297
[TRAIN] Iter: 260900 Loss: 0.004310534335672855  PSNR: 29.07871437072754
[TRAIN] Iter: 261000 Loss: 0.004475999623537064  PSNR: 28.27214241027832
[TRAIN] Iter: 261100 Loss: 0.005955974105745554  PSNR: 26.209259033203125
[TRAIN] Iter: 261200 Loss: 0.004798388574272394  PSNR: 27.95500946044922
[TRAIN] Iter: 261300 Loss: 0.005742219276726246  PSNR: 26.777658462524414
[TRAIN] Iter: 261400 Loss: 0.0042277914471924305  PSNR: 29.786869049072266
[TRAIN] Iter: 261500 Loss: 0.0051807682029902935  PSNR: 27.360061645507812
[TRAIN] Iter: 261600 Loss: 0.005713018123060465  PSNR: 27.754528045654297
[TRAIN] Iter: 261700 Loss: 0.004473284352570772  PSNR: 28.739212036132812
[TRAIN] Iter: 261800 Loss: 0.004335879813879728  PSNR: 29.220298767089844
[TRAIN] Iter: 261900 Loss: 0.004743092693388462  PSNR: 28.731101989746094
[TRAIN] Iter: 262000 Loss: 0.005297978408634663  PSNR: 27.992998123168945
[TRAIN] Iter: 262100 Loss: 0.0036234401632100344  PSNR: 30.055587768554688
[TRAIN] Iter: 262200 Loss: 0.005026573780924082  PSNR: 29.210840225219727
[TRAIN] Iter: 262300 Loss: 0.006127772852778435  PSNR: 26.683351516723633
[TRAIN] Iter: 262400 Loss: 0.00460305018350482  PSNR: 28.087356567382812
[TRAIN] Iter: 262500 Loss: 0.005267027765512466  PSNR: 28.42302894592285
[TRAIN] Iter: 262600 Loss: 0.005543154664337635  PSNR: 27.913841247558594
[TRAIN] Iter: 262700 Loss: 0.005129015073180199  PSNR: 27.837533950805664
[TRAIN] Iter: 262800 Loss: 0.006224953569471836  PSNR: 26.85447883605957
[TRAIN] Iter: 262900 Loss: 0.005563382990658283  PSNR: 26.502145767211914
[TRAIN] Iter: 263000 Loss: 0.0057572368532419205  PSNR: 27.433317184448242
[TRAIN] Iter: 263100 Loss: 0.004295001737773418  PSNR: 29.530324935913086
[TRAIN] Iter: 263200 Loss: 0.004179801791906357  PSNR: 29.113969802856445
[TRAIN] Iter: 263300 Loss: 0.004100707825273275  PSNR: 28.765209197998047
[TRAIN] Iter: 263400 Loss: 0.005416719242930412  PSNR: 27.369230270385742
[TRAIN] Iter: 263500 Loss: 0.006635881029069424  PSNR: 26.64035987854004
[TRAIN] Iter: 263600 Loss: 0.005335279740393162  PSNR: 27.43655776977539
[TRAIN] Iter: 263700 Loss: 0.00542469322681427  PSNR: 28.737205505371094
[TRAIN] Iter: 263800 Loss: 0.004409868735820055  PSNR: 29.480623245239258
[TRAIN] Iter: 263900 Loss: 0.00406733900308609  PSNR: 28.981494903564453
[TRAIN] Iter: 264000 Loss: 0.004719243850558996  PSNR: 28.480541229248047
[TRAIN] Iter: 264100 Loss: 0.006470927502959967  PSNR: 27.094175338745117
[TRAIN] Iter: 264200 Loss: 0.005604480393230915  PSNR: 27.862340927124023
[TRAIN] Iter: 264300 Loss: 0.0058515495620667934  PSNR: 27.163070678710938
[TRAIN] Iter: 264400 Loss: 0.004478893242776394  PSNR: 28.871965408325195
[TRAIN] Iter: 264500 Loss: 0.004173764027655125  PSNR: 29.646869659423828
[TRAIN] Iter: 264600 Loss: 0.005976737476885319  PSNR: 27.07353401184082
[TRAIN] Iter: 264700 Loss: 0.006365135312080383  PSNR: 26.0389461517334
[TRAIN] Iter: 264800 Loss: 0.006237736903131008  PSNR: 26.384714126586914
[TRAIN] Iter: 264900 Loss: 0.004490599036216736  PSNR: 28.385122299194336
[TRAIN] Iter: 265000 Loss: 0.006486096885055304  PSNR: 26.622480392456055
[TRAIN] Iter: 265100 Loss: 0.004533164203166962  PSNR: 28.020658493041992
[TRAIN] Iter: 265200 Loss: 0.005890815518796444  PSNR: 27.329219818115234
[TRAIN] Iter: 265300 Loss: 0.005538929253816605  PSNR: 27.129554748535156
[TRAIN] Iter: 265400 Loss: 0.006227937527000904  PSNR: 26.472864151000977
[TRAIN] Iter: 265500 Loss: 0.004142502788454294  PSNR: 29.85982322692871
[TRAIN] Iter: 265600 Loss: 0.005788022186607122  PSNR: 26.963314056396484
[TRAIN] Iter: 265700 Loss: 0.00657021114602685  PSNR: 27.286115646362305
[TRAIN] Iter: 265800 Loss: 0.006314171012490988  PSNR: 26.296533584594727
[TRAIN] Iter: 265900 Loss: 0.006811153143644333  PSNR: 26.396432876586914
[TRAIN] Iter: 266000 Loss: 0.003912673331797123  PSNR: 29.101242065429688
[TRAIN] Iter: 266100 Loss: 0.005244849249720573  PSNR: 27.471813201904297
[TRAIN] Iter: 266200 Loss: 0.0060844700783491135  PSNR: 27.007953643798828
[TRAIN] Iter: 266300 Loss: 0.005619164556264877  PSNR: 27.26220703125
[TRAIN] Iter: 266400 Loss: 0.005964941345155239  PSNR: 27.07716178894043
[TRAIN] Iter: 266500 Loss: 0.005781498737633228  PSNR: 26.812530517578125
[TRAIN] Iter: 266600 Loss: 0.004547943361103535  PSNR: 27.784038543701172
[TRAIN] Iter: 266700 Loss: 0.005409256089478731  PSNR: 27.319169998168945
[TRAIN] Iter: 266800 Loss: 0.005179230123758316  PSNR: 27.770198822021484
[TRAIN] Iter: 266900 Loss: 0.004018673673272133  PSNR: 29.688566207885742
[TRAIN] Iter: 267000 Loss: 0.005733885802328587  PSNR: 27.751672744750977
[TRAIN] Iter: 267100 Loss: 0.004863045644015074  PSNR: 28.30815887451172
[TRAIN] Iter: 267200 Loss: 0.005249767564237118  PSNR: 27.827930450439453
[TRAIN] Iter: 267300 Loss: 0.0055819181725382805  PSNR: 27.200536727905273
[TRAIN] Iter: 267400 Loss: 0.005213592667132616  PSNR: 26.92390251159668
[TRAIN] Iter: 267500 Loss: 0.005528931971639395  PSNR: 26.644006729125977
[TRAIN] Iter: 267600 Loss: 0.006124146748334169  PSNR: 26.89539909362793
[TRAIN] Iter: 267700 Loss: 0.005888041574507952  PSNR: 27.34283447265625
[TRAIN] Iter: 267800 Loss: 0.00663228053599596  PSNR: 26.706716537475586
[TRAIN] Iter: 267900 Loss: 0.004056619480252266  PSNR: 29.585105895996094
[TRAIN] Iter: 268000 Loss: 0.005277357995510101  PSNR: 27.404254913330078
[TRAIN] Iter: 268100 Loss: 0.0064343782141804695  PSNR: 26.240144729614258
[TRAIN] Iter: 268200 Loss: 0.005493577104061842  PSNR: 27.419031143188477
[TRAIN] Iter: 268300 Loss: 0.005460683256387711  PSNR: 27.053857803344727
[TRAIN] Iter: 268400 Loss: 0.0041427817195653915  PSNR: 29.061826705932617
[TRAIN] Iter: 268500 Loss: 0.0059331557713449  PSNR: 27.090837478637695
[TRAIN] Iter: 268600 Loss: 0.00527822133153677  PSNR: 27.21065902709961
[TRAIN] Iter: 268700 Loss: 0.004437739960849285  PSNR: 28.472381591796875
[TRAIN] Iter: 268800 Loss: 0.005999622400850058  PSNR: 26.731834411621094
[TRAIN] Iter: 268900 Loss: 0.005355905741453171  PSNR: 28.048242568969727
[TRAIN] Iter: 269000 Loss: 0.004628545138984919  PSNR: 29.263446807861328
[TRAIN] Iter: 269100 Loss: 0.008332855068147182  PSNR: 25.07430648803711
[TRAIN] Iter: 269200 Loss: 0.004788280464708805  PSNR: 28.750520706176758
[TRAIN] Iter: 269300 Loss: 0.005315751768648624  PSNR: 27.46299934387207
[TRAIN] Iter: 269400 Loss: 0.0036865274887531996  PSNR: 29.576311111450195
[TRAIN] Iter: 269500 Loss: 0.005213289987295866  PSNR: 28.422454833984375
[TRAIN] Iter: 269600 Loss: 0.005096157547086477  PSNR: 27.15198516845703
[TRAIN] Iter: 269700 Loss: 0.006270864047110081  PSNR: 26.648408889770508
[TRAIN] Iter: 269800 Loss: 0.006223318632692099  PSNR: 27.456100463867188
[TRAIN] Iter: 269900 Loss: 0.005610969383269548  PSNR: 26.876949310302734
Saved checkpoints at ./logs/TUT-KE101-nerf/270000.tar
[TRAIN] Iter: 270000 Loss: 0.005954527761787176  PSNR: 27.291818618774414
[TRAIN] Iter: 270100 Loss: 0.0054813651368021965  PSNR: 26.94340705871582
[TRAIN] Iter: 270200 Loss: 0.005387431941926479  PSNR: 27.4511775970459
[TRAIN] Iter: 270300 Loss: 0.005003821104764938  PSNR: 27.281513214111328
[TRAIN] Iter: 270400 Loss: 0.00585048645734787  PSNR: 26.868452072143555
[TRAIN] Iter: 270500 Loss: 0.004973161034286022  PSNR: 27.975526809692383
[TRAIN] Iter: 270600 Loss: 0.005482678301632404  PSNR: 27.510759353637695
[TRAIN] Iter: 270700 Loss: 0.006094709504395723  PSNR: 27.213491439819336
[TRAIN] Iter: 270800 Loss: 0.005983926355838776  PSNR: 27.582321166992188
[TRAIN] Iter: 270900 Loss: 0.006606747396290302  PSNR: 27.277362823486328
[TRAIN] Iter: 271000 Loss: 0.00479149492457509  PSNR: 27.929513931274414
[TRAIN] Iter: 271100 Loss: 0.004231671337038279  PSNR: 29.01388168334961
[TRAIN] Iter: 271200 Loss: 0.005553172901272774  PSNR: 27.3890380859375
[TRAIN] Iter: 271300 Loss: 0.004844526760280132  PSNR: 27.67816162109375
[TRAIN] Iter: 271400 Loss: 0.006173650734126568  PSNR: 26.58797264099121
[TRAIN] Iter: 271500 Loss: 0.0049676368944346905  PSNR: 27.81437873840332
[TRAIN] Iter: 271600 Loss: 0.00599070405587554  PSNR: 26.864587783813477
[TRAIN] Iter: 271700 Loss: 0.005603932775557041  PSNR: 27.05440330505371
[TRAIN] Iter: 271800 Loss: 0.004481565672904253  PSNR: 29.79924774169922
[TRAIN] Iter: 271900 Loss: 0.00610787607729435  PSNR: 27.22796630859375
[TRAIN] Iter: 272000 Loss: 0.004571183584630489  PSNR: 28.709442138671875
[TRAIN] Iter: 272100 Loss: 0.005093475338071585  PSNR: 27.35854148864746
[TRAIN] Iter: 272200 Loss: 0.0064191012643277645  PSNR: 27.092252731323242
[TRAIN] Iter: 272300 Loss: 0.004799237474799156  PSNR: 28.532337188720703
[TRAIN] Iter: 272400 Loss: 0.0034586384426802397  PSNR: 29.852781295776367
[TRAIN] Iter: 272500 Loss: 0.0038929872680455446  PSNR: 28.808361053466797
[TRAIN] Iter: 272600 Loss: 0.005043935962021351  PSNR: 29.0906982421875
[TRAIN] Iter: 272700 Loss: 0.005921565927565098  PSNR: 26.563318252563477
[TRAIN] Iter: 272800 Loss: 0.006074022967368364  PSNR: 26.44222640991211
[TRAIN] Iter: 272900 Loss: 0.005824554245918989  PSNR: 27.06981658935547
[TRAIN] Iter: 273000 Loss: 0.004255940672010183  PSNR: 29.49247932434082
[TRAIN] Iter: 273100 Loss: 0.004461003001779318  PSNR: 28.212339401245117
[TRAIN] Iter: 273200 Loss: 0.00429436843842268  PSNR: 29.461393356323242
[TRAIN] Iter: 273300 Loss: 0.0056709717027843  PSNR: 26.952272415161133
[TRAIN] Iter: 273400 Loss: 0.006644861772656441  PSNR: 26.65310287475586
[TRAIN] Iter: 273500 Loss: 0.004707933869212866  PSNR: 27.7047119140625
[TRAIN] Iter: 273600 Loss: 0.005113933701068163  PSNR: 27.43077850341797
[TRAIN] Iter: 273700 Loss: 0.005710540805011988  PSNR: 27.148563385009766
[TRAIN] Iter: 273800 Loss: 0.006000176537781954  PSNR: 27.073104858398438
[TRAIN] Iter: 273900 Loss: 0.0051621198654174805  PSNR: 27.956003189086914
[TRAIN] Iter: 274000 Loss: 0.005487668793648481  PSNR: 27.868606567382812
[TRAIN] Iter: 274100 Loss: 0.004386101383715868  PSNR: 29.250404357910156
[TRAIN] Iter: 274200 Loss: 0.006490698084235191  PSNR: 26.046178817749023
[TRAIN] Iter: 274300 Loss: 0.005758876912295818  PSNR: 28.103370666503906
[TRAIN] Iter: 274400 Loss: 0.0063529605977237225  PSNR: 27.457242965698242
[TRAIN] Iter: 274500 Loss: 0.004794880282133818  PSNR: 27.686677932739258
[TRAIN] Iter: 274600 Loss: 0.004675297066569328  PSNR: 28.967138290405273
[TRAIN] Iter: 274700 Loss: 0.003648750251159072  PSNR: 29.720623016357422
[TRAIN] Iter: 274800 Loss: 0.0067318156361579895  PSNR: 26.331819534301758
[TRAIN] Iter: 274900 Loss: 0.006574216298758984  PSNR: 26.560155868530273
[TRAIN] Iter: 275000 Loss: 0.004914304707199335  PSNR: 28.313583374023438
[TRAIN] Iter: 275100 Loss: 0.0039046246092766523  PSNR: 29.654407501220703
[TRAIN] Iter: 275200 Loss: 0.005324209108948708  PSNR: 27.162107467651367
[TRAIN] Iter: 275300 Loss: 0.005365781486034393  PSNR: 26.907194137573242
[TRAIN] Iter: 275400 Loss: 0.0037656587082892656  PSNR: 29.694059371948242
[TRAIN] Iter: 275500 Loss: 0.004105068277567625  PSNR: 29.03818130493164
[TRAIN] Iter: 275600 Loss: 0.005779891740530729  PSNR: 26.733186721801758
[TRAIN] Iter: 275700 Loss: 0.003912706393748522  PSNR: 29.775798797607422
[TRAIN] Iter: 275800 Loss: 0.005809183232486248  PSNR: 26.887699127197266
[TRAIN] Iter: 275900 Loss: 0.006686449982225895  PSNR: 26.497089385986328
[TRAIN] Iter: 276000 Loss: 0.005482326727360487  PSNR: 27.686084747314453
[TRAIN] Iter: 276100 Loss: 0.004639612045139074  PSNR: 28.56433868408203
[TRAIN] Iter: 276200 Loss: 0.005271537695080042  PSNR: 27.187292098999023
[TRAIN] Iter: 276300 Loss: 0.004308181349188089  PSNR: 29.361003875732422
[TRAIN] Iter: 276400 Loss: 0.004307168535888195  PSNR: 29.11295509338379
[TRAIN] Iter: 276500 Loss: 0.0057611423544585705  PSNR: 27.070735931396484
[TRAIN] Iter: 276600 Loss: 0.005548939108848572  PSNR: 27.50840187072754
[TRAIN] Iter: 276700 Loss: 0.005140103865414858  PSNR: 27.875669479370117
[TRAIN] Iter: 276800 Loss: 0.005732306744903326  PSNR: 26.6943302154541
[TRAIN] Iter: 276900 Loss: 0.006202877499163151  PSNR: 27.10173797607422
[TRAIN] Iter: 277000 Loss: 0.004617948085069656  PSNR: 28.261459350585938
[TRAIN] Iter: 277100 Loss: 0.005355441477149725  PSNR: 27.366943359375
[TRAIN] Iter: 277200 Loss: 0.004797779023647308  PSNR: 29.359149932861328
[TRAIN] Iter: 277300 Loss: 0.005444871261715889  PSNR: 26.915374755859375
[TRAIN] Iter: 277400 Loss: 0.0055215563625097275  PSNR: 27.382293701171875
[TRAIN] Iter: 277500 Loss: 0.00494287209585309  PSNR: 28.131214141845703
[TRAIN] Iter: 277600 Loss: 0.003976999316364527  PSNR: 29.45638656616211
[TRAIN] Iter: 277700 Loss: 0.004087082576006651  PSNR: 28.570505142211914
[TRAIN] Iter: 277800 Loss: 0.0061118146404623985  PSNR: 26.398164749145508
[TRAIN] Iter: 277900 Loss: 0.004887212999165058  PSNR: 28.02985954284668
[TRAIN] Iter: 278000 Loss: 0.004044559318572283  PSNR: 30.026748657226562
[TRAIN] Iter: 278100 Loss: 0.004936136305332184  PSNR: 27.19439697265625
[TRAIN] Iter: 278200 Loss: 0.0055165598168969154  PSNR: 27.855684280395508
[TRAIN] Iter: 278300 Loss: 0.004347650799900293  PSNR: 28.08465576171875
[TRAIN] Iter: 278400 Loss: 0.005431652069091797  PSNR: 27.7822322845459
[TRAIN] Iter: 278500 Loss: 0.006009341217577457  PSNR: 26.813432693481445
[TRAIN] Iter: 278600 Loss: 0.005130859091877937  PSNR: 27.644527435302734
[TRAIN] Iter: 278700 Loss: 0.005635393783450127  PSNR: 27.23672866821289
[TRAIN] Iter: 278800 Loss: 0.0058951182290911674  PSNR: 26.566213607788086
[TRAIN] Iter: 278900 Loss: 0.005417399108409882  PSNR: 27.708688735961914
[TRAIN] Iter: 279000 Loss: 0.004018752835690975  PSNR: 29.116565704345703
[TRAIN] Iter: 279100 Loss: 0.0047182561829686165  PSNR: 28.471281051635742
[TRAIN] Iter: 279200 Loss: 0.005606185644865036  PSNR: 27.046777725219727
[TRAIN] Iter: 279300 Loss: 0.004240648355334997  PSNR: 29.40643882751465
[TRAIN] Iter: 279400 Loss: 0.004407130181789398  PSNR: 29.494524002075195
[TRAIN] Iter: 279500 Loss: 0.003714237827807665  PSNR: 29.772056579589844
[TRAIN] Iter: 279600 Loss: 0.004586382769048214  PSNR: 28.535091400146484
[TRAIN] Iter: 279700 Loss: 0.006042190361768007  PSNR: 26.22208595275879
[TRAIN] Iter: 279800 Loss: 0.0047294506803154945  PSNR: 27.600215911865234
[TRAIN] Iter: 279900 Loss: 0.004364462569355965  PSNR: 28.303728103637695
Saved checkpoints at ./logs/TUT-KE101-nerf/280000.tar
[TRAIN] Iter: 280000 Loss: 0.006143324542790651  PSNR: 26.654067993164062
[TRAIN] Iter: 280100 Loss: 0.007088164798915386  PSNR: 26.374223709106445
[TRAIN] Iter: 280200 Loss: 0.004792564548552036  PSNR: 28.49538230895996
[TRAIN] Iter: 280300 Loss: 0.004500355571508408  PSNR: 29.089439392089844
[TRAIN] Iter: 280400 Loss: 0.0048190392553806305  PSNR: 28.03771209716797
[TRAIN] Iter: 280500 Loss: 0.0048440247774124146  PSNR: 28.216238021850586
[TRAIN] Iter: 280600 Loss: 0.006423530168831348  PSNR: 26.5950870513916
[TRAIN] Iter: 280700 Loss: 0.005227173678576946  PSNR: 27.38589859008789
[TRAIN] Iter: 280800 Loss: 0.00528133288025856  PSNR: 27.71982192993164
[TRAIN] Iter: 280900 Loss: 0.004790899343788624  PSNR: 27.955665588378906
[TRAIN] Iter: 281000 Loss: 0.006178534124046564  PSNR: 26.73004913330078
[TRAIN] Iter: 281100 Loss: 0.006362167652696371  PSNR: 27.330442428588867
[TRAIN] Iter: 281200 Loss: 0.003786266315728426  PSNR: 29.71021842956543
[TRAIN] Iter: 281300 Loss: 0.004981440491974354  PSNR: 27.433086395263672
[TRAIN] Iter: 281400 Loss: 0.004694407805800438  PSNR: 28.26693344116211
[TRAIN] Iter: 281500 Loss: 0.005723792128264904  PSNR: 26.909713745117188
[TRAIN] Iter: 281600 Loss: 0.004996499512344599  PSNR: 29.012039184570312
[TRAIN] Iter: 281700 Loss: 0.004237926099449396  PSNR: 29.25538444519043
[TRAIN] Iter: 281800 Loss: 0.003935891203582287  PSNR: 29.134876251220703
[TRAIN] Iter: 281900 Loss: 0.00369720789603889  PSNR: 29.934947967529297
[TRAIN] Iter: 282000 Loss: 0.004146842285990715  PSNR: 29.958532333374023
[TRAIN] Iter: 282100 Loss: 0.004392404109239578  PSNR: 29.0670108795166
[TRAIN] Iter: 282200 Loss: 0.0055947257205843925  PSNR: 27.21583366394043
[TRAIN] Iter: 282300 Loss: 0.004190679639577866  PSNR: 28.485048294067383
[TRAIN] Iter: 282400 Loss: 0.00516844354569912  PSNR: 27.701906204223633
[TRAIN] Iter: 282500 Loss: 0.005865426734089851  PSNR: 26.423856735229492
[TRAIN] Iter: 282600 Loss: 0.004548306576907635  PSNR: 27.781435012817383
[TRAIN] Iter: 282700 Loss: 0.00499314721673727  PSNR: 27.703842163085938
[TRAIN] Iter: 282800 Loss: 0.004713218659162521  PSNR: 28.723613739013672
[TRAIN] Iter: 282900 Loss: 0.004166430793702602  PSNR: 29.801082611083984
[TRAIN] Iter: 283000 Loss: 0.005319774616509676  PSNR: 27.303197860717773
[TRAIN] Iter: 283100 Loss: 0.004435020033270121  PSNR: 28.49077033996582
[TRAIN] Iter: 283200 Loss: 0.005855858325958252  PSNR: 27.015459060668945
[TRAIN] Iter: 283300 Loss: 0.004482697695493698  PSNR: 28.85472869873047
[TRAIN] Iter: 283400 Loss: 0.006273510865867138  PSNR: 27.032058715820312
[TRAIN] Iter: 283500 Loss: 0.005088837817311287  PSNR: 27.818845748901367
[TRAIN] Iter: 283600 Loss: 0.004328272305428982  PSNR: 28.709787368774414
[TRAIN] Iter: 283700 Loss: 0.0043144430965185165  PSNR: 29.291311264038086
[TRAIN] Iter: 283800 Loss: 0.005423839204013348  PSNR: 27.625015258789062
[TRAIN] Iter: 283900 Loss: 0.004956824239343405  PSNR: 27.820194244384766
[TRAIN] Iter: 284000 Loss: 0.00406889570876956  PSNR: 29.437280654907227
[TRAIN] Iter: 284100 Loss: 0.006229728925973177  PSNR: 26.662986755371094
[TRAIN] Iter: 284200 Loss: 0.005724722519516945  PSNR: 27.1605167388916
[TRAIN] Iter: 284300 Loss: 0.006001295987516642  PSNR: 26.96236228942871
[TRAIN] Iter: 284400 Loss: 0.005503897089511156  PSNR: 26.865922927856445
[TRAIN] Iter: 284500 Loss: 0.004070743452757597  PSNR: 29.888212203979492
[TRAIN] Iter: 284600 Loss: 0.005254444666206837  PSNR: 27.838245391845703
[TRAIN] Iter: 284700 Loss: 0.005643220618367195  PSNR: 26.537588119506836
[TRAIN] Iter: 284800 Loss: 0.005101033486425877  PSNR: 27.78651237487793
[TRAIN] Iter: 284900 Loss: 0.004213808104395866  PSNR: 29.064701080322266
[TRAIN] Iter: 285000 Loss: 0.0038306755013763905  PSNR: 29.346033096313477
[TRAIN] Iter: 285100 Loss: 0.004058618564158678  PSNR: 29.695537567138672
[TRAIN] Iter: 285200 Loss: 0.0036979676224291325  PSNR: 29.70306968688965
[TRAIN] Iter: 285300 Loss: 0.00541536184027791  PSNR: 27.899629592895508
[TRAIN] Iter: 285400 Loss: 0.0047467369586229324  PSNR: 27.522472381591797
[TRAIN] Iter: 285500 Loss: 0.004961973987519741  PSNR: 27.82765007019043
[TRAIN] Iter: 285600 Loss: 0.005226865876466036  PSNR: 27.74639320373535
[TRAIN] Iter: 285700 Loss: 0.004996207542717457  PSNR: 28.047367095947266
[TRAIN] Iter: 285800 Loss: 0.005088036879897118  PSNR: 27.32766342163086
[TRAIN] Iter: 285900 Loss: 0.006048423238098621  PSNR: 26.697208404541016
[TRAIN] Iter: 286000 Loss: 0.0044063604436814785  PSNR: 27.757949829101562
[TRAIN] Iter: 286100 Loss: 0.006185354199260473  PSNR: 26.991369247436523
[TRAIN] Iter: 286200 Loss: 0.0054349652491509914  PSNR: 27.303953170776367
[TRAIN] Iter: 286300 Loss: 0.004846247844398022  PSNR: 27.92072296142578
[TRAIN] Iter: 286400 Loss: 0.004721463192254305  PSNR: 27.841266632080078
[TRAIN] Iter: 286500 Loss: 0.004826581571251154  PSNR: 27.58461570739746
[TRAIN] Iter: 286600 Loss: 0.004986759275197983  PSNR: 27.70921516418457
[TRAIN] Iter: 286700 Loss: 0.004237603861838579  PSNR: 29.199766159057617
[TRAIN] Iter: 286800 Loss: 0.004365227185189724  PSNR: 29.11627769470215
[TRAIN] Iter: 286900 Loss: 0.005833650939166546  PSNR: 27.153629302978516
[TRAIN] Iter: 287000 Loss: 0.004098355770111084  PSNR: 29.06092071533203
[TRAIN] Iter: 287100 Loss: 0.006023522466421127  PSNR: 27.22194480895996
[TRAIN] Iter: 287200 Loss: 0.005416583269834518  PSNR: 27.4454288482666
[TRAIN] Iter: 287300 Loss: 0.003921849653124809  PSNR: 29.182546615600586
[TRAIN] Iter: 287400 Loss: 0.005042306147515774  PSNR: 27.398351669311523
[TRAIN] Iter: 287500 Loss: 0.005236785858869553  PSNR: 27.697233200073242
[TRAIN] Iter: 287600 Loss: 0.0047250972129404545  PSNR: 28.633323669433594
[TRAIN] Iter: 287700 Loss: 0.004729696083813906  PSNR: 27.880783081054688
[TRAIN] Iter: 287800 Loss: 0.0036864164285361767  PSNR: 29.60053253173828
[TRAIN] Iter: 287900 Loss: 0.004618552979081869  PSNR: 29.39406394958496
[TRAIN] Iter: 288000 Loss: 0.004262523725628853  PSNR: 29.339012145996094
[TRAIN] Iter: 288100 Loss: 0.004099740646779537  PSNR: 29.370569229125977
[TRAIN] Iter: 288200 Loss: 0.00406945776194334  PSNR: 29.063562393188477
[TRAIN] Iter: 288300 Loss: 0.0066490862518548965  PSNR: 26.53564453125
[TRAIN] Iter: 288400 Loss: 0.004424885846674442  PSNR: 29.248506546020508
[TRAIN] Iter: 288500 Loss: 0.005440630950033665  PSNR: 27.932817459106445
[TRAIN] Iter: 288600 Loss: 0.004329180344939232  PSNR: 27.758108139038086
[TRAIN] Iter: 288700 Loss: 0.005552018526941538  PSNR: 27.687219619750977
[TRAIN] Iter: 288800 Loss: 0.005607495550066233  PSNR: 27.01187515258789
[TRAIN] Iter: 288900 Loss: 0.0048775202594697475  PSNR: 27.65007781982422
[TRAIN] Iter: 289000 Loss: 0.004816724918782711  PSNR: 27.500764846801758
[TRAIN] Iter: 289100 Loss: 0.0036020835395902395  PSNR: 29.755231857299805
[TRAIN] Iter: 289200 Loss: 0.005168285220861435  PSNR: 27.49578285217285
[TRAIN] Iter: 289300 Loss: 0.004095318727195263  PSNR: 29.044782638549805
[TRAIN] Iter: 289400 Loss: 0.0057677291333675385  PSNR: 27.119613647460938
[TRAIN] Iter: 289500 Loss: 0.006143556907773018  PSNR: 27.370878219604492
[TRAIN] Iter: 289600 Loss: 0.004908502567559481  PSNR: 28.91489028930664
[TRAIN] Iter: 289700 Loss: 0.005188719369471073  PSNR: 27.698183059692383
[TRAIN] Iter: 289800 Loss: 0.00570138031616807  PSNR: 27.40363121032715
[TRAIN] Iter: 289900 Loss: 0.005960706155747175  PSNR: 26.169475555419922
Saved checkpoints at ./logs/TUT-KE101-nerf/290000.tar
[TRAIN] Iter: 290000 Loss: 0.0041471803560853004  PSNR: 29.886137008666992
[TRAIN] Iter: 290100 Loss: 0.005709884688258171  PSNR: 27.154338836669922
[TRAIN] Iter: 290200 Loss: 0.00473930686712265  PSNR: 28.95749282836914
[TRAIN] Iter: 290300 Loss: 0.004680061247199774  PSNR: 28.72394561767578
[TRAIN] Iter: 290400 Loss: 0.005653820466250181  PSNR: 27.893108367919922
[TRAIN] Iter: 290500 Loss: 0.006275573745369911  PSNR: 26.698524475097656
[TRAIN] Iter: 290600 Loss: 0.005760300904512405  PSNR: 26.802242279052734
[TRAIN] Iter: 290700 Loss: 0.00446275295689702  PSNR: 28.68777847290039
[TRAIN] Iter: 290800 Loss: 0.006019764579832554  PSNR: 26.55260467529297
[TRAIN] Iter: 290900 Loss: 0.005917273461818695  PSNR: 27.023681640625
[TRAIN] Iter: 291000 Loss: 0.004378792829811573  PSNR: 29.943449020385742
[TRAIN] Iter: 291100 Loss: 0.005664053373038769  PSNR: 27.354320526123047
[TRAIN] Iter: 291200 Loss: 0.006455768831074238  PSNR: 26.477033615112305
[TRAIN] Iter: 291300 Loss: 0.0031183285173028708  PSNR: 29.91492462158203
[TRAIN] Iter: 291400 Loss: 0.0038525445852428675  PSNR: 28.96426010131836
[TRAIN] Iter: 291500 Loss: 0.004178223200142384  PSNR: 29.13540267944336
[TRAIN] Iter: 291600 Loss: 0.006479454226791859  PSNR: 26.26228904724121
[TRAIN] Iter: 291700 Loss: 0.0050077661871910095  PSNR: 27.264875411987305
[TRAIN] Iter: 291800 Loss: 0.004814829211682081  PSNR: 27.444293975830078
[TRAIN] Iter: 291900 Loss: 0.004655430559068918  PSNR: 28.162858963012695
[TRAIN] Iter: 292000 Loss: 0.004356867168098688  PSNR: 29.156675338745117
[TRAIN] Iter: 292100 Loss: 0.0050872983410954475  PSNR: 26.965259552001953
[TRAIN] Iter: 292200 Loss: 0.004771368578076363  PSNR: 28.024219512939453
[TRAIN] Iter: 292300 Loss: 0.005726316943764687  PSNR: 27.26706314086914
[TRAIN] Iter: 292400 Loss: 0.004089732654392719  PSNR: 29.409448623657227
[TRAIN] Iter: 292500 Loss: 0.003986195661127567  PSNR: 28.995521545410156
[TRAIN] Iter: 292600 Loss: 0.004799173679202795  PSNR: 27.741214752197266
[TRAIN] Iter: 292700 Loss: 0.005516819190233946  PSNR: 27.516357421875
[TRAIN] Iter: 292800 Loss: 0.003936988301575184  PSNR: 29.457399368286133
[TRAIN] Iter: 292900 Loss: 0.004128850065171719  PSNR: 29.136812210083008
[TRAIN] Iter: 293000 Loss: 0.004988042172044516  PSNR: 27.85993766784668
[TRAIN] Iter: 293100 Loss: 0.003975724335759878  PSNR: 29.332082748413086
[TRAIN] Iter: 293200 Loss: 0.0037416659761220217  PSNR: 29.37940216064453
[TRAIN] Iter: 293300 Loss: 0.005211145617067814  PSNR: 28.062562942504883
[TRAIN] Iter: 293400 Loss: 0.005198297090828419  PSNR: 27.351634979248047
[TRAIN] Iter: 293500 Loss: 0.004888857714831829  PSNR: 28.076452255249023
[TRAIN] Iter: 293600 Loss: 0.003403765382245183  PSNR: 30.07483673095703
[TRAIN] Iter: 293700 Loss: 0.006503345910459757  PSNR: 26.477890014648438
[TRAIN] Iter: 293800 Loss: 0.004922587424516678  PSNR: 28.480138778686523
[TRAIN] Iter: 293900 Loss: 0.003951257094740868  PSNR: 29.850025177001953
[TRAIN] Iter: 294000 Loss: 0.0038810400292277336  PSNR: 29.588909149169922
[TRAIN] Iter: 294100 Loss: 0.003702657762914896  PSNR: 30.29159164428711
[TRAIN] Iter: 294200 Loss: 0.00505845807492733  PSNR: 28.4925594329834
[TRAIN] Iter: 294300 Loss: 0.005417392589151859  PSNR: 27.143022537231445
[TRAIN] Iter: 294400 Loss: 0.004288768861442804  PSNR: 28.686492919921875
[TRAIN] Iter: 294500 Loss: 0.005124207586050034  PSNR: 27.171875
[TRAIN] Iter: 294600 Loss: 0.007189142983406782  PSNR: 26.055713653564453
[TRAIN] Iter: 294700 Loss: 0.003396848915144801  PSNR: 29.926227569580078
[TRAIN] Iter: 294800 Loss: 0.005484017543494701  PSNR: 27.736635208129883
[TRAIN] Iter: 294900 Loss: 0.005597114562988281  PSNR: 26.842042922973633
[TRAIN] Iter: 295000 Loss: 0.0050743515603244305  PSNR: 27.48318099975586
[TRAIN] Iter: 295100 Loss: 0.004690962377935648  PSNR: 28.0162353515625
[TRAIN] Iter: 295200 Loss: 0.00575853418558836  PSNR: 26.68834114074707
[TRAIN] Iter: 295300 Loss: 0.005828763358294964  PSNR: 26.934627532958984
[TRAIN] Iter: 295400 Loss: 0.0038011556025594473  PSNR: 28.796998977661133
[TRAIN] Iter: 295500 Loss: 0.0033866798039525747  PSNR: 30.20311164855957
[TRAIN] Iter: 295600 Loss: 0.006613778881728649  PSNR: 26.584232330322266
[TRAIN] Iter: 295700 Loss: 0.004266014322638512  PSNR: 29.477657318115234
[TRAIN] Iter: 295800 Loss: 0.0057868254370987415  PSNR: 27.64292335510254
[TRAIN] Iter: 295900 Loss: 0.00567766185849905  PSNR: 26.938501358032227
[TRAIN] Iter: 296000 Loss: 0.004960431717336178  PSNR: 27.594331741333008
[TRAIN] Iter: 296100 Loss: 0.004324573092162609  PSNR: 29.975997924804688
[TRAIN] Iter: 296200 Loss: 0.004681305028498173  PSNR: 29.160154342651367
[TRAIN] Iter: 296300 Loss: 0.005802586209028959  PSNR: 27.122718811035156
[TRAIN] Iter: 296400 Loss: 0.004601363092660904  PSNR: 29.068897247314453
[TRAIN] Iter: 296500 Loss: 0.004891500808298588  PSNR: 27.601306915283203
[TRAIN] Iter: 296600 Loss: 0.006173158064484596  PSNR: 26.849843978881836
[TRAIN] Iter: 296700 Loss: 0.0046814302913844585  PSNR: 29.14522361755371
[TRAIN] Iter: 296800 Loss: 0.005711082369089127  PSNR: 27.191198348999023
[TRAIN] Iter: 296900 Loss: 0.0039632972329854965  PSNR: 29.011384963989258
[TRAIN] Iter: 297000 Loss: 0.00577320484444499  PSNR: 27.34981918334961
[TRAIN] Iter: 297100 Loss: 0.004625260829925537  PSNR: 28.439313888549805
[TRAIN] Iter: 297200 Loss: 0.007036728318780661  PSNR: 25.965347290039062
[TRAIN] Iter: 297300 Loss: 0.0048488290049135685  PSNR: 28.72047233581543
[TRAIN] Iter: 297400 Loss: 0.005450400523841381  PSNR: 27.348072052001953
[TRAIN] Iter: 297500 Loss: 0.006989000365138054  PSNR: 26.300750732421875
[TRAIN] Iter: 297600 Loss: 0.004324679262936115  PSNR: 29.344594955444336
[TRAIN] Iter: 297700 Loss: 0.004333760589361191  PSNR: 29.614429473876953
[TRAIN] Iter: 297800 Loss: 0.0042634643614292145  PSNR: 28.997432708740234
[TRAIN] Iter: 297900 Loss: 0.003799028228968382  PSNR: 30.07314682006836
[TRAIN] Iter: 298000 Loss: 0.0060661411844193935  PSNR: 27.071935653686523
[TRAIN] Iter: 298100 Loss: 0.005505098029971123  PSNR: 27.341184616088867
[TRAIN] Iter: 298200 Loss: 0.004339098930358887  PSNR: 28.270660400390625
[TRAIN] Iter: 298300 Loss: 0.005939931608736515  PSNR: 27.047216415405273
[TRAIN] Iter: 298400 Loss: 0.00553799606859684  PSNR: 27.260366439819336
[TRAIN] Iter: 298500 Loss: 0.0037356987595558167  PSNR: 29.279470443725586
[TRAIN] Iter: 298600 Loss: 0.0042617423459887505  PSNR: 29.059019088745117
[TRAIN] Iter: 298700 Loss: 0.005717677995562553  PSNR: 27.226951599121094
[TRAIN] Iter: 298800 Loss: 0.0035388607066124678  PSNR: 29.922016143798828
[TRAIN] Iter: 298900 Loss: 0.003805718384683132  PSNR: 28.995210647583008
[TRAIN] Iter: 299000 Loss: 0.004240464884787798  PSNR: 28.675371170043945
[TRAIN] Iter: 299100 Loss: 0.005155513063073158  PSNR: 27.380878448486328
[TRAIN] Iter: 299200 Loss: 0.0042527904734015465  PSNR: 29.11234474182129
[TRAIN] Iter: 299300 Loss: 0.005894322879612446  PSNR: 26.78122901916504
[TRAIN] Iter: 299400 Loss: 0.0058315470814704895  PSNR: 26.734346389770508
[TRAIN] Iter: 299500 Loss: 0.003857711562886834  PSNR: 29.656478881835938
[TRAIN] Iter: 299600 Loss: 0.00424384418874979  PSNR: 27.879484176635742
[TRAIN] Iter: 299700 Loss: 0.005106582771986723  PSNR: 27.56075096130371
[TRAIN] Iter: 299800 Loss: 0.005657875444740057  PSNR: 27.08531951904297
[TRAIN] Iter: 299900 Loss: 0.005070400889962912  PSNR: 27.81781768798828
Saved checkpoints at ./logs/TUT-KE101-nerf/300000.tar
0 0.0003991127014160156
torch.Size([320, 640, 3]) torch.Size([320, 640])
1 12.97891616821289
2 15.125487804412842
3 12.743627309799194
4 12.874449253082275
5 16.64451766014099
6 12.743354797363281
7 15.938889980316162
8 12.865309238433838
9 12.748666286468506
10 16.474878549575806
11 12.924388647079468
12 15.904825925827026
13 12.823855876922607
14 12.75662636756897
15 16.396005868911743
16 12.719696760177612
17 16.216864585876465
18 13.056241512298584
19 13.051686525344849
20 15.81129789352417
21 13.04912519454956
22 15.910707950592041
23 13.027257204055786
24 12.952205657958984
25 15.934080362319946
26 12.968902349472046
27 15.957462549209595
28 12.988502740859985
29 12.951033353805542
30 16.02434492111206
31 12.973121881484985
32 15.963140487670898
33 13.00548243522644
34 12.927386283874512
35 15.991887092590332
36 12.969110012054443
37 15.97292971611023
38 12.958101987838745
39 12.979212522506714
40 16.030264139175415
41 12.995699167251587
42 16.019697427749634
43 13.004688501358032
44 12.91365098953247
45 16.016618490219116
46 13.036290168762207
47 16.048641681671143
48 12.918576955795288
49 15.943415880203247
50 13.100845575332642
51 12.934643745422363
52 15.910328388214111
53 12.81199598312378
54 16.35866093635559
55 12.774189472198486
56 12.855252742767334
57 16.25767946243286
58 12.786320686340332
59 16.301067113876343
60 12.969867944717407
61 12.864589929580688
62 16.103027820587158
63 12.941961526870728
64 16.073253393173218
65 12.980130910873413
66 12.918887615203857
67 16.120675802230835
68 12.924619674682617
69 16.116193294525146
70 12.913785696029663
71 12.815786123275757
72 16.36457371711731
73 12.812903881072998
74 16.168059825897217
75 13.122438907623291
76 12.897640705108643
77 16.018799543380737
78 12.906785488128662
79 16.05631709098816
80 13.152012586593628
81 12.831400394439697
82 15.867581844329834
83 12.909393548965454
84 16.151126623153687
85 13.227108716964722
86 12.885210514068604
87 15.600312232971191
88 12.774695873260498
89 16.330175161361694
90 13.144987344741821
91 12.786515474319458
92 15.88811206817627
93 12.7975594997406
94 16.317527055740356
95 13.124642848968506
96 12.84762191772461
97 16.083736419677734
98 12.854860067367554
99 16.15614080429077
100 12.986270427703857
101 12.734041213989258
102 16.165614366531372
103 12.896369218826294
104 16.15074896812439
105 12.96150016784668
106 13.915412902832031
107 17.744077682495117
108 14.809118032455444
109 17.75127124786377
110 14.650122165679932
111 17.846874952316284
112 14.688498973846436
113 17.859539270401
114 14.708049774169922
115 14.664098262786865
116 17.99847650527954
117 14.563107013702393
118 17.927871704101562
119 14.584859609603882
Done, saving (120, 320, 640, 3) (120, 320, 640)
extras:{'raw': tensor([[[-1.6990e+00, -1.4878e+00, -9.4405e-01, -5.6181e+01],
         [-5.4773e-01, -1.0006e-01,  7.2470e-01, -5.7063e+01],
         [ 6.6906e-01,  7.1142e-01,  6.5886e-01, -4.2783e+00],
         ...,
         [ 1.3662e+00, -3.4882e+00, -1.5694e+01,  1.7902e+02],
         [ 1.7993e+00, -3.1720e+00, -1.5641e+01,  1.7481e+02],
         [ 1.3000e+00, -3.5213e+00, -1.5675e+01,  2.3897e+02]],

        [[ 9.0205e+00,  1.0273e+01,  1.3866e+01, -5.5803e+01],
         [-6.4969e-02,  2.5242e-01,  9.3597e-01, -5.1395e+01],
         [-6.2869e-02,  2.4263e-01,  8.9932e-01, -5.1027e+01],
         ...,
         [ 3.6908e+00,  4.4988e+00,  6.6090e+00,  2.5205e+02],
         [ 3.4762e+00,  3.8246e+00,  5.7771e+00,  2.5185e+02],
         [ 3.8145e+00,  4.2972e+00,  6.0162e+00,  3.0768e+02]],

        [[-1.1723e+00,  1.6335e-01,  2.7803e+00, -6.3209e+01],
         [-1.9792e+00, -1.6825e+00, -1.1290e+00, -2.8763e+01],
         [-8.8432e-01, -9.2194e-01, -1.0276e+00, -3.1881e+00],
         ...,
         [-1.9563e+01, -2.0775e+01, -2.5516e+01, -2.5213e+02],
         [-2.0326e+01, -2.1517e+01, -2.6188e+01, -9.9082e+01],
         [-1.8979e+01, -2.0334e+01, -2.4797e+01, -2.6586e+02]],

        ...,

        [[ 1.5160e+01,  1.7431e+01,  2.2491e+01, -8.0691e+01],
         [-1.3378e-01, -2.7429e-01, -4.7178e-01, -6.1280e+00],
         [-6.9904e-02, -2.4094e-01, -4.7352e-01,  1.5936e+01],
         ...,
         [ 1.4529e+01,  8.1489e+00, -2.3857e+00,  4.9204e+01],
         [ 1.3506e+01,  6.3088e+00, -5.7452e+00,  9.9033e+01],
         [ 1.7110e+01,  9.8058e+00, -8.1192e-01,  7.4201e+01]],

        [[-3.2779e-01,  6.2741e-01,  2.0364e+00, -7.3144e+01],
         [-6.2474e+00, -5.9725e+00, -5.4613e+00, -2.5296e+01],
         [-3.1173e+00, -3.4136e+00, -3.3648e+00, -2.5321e+01],
         ...,
         [-5.9845e+00, -6.8054e+00, -9.0665e+00, -3.6452e+02],
         [-7.1261e+00, -7.6631e+00, -9.8292e+00, -2.5520e+02],
         [-8.1228e+00, -8.9819e+00, -1.1424e+01, -2.8580e+02]],

        [[ 7.3867e-01,  1.6923e+00,  3.2332e+00, -6.3823e+01],
         [-1.1781e+00, -1.4457e+00, -1.9437e+00, -1.4274e+01],
         [-5.1142e-01, -6.8899e-01, -9.9155e-01, -5.7848e+00],
         ...,
         [ 5.1342e-01, -1.6951e-01, -2.0223e+00,  1.8005e+02],
         [ 3.4342e-01, -5.6455e-01, -2.9090e+00,  2.0101e+02],
         [ 7.7821e-01, -3.3070e-01, -3.1482e+00,  2.7067e+02]]],
       grad_fn=<ReshapeAliasBackward0>), 'rgb0': tensor([[0.6342, 0.6396, 0.6260],
        [0.3903, 0.3626, 0.3305],
        [0.2991, 0.2877, 0.2688],
        ...,
        [0.4628, 0.4196, 0.3564],
        [0.1837, 0.2094, 0.2697],
        [0.5888, 0.5898, 0.5861]], grad_fn=<ReshapeAliasBackward0>), 'disp0': tensor([183.6682,  98.3405,  49.3294,  ..., 312.6472,  82.6458,  73.9037],
       grad_fn=<ReshapeAliasBackward0>), 'acc0': tensor([1., 1., 1.,  ..., 1., 1., 1.], grad_fn=<ReshapeAliasBackward0>), 'z_std': tensor([0.0028, 0.0021, 0.0025,  ..., 0.0491, 0.0029, 0.0023])}
0 0.00047135353088378906
torch.Size([320, 640, 3]) torch.Size([320, 640])
1 14.586534976959229
2 18.011981964111328
3 14.606895208358765
4 17.93801188468933
5 14.656063079833984
6 17.875271558761597
7 14.643683910369873
8 14.574434280395508
9 18.062540292739868
10 14.52043890953064
11 18.055643796920776
12 14.569794416427612
13 18.022231340408325
14 14.593841075897217
15 18.01634693145752
16 14.634044170379639
17 14.550092458724976
18 17.941105604171753
19 14.583155393600464
20 18.019439458847046
21 14.564697265625
22 18.066319465637207
23 14.55444622039795
24 18.067173957824707
25 14.495651483535767
26 18.012959241867065
27 14.566736221313477
28 14.576115846633911
29 18.05852437019348
30 14.540095567703247
31 18.126683235168457
32 14.512726068496704
33 18.090938329696655
34 14.535850048065186
35 18.095383167266846
36 14.687644720077515
37 17.76759433746338
38 14.763917207717896
39 14.529140949249268
40 18.149037837982178
41 14.666929483413696
42 17.85986852645874
43 14.628823518753052
44 17.999460697174072
45 14.747324228286743
46 17.648603439331055
47 14.601630210876465
48 18.159878253936768
49 15.329837560653687
50 15.049387693405151
51 17.67347002029419
52 14.609099864959717
53 18.509095430374146
54 14.598562717437744
55 17.69334077835083
56 14.463565826416016
57 18.192695379257202
58 14.783828020095825
59 17.654377460479736
60 14.759671688079834
61 17.822161197662354
62 14.764222860336304
63 14.755971431732178
64 17.74516224861145
65 14.711737394332886
66 17.751864671707153
67 14.58275032043457
68 17.994809865951538
69 14.630594730377197
70 17.892671823501587
71 14.668638706207275
72 17.889422178268433
73 14.764809131622314
74 14.559606552124023
75 17.988017559051514
76 14.528323888778687
77 18.013928413391113
78 14.571370124816895
79 18.04065442085266
80 14.605141162872314
81 17.974501132965088
82 14.639634847640991
83 14.510456323623657
84 18.02298641204834
85 14.550115823745728
86 18.042216300964355
87 14.507720708847046
88 18.046619653701782
89 14.539250135421753
90 18.04530930519104
91 14.575765609741211
92 18.06980013847351
93 14.635883569717407
94 14.519720315933228
95 18.003337144851685
96 14.533429384231567
97 18.083680868148804
98 14.534366607666016
99 18.10164213180542
100 14.577728271484375
101 18.042786359786987
102 14.484129190444946
103 18.04848313331604
104 14.579576969146729
105 14.578552722930908
106 18.167022228240967
107 14.787707090377808
108 14.958713054656982
109 14.570833444595337
110 18.04608154296875
111 14.634704113006592
112 17.922797203063965
113 14.763556718826294
114 14.532725095748901
115 17.971848249435425
116 14.59438443183899
117 18.1122784614563
118 14.690632104873657
119 17.829877138137817
test poses shape torch.Size([4, 3, 4])
0 0.0005726814270019531
torch.Size([320, 640, 3]) torch.Size([320, 640])
1 18.018842220306396
2 14.774084329605103
3 17.780975818634033
Saved test set
[TRAIN] Iter: 300000 Loss: 0.00445700716227293  PSNR: 28.577604293823242
[TRAIN] Iter: 300100 Loss: 0.006263957358896732  PSNR: 26.879907608032227
[TRAIN] Iter: 300200 Loss: 0.00502857007086277  PSNR: 27.93206787109375
[TRAIN] Iter: 300300 Loss: 0.006471262313425541  PSNR: 26.526615142822266
[TRAIN] Iter: 300400 Loss: 0.004454553127288818  PSNR: 28.18074607849121
[TRAIN] Iter: 300500 Loss: 0.0037898821756243706  PSNR: 29.71735954284668
[TRAIN] Iter: 300600 Loss: 0.004915174096822739  PSNR: 27.666540145874023
[TRAIN] Iter: 300700 Loss: 0.004677254240959883  PSNR: 28.399621963500977
[TRAIN] Iter: 300800 Loss: 0.004951281473040581  PSNR: 27.88800621032715
[TRAIN] Iter: 300900 Loss: 0.0035466887056827545  PSNR: 29.768892288208008
[TRAIN] Iter: 301000 Loss: 0.004059990867972374  PSNR: 29.610933303833008
[TRAIN] Iter: 301100 Loss: 0.005943115334957838  PSNR: 27.3826847076416
[TRAIN] Iter: 301200 Loss: 0.005985344760119915  PSNR: 27.251413345336914
[TRAIN] Iter: 301300 Loss: 0.0053205774165689945  PSNR: 27.440073013305664
[TRAIN] Iter: 301400 Loss: 0.005569878965616226  PSNR: 27.286766052246094
[TRAIN] Iter: 301500 Loss: 0.007260845508426428  PSNR: 26.25373077392578
[TRAIN] Iter: 301600 Loss: 0.00595731008797884  PSNR: 27.23042869567871
[TRAIN] Iter: 301700 Loss: 0.005266902036964893  PSNR: 27.784448623657227
[TRAIN] Iter: 301800 Loss: 0.004547540098428726  PSNR: 28.382680892944336
[TRAIN] Iter: 301900 Loss: 0.005377587396651506  PSNR: 26.857940673828125
[TRAIN] Iter: 302000 Loss: 0.005367607809603214  PSNR: 27.720251083374023
[TRAIN] Iter: 302100 Loss: 0.004426978994160891  PSNR: 28.925167083740234
[TRAIN] Iter: 302200 Loss: 0.003808319568634033  PSNR: 29.702239990234375
[TRAIN] Iter: 302300 Loss: 0.005115218460559845  PSNR: 27.703758239746094
[TRAIN] Iter: 302400 Loss: 0.003947783261537552  PSNR: 29.986970901489258
[TRAIN] Iter: 302500 Loss: 0.006046430207788944  PSNR: 26.300495147705078
[TRAIN] Iter: 302600 Loss: 0.005367012228816748  PSNR: 27.923254013061523
[TRAIN] Iter: 302700 Loss: 0.005634728819131851  PSNR: 26.71649742126465
[TRAIN] Iter: 302800 Loss: 0.004651951603591442  PSNR: 28.344310760498047
[TRAIN] Iter: 302900 Loss: 0.0061625465750694275  PSNR: 26.246068954467773
[TRAIN] Iter: 303000 Loss: 0.004225366748869419  PSNR: 29.394323348999023
[TRAIN] Iter: 303100 Loss: 0.005938143003731966  PSNR: 27.02971839904785
[TRAIN] Iter: 303200 Loss: 0.007536041550338268  PSNR: 26.0575008392334
[TRAIN] Iter: 303300 Loss: 0.003995887935161591  PSNR: 29.951461791992188
[TRAIN] Iter: 303400 Loss: 0.005027825012803078  PSNR: 27.678569793701172
[TRAIN] Iter: 303500 Loss: 0.004684248473495245  PSNR: 28.02593231201172
[TRAIN] Iter: 303600 Loss: 0.003999380860477686  PSNR: 30.16720962524414
[TRAIN] Iter: 303700 Loss: 0.004450053907930851  PSNR: 29.060443878173828
[TRAIN] Iter: 303800 Loss: 0.005080460570752621  PSNR: 27.977510452270508
[TRAIN] Iter: 303900 Loss: 0.004646238870918751  PSNR: 28.243349075317383
[TRAIN] Iter: 304000 Loss: 0.0055404650047421455  PSNR: 27.69989013671875
[TRAIN] Iter: 304100 Loss: 0.005155433900654316  PSNR: 27.878616333007812
[TRAIN] Iter: 304200 Loss: 0.0038235485553741455  PSNR: 29.58674430847168
[TRAIN] Iter: 304300 Loss: 0.003726368770003319  PSNR: 29.847288131713867
[TRAIN] Iter: 304400 Loss: 0.004477871581912041  PSNR: 28.409164428710938
[TRAIN] Iter: 304500 Loss: 0.004471227992326021  PSNR: 28.85015869140625
[TRAIN] Iter: 304600 Loss: 0.006249281577765942  PSNR: 26.052078247070312
[TRAIN] Iter: 304700 Loss: 0.004434770438820124  PSNR: 29.43954086303711
[TRAIN] Iter: 304800 Loss: 0.0044849831610918045  PSNR: 28.260814666748047
[TRAIN] Iter: 304900 Loss: 0.005598556250333786  PSNR: 27.053621292114258
[TRAIN] Iter: 305000 Loss: 0.005195082630962133  PSNR: 28.1495361328125
[TRAIN] Iter: 305100 Loss: 0.004005815368145704  PSNR: 28.969772338867188
[TRAIN] Iter: 305200 Loss: 0.005587231367826462  PSNR: 27.56894874572754
[TRAIN] Iter: 305300 Loss: 0.004796541295945644  PSNR: 28.87508201599121
[TRAIN] Iter: 305400 Loss: 0.005174858495593071  PSNR: 27.869232177734375
[TRAIN] Iter: 305500 Loss: 0.0048124599270522594  PSNR: 27.39137077331543
[TRAIN] Iter: 305600 Loss: 0.004102780483663082  PSNR: 29.66887855529785
[TRAIN] Iter: 305700 Loss: 0.00466013140976429  PSNR: 28.87225341796875
[TRAIN] Iter: 305800 Loss: 0.005164417903870344  PSNR: 27.322935104370117
[TRAIN] Iter: 305900 Loss: 0.0041915131732821465  PSNR: 28.96356964111328
[TRAIN] Iter: 306000 Loss: 0.003996811341494322  PSNR: 28.06427001953125
[TRAIN] Iter: 306100 Loss: 0.004303286783397198  PSNR: 28.338645935058594
[TRAIN] Iter: 306200 Loss: 0.004151163622736931  PSNR: 28.865840911865234
[TRAIN] Iter: 306300 Loss: 0.004233446903526783  PSNR: 28.95294761657715
[TRAIN] Iter: 306400 Loss: 0.00456035602837801  PSNR: 28.352880477905273
[TRAIN] Iter: 306500 Loss: 0.005333946086466312  PSNR: 27.771656036376953
[TRAIN] Iter: 306600 Loss: 0.005392297171056271  PSNR: 27.595195770263672
[TRAIN] Iter: 306700 Loss: 0.005443900357931852  PSNR: 26.5419979095459
[TRAIN] Iter: 306800 Loss: 0.004197400528937578  PSNR: 29.24422264099121
[TRAIN] Iter: 306900 Loss: 0.004024820402264595  PSNR: 29.239606857299805
[TRAIN] Iter: 307000 Loss: 0.005883673205971718  PSNR: 27.229188919067383
[TRAIN] Iter: 307100 Loss: 0.0057640052400529385  PSNR: 27.54300880432129
[TRAIN] Iter: 307200 Loss: 0.004512245766818523  PSNR: 27.747356414794922
[TRAIN] Iter: 307300 Loss: 0.005590961314737797  PSNR: 27.28435516357422
[TRAIN] Iter: 307400 Loss: 0.005104110576212406  PSNR: 27.374935150146484
[TRAIN] Iter: 307500 Loss: 0.004137304145842791  PSNR: 29.31574249267578
[TRAIN] Iter: 307600 Loss: 0.005618949420750141  PSNR: 26.8493595123291
[TRAIN] Iter: 307700 Loss: 0.0037362107541412115  PSNR: 29.25703239440918
[TRAIN] Iter: 307800 Loss: 0.005605523474514484  PSNR: 27.017147064208984
[TRAIN] Iter: 307900 Loss: 0.004307847004383802  PSNR: 28.85079574584961
[TRAIN] Iter: 308000 Loss: 0.006633578334003687  PSNR: 26.189847946166992
[TRAIN] Iter: 308100 Loss: 0.0055785574950277805  PSNR: 27.560922622680664
[TRAIN] Iter: 308200 Loss: 0.007179076783359051  PSNR: 26.106678009033203
[TRAIN] Iter: 308300 Loss: 0.00475908350199461  PSNR: 28.142635345458984
[TRAIN] Iter: 308400 Loss: 0.004516248591244221  PSNR: 27.999034881591797
[TRAIN] Iter: 308500 Loss: 0.0047026691026985645  PSNR: 29.226581573486328
[TRAIN] Iter: 308600 Loss: 0.004185871686786413  PSNR: 29.03846549987793
[TRAIN] Iter: 308700 Loss: 0.005799554288387299  PSNR: 26.8448429107666
[TRAIN] Iter: 308800 Loss: 0.006136316806077957  PSNR: 26.71659278869629
[TRAIN] Iter: 308900 Loss: 0.005830568727105856  PSNR: 26.862232208251953
[TRAIN] Iter: 309000 Loss: 0.0052982522174716  PSNR: 27.738567352294922
[TRAIN] Iter: 309100 Loss: 0.006765055935829878  PSNR: 26.252105712890625
[TRAIN] Iter: 309200 Loss: 0.004278961103409529  PSNR: 29.40143585205078
[TRAIN] Iter: 309300 Loss: 0.003794693388044834  PSNR: 30.039752960205078
[TRAIN] Iter: 309400 Loss: 0.0053249504417181015  PSNR: 27.186710357666016
[TRAIN] Iter: 309500 Loss: 0.0037369602359831333  PSNR: 30.094358444213867
[TRAIN] Iter: 309600 Loss: 0.004466202110052109  PSNR: 29.353490829467773
[TRAIN] Iter: 309700 Loss: 0.004204524215310812  PSNR: 29.52016830444336
[TRAIN] Iter: 309800 Loss: 0.004791929852217436  PSNR: 27.792570114135742
[TRAIN] Iter: 309900 Loss: 0.005200190469622612  PSNR: 27.473968505859375
Saved checkpoints at ./logs/TUT-KE101-nerf/310000.tar
[TRAIN] Iter: 310000 Loss: 0.005488712340593338  PSNR: 27.230653762817383
[TRAIN] Iter: 310100 Loss: 0.004223986063152552  PSNR: 29.677078247070312
[TRAIN] Iter: 310200 Loss: 0.005403000861406326  PSNR: 27.352941513061523
[TRAIN] Iter: 310300 Loss: 0.004049723502248526  PSNR: 29.99081802368164
[TRAIN] Iter: 310400 Loss: 0.005523836240172386  PSNR: 27.815475463867188
[TRAIN] Iter: 310500 Loss: 0.00481075793504715  PSNR: 27.712894439697266
[TRAIN] Iter: 310600 Loss: 0.004841776564717293  PSNR: 27.8264102935791
[TRAIN] Iter: 310700 Loss: 0.004888656549155712  PSNR: 27.945056915283203
[TRAIN] Iter: 310800 Loss: 0.0035362066701054573  PSNR: 30.711877822875977
[TRAIN] Iter: 310900 Loss: 0.00398698216304183  PSNR: 29.977108001708984
[TRAIN] Iter: 311000 Loss: 0.004199223592877388  PSNR: 29.570648193359375
[TRAIN] Iter: 311100 Loss: 0.005920437630265951  PSNR: 26.332275390625
[TRAIN] Iter: 311200 Loss: 0.004605125170201063  PSNR: 28.27619171142578
[TRAIN] Iter: 311300 Loss: 0.006041586399078369  PSNR: 26.456403732299805
[TRAIN] Iter: 311400 Loss: 0.00575697235763073  PSNR: 27.0058650970459
[TRAIN] Iter: 311500 Loss: 0.0035355035215616226  PSNR: 30.093364715576172
[TRAIN] Iter: 311600 Loss: 0.005799179431051016  PSNR: 27.431821823120117
[TRAIN] Iter: 311700 Loss: 0.0035400092601776123  PSNR: 29.419158935546875
[TRAIN] Iter: 311800 Loss: 0.004254466854035854  PSNR: 28.330577850341797
[TRAIN] Iter: 311900 Loss: 0.005620947573333979  PSNR: 27.558351516723633
[TRAIN] Iter: 312000 Loss: 0.006130832247436047  PSNR: 26.28611946105957
[TRAIN] Iter: 312100 Loss: 0.006039902567863464  PSNR: 26.330684661865234
[TRAIN] Iter: 312200 Loss: 0.0034315951634198427  PSNR: 29.811851501464844
[TRAIN] Iter: 312300 Loss: 0.004804201424121857  PSNR: 28.109315872192383
[TRAIN] Iter: 312400 Loss: 0.004559164401143789  PSNR: 28.751794815063477
[TRAIN] Iter: 312500 Loss: 0.004673402290791273  PSNR: 28.0826416015625
[TRAIN] Iter: 312600 Loss: 0.004535827785730362  PSNR: 27.920625686645508
[TRAIN] Iter: 312700 Loss: 0.00507752038538456  PSNR: 27.676164627075195
[TRAIN] Iter: 312800 Loss: 0.004578391555696726  PSNR: 28.2274112701416
[TRAIN] Iter: 312900 Loss: 0.0043594264425337315  PSNR: 29.135684967041016
[TRAIN] Iter: 313000 Loss: 0.005532850511372089  PSNR: 26.58173179626465
[TRAIN] Iter: 313100 Loss: 0.004543652758002281  PSNR: 27.668432235717773
[TRAIN] Iter: 313200 Loss: 0.004644423257559538  PSNR: 28.473060607910156
[TRAIN] Iter: 313300 Loss: 0.006070093717426062  PSNR: 26.730653762817383
[TRAIN] Iter: 313400 Loss: 0.004379329737275839  PSNR: 29.20398712158203
[TRAIN] Iter: 313500 Loss: 0.0056464457884430885  PSNR: 27.441255569458008
[TRAIN] Iter: 313600 Loss: 0.004253102000802755  PSNR: 29.364316940307617
[TRAIN] Iter: 313700 Loss: 0.0049829608760774136  PSNR: 27.34486198425293
[TRAIN] Iter: 313800 Loss: 0.0055121080949902534  PSNR: 27.730987548828125
[TRAIN] Iter: 313900 Loss: 0.005621688440442085  PSNR: 27.633718490600586
[TRAIN] Iter: 314000 Loss: 0.0055911787785589695  PSNR: 27.162878036499023
[TRAIN] Iter: 314100 Loss: 0.004559283144772053  PSNR: 28.27849769592285
[TRAIN] Iter: 314200 Loss: 0.005373835563659668  PSNR: 28.16207504272461
[TRAIN] Iter: 314300 Loss: 0.005853095557540655  PSNR: 26.905508041381836
[TRAIN] Iter: 314400 Loss: 0.0051059601828455925  PSNR: 27.284128189086914
[TRAIN] Iter: 314500 Loss: 0.006103523075580597  PSNR: 27.10409927368164
[TRAIN] Iter: 314600 Loss: 0.004599867854267359  PSNR: 28.38591766357422
[TRAIN] Iter: 314700 Loss: 0.003592959139496088  PSNR: 30.207971572875977
[TRAIN] Iter: 314800 Loss: 0.004534061998128891  PSNR: 28.35511016845703
[TRAIN] Iter: 314900 Loss: 0.006062246859073639  PSNR: 26.56674575805664
[TRAIN] Iter: 315000 Loss: 0.0038326445501297712  PSNR: 29.19437599182129
[TRAIN] Iter: 315100 Loss: 0.0053237732499837875  PSNR: 27.70231056213379
[TRAIN] Iter: 315200 Loss: 0.004607324488461018  PSNR: 28.198863983154297
[TRAIN] Iter: 315300 Loss: 0.005933796521276236  PSNR: 27.283796310424805
[TRAIN] Iter: 315400 Loss: 0.005204509478062391  PSNR: 27.552053451538086
[TRAIN] Iter: 315500 Loss: 0.005309977103024721  PSNR: 27.52507781982422
[TRAIN] Iter: 315600 Loss: 0.006054742261767387  PSNR: 25.820512771606445
[TRAIN] Iter: 315700 Loss: 0.0036981196608394384  PSNR: 29.3747501373291
[TRAIN] Iter: 315800 Loss: 0.005546979606151581  PSNR: 27.727588653564453
[TRAIN] Iter: 315900 Loss: 0.0059144580736756325  PSNR: 26.401779174804688
[TRAIN] Iter: 316000 Loss: 0.00554992351680994  PSNR: 27.428524017333984
[TRAIN] Iter: 316100 Loss: 0.004785616882145405  PSNR: 27.663545608520508
[TRAIN] Iter: 316200 Loss: 0.004318922758102417  PSNR: 29.19761848449707
[TRAIN] Iter: 316300 Loss: 0.004376104101538658  PSNR: 28.252779006958008
[TRAIN] Iter: 316400 Loss: 0.00503669073805213  PSNR: 27.582998275756836
[TRAIN] Iter: 316500 Loss: 0.004824602976441383  PSNR: 28.027530670166016
[TRAIN] Iter: 316600 Loss: 0.00556123536080122  PSNR: 26.56785011291504
[TRAIN] Iter: 316700 Loss: 0.005419348366558552  PSNR: 27.52303695678711
[TRAIN] Iter: 316800 Loss: 0.0066568427719175816  PSNR: 26.583194732666016
[TRAIN] Iter: 316900 Loss: 0.006344590336084366  PSNR: 26.416292190551758
[TRAIN] Iter: 317000 Loss: 0.004441916476935148  PSNR: 28.868104934692383
[TRAIN] Iter: 317100 Loss: 0.0038744087796658278  PSNR: 28.97388458251953
[TRAIN] Iter: 317200 Loss: 0.005282229743897915  PSNR: 27.555463790893555
[TRAIN] Iter: 317300 Loss: 0.005793541669845581  PSNR: 26.483774185180664
[TRAIN] Iter: 317400 Loss: 0.005120501387864351  PSNR: 28.070999145507812
[TRAIN] Iter: 317500 Loss: 0.004648993257433176  PSNR: 27.986459732055664
[TRAIN] Iter: 317600 Loss: 0.0041351960971951485  PSNR: 28.972314834594727
[TRAIN] Iter: 317700 Loss: 0.00547531433403492  PSNR: 27.861101150512695
[TRAIN] Iter: 317800 Loss: 0.0037471111863851547  PSNR: 29.733333587646484
[TRAIN] Iter: 317900 Loss: 0.005717935040593147  PSNR: 26.696035385131836
[TRAIN] Iter: 318000 Loss: 0.0038220700807869434  PSNR: 29.668115615844727
[TRAIN] Iter: 318100 Loss: 0.005153513513505459  PSNR: 27.500410079956055
[TRAIN] Iter: 318200 Loss: 0.0035633051302284002  PSNR: 29.697505950927734
[TRAIN] Iter: 318300 Loss: 0.005642775446176529  PSNR: 26.99117660522461
[TRAIN] Iter: 318400 Loss: 0.006193602457642555  PSNR: 26.25985336303711
[TRAIN] Iter: 318500 Loss: 0.006616547703742981  PSNR: 26.355823516845703
[TRAIN] Iter: 318600 Loss: 0.004553772509098053  PSNR: 29.23179054260254
[TRAIN] Iter: 318700 Loss: 0.005168803967535496  PSNR: 27.71929931640625
[TRAIN] Iter: 318800 Loss: 0.003793583484366536  PSNR: 29.874027252197266
[TRAIN] Iter: 318900 Loss: 0.005454621277749538  PSNR: 27.71256446838379
[TRAIN] Iter: 319000 Loss: 0.0055181533098220825  PSNR: 27.460065841674805
[TRAIN] Iter: 319100 Loss: 0.0045233434066176414  PSNR: 29.443904876708984
[TRAIN] Iter: 319200 Loss: 0.004597784020006657  PSNR: 28.502334594726562
[TRAIN] Iter: 319300 Loss: 0.006207664497196674  PSNR: 26.7303409576416
[TRAIN] Iter: 319400 Loss: 0.005425949580967426  PSNR: 26.89646339416504
[TRAIN] Iter: 319500 Loss: 0.005020226817578077  PSNR: 29.105907440185547
[TRAIN] Iter: 319600 Loss: 0.006739236414432526  PSNR: 26.12526512145996
[TRAIN] Iter: 319700 Loss: 0.0061386507004499435  PSNR: 26.133718490600586
[TRAIN] Iter: 319800 Loss: 0.005297635681927204  PSNR: 28.11385154724121
[TRAIN] Iter: 319900 Loss: 0.0042110830545425415  PSNR: 28.562227249145508
Saved checkpoints at ./logs/TUT-KE101-nerf/320000.tar
[TRAIN] Iter: 320000 Loss: 0.00443224236369133  PSNR: 28.38673210144043
[TRAIN] Iter: 320100 Loss: 0.005743292160332203  PSNR: 27.131010055541992
[TRAIN] Iter: 320200 Loss: 0.004051683936268091  PSNR: 29.56514549255371
[TRAIN] Iter: 320300 Loss: 0.00512413214892149  PSNR: 27.1829776763916
[TRAIN] Iter: 320400 Loss: 0.004883754998445511  PSNR: 27.760059356689453
[TRAIN] Iter: 320500 Loss: 0.0043325526639819145  PSNR: 29.16485595703125
[TRAIN] Iter: 320600 Loss: 0.005782921798527241  PSNR: 28.14596176147461
[TRAIN] Iter: 320700 Loss: 0.004148936364799738  PSNR: 28.4976749420166
[TRAIN] Iter: 320800 Loss: 0.006538345944136381  PSNR: 26.754487991333008
[TRAIN] Iter: 320900 Loss: 0.005338267423212528  PSNR: 27.379093170166016
[TRAIN] Iter: 321000 Loss: 0.004249263554811478  PSNR: 28.40750503540039
[TRAIN] Iter: 321100 Loss: 0.004984569735825062  PSNR: 27.552783966064453
[TRAIN] Iter: 321200 Loss: 0.006077160127460957  PSNR: 27.073333740234375
[TRAIN] Iter: 321300 Loss: 0.004290117882192135  PSNR: 28.227802276611328
[TRAIN] Iter: 321400 Loss: 0.004536770284175873  PSNR: 28.084646224975586
[TRAIN] Iter: 321500 Loss: 0.005228943191468716  PSNR: 27.773256301879883
[TRAIN] Iter: 321600 Loss: 0.0047017671167850494  PSNR: 28.134052276611328
[TRAIN] Iter: 321700 Loss: 0.0052442848682403564  PSNR: 27.718379974365234
[TRAIN] Iter: 321800 Loss: 0.005602475721389055  PSNR: 27.083309173583984
[TRAIN] Iter: 321900 Loss: 0.0034830544609576464  PSNR: 30.25108528137207
[TRAIN] Iter: 322000 Loss: 0.0045149666257202625  PSNR: 28.424612045288086
[TRAIN] Iter: 322100 Loss: 0.004540423396974802  PSNR: 28.177623748779297
[TRAIN] Iter: 322200 Loss: 0.006064047571271658  PSNR: 27.255510330200195
[TRAIN] Iter: 322300 Loss: 0.004686462692916393  PSNR: 28.302711486816406
[TRAIN] Iter: 322400 Loss: 0.006023807916790247  PSNR: 26.525949478149414
[TRAIN] Iter: 322500 Loss: 0.003677595406770706  PSNR: 29.816944122314453
[TRAIN] Iter: 322600 Loss: 0.0038927304558455944  PSNR: 29.594839096069336
[TRAIN] Iter: 322700 Loss: 0.005394334904849529  PSNR: 27.5282039642334
[TRAIN] Iter: 322800 Loss: 0.0035439711064100266  PSNR: 30.16607093811035
[TRAIN] Iter: 322900 Loss: 0.003737021703273058  PSNR: 29.786149978637695
[TRAIN] Iter: 323000 Loss: 0.003669019788503647  PSNR: 29.769685745239258
[TRAIN] Iter: 323100 Loss: 0.005245231091976166  PSNR: 27.991798400878906
[TRAIN] Iter: 323200 Loss: 0.005678136833012104  PSNR: 28.135021209716797
[TRAIN] Iter: 323300 Loss: 0.004282963462173939  PSNR: 28.8581600189209
[TRAIN] Iter: 323400 Loss: 0.004146123304963112  PSNR: 28.386192321777344
[TRAIN] Iter: 323500 Loss: 0.006090693641453981  PSNR: 27.441377639770508
[TRAIN] Iter: 323600 Loss: 0.0040542371571063995  PSNR: 28.979488372802734
[TRAIN] Iter: 323700 Loss: 0.006287009920924902  PSNR: 27.219694137573242
[TRAIN] Iter: 323800 Loss: 0.0042975530959665775  PSNR: 28.71870231628418
[TRAIN] Iter: 323900 Loss: 0.003658229485154152  PSNR: 30.524723052978516
[TRAIN] Iter: 324000 Loss: 0.0053249988704919815  PSNR: 27.4326171875
[TRAIN] Iter: 324100 Loss: 0.005227755755186081  PSNR: 27.046001434326172
[TRAIN] Iter: 324200 Loss: 0.005212919786572456  PSNR: 27.49256706237793
[TRAIN] Iter: 324300 Loss: 0.006617176812142134  PSNR: 26.464834213256836
[TRAIN] Iter: 324400 Loss: 0.005743257235735655  PSNR: 27.240942001342773
[TRAIN] Iter: 324500 Loss: 0.005028652958571911  PSNR: 28.04601287841797
[TRAIN] Iter: 324600 Loss: 0.0041257403790950775  PSNR: 29.323421478271484
[TRAIN] Iter: 324700 Loss: 0.004105931613594294  PSNR: 29.293975830078125
[TRAIN] Iter: 324800 Loss: 0.003645886667072773  PSNR: 30.416574478149414
[TRAIN] Iter: 324900 Loss: 0.0043960195034742355  PSNR: 28.696592330932617
[TRAIN] Iter: 325000 Loss: 0.004111219197511673  PSNR: 30.041568756103516
[TRAIN] Iter: 325100 Loss: 0.005643212236464024  PSNR: 26.86116600036621
[TRAIN] Iter: 325200 Loss: 0.0046025426127016544  PSNR: 29.304452896118164
[TRAIN] Iter: 325300 Loss: 0.003709157695993781  PSNR: 29.803743362426758
[TRAIN] Iter: 325400 Loss: 0.005348047707229853  PSNR: 26.939292907714844
[TRAIN] Iter: 325500 Loss: 0.005277211777865887  PSNR: 27.593536376953125
[TRAIN] Iter: 325600 Loss: 0.00534505769610405  PSNR: 28.696447372436523
[TRAIN] Iter: 325700 Loss: 0.004711736924946308  PSNR: 27.962242126464844
[TRAIN] Iter: 325800 Loss: 0.004110661335289478  PSNR: 29.144018173217773
[TRAIN] Iter: 325900 Loss: 0.004116712603718042  PSNR: 29.051799774169922
[TRAIN] Iter: 326000 Loss: 0.004904911853373051  PSNR: 28.310382843017578
[TRAIN] Iter: 326100 Loss: 0.004078819416463375  PSNR: 29.983144760131836
[TRAIN] Iter: 326200 Loss: 0.004661439917981625  PSNR: 27.539674758911133
[TRAIN] Iter: 326300 Loss: 0.005785307846963406  PSNR: 27.209348678588867
[TRAIN] Iter: 326400 Loss: 0.005839521065354347  PSNR: 27.8012752532959
[TRAIN] Iter: 326500 Loss: 0.0038598778191953897  PSNR: 28.880287170410156
[TRAIN] Iter: 326600 Loss: 0.005081038922071457  PSNR: 27.617931365966797
[TRAIN] Iter: 326700 Loss: 0.00670976098626852  PSNR: 26.18791961669922
[TRAIN] Iter: 326800 Loss: 0.004891848191618919  PSNR: 27.914493560791016
[TRAIN] Iter: 326900 Loss: 0.004550800658762455  PSNR: 29.4891357421875
[TRAIN] Iter: 327000 Loss: 0.005727075971662998  PSNR: 26.79488182067871
[TRAIN] Iter: 327100 Loss: 0.0052815997041761875  PSNR: 28.09083366394043
[TRAIN] Iter: 327200 Loss: 0.004108313005417585  PSNR: 29.359594345092773
[TRAIN] Iter: 327300 Loss: 0.004374650306999683  PSNR: 28.87828254699707
[TRAIN] Iter: 327400 Loss: 0.004741596523672342  PSNR: 28.20258140563965
[TRAIN] Iter: 327500 Loss: 0.0043908655643463135  PSNR: 27.79108238220215
[TRAIN] Iter: 327600 Loss: 0.0039786663837730885  PSNR: 28.11113739013672
[TRAIN] Iter: 327700 Loss: 0.005532633978873491  PSNR: 27.415616989135742
[TRAIN] Iter: 327800 Loss: 0.003963518887758255  PSNR: 29.900577545166016
[TRAIN] Iter: 327900 Loss: 0.004083259031176567  PSNR: 28.74631118774414
[TRAIN] Iter: 328000 Loss: 0.005187040660530329  PSNR: 27.792137145996094
[TRAIN] Iter: 328100 Loss: 0.005267081782221794  PSNR: 27.745086669921875
[TRAIN] Iter: 328200 Loss: 0.0046318890526890755  PSNR: 27.757221221923828
[TRAIN] Iter: 328300 Loss: 0.006615749094635248  PSNR: 25.622724533081055
[TRAIN] Iter: 328400 Loss: 0.005274513736367226  PSNR: 26.919254302978516
[TRAIN] Iter: 328500 Loss: 0.005473590921610594  PSNR: 27.20722770690918
[TRAIN] Iter: 328600 Loss: 0.005966379307210445  PSNR: 26.392560958862305
[TRAIN] Iter: 328700 Loss: 0.005090145859867334  PSNR: 27.903553009033203
[TRAIN] Iter: 328800 Loss: 0.003985161893069744  PSNR: 30.697742462158203
[TRAIN] Iter: 328900 Loss: 0.005478859879076481  PSNR: 27.704877853393555
[TRAIN] Iter: 329000 Loss: 0.005576536990702152  PSNR: 27.75408172607422
[TRAIN] Iter: 329100 Loss: 0.005367472767829895  PSNR: 27.829030990600586
[TRAIN] Iter: 329200 Loss: 0.004173886962234974  PSNR: 28.79533576965332
[TRAIN] Iter: 329300 Loss: 0.004248208366334438  PSNR: 28.8105411529541
[TRAIN] Iter: 329400 Loss: 0.0043137334287166595  PSNR: 29.472667694091797
[TRAIN] Iter: 329500 Loss: 0.004244527779519558  PSNR: 28.50105857849121
[TRAIN] Iter: 329600 Loss: 0.004590224474668503  PSNR: 28.215824127197266
[TRAIN] Iter: 329700 Loss: 0.004268739838153124  PSNR: 29.15776252746582
[TRAIN] Iter: 329800 Loss: 0.005292431451380253  PSNR: 27.28497314453125
[TRAIN] Iter: 329900 Loss: 0.006343858316540718  PSNR: 27.12987518310547
Saved checkpoints at ./logs/TUT-KE101-nerf/330000.tar
[TRAIN] Iter: 330000 Loss: 0.004005254711955786  PSNR: 29.424497604370117
[TRAIN] Iter: 330100 Loss: 0.004118424840271473  PSNR: 29.3998966217041
[TRAIN] Iter: 330200 Loss: 0.004996339324861765  PSNR: 27.52022361755371
[TRAIN] Iter: 330300 Loss: 0.004171175882220268  PSNR: 29.22188949584961
[TRAIN] Iter: 330400 Loss: 0.0037033362314105034  PSNR: 30.015134811401367
[TRAIN] Iter: 330500 Loss: 0.0034998066257685423  PSNR: 29.322677612304688
[TRAIN] Iter: 330600 Loss: 0.004670152440667152  PSNR: 28.298831939697266
[TRAIN] Iter: 330700 Loss: 0.004140699747949839  PSNR: 29.414583206176758
[TRAIN] Iter: 330800 Loss: 0.003849770175293088  PSNR: 29.928081512451172
[TRAIN] Iter: 330900 Loss: 0.004197809379547834  PSNR: 28.91689109802246
[TRAIN] Iter: 331000 Loss: 0.005204321816563606  PSNR: 27.577579498291016
[TRAIN] Iter: 331100 Loss: 0.00485818088054657  PSNR: 28.055294036865234
[TRAIN] Iter: 331200 Loss: 0.004841689020395279  PSNR: 27.91123390197754
[TRAIN] Iter: 331300 Loss: 0.00432355422526598  PSNR: 29.1361026763916
[TRAIN] Iter: 331400 Loss: 0.006147218868136406  PSNR: 27.151927947998047
[TRAIN] Iter: 331500 Loss: 0.005384727846831083  PSNR: 27.68122100830078
[TRAIN] Iter: 331600 Loss: 0.005569884553551674  PSNR: 27.550029754638672
[TRAIN] Iter: 331700 Loss: 0.0037003038451075554  PSNR: 30.316940307617188
[TRAIN] Iter: 331800 Loss: 0.005324601195752621  PSNR: 27.5675106048584
[TRAIN] Iter: 331900 Loss: 0.0048489999026060104  PSNR: 27.648176193237305
[TRAIN] Iter: 332000 Loss: 0.0043696388602256775  PSNR: 29.249258041381836
[TRAIN] Iter: 332100 Loss: 0.005288239568471909  PSNR: 27.306228637695312
[TRAIN] Iter: 332200 Loss: 0.0053322408348321915  PSNR: 28.159378051757812
[TRAIN] Iter: 332300 Loss: 0.0046051787212491035  PSNR: 28.282207489013672
[TRAIN] Iter: 332400 Loss: 0.005495614372193813  PSNR: 27.81529426574707
[TRAIN] Iter: 332500 Loss: 0.004430701956152916  PSNR: 28.699176788330078
[TRAIN] Iter: 332600 Loss: 0.006090407259762287  PSNR: 27.56230926513672
[TRAIN] Iter: 332700 Loss: 0.004302440211176872  PSNR: 28.77048110961914
[TRAIN] Iter: 332800 Loss: 0.0053268419578671455  PSNR: 27.491846084594727
[TRAIN] Iter: 332900 Loss: 0.004465863108634949  PSNR: 28.190366744995117
[TRAIN] Iter: 333000 Loss: 0.0036182785406708717  PSNR: 30.30971908569336
[TRAIN] Iter: 333100 Loss: 0.004823262337595224  PSNR: 28.6492862701416
[TRAIN] Iter: 333200 Loss: 0.0045026266016066074  PSNR: 28.6466007232666
[TRAIN] Iter: 333300 Loss: 0.005532467272132635  PSNR: 27.043231964111328
[TRAIN] Iter: 333400 Loss: 0.003966609947383404  PSNR: 29.778358459472656
[TRAIN] Iter: 333500 Loss: 0.0067068603821098804  PSNR: 26.619033813476562
[TRAIN] Iter: 333600 Loss: 0.0037660652305930853  PSNR: 29.764907836914062
[TRAIN] Iter: 333700 Loss: 0.004911258351057768  PSNR: 27.698591232299805
[TRAIN] Iter: 333800 Loss: 0.0037108848337084055  PSNR: 30.74785804748535
[TRAIN] Iter: 333900 Loss: 0.005136803723871708  PSNR: 28.061058044433594
[TRAIN] Iter: 334000 Loss: 0.0052832551300525665  PSNR: 27.437313079833984
[TRAIN] Iter: 334100 Loss: 0.006580133456736803  PSNR: 26.032188415527344
[TRAIN] Iter: 334200 Loss: 0.004430423025041819  PSNR: 28.92181396484375
[TRAIN] Iter: 334300 Loss: 0.003447822295129299  PSNR: 29.83045196533203
[TRAIN] Iter: 334400 Loss: 0.0054701040498912334  PSNR: 26.702590942382812
[TRAIN] Iter: 334500 Loss: 0.0066567822359502316  PSNR: 26.10431480407715
[TRAIN] Iter: 334600 Loss: 0.0044770617969334126  PSNR: 28.20872688293457
[TRAIN] Iter: 334700 Loss: 0.0044978344812989235  PSNR: 28.492090225219727
[TRAIN] Iter: 334800 Loss: 0.005695447325706482  PSNR: 27.481489181518555
[TRAIN] Iter: 334900 Loss: 0.0064529236406087875  PSNR: 27.239492416381836
[TRAIN] Iter: 335000 Loss: 0.004557798616588116  PSNR: 28.067245483398438
[TRAIN] Iter: 335100 Loss: 0.004150650463998318  PSNR: 29.623666763305664
[TRAIN] Iter: 335200 Loss: 0.006249497644603252  PSNR: 26.846467971801758
[TRAIN] Iter: 335300 Loss: 0.004489469807595015  PSNR: 29.79511833190918
[TRAIN] Iter: 335400 Loss: 0.005849434994161129  PSNR: 27.019079208374023
[TRAIN] Iter: 335500 Loss: 0.0038204300217330456  PSNR: 29.9309024810791
[TRAIN] Iter: 335600 Loss: 0.005811197683215141  PSNR: 27.6234188079834
[TRAIN] Iter: 335700 Loss: 0.005741655360907316  PSNR: 27.194686889648438
[TRAIN] Iter: 335800 Loss: 0.004014590289443731  PSNR: 29.811403274536133
[TRAIN] Iter: 335900 Loss: 0.004211605992168188  PSNR: 29.919769287109375
[TRAIN] Iter: 336000 Loss: 0.005271678324788809  PSNR: 27.877012252807617
[TRAIN] Iter: 336100 Loss: 0.0045415861532092094  PSNR: 28.431875228881836
[TRAIN] Iter: 336200 Loss: 0.003945840522646904  PSNR: 29.648841857910156
[TRAIN] Iter: 336300 Loss: 0.004492892883718014  PSNR: 28.16674041748047
[TRAIN] Iter: 336400 Loss: 0.004080735146999359  PSNR: 29.55115509033203
[TRAIN] Iter: 336500 Loss: 0.004285614006221294  PSNR: 28.46455192565918
[TRAIN] Iter: 336600 Loss: 0.00544551108032465  PSNR: 28.018892288208008
[TRAIN] Iter: 336700 Loss: 0.004590798169374466  PSNR: 28.721614837646484
[TRAIN] Iter: 336800 Loss: 0.004339732229709625  PSNR: 28.50571060180664
[TRAIN] Iter: 336900 Loss: 0.004763716831803322  PSNR: 27.60671043395996
[TRAIN] Iter: 337000 Loss: 0.005077090114355087  PSNR: 27.639272689819336
[TRAIN] Iter: 337100 Loss: 0.00506432494148612  PSNR: 28.06402015686035
[TRAIN] Iter: 337200 Loss: 0.004949703346937895  PSNR: 27.82269859313965
[TRAIN] Iter: 337300 Loss: 0.006230555474758148  PSNR: 26.370681762695312
[TRAIN] Iter: 337400 Loss: 0.005211988463997841  PSNR: 27.64769744873047
[TRAIN] Iter: 337500 Loss: 0.004992097616195679  PSNR: 27.726308822631836
[TRAIN] Iter: 337600 Loss: 0.004608099348843098  PSNR: 28.102571487426758
[TRAIN] Iter: 337700 Loss: 0.004606527741998434  PSNR: 29.853490829467773
[TRAIN] Iter: 337800 Loss: 0.004987427033483982  PSNR: 28.011089324951172
[TRAIN] Iter: 337900 Loss: 0.006099318154156208  PSNR: 26.80388641357422
[TRAIN] Iter: 338000 Loss: 0.004896565806120634  PSNR: 27.805145263671875
[TRAIN] Iter: 338100 Loss: 0.0055237095803022385  PSNR: 26.933759689331055
[TRAIN] Iter: 338200 Loss: 0.005606526508927345  PSNR: 27.536108016967773
[TRAIN] Iter: 338300 Loss: 0.005633738823235035  PSNR: 27.067888259887695
[TRAIN] Iter: 338400 Loss: 0.005078714340925217  PSNR: 27.203048706054688
[TRAIN] Iter: 338500 Loss: 0.004209044389426708  PSNR: 28.651065826416016
[TRAIN] Iter: 338600 Loss: 0.0048842341639101505  PSNR: 28.11286735534668
[TRAIN] Iter: 338700 Loss: 0.004661278799176216  PSNR: 28.422374725341797
[TRAIN] Iter: 338800 Loss: 0.003996521234512329  PSNR: 29.49192237854004
[TRAIN] Iter: 338900 Loss: 0.005443636793643236  PSNR: 27.486919403076172
[TRAIN] Iter: 339000 Loss: 0.00461000669747591  PSNR: 28.253677368164062
[TRAIN] Iter: 339100 Loss: 0.005364280194044113  PSNR: 27.51544952392578
[TRAIN] Iter: 339200 Loss: 0.006060104817152023  PSNR: 27.50071907043457
[TRAIN] Iter: 339300 Loss: 0.0051079411059618  PSNR: 28.086896896362305
[TRAIN] Iter: 339400 Loss: 0.005402782000601292  PSNR: 27.76758575439453
[TRAIN] Iter: 339500 Loss: 0.004980381578207016  PSNR: 27.421987533569336
[TRAIN] Iter: 339600 Loss: 0.00499168923124671  PSNR: 28.7803955078125
[TRAIN] Iter: 339700 Loss: 0.003631910774856806  PSNR: 29.575069427490234
[TRAIN] Iter: 339800 Loss: 0.004854614846408367  PSNR: 28.2373046875
[TRAIN] Iter: 339900 Loss: 0.00551875913515687  PSNR: 27.338987350463867
Saved checkpoints at ./logs/TUT-KE101-nerf/340000.tar
[TRAIN] Iter: 340000 Loss: 0.003885064274072647  PSNR: 28.916526794433594
[TRAIN] Iter: 340100 Loss: 0.003877673763781786  PSNR: 29.21955108642578
[TRAIN] Iter: 340200 Loss: 0.00415860814973712  PSNR: 29.030494689941406
[TRAIN] Iter: 340300 Loss: 0.005527046509087086  PSNR: 26.9987850189209
[TRAIN] Iter: 340400 Loss: 0.005520360544323921  PSNR: 27.55100440979004
[TRAIN] Iter: 340500 Loss: 0.004259534180164337  PSNR: 28.90745735168457
[TRAIN] Iter: 340600 Loss: 0.003262151265516877  PSNR: 30.590286254882812
[TRAIN] Iter: 340700 Loss: 0.004496051464229822  PSNR: 28.067546844482422
[TRAIN] Iter: 340800 Loss: 0.004493478685617447  PSNR: 28.536895751953125
[TRAIN] Iter: 340900 Loss: 0.0053106145933270454  PSNR: 27.639514923095703
[TRAIN] Iter: 341000 Loss: 0.0039036732632666826  PSNR: 28.83433723449707
[TRAIN] Iter: 341100 Loss: 0.0062485611997544765  PSNR: 26.523916244506836
[TRAIN] Iter: 341200 Loss: 0.005451998207718134  PSNR: 27.793188095092773
[TRAIN] Iter: 341300 Loss: 0.005561491474509239  PSNR: 27.610462188720703
[TRAIN] Iter: 341400 Loss: 0.004320024512708187  PSNR: 28.486854553222656
[TRAIN] Iter: 341500 Loss: 0.0066595543175935745  PSNR: 26.54166603088379
[TRAIN] Iter: 341600 Loss: 0.004066581837832928  PSNR: 29.832643508911133
[TRAIN] Iter: 341700 Loss: 0.005336180329322815  PSNR: 27.49980354309082
[TRAIN] Iter: 341800 Loss: 0.005582233425229788  PSNR: 27.323305130004883
[TRAIN] Iter: 341900 Loss: 0.003971514757722616  PSNR: 29.105609893798828
[TRAIN] Iter: 342000 Loss: 0.003974223975092173  PSNR: 29.423635482788086
[TRAIN] Iter: 342100 Loss: 0.005985804833471775  PSNR: 26.87266731262207
[TRAIN] Iter: 342200 Loss: 0.004127064254134893  PSNR: 29.312026977539062
[TRAIN] Iter: 342300 Loss: 0.005293995141983032  PSNR: 27.94644546508789
[TRAIN] Iter: 342400 Loss: 0.0038792830891907215  PSNR: 29.214460372924805
[TRAIN] Iter: 342500 Loss: 0.004154804162681103  PSNR: 29.441064834594727
[TRAIN] Iter: 342600 Loss: 0.005950874648988247  PSNR: 27.331188201904297
[TRAIN] Iter: 342700 Loss: 0.0038166248705238104  PSNR: 29.761619567871094
[TRAIN] Iter: 342800 Loss: 0.006218497641384602  PSNR: 26.600801467895508
[TRAIN] Iter: 342900 Loss: 0.004619215615093708  PSNR: 28.415910720825195
[TRAIN] Iter: 343000 Loss: 0.00513109378516674  PSNR: 27.88361167907715
[TRAIN] Iter: 343100 Loss: 0.004278356209397316  PSNR: 28.637432098388672
[TRAIN] Iter: 343200 Loss: 0.004348421469330788  PSNR: 28.353628158569336
[TRAIN] Iter: 343300 Loss: 0.00391541700810194  PSNR: 29.603818893432617
[TRAIN] Iter: 343400 Loss: 0.005507559049874544  PSNR: 27.29546356201172
[TRAIN] Iter: 343500 Loss: 0.0032628607004880905  PSNR: 29.716899871826172
[TRAIN] Iter: 343600 Loss: 0.005248117260634899  PSNR: 28.112356185913086
[TRAIN] Iter: 343700 Loss: 0.005207391455769539  PSNR: 27.1542911529541
[TRAIN] Iter: 343800 Loss: 0.004900118801742792  PSNR: 26.996227264404297
[TRAIN] Iter: 343900 Loss: 0.004521836992353201  PSNR: 28.44480323791504
[TRAIN] Iter: 344000 Loss: 0.004663721192628145  PSNR: 28.319345474243164
[TRAIN] Iter: 344100 Loss: 0.0036091869696974754  PSNR: 29.79785919189453
[TRAIN] Iter: 344200 Loss: 0.0044994475319981575  PSNR: 28.344844818115234
[TRAIN] Iter: 344300 Loss: 0.004038476385176182  PSNR: 29.42420196533203
[TRAIN] Iter: 344400 Loss: 0.004826038144528866  PSNR: 27.535072326660156
[TRAIN] Iter: 344500 Loss: 0.004212653264403343  PSNR: 29.435007095336914
[TRAIN] Iter: 344600 Loss: 0.0036720302887260914  PSNR: 29.386646270751953
[TRAIN] Iter: 344700 Loss: 0.006179326213896275  PSNR: 27.139934539794922
[TRAIN] Iter: 344800 Loss: 0.003932991065084934  PSNR: 29.605104446411133
[TRAIN] Iter: 344900 Loss: 0.00458707008510828  PSNR: 28.849939346313477
[TRAIN] Iter: 345000 Loss: 0.004988967441022396  PSNR: 27.62990951538086
[TRAIN] Iter: 345100 Loss: 0.0040018800646066666  PSNR: 28.696056365966797
[TRAIN] Iter: 345200 Loss: 0.005065165925770998  PSNR: 28.08989906311035
[TRAIN] Iter: 345300 Loss: 0.005288359709084034  PSNR: 27.365638732910156
[TRAIN] Iter: 345400 Loss: 0.003567224368453026  PSNR: 28.635009765625
[TRAIN] Iter: 345500 Loss: 0.004332432989031076  PSNR: 29.089956283569336
[TRAIN] Iter: 345600 Loss: 0.004748010076582432  PSNR: 28.529279708862305
[TRAIN] Iter: 345700 Loss: 0.0045499480329453945  PSNR: 27.908044815063477
[TRAIN] Iter: 345800 Loss: 0.004151714965701103  PSNR: 28.71727752685547
[TRAIN] Iter: 345900 Loss: 0.00437587033957243  PSNR: 28.415376663208008
[TRAIN] Iter: 346000 Loss: 0.004371092654764652  PSNR: 28.779151916503906
[TRAIN] Iter: 346100 Loss: 0.0035963146947324276  PSNR: 29.581298828125
[TRAIN] Iter: 346200 Loss: 0.00475818058475852  PSNR: 28.150650024414062
[TRAIN] Iter: 346300 Loss: 0.003925850614905357  PSNR: 28.691410064697266
[TRAIN] Iter: 346400 Loss: 0.005505512468516827  PSNR: 27.330652236938477
[TRAIN] Iter: 346500 Loss: 0.004647283814847469  PSNR: 27.911977767944336
[TRAIN] Iter: 346600 Loss: 0.004572980105876923  PSNR: 28.305498123168945
[TRAIN] Iter: 346700 Loss: 0.0053297244012355804  PSNR: 27.579601287841797
[TRAIN] Iter: 346800 Loss: 0.004264176823198795  PSNR: 28.603160858154297
[TRAIN] Iter: 346900 Loss: 0.004752445500344038  PSNR: 28.30340003967285
[TRAIN] Iter: 347000 Loss: 0.004972653463482857  PSNR: 28.225114822387695
[TRAIN] Iter: 347100 Loss: 0.005834242329001427  PSNR: 26.763906478881836
[TRAIN] Iter: 347200 Loss: 0.005287747830152512  PSNR: 27.6279296875
[TRAIN] Iter: 347300 Loss: 0.004502099007368088  PSNR: 28.49955177307129
[TRAIN] Iter: 347400 Loss: 0.003629035782068968  PSNR: 29.466964721679688
[TRAIN] Iter: 347500 Loss: 0.005471956916153431  PSNR: 27.55970001220703
[TRAIN] Iter: 347600 Loss: 0.004173309542238712  PSNR: 29.852474212646484
[TRAIN] Iter: 347700 Loss: 0.0054419562220573425  PSNR: 27.340173721313477
[TRAIN] Iter: 347800 Loss: 0.006169147323817015  PSNR: 27.254844665527344
[TRAIN] Iter: 347900 Loss: 0.004825284238904715  PSNR: 27.551292419433594
[TRAIN] Iter: 348000 Loss: 0.005355731584131718  PSNR: 27.669734954833984
[TRAIN] Iter: 348100 Loss: 0.004494950175285339  PSNR: 28.441436767578125
[TRAIN] Iter: 348200 Loss: 0.004278942942619324  PSNR: 28.36315155029297
[TRAIN] Iter: 348300 Loss: 0.005044193472713232  PSNR: 27.828811645507812
[TRAIN] Iter: 348400 Loss: 0.004542812239378691  PSNR: 28.328340530395508
[TRAIN] Iter: 348500 Loss: 0.004384838975965977  PSNR: 27.841270446777344
[TRAIN] Iter: 348600 Loss: 0.004311802797019482  PSNR: 29.182775497436523
[TRAIN] Iter: 348700 Loss: 0.004182947799563408  PSNR: 29.29484748840332
[TRAIN] Iter: 348800 Loss: 0.005331765860319138  PSNR: 27.570096969604492
[TRAIN] Iter: 348900 Loss: 0.003503875806927681  PSNR: 29.795135498046875
[TRAIN] Iter: 349000 Loss: 0.005565363448113203  PSNR: 27.468217849731445
[TRAIN] Iter: 349100 Loss: 0.005908305756747723  PSNR: 26.640323638916016
[TRAIN] Iter: 349200 Loss: 0.005539420992136002  PSNR: 27.046966552734375
[TRAIN] Iter: 349300 Loss: 0.006157365627586842  PSNR: 26.757659912109375
[TRAIN] Iter: 349400 Loss: 0.006024875678122044  PSNR: 27.271268844604492
[TRAIN] Iter: 349500 Loss: 0.004794863983988762  PSNR: 28.10524559020996
[TRAIN] Iter: 349600 Loss: 0.004458378069102764  PSNR: 29.08405876159668
[TRAIN] Iter: 349700 Loss: 0.004993471782654524  PSNR: 27.976516723632812
[TRAIN] Iter: 349800 Loss: 0.004511241335421801  PSNR: 28.298181533813477
[TRAIN] Iter: 349900 Loss: 0.0041541894897818565  PSNR: 29.724096298217773
Saved checkpoints at ./logs/TUT-KE101-nerf/350000.tar
0 0.00034618377685546875
torch.Size([320, 640, 3]) torch.Size([320, 640])
1 12.847030639648438
2 12.799074172973633
3 16.280875205993652
4 12.702049016952515
5 16.22582983970642
6 12.825786352157593
7 12.770791053771973
8 16.277437448501587
9 12.746727705001831
10 16.19872283935547
11 12.808517694473267
12 12.744526863098145
13 16.265995979309082
14 12.79053258895874
15 16.20969033241272
16 12.813441276550293
17 12.761233568191528
18 16.289183139801025
19 12.770653247833252
20 16.18142008781433
21 12.828053712844849
22 12.773788213729858
23 16.29864192008972
24 12.750172853469849
25 16.232173919677734
26 12.80105447769165
27 12.711964845657349
28 16.275012493133545
29 12.754570722579956
30 16.33253526687622
31 12.730218648910522
32 12.730521440505981
33 16.340649604797363
34 12.776793479919434
35 16.299605131149292
36 12.798704862594604
37 12.719242095947266
38 16.253887176513672
39 12.77562141418457
40 16.24464702606201
41 12.966276168823242
42 12.756767511367798
43 16.121582746505737
44 12.889940023422241
45 16.002535581588745
46 13.027387857437134
47 12.752040386199951
48 16.14017391204834
49 12.909810066223145
50 16.032701015472412
51 12.877212047576904
52 12.744524478912354
53 16.28916358947754
54 12.921305894851685
55 16.03241801261902
56 12.825838327407837
57 12.729102611541748
58 16.33025097846985
59 12.931950330734253
60 16.0432550907135
61 12.774868726730347
62 12.782583236694336
63 16.36463975906372
64 12.897827625274658
65 12.72974681854248
66 16.071868896484375
67 12.741553783416748
68 16.404592275619507
69 12.904474258422852
70 12.725499629974365
71 16.143026113510132
72 12.949178457260132
73 16.062235593795776
74 12.978851795196533
75 12.956188201904297
76 15.92480731010437
77 12.93324065208435
78 15.974425792694092
79 12.918753623962402
80 12.953155755996704
81 15.923030853271484
82 12.964781522750854
83 15.928690910339355
84 12.90681505203247
85 12.985193490982056
86 15.963419675827026
87 12.92391562461853
88 15.986119270324707
89 12.917997598648071
90 15.948147296905518
91 12.991850137710571
92 12.99619722366333
93 15.995300769805908
94 12.981993436813354
95 15.939706325531006
96 12.960286378860474
97 12.96146845817566
98 16.00724506378174
99 12.971332550048828
100 15.925390720367432
101 12.961184978485107
102 12.959345817565918
103 15.981877326965332
104 12.948051452636719
105 15.983355522155762
106 12.759829998016357
107 12.757030963897705
108 16.422808170318604
109 12.90141248703003
110 16.040165424346924
111 12.920421600341797
112 12.846039295196533
113 16.095035076141357
114 12.823608636856079
115 16.099517822265625
116 12.898009300231934
117 12.852793216705322
118 16.135896921157837
119 12.805219411849976
Done, saving (120, 320, 640, 3) (120, 320, 640)
extras:{'raw': tensor([[[-7.5273e-01, -2.4803e-01,  6.4324e-01, -6.1128e+01],
         [-9.7844e-02,  3.9403e-01,  1.3529e+00, -6.0453e+01],
         [-1.7145e+00, -1.8421e+00, -2.1574e+00, -1.0973e+01],
         ...,
         [-3.0096e+00, -4.1206e+00, -6.1907e+00,  3.3986e+01],
         [-1.2296e+00, -2.2280e+00, -3.8913e+00,  5.3211e+01],
         [-9.5560e-01, -1.6025e+00, -3.0000e+00,  7.7811e+01]],

        [[-1.9236e+00, -1.0702e+00,  5.2277e-01, -6.9794e+01],
         [-4.8108e-01, -2.5585e-01,  1.6641e-01, -5.7376e+01],
         [-3.5067e-01, -6.5206e-01, -1.3075e+00, -1.4144e+01],
         ...,
         [-5.7509e-01, -2.9023e+00, -7.9039e+00, -1.1817e+02],
         [-2.6321e-01, -2.7238e+00, -8.5307e+00, -1.2690e+02],
         [-4.5069e+00, -6.2889e+00, -9.4046e+00, -4.8327e+01]],

        [[-1.4943e+00, -1.4167e+00, -1.1684e+00, -5.8731e+01],
         [ 3.7609e+00,  3.8856e+00,  4.5943e+00, -2.0274e+01],
         [ 3.1717e+00,  3.1338e+00,  3.1217e+00, -1.1094e+01],
         ...,
         [ 2.0317e+00, -2.0913e+00, -1.3079e+01,  2.3832e+02],
         [ 1.8897e+00, -2.0385e+00, -1.2674e+01,  2.9072e+02],
         [ 1.9335e+00, -2.0735e+00, -1.2974e+01,  2.9015e+02]],

        ...,

        [[ 2.6557e+00,  2.4260e+00,  1.9461e+00, -6.5672e+01],
         [ 3.1426e-01,  5.0225e-01,  2.6240e-01, -4.6583e+01],
         [ 6.1680e-02,  2.0280e-01, -7.2463e-02, -3.6982e+01],
         ...,
         [-7.2500e+00, -5.9267e+00, -6.0592e+00, -5.1588e+01],
         [-7.5684e+00, -6.5128e+00, -7.0422e+00, -9.1359e+01],
         [-6.3111e+00, -5.8263e+00, -6.5894e+00, -4.6982e+01]],

        [[-1.7701e+00, -9.3329e-01,  5.4956e-01, -4.6948e+01],
         [-5.3521e-01,  8.6641e-02,  1.4920e+00, -5.8016e+01],
         [-2.6052e-01,  2.4550e-01,  1.3665e+00, -5.9035e+01],
         ...,
         [-9.7596e+00, -1.0346e+01, -1.1848e+01, -1.8090e+02],
         [-1.1473e+01, -1.1603e+01, -1.2626e+01, -6.7953e+01],
         [-1.0215e+01, -9.5178e+00, -8.9481e+00, -2.0910e+02]],

        [[-5.8086e-01, -5.9540e-01, -1.1329e+00, -7.3782e+01],
         [-7.7976e-03,  1.3108e-03,  7.5016e-02, -4.9628e+01],
         [ 1.4435e-02, -1.2979e-02, -4.6650e-02, -4.3914e+01],
         ...,
         [ 9.3413e-01, -3.4369e+00, -1.3421e+01, -1.6738e+02],
         [ 6.2421e-02, -4.1798e+00, -1.3696e+01, -1.5163e+02],
         [-1.7929e+00, -5.8782e+00, -1.4663e+01, -1.6699e+02]]],
       grad_fn=<ReshapeAliasBackward0>), 'rgb0': tensor([[0.4793, 0.4515, 0.3943],
        [0.4898, 0.4759, 0.4867],
        [0.6018, 0.5914, 0.5566],
        ...,
        [0.7220, 0.7252, 0.7581],
        [0.6577, 0.6829, 0.7746],
        [0.3189, 0.2602, 0.1572]], grad_fn=<ReshapeAliasBackward0>), 'disp0': tensor([ 69.5076,  72.5181,  62.8030,  ...,  95.7655, 173.8693, 458.2746],
       grad_fn=<ReshapeAliasBackward0>), 'acc0': tensor([1., 1., 1.,  ..., 1., 1., 1.], grad_fn=<ReshapeAliasBackward0>), 'z_std': tensor([0.0024, 0.0025, 0.0036,  ..., 0.0022, 0.0020, 0.3066])}
0 0.00048279762268066406
torch.Size([320, 640, 3]) torch.Size([320, 640])
1 14.646574020385742
2 14.56952977180481
3 18.077912092208862
4 14.545022249221802
5 18.140786170959473
6 14.55927300453186
7 17.91742706298828
8 14.6569504737854
9 17.877615451812744
10 14.739588975906372
11 17.80676555633545
12 14.742670059204102
13 14.533705234527588
14 18.121010065078735
15 14.567471265792847
16 17.883997678756714
17 14.608861923217773
18 18.059674739837646
19 14.639518022537231
20 17.80760669708252
21 14.613138914108276
22 18.04281759262085
23 14.76756763458252
24 14.585816860198975
25 17.75135850906372
26 14.535692930221558
27 18.33019709587097
28 14.589622497558594
29 17.79087495803833
30 14.58707571029663
31 18.22537589073181
32 14.756670236587524
33 18.03313708305359
34 15.08496880531311
35 14.511173963546753
36 17.510150909423828
37 14.532724857330322
38 17.887701272964478
39 14.516072511672974
40 18.05911111831665
41 14.50793743133545
42 18.17189884185791
43 14.531250953674316
44 17.812561750411987
45 14.95857310295105
46 17.453949689865112
47 14.963706970214844
48 14.895767211914062
49 17.873273134231567
50 14.985127687454224
51 18.297186374664307
52 14.863242864608765
53 17.986021280288696
54 14.631560802459717
55 17.880393743515015
56 14.725479364395142
57 17.85526704788208
58 14.65169620513916
59 17.753398656845093
60 14.783181190490723
61 14.752879619598389
62 17.820507049560547
63 14.624966621398926
64 17.951520204544067
65 14.469838619232178
66 18.067721843719482
67 14.534607172012329
68 18.069180727005005
69 14.530424356460571
70 18.154690504074097
71 14.451133012771606
72 14.514703035354614
73 18.0132417678833
74 14.54941177368164
75 18.048457145690918
76 14.530797719955444
77 18.071336030960083
78 14.532694578170776
79 18.076825618743896
80 14.510169982910156
81 18.21124529838562
82 16.083334922790527
83 16.17190647125244
84 14.518994331359863
85 15.11358904838562
86 18.712971687316895
87 15.880004167556763
88 15.167003631591797
89 14.855238676071167
90 18.296478986740112
91 14.522836685180664
92 17.875232696533203
93 14.525926113128662
94 17.963287115097046
95 14.670064449310303
96 17.71805453300476
97 14.722434043884277
98 14.583055019378662
99 18.099223613739014
100 14.521466255187988
101 17.924319982528687
102 14.572627544403076
103 18.074115991592407
104 14.513131141662598
105 17.966022729873657
106 14.537920951843262
107 17.893358945846558
108 14.710245370864868
109 17.72022032737732
110 15.021027565002441
111 14.814275979995728
112 18.422041416168213
113 14.995098114013672
114 18.420530080795288
115 14.692513465881348
116 18.00426936149597
117 14.525400876998901
118 17.950016260147095
119 14.50849175453186
test poses shape torch.Size([4, 3, 4])
0 0.0006890296936035156
torch.Size([320, 640, 3]) torch.Size([320, 640])
1 14.27701997756958
2 13.373026609420776
3 17.616145849227905
Saved test set
[TRAIN] Iter: 350000 Loss: 0.004817979875952005  PSNR: 28.233192443847656
[TRAIN] Iter: 350100 Loss: 0.003962497226893902  PSNR: 29.94051742553711
[TRAIN] Iter: 350200 Loss: 0.004869162105023861  PSNR: 27.94232940673828
[TRAIN] Iter: 350300 Loss: 0.004317383747547865  PSNR: 29.138113021850586
[TRAIN] Iter: 350400 Loss: 0.005047366954386234  PSNR: 27.618337631225586
[TRAIN] Iter: 350500 Loss: 0.003773817326873541  PSNR: 29.528230667114258
[TRAIN] Iter: 350600 Loss: 0.0056947506964206696  PSNR: 26.723947525024414
[TRAIN] Iter: 350700 Loss: 0.004143800586462021  PSNR: 28.986501693725586
[TRAIN] Iter: 350800 Loss: 0.0030520507134497166  PSNR: 30.590425491333008
[TRAIN] Iter: 350900 Loss: 0.005062360316514969  PSNR: 28.296531677246094
[TRAIN] Iter: 351000 Loss: 0.004316992126405239  PSNR: 29.201828002929688
[TRAIN] Iter: 351100 Loss: 0.003819535719230771  PSNR: 29.850330352783203
[TRAIN] Iter: 351200 Loss: 0.006112339906394482  PSNR: 27.15489959716797
[TRAIN] Iter: 351300 Loss: 0.0055016037076711655  PSNR: 27.154071807861328
[TRAIN] Iter: 351400 Loss: 0.005679041147232056  PSNR: 27.641923904418945
[TRAIN] Iter: 351500 Loss: 0.004125374369323254  PSNR: 29.193632125854492
[TRAIN] Iter: 351600 Loss: 0.0061158593744039536  PSNR: 26.503686904907227
[TRAIN] Iter: 351700 Loss: 0.005667771212756634  PSNR: 27.22517204284668
[TRAIN] Iter: 351800 Loss: 0.005381189286708832  PSNR: 27.207136154174805
[TRAIN] Iter: 351900 Loss: 0.004526662640273571  PSNR: 28.918060302734375
[TRAIN] Iter: 352000 Loss: 0.00333162909373641  PSNR: 30.207677841186523
[TRAIN] Iter: 352100 Loss: 0.0043199011124670506  PSNR: 28.93967056274414
[TRAIN] Iter: 352200 Loss: 0.005254653282463551  PSNR: 27.57164764404297
[TRAIN] Iter: 352300 Loss: 0.003737678751349449  PSNR: 29.988862991333008
[TRAIN] Iter: 352400 Loss: 0.005470219533890486  PSNR: 27.682476043701172
[TRAIN] Iter: 352500 Loss: 0.004906952381134033  PSNR: 27.469778060913086
[TRAIN] Iter: 352600 Loss: 0.005642581731081009  PSNR: 27.318750381469727
[TRAIN] Iter: 352700 Loss: 0.004325503017753363  PSNR: 29.46660804748535
[TRAIN] Iter: 352800 Loss: 0.006408515386283398  PSNR: 26.715492248535156
[TRAIN] Iter: 352900 Loss: 0.0035997615195810795  PSNR: 30.380752563476562
[TRAIN] Iter: 353000 Loss: 0.005649513099342585  PSNR: 26.958084106445312
[TRAIN] Iter: 353100 Loss: 0.005263795144855976  PSNR: 27.474414825439453
[TRAIN] Iter: 353200 Loss: 0.004593193531036377  PSNR: 28.114900588989258
[TRAIN] Iter: 353300 Loss: 0.004779017996042967  PSNR: 27.347476959228516
[TRAIN] Iter: 353400 Loss: 0.005705174058675766  PSNR: 27.039331436157227
[TRAIN] Iter: 353500 Loss: 0.005230561830103397  PSNR: 28.20543670654297
[TRAIN] Iter: 353600 Loss: 0.004679754376411438  PSNR: 28.009254455566406
[TRAIN] Iter: 353700 Loss: 0.004562702961266041  PSNR: 27.904762268066406
[TRAIN] Iter: 353800 Loss: 0.005010948516428471  PSNR: 28.01253318786621
[TRAIN] Iter: 353900 Loss: 0.004379101563245058  PSNR: 29.426111221313477
[TRAIN] Iter: 354000 Loss: 0.004130254965275526  PSNR: 28.991134643554688
[TRAIN] Iter: 354100 Loss: 0.005015539936721325  PSNR: 27.63796043395996
[TRAIN] Iter: 354200 Loss: 0.005010016728192568  PSNR: 27.745708465576172
[TRAIN] Iter: 354300 Loss: 0.0044037639163434505  PSNR: 29.281448364257812
[TRAIN] Iter: 354400 Loss: 0.00430257897824049  PSNR: 29.538236618041992
[TRAIN] Iter: 354500 Loss: 0.004495783243328333  PSNR: 28.82000732421875
[TRAIN] Iter: 354600 Loss: 0.004306629300117493  PSNR: 28.309696197509766
[TRAIN] Iter: 354700 Loss: 0.005795112811028957  PSNR: 26.9902400970459
[TRAIN] Iter: 354800 Loss: 0.004864636808633804  PSNR: 27.71587562561035
[TRAIN] Iter: 354900 Loss: 0.003879133379086852  PSNR: 30.046497344970703
[TRAIN] Iter: 355000 Loss: 0.005491900257766247  PSNR: 27.486190795898438
[TRAIN] Iter: 355100 Loss: 0.00522132171317935  PSNR: 27.203800201416016
[TRAIN] Iter: 355200 Loss: 0.005390780512243509  PSNR: 27.102834701538086
[TRAIN] Iter: 355300 Loss: 0.005095542874187231  PSNR: 28.3079776763916
[TRAIN] Iter: 355400 Loss: 0.0039017461240291595  PSNR: 28.86347198486328
[TRAIN] Iter: 355500 Loss: 0.006062614265829325  PSNR: 27.360315322875977
[TRAIN] Iter: 355600 Loss: 0.003466374007984996  PSNR: 30.462038040161133
[TRAIN] Iter: 355700 Loss: 0.004655093420296907  PSNR: 28.033437728881836
[TRAIN] Iter: 355800 Loss: 0.0034661279059946537  PSNR: 29.6545467376709
[TRAIN] Iter: 355900 Loss: 0.005925238132476807  PSNR: 27.304025650024414
[TRAIN] Iter: 356000 Loss: 0.0035024997778236866  PSNR: 30.114967346191406
[TRAIN] Iter: 356100 Loss: 0.00416420865803957  PSNR: 28.724939346313477
[TRAIN] Iter: 356200 Loss: 0.0053021712228655815  PSNR: 27.95496940612793
[TRAIN] Iter: 356300 Loss: 0.005025283433496952  PSNR: 28.218097686767578
[TRAIN] Iter: 356400 Loss: 0.0051087443716824055  PSNR: 27.395105361938477
[TRAIN] Iter: 356500 Loss: 0.005109922494739294  PSNR: 28.31580352783203
[TRAIN] Iter: 356600 Loss: 0.005052010994404554  PSNR: 27.738588333129883
[TRAIN] Iter: 356700 Loss: 0.003982714377343655  PSNR: 29.150009155273438
[TRAIN] Iter: 356800 Loss: 0.004297289997339249  PSNR: 28.382471084594727
[TRAIN] Iter: 356900 Loss: 0.005968336947262287  PSNR: 27.227745056152344
[TRAIN] Iter: 357000 Loss: 0.005051140673458576  PSNR: 28.426742553710938
[TRAIN] Iter: 357100 Loss: 0.004938274621963501  PSNR: 27.88139533996582
[TRAIN] Iter: 357200 Loss: 0.0035852990113198757  PSNR: 30.25223159790039
[TRAIN] Iter: 357300 Loss: 0.005567270331084728  PSNR: 27.337987899780273
[TRAIN] Iter: 357400 Loss: 0.005009341519325972  PSNR: 27.640871047973633
[TRAIN] Iter: 357500 Loss: 0.005182142835110426  PSNR: 27.879133224487305
[TRAIN] Iter: 357600 Loss: 0.005296484567224979  PSNR: 27.386920928955078
[TRAIN] Iter: 357700 Loss: 0.004748200066387653  PSNR: 27.748239517211914
[TRAIN] Iter: 357800 Loss: 0.004213252104818821  PSNR: 29.12804412841797
[TRAIN] Iter: 357900 Loss: 0.0038222491275519133  PSNR: 29.534740447998047
[TRAIN] Iter: 358000 Loss: 0.0034332808572798967  PSNR: 29.339515686035156
[TRAIN] Iter: 358100 Loss: 0.005106917582452297  PSNR: 28.25728416442871
[TRAIN] Iter: 358200 Loss: 0.004417859483510256  PSNR: 28.065568923950195
[TRAIN] Iter: 358300 Loss: 0.006295111961662769  PSNR: 26.834884643554688
[TRAIN] Iter: 358400 Loss: 0.005988242104649544  PSNR: 27.221458435058594
[TRAIN] Iter: 358500 Loss: 0.00600470369681716  PSNR: 26.341176986694336
[TRAIN] Iter: 358600 Loss: 0.004568072035908699  PSNR: 28.467514038085938
[TRAIN] Iter: 358700 Loss: 0.005401666276156902  PSNR: 27.729520797729492
[TRAIN] Iter: 358800 Loss: 0.005630397703498602  PSNR: 27.46576499938965
[TRAIN] Iter: 358900 Loss: 0.004096600227057934  PSNR: 28.07524299621582
[TRAIN] Iter: 359000 Loss: 0.0041558146476745605  PSNR: 29.31264305114746
[TRAIN] Iter: 359100 Loss: 0.005387766286730766  PSNR: 27.323461532592773
[TRAIN] Iter: 359200 Loss: 0.004096376709640026  PSNR: 29.555500030517578
[TRAIN] Iter: 359300 Loss: 0.004616408608853817  PSNR: 27.616527557373047
[TRAIN] Iter: 359400 Loss: 0.004384530708193779  PSNR: 28.55291748046875
[TRAIN] Iter: 359500 Loss: 0.005756434518843889  PSNR: 27.23524284362793
[TRAIN] Iter: 359600 Loss: 0.006300191394984722  PSNR: 25.486328125
[TRAIN] Iter: 359700 Loss: 0.0040321946144104  PSNR: 29.473691940307617
[TRAIN] Iter: 359800 Loss: 0.005181531421840191  PSNR: 27.733116149902344
[TRAIN] Iter: 359900 Loss: 0.0062411995604634285  PSNR: 26.921480178833008
Saved checkpoints at ./logs/TUT-KE101-nerf/360000.tar
[TRAIN] Iter: 360000 Loss: 0.0055018700659275055  PSNR: 27.33488655090332
[TRAIN] Iter: 360100 Loss: 0.005136965308338404  PSNR: 27.99619483947754
[TRAIN] Iter: 360200 Loss: 0.004199557472020388  PSNR: 28.575145721435547
[TRAIN] Iter: 360300 Loss: 0.003238128265365958  PSNR: 30.144380569458008
[TRAIN] Iter: 360400 Loss: 0.0062729124911129475  PSNR: 26.42291831970215
[TRAIN] Iter: 360500 Loss: 0.005336361937224865  PSNR: 27.225248336791992
[TRAIN] Iter: 360600 Loss: 0.00528192613273859  PSNR: 27.860816955566406
[TRAIN] Iter: 360700 Loss: 0.004457845352590084  PSNR: 28.937725067138672
[TRAIN] Iter: 360800 Loss: 0.004286876879632473  PSNR: 27.935527801513672
[TRAIN] Iter: 360900 Loss: 0.0035827516112476587  PSNR: 30.48851203918457
[TRAIN] Iter: 361000 Loss: 0.005806241184473038  PSNR: 26.93992042541504
[TRAIN] Iter: 361100 Loss: 0.0051153614185750484  PSNR: 27.265148162841797
[TRAIN] Iter: 361200 Loss: 0.00522231962531805  PSNR: 27.28464126586914
[TRAIN] Iter: 361300 Loss: 0.004602724686264992  PSNR: 28.632102966308594
[TRAIN] Iter: 361400 Loss: 0.005571453832089901  PSNR: 27.569791793823242
[TRAIN] Iter: 361500 Loss: 0.004547500051558018  PSNR: 27.87510108947754
[TRAIN] Iter: 361600 Loss: 0.006043391302227974  PSNR: 26.564828872680664
[TRAIN] Iter: 361700 Loss: 0.0048265838995575905  PSNR: 28.286027908325195
[TRAIN] Iter: 361800 Loss: 0.0050453124567866325  PSNR: 27.23410987854004
[TRAIN] Iter: 361900 Loss: 0.0035988385789096355  PSNR: 29.71092414855957
[TRAIN] Iter: 362000 Loss: 0.005349120125174522  PSNR: 27.8112735748291
[TRAIN] Iter: 362100 Loss: 0.005788259208202362  PSNR: 26.81526756286621
[TRAIN] Iter: 362200 Loss: 0.004311609081923962  PSNR: 29.0981502532959
[TRAIN] Iter: 362300 Loss: 0.00440249964594841  PSNR: 27.776247024536133
[TRAIN] Iter: 362400 Loss: 0.00472101429477334  PSNR: 27.874895095825195
[TRAIN] Iter: 362500 Loss: 0.005496067926287651  PSNR: 27.426799774169922
[TRAIN] Iter: 362600 Loss: 0.005157027393579483  PSNR: 28.070581436157227
[TRAIN] Iter: 362700 Loss: 0.005154208280146122  PSNR: 27.661104202270508
[TRAIN] Iter: 362800 Loss: 0.005266412626951933  PSNR: 27.594444274902344
[TRAIN] Iter: 362900 Loss: 0.003863754216581583  PSNR: 28.655122756958008
[TRAIN] Iter: 363000 Loss: 0.004829966463148594  PSNR: 28.0860652923584
[TRAIN] Iter: 363100 Loss: 0.003643782576546073  PSNR: 29.983436584472656
[TRAIN] Iter: 363200 Loss: 0.004190893843770027  PSNR: 28.729740142822266
[TRAIN] Iter: 363300 Loss: 0.004807356745004654  PSNR: 27.723142623901367
[TRAIN] Iter: 363400 Loss: 0.004633640870451927  PSNR: 28.373498916625977
[TRAIN] Iter: 363500 Loss: 0.004441206343472004  PSNR: 28.02491569519043
[TRAIN] Iter: 363600 Loss: 0.0041191428899765015  PSNR: 29.3922061920166
[TRAIN] Iter: 363700 Loss: 0.005024037789553404  PSNR: 27.343124389648438
[TRAIN] Iter: 363800 Loss: 0.003813730552792549  PSNR: 29.87711524963379
[TRAIN] Iter: 363900 Loss: 0.005117006134241819  PSNR: 27.61373519897461
[TRAIN] Iter: 364000 Loss: 0.00519203394651413  PSNR: 27.740665435791016
[TRAIN] Iter: 364100 Loss: 0.005387080367654562  PSNR: 27.624792098999023
[TRAIN] Iter: 364200 Loss: 0.00541233504191041  PSNR: 27.6165714263916
[TRAIN] Iter: 364300 Loss: 0.004185535479336977  PSNR: 28.936307907104492
[TRAIN] Iter: 364400 Loss: 0.005117873661220074  PSNR: 27.60599708557129
[TRAIN] Iter: 364500 Loss: 0.004055434837937355  PSNR: 29.229188919067383
[TRAIN] Iter: 364600 Loss: 0.004995310213416815  PSNR: 28.069089889526367
[TRAIN] Iter: 364700 Loss: 0.005661823321133852  PSNR: 27.41318130493164
[TRAIN] Iter: 364800 Loss: 0.004731975495815277  PSNR: 29.500944137573242
[TRAIN] Iter: 364900 Loss: 0.004027471877634525  PSNR: 29.05270767211914
[TRAIN] Iter: 365000 Loss: 0.004849426448345184  PSNR: 27.974224090576172
[TRAIN] Iter: 365100 Loss: 0.004345935303717852  PSNR: 29.156890869140625
[TRAIN] Iter: 365200 Loss: 0.005139568820595741  PSNR: 26.988792419433594
[TRAIN] Iter: 365300 Loss: 0.0039546191692352295  PSNR: 29.182947158813477
[TRAIN] Iter: 365400 Loss: 0.004227875731885433  PSNR: 28.822956085205078
[TRAIN] Iter: 365500 Loss: 0.0037516923621296883  PSNR: 29.295679092407227
[TRAIN] Iter: 365600 Loss: 0.0036540371365845203  PSNR: 29.80086326599121
[TRAIN] Iter: 365700 Loss: 0.00337825040332973  PSNR: 30.273408889770508
[TRAIN] Iter: 365800 Loss: 0.00415846798568964  PSNR: 28.27376937866211
[TRAIN] Iter: 365900 Loss: 0.004719750955700874  PSNR: 27.897274017333984
[TRAIN] Iter: 366000 Loss: 0.004825211595743895  PSNR: 28.080896377563477
[TRAIN] Iter: 366100 Loss: 0.003634109627455473  PSNR: 29.54698371887207
[TRAIN] Iter: 366200 Loss: 0.004105954430997372  PSNR: 28.909469604492188
[TRAIN] Iter: 366300 Loss: 0.004216046538203955  PSNR: 29.588424682617188
[TRAIN] Iter: 366400 Loss: 0.0049367728643119335  PSNR: 28.185993194580078
[TRAIN] Iter: 366500 Loss: 0.005067263264209032  PSNR: 29.361886978149414
[TRAIN] Iter: 366600 Loss: 0.004607914946973324  PSNR: 28.000112533569336
[TRAIN] Iter: 366700 Loss: 0.004801018163561821  PSNR: 27.90113067626953
[TRAIN] Iter: 366800 Loss: 0.004160259384661913  PSNR: 28.621679306030273
[TRAIN] Iter: 366900 Loss: 0.0038636033423244953  PSNR: 29.742843627929688
[TRAIN] Iter: 367000 Loss: 0.004172104876488447  PSNR: 29.430030822753906
[TRAIN] Iter: 367100 Loss: 0.004048852249979973  PSNR: 29.470643997192383
[TRAIN] Iter: 367200 Loss: 0.004807800054550171  PSNR: 27.723440170288086
[TRAIN] Iter: 367300 Loss: 0.0038468181155622005  PSNR: 28.41213607788086
[TRAIN] Iter: 367400 Loss: 0.004495465196669102  PSNR: 28.495607376098633
[TRAIN] Iter: 367500 Loss: 0.004571996163576841  PSNR: 28.791061401367188
[TRAIN] Iter: 367600 Loss: 0.005179582163691521  PSNR: 27.854745864868164
[TRAIN] Iter: 367700 Loss: 0.0038033481687307358  PSNR: 29.656604766845703
[TRAIN] Iter: 367800 Loss: 0.004085083492100239  PSNR: 29.336782455444336
[TRAIN] Iter: 367900 Loss: 0.004915688186883926  PSNR: 27.703243255615234
[TRAIN] Iter: 368000 Loss: 0.004955763928592205  PSNR: 28.41205406188965
[TRAIN] Iter: 368100 Loss: 0.005108139012008905  PSNR: 28.19556999206543
[TRAIN] Iter: 368200 Loss: 0.0052897511050105095  PSNR: 27.187585830688477
[TRAIN] Iter: 368300 Loss: 0.006229012738913298  PSNR: 26.866518020629883
[TRAIN] Iter: 368400 Loss: 0.005414540879428387  PSNR: 27.95804786682129
[TRAIN] Iter: 368500 Loss: 0.0043396493420004845  PSNR: 28.396175384521484
[TRAIN] Iter: 368600 Loss: 0.005313334986567497  PSNR: 27.646806716918945
[TRAIN] Iter: 368700 Loss: 0.005371796898543835  PSNR: 27.611064910888672
[TRAIN] Iter: 368800 Loss: 0.005380318500101566  PSNR: 27.766281127929688
[TRAIN] Iter: 368900 Loss: 0.003911744337528944  PSNR: 29.200714111328125
[TRAIN] Iter: 369000 Loss: 0.0037411008961498737  PSNR: 30.491573333740234
[TRAIN] Iter: 369100 Loss: 0.003976835869252682  PSNR: 29.089433670043945
[TRAIN] Iter: 369200 Loss: 0.005891266278922558  PSNR: 26.675840377807617
[TRAIN] Iter: 369300 Loss: 0.005150272510945797  PSNR: 26.435270309448242
[TRAIN] Iter: 369400 Loss: 0.003855663351714611  PSNR: 29.33177947998047
[TRAIN] Iter: 369500 Loss: 0.003740728599950671  PSNR: 29.008344650268555
[TRAIN] Iter: 369600 Loss: 0.004509362392127514  PSNR: 28.692955017089844
[TRAIN] Iter: 369700 Loss: 0.004442031495273113  PSNR: 29.087846755981445
[TRAIN] Iter: 369800 Loss: 0.005169043317437172  PSNR: 27.25970458984375
[TRAIN] Iter: 369900 Loss: 0.004499215632677078  PSNR: 28.291627883911133
Saved checkpoints at ./logs/TUT-KE101-nerf/370000.tar
[TRAIN] Iter: 370000 Loss: 0.004695895593613386  PSNR: 27.272602081298828
[TRAIN] Iter: 370100 Loss: 0.005093253683298826  PSNR: 28.200439453125
[TRAIN] Iter: 370200 Loss: 0.00519373407587409  PSNR: 27.56766700744629
[TRAIN] Iter: 370300 Loss: 0.005121093709021807  PSNR: 27.522550582885742
[TRAIN] Iter: 370400 Loss: 0.0037016579881310463  PSNR: 29.128978729248047
[TRAIN] Iter: 370500 Loss: 0.004602842964231968  PSNR: 28.15802574157715
[TRAIN] Iter: 370600 Loss: 0.004036138765513897  PSNR: 29.009056091308594
[TRAIN] Iter: 370700 Loss: 0.0032832350116223097  PSNR: 30.46816635131836
[TRAIN] Iter: 370800 Loss: 0.006206925492733717  PSNR: 27.088424682617188
[TRAIN] Iter: 370900 Loss: 0.006190057843923569  PSNR: 26.485769271850586
[TRAIN] Iter: 371000 Loss: 0.004814415238797665  PSNR: 28.240276336669922
[TRAIN] Iter: 371100 Loss: 0.004814136773347855  PSNR: 27.334028244018555
[TRAIN] Iter: 371200 Loss: 0.003909062594175339  PSNR: 29.843547821044922
[TRAIN] Iter: 371300 Loss: 0.003919095732271671  PSNR: 29.77552604675293
[TRAIN] Iter: 371400 Loss: 0.005554273724555969  PSNR: 27.69928550720215
[TRAIN] Iter: 371500 Loss: 0.003812537994235754  PSNR: 29.15386199951172
[TRAIN] Iter: 371600 Loss: 0.005034259054809809  PSNR: 27.54801368713379
[TRAIN] Iter: 371700 Loss: 0.004369804635643959  PSNR: 28.098512649536133
[TRAIN] Iter: 371800 Loss: 0.0032961820252239704  PSNR: 30.75791358947754
[TRAIN] Iter: 371900 Loss: 0.004149219021201134  PSNR: 29.768310546875
[TRAIN] Iter: 372000 Loss: 0.004013495519757271  PSNR: 28.45293426513672
[TRAIN] Iter: 372100 Loss: 0.004739356692880392  PSNR: 27.680652618408203
[TRAIN] Iter: 372200 Loss: 0.004812208004295826  PSNR: 27.407617568969727
[TRAIN] Iter: 372300 Loss: 0.0050314925611019135  PSNR: 27.689435958862305
[TRAIN] Iter: 372400 Loss: 0.004012409131973982  PSNR: 28.910350799560547
[TRAIN] Iter: 372500 Loss: 0.00611438974738121  PSNR: 27.042434692382812
[TRAIN] Iter: 372600 Loss: 0.004432765766978264  PSNR: 28.54637336730957
[TRAIN] Iter: 372700 Loss: 0.005891094915568829  PSNR: 27.344295501708984
[TRAIN] Iter: 372800 Loss: 0.004144576843827963  PSNR: 29.42021942138672
[TRAIN] Iter: 372900 Loss: 0.004974629729986191  PSNR: 27.855375289916992
[TRAIN] Iter: 373000 Loss: 0.003935975022614002  PSNR: 30.01076889038086
[TRAIN] Iter: 373100 Loss: 0.004087834153324366  PSNR: 28.5118465423584
[TRAIN] Iter: 373200 Loss: 0.004121372476220131  PSNR: 29.66322898864746
[TRAIN] Iter: 373300 Loss: 0.005432046949863434  PSNR: 27.041242599487305
[TRAIN] Iter: 373400 Loss: 0.006043185014277697  PSNR: 27.207019805908203
[TRAIN] Iter: 373500 Loss: 0.0035698742140084505  PSNR: 30.39592742919922
[TRAIN] Iter: 373600 Loss: 0.0050111329182982445  PSNR: 27.39170265197754
[TRAIN] Iter: 373700 Loss: 0.005891510285437107  PSNR: 27.193435668945312
[TRAIN] Iter: 373800 Loss: 0.005061257630586624  PSNR: 27.65517234802246
[TRAIN] Iter: 373900 Loss: 0.0065092360600829124  PSNR: 26.86176300048828
[TRAIN] Iter: 374000 Loss: 0.00323345884680748  PSNR: 31.426307678222656
[TRAIN] Iter: 374100 Loss: 0.004774943925440311  PSNR: 27.9158878326416
[TRAIN] Iter: 374200 Loss: 0.006288864649832249  PSNR: 26.344858169555664
[TRAIN] Iter: 374300 Loss: 0.0034542446956038475  PSNR: 31.238155364990234
[TRAIN] Iter: 374400 Loss: 0.003976862411946058  PSNR: 28.9290771484375
[TRAIN] Iter: 374500 Loss: 0.004246733151376247  PSNR: 28.874963760375977
[TRAIN] Iter: 374600 Loss: 0.004132038913667202  PSNR: 29.42967414855957
[TRAIN] Iter: 374700 Loss: 0.004177420865744352  PSNR: 28.155179977416992
[TRAIN] Iter: 374800 Loss: 0.005601778160780668  PSNR: 27.144399642944336
[TRAIN] Iter: 374900 Loss: 0.00611826591193676  PSNR: 26.640676498413086
[TRAIN] Iter: 375000 Loss: 0.006216806825250387  PSNR: 26.270336151123047
[TRAIN] Iter: 375100 Loss: 0.00404634652659297  PSNR: 29.517932891845703
[TRAIN] Iter: 375200 Loss: 0.0034834323450922966  PSNR: 30.077749252319336
[TRAIN] Iter: 375300 Loss: 0.004081291612237692  PSNR: 27.857622146606445
[TRAIN] Iter: 375400 Loss: 0.003938998095691204  PSNR: 28.816743850708008
[TRAIN] Iter: 375500 Loss: 0.003974731545895338  PSNR: 29.0804443359375
[TRAIN] Iter: 375600 Loss: 0.005430163349956274  PSNR: 28.09992790222168
[TRAIN] Iter: 375700 Loss: 0.005760099273175001  PSNR: 26.984519958496094
[TRAIN] Iter: 375800 Loss: 0.0045099398121237755  PSNR: 27.719608306884766
[TRAIN] Iter: 375900 Loss: 0.005228223279118538  PSNR: 27.47994041442871
[TRAIN] Iter: 376000 Loss: 0.004762677475810051  PSNR: 27.509477615356445
[TRAIN] Iter: 376100 Loss: 0.0041608442552387714  PSNR: 28.318500518798828
[TRAIN] Iter: 376200 Loss: 0.0038486195262521505  PSNR: 28.974424362182617
[TRAIN] Iter: 376300 Loss: 0.004807859659194946  PSNR: 27.336122512817383
[TRAIN] Iter: 376400 Loss: 0.0049543664790689945  PSNR: 28.131732940673828
[TRAIN] Iter: 376500 Loss: 0.004905355162918568  PSNR: 28.74384117126465
[TRAIN] Iter: 376600 Loss: 0.005148577503859997  PSNR: 27.654016494750977
[TRAIN] Iter: 376700 Loss: 0.004822160117328167  PSNR: 27.044979095458984
[TRAIN] Iter: 376800 Loss: 0.004400117322802544  PSNR: 29.449878692626953
[TRAIN] Iter: 376900 Loss: 0.006166546139866114  PSNR: 26.735990524291992
[TRAIN] Iter: 377000 Loss: 0.00431094178929925  PSNR: 29.796937942504883
[TRAIN] Iter: 377100 Loss: 0.00541706895455718  PSNR: 27.503820419311523
[TRAIN] Iter: 377200 Loss: 0.005517126061022282  PSNR: 27.25103187561035
[TRAIN] Iter: 377300 Loss: 0.004382169805467129  PSNR: 28.704668045043945
[TRAIN] Iter: 377400 Loss: 0.0036479677073657513  PSNR: 30.062801361083984
[TRAIN] Iter: 377500 Loss: 0.003682773793116212  PSNR: 29.657611846923828
[TRAIN] Iter: 377600 Loss: 0.0044179316610097885  PSNR: 28.157365798950195
[TRAIN] Iter: 377700 Loss: 0.005787152796983719  PSNR: 26.70555305480957
[TRAIN] Iter: 377800 Loss: 0.005442555993795395  PSNR: 26.99903106689453
[TRAIN] Iter: 377900 Loss: 0.0036408272571861744  PSNR: 30.170785903930664
[TRAIN] Iter: 378000 Loss: 0.004892104305326939  PSNR: 28.527698516845703
[TRAIN] Iter: 378100 Loss: 0.003891496919095516  PSNR: 29.592784881591797
[TRAIN] Iter: 378200 Loss: 0.0037293110508471727  PSNR: 29.160011291503906
[TRAIN] Iter: 378300 Loss: 0.004180599469691515  PSNR: 29.157012939453125
[TRAIN] Iter: 378400 Loss: 0.004694120027124882  PSNR: 28.566823959350586
[TRAIN] Iter: 378500 Loss: 0.005893121473491192  PSNR: 27.292781829833984
[TRAIN] Iter: 378600 Loss: 0.004929886665195227  PSNR: 28.03729820251465
[TRAIN] Iter: 378700 Loss: 0.0051558539271354675  PSNR: 28.02741813659668
[TRAIN] Iter: 378800 Loss: 0.005702519789338112  PSNR: 27.488414764404297
[TRAIN] Iter: 378900 Loss: 0.005935841705650091  PSNR: 27.657695770263672
[TRAIN] Iter: 379000 Loss: 0.004109294153749943  PSNR: 29.275590896606445
[TRAIN] Iter: 379100 Loss: 0.005142539273947477  PSNR: 27.9232177734375
[TRAIN] Iter: 379200 Loss: 0.0038389423862099648  PSNR: 29.828927993774414
[TRAIN] Iter: 379300 Loss: 0.004017019644379616  PSNR: 29.47716522216797
[TRAIN] Iter: 379400 Loss: 0.005263263359665871  PSNR: 27.631898880004883
[TRAIN] Iter: 379500 Loss: 0.0044102175161242485  PSNR: 28.138940811157227
[TRAIN] Iter: 379600 Loss: 0.004888416733592749  PSNR: 27.461902618408203
[TRAIN] Iter: 379700 Loss: 0.0039231255650520325  PSNR: 29.897525787353516
[TRAIN] Iter: 379800 Loss: 0.004619361832737923  PSNR: 28.144569396972656
[TRAIN] Iter: 379900 Loss: 0.0051391711458563805  PSNR: 27.044387817382812
Saved checkpoints at ./logs/TUT-KE101-nerf/380000.tar
[TRAIN] Iter: 380000 Loss: 0.004317758604884148  PSNR: 28.579957962036133
[TRAIN] Iter: 380100 Loss: 0.004416739568114281  PSNR: 28.544458389282227
[TRAIN] Iter: 380200 Loss: 0.006045251619070768  PSNR: 26.921300888061523
[TRAIN] Iter: 380300 Loss: 0.0044176108203828335  PSNR: 28.680591583251953
[TRAIN] Iter: 380400 Loss: 0.004266951233148575  PSNR: 28.463092803955078
[TRAIN] Iter: 380500 Loss: 0.005260227248072624  PSNR: 27.563228607177734
[TRAIN] Iter: 380600 Loss: 0.005374029278755188  PSNR: 27.738805770874023
[TRAIN] Iter: 380700 Loss: 0.006042025052011013  PSNR: 26.050107955932617
[TRAIN] Iter: 380800 Loss: 0.0054416656494140625  PSNR: 27.137935638427734
[TRAIN] Iter: 380900 Loss: 0.0057459063827991486  PSNR: 27.599952697753906
[TRAIN] Iter: 381000 Loss: 0.004889099393039942  PSNR: 28.365604400634766
[TRAIN] Iter: 381100 Loss: 0.004533750470727682  PSNR: 28.069902420043945
[TRAIN] Iter: 381200 Loss: 0.005015100352466106  PSNR: 27.752260208129883
[TRAIN] Iter: 381300 Loss: 0.005071605090051889  PSNR: 27.512657165527344
[TRAIN] Iter: 381400 Loss: 0.00450480729341507  PSNR: 27.93178367614746
[TRAIN] Iter: 381500 Loss: 0.004300939384847879  PSNR: 28.151432037353516
[TRAIN] Iter: 381600 Loss: 0.006143263075500727  PSNR: 26.828598022460938
[TRAIN] Iter: 381700 Loss: 0.005574285518378019  PSNR: 27.935977935791016
[TRAIN] Iter: 381800 Loss: 0.004227956756949425  PSNR: 29.86249351501465
[TRAIN] Iter: 381900 Loss: 0.005335059016942978  PSNR: 27.97127914428711
[TRAIN] Iter: 382000 Loss: 0.003474453231319785  PSNR: 30.385040283203125
[TRAIN] Iter: 382100 Loss: 0.0040434799157083035  PSNR: 29.0332088470459
[TRAIN] Iter: 382200 Loss: 0.0059548960998654366  PSNR: 27.093053817749023
[TRAIN] Iter: 382300 Loss: 0.00346254906617105  PSNR: 30.241682052612305
[TRAIN] Iter: 382400 Loss: 0.005179260857403278  PSNR: 27.581850051879883
[TRAIN] Iter: 382500 Loss: 0.003771423362195492  PSNR: 29.844388961791992
[TRAIN] Iter: 382600 Loss: 0.004620895721018314  PSNR: 27.979135513305664
[TRAIN] Iter: 382700 Loss: 0.004493899177759886  PSNR: 27.793611526489258
[TRAIN] Iter: 382800 Loss: 0.004066081251949072  PSNR: 29.704238891601562
[TRAIN] Iter: 382900 Loss: 0.005094129592180252  PSNR: 27.234968185424805
[TRAIN] Iter: 383000 Loss: 0.0039074597880244255  PSNR: 28.374622344970703
[TRAIN] Iter: 383100 Loss: 0.005316412076354027  PSNR: 26.958532333374023
[TRAIN] Iter: 383200 Loss: 0.006104289088398218  PSNR: 26.88151741027832
[TRAIN] Iter: 383300 Loss: 0.004052870906889439  PSNR: 29.36496925354004
[TRAIN] Iter: 383400 Loss: 0.0043479991145431995  PSNR: 28.71997833251953
[TRAIN] Iter: 383500 Loss: 0.005878188647329807  PSNR: 26.68255615234375
[TRAIN] Iter: 383600 Loss: 0.005041681230068207  PSNR: 27.335290908813477
[TRAIN] Iter: 383700 Loss: 0.004041720647364855  PSNR: 30.49298095703125
[TRAIN] Iter: 383800 Loss: 0.004591495264321566  PSNR: 28.521753311157227
[TRAIN] Iter: 383900 Loss: 0.003959815949201584  PSNR: 29.984268188476562
[TRAIN] Iter: 384000 Loss: 0.0035909584257751703  PSNR: 30.48293113708496
[TRAIN] Iter: 384100 Loss: 0.005020966753363609  PSNR: 27.784860610961914
[TRAIN] Iter: 384200 Loss: 0.0036750645376741886  PSNR: 29.899700164794922
[TRAIN] Iter: 384300 Loss: 0.0046898904256522655  PSNR: 27.911495208740234
[TRAIN] Iter: 384400 Loss: 0.0035859672352671623  PSNR: 29.941354751586914
[TRAIN] Iter: 384500 Loss: 0.005677417851984501  PSNR: 27.956361770629883
[TRAIN] Iter: 384600 Loss: 0.006310614757239819  PSNR: 26.02431869506836
[TRAIN] Iter: 384700 Loss: 0.00506183784455061  PSNR: 28.26458740234375
[TRAIN] Iter: 384800 Loss: 0.004698533099144697  PSNR: 28.992191314697266
[TRAIN] Iter: 384900 Loss: 0.005973740015178919  PSNR: 27.514408111572266
[TRAIN] Iter: 385000 Loss: 0.0060443999245762825  PSNR: 26.62799835205078
[TRAIN] Iter: 385100 Loss: 0.004671192262321711  PSNR: 27.43560028076172
[TRAIN] Iter: 385200 Loss: 0.006165117025375366  PSNR: 26.348743438720703
[TRAIN] Iter: 385300 Loss: 0.004465942271053791  PSNR: 28.56293296813965
[TRAIN] Iter: 385400 Loss: 0.004850524011999369  PSNR: 28.29582405090332
[TRAIN] Iter: 385500 Loss: 0.003254610113799572  PSNR: 30.146312713623047
[TRAIN] Iter: 385600 Loss: 0.004694370087236166  PSNR: 27.695613861083984
[TRAIN] Iter: 385700 Loss: 0.004830487538129091  PSNR: 28.112957000732422
[TRAIN] Iter: 385800 Loss: 0.0053475541062653065  PSNR: 27.72840690612793
[TRAIN] Iter: 385900 Loss: 0.005835916846990585  PSNR: 26.86369514465332
[TRAIN] Iter: 386000 Loss: 0.004325510933995247  PSNR: 28.068334579467773
[TRAIN] Iter: 386100 Loss: 0.0041717058047652245  PSNR: 29.591970443725586
[TRAIN] Iter: 386200 Loss: 0.0047292085364460945  PSNR: 28.31220817565918
[TRAIN] Iter: 386300 Loss: 0.0038250030484050512  PSNR: 29.876239776611328
[TRAIN] Iter: 386400 Loss: 0.0047446442767977715  PSNR: 27.736093521118164
[TRAIN] Iter: 386500 Loss: 0.0037565380334854126  PSNR: 29.79912757873535
[TRAIN] Iter: 386600 Loss: 0.004260025918483734  PSNR: 28.39097023010254
[TRAIN] Iter: 386700 Loss: 0.0037857128772884607  PSNR: 29.664196014404297
[TRAIN] Iter: 386800 Loss: 0.0049195727333426476  PSNR: 27.168907165527344
[TRAIN] Iter: 386900 Loss: 0.004222494550049305  PSNR: 29.758283615112305
[TRAIN] Iter: 387000 Loss: 0.004631246440112591  PSNR: 28.12640953063965
[TRAIN] Iter: 387100 Loss: 0.0053763012401759624  PSNR: 27.266170501708984
[TRAIN] Iter: 387200 Loss: 0.004578736145049334  PSNR: 28.507165908813477
[TRAIN] Iter: 387300 Loss: 0.005594294983893633  PSNR: 27.674877166748047
[TRAIN] Iter: 387400 Loss: 0.0038735405541956425  PSNR: 30.16443634033203
[TRAIN] Iter: 387500 Loss: 0.004144993610680103  PSNR: 28.52197265625
[TRAIN] Iter: 387600 Loss: 0.004401822574436665  PSNR: 28.87104034423828
[TRAIN] Iter: 387700 Loss: 0.004283388145267963  PSNR: 29.331205368041992
[TRAIN] Iter: 387800 Loss: 0.004791845101863146  PSNR: 27.701902389526367
[TRAIN] Iter: 387900 Loss: 0.004628761205822229  PSNR: 28.425413131713867
[TRAIN] Iter: 388000 Loss: 0.004861362744122744  PSNR: 27.403547286987305
[TRAIN] Iter: 388100 Loss: 0.005081288516521454  PSNR: 28.388111114501953
[TRAIN] Iter: 388200 Loss: 0.005222462583333254  PSNR: 27.456886291503906
[TRAIN] Iter: 388300 Loss: 0.00362388719804585  PSNR: 30.195825576782227
[TRAIN] Iter: 388400 Loss: 0.0042679086327552795  PSNR: 29.04688835144043
[TRAIN] Iter: 388500 Loss: 0.0041466811671853065  PSNR: 29.451194763183594
[TRAIN] Iter: 388600 Loss: 0.004261951893568039  PSNR: 28.06936264038086
[TRAIN] Iter: 388700 Loss: 0.004085894674062729  PSNR: 29.274782180786133
[TRAIN] Iter: 388800 Loss: 0.00502663804218173  PSNR: 28.00293731689453
[TRAIN] Iter: 388900 Loss: 0.003272277768701315  PSNR: 29.197860717773438
[TRAIN] Iter: 389000 Loss: 0.0038681954611092806  PSNR: 29.397951126098633
[TRAIN] Iter: 389100 Loss: 0.004975185263901949  PSNR: 27.1983585357666
[TRAIN] Iter: 389200 Loss: 0.004395955707877874  PSNR: 28.06991195678711
[TRAIN] Iter: 389300 Loss: 0.005077328532934189  PSNR: 27.599966049194336
[TRAIN] Iter: 389400 Loss: 0.003276097122579813  PSNR: 30.53443145751953
[TRAIN] Iter: 389500 Loss: 0.004694866947829723  PSNR: 28.255151748657227
[TRAIN] Iter: 389600 Loss: 0.004917554557323456  PSNR: 27.705978393554688
[TRAIN] Iter: 389700 Loss: 0.004747207276523113  PSNR: 27.91610336303711
[TRAIN] Iter: 389800 Loss: 0.004740695934742689  PSNR: 28.135881423950195
[TRAIN] Iter: 389900 Loss: 0.005307300481945276  PSNR: 27.790483474731445
Saved checkpoints at ./logs/TUT-KE101-nerf/390000.tar
[TRAIN] Iter: 390000 Loss: 0.005905823782086372  PSNR: 26.967531204223633
[TRAIN] Iter: 390100 Loss: 0.004401099402457476  PSNR: 28.760456085205078
[TRAIN] Iter: 390200 Loss: 0.0038015202153474092  PSNR: 29.529449462890625
[TRAIN] Iter: 390300 Loss: 0.005676891654729843  PSNR: 27.362321853637695
[TRAIN] Iter: 390400 Loss: 0.005406979471445084  PSNR: 27.00762176513672
[TRAIN] Iter: 390500 Loss: 0.003984976559877396  PSNR: 29.760351181030273
[TRAIN] Iter: 390600 Loss: 0.004837818909436464  PSNR: 28.03215217590332
[TRAIN] Iter: 390700 Loss: 0.0055722747929394245  PSNR: 27.534135818481445
[TRAIN] Iter: 390800 Loss: 0.004745698533952236  PSNR: 27.591842651367188
[TRAIN] Iter: 390900 Loss: 0.004022303503006697  PSNR: 28.687776565551758
[TRAIN] Iter: 391000 Loss: 0.00529718492180109  PSNR: 27.007158279418945
[TRAIN] Iter: 391100 Loss: 0.004380394704639912  PSNR: 28.03946876525879
[TRAIN] Iter: 391200 Loss: 0.0056585222482681274  PSNR: 26.403926849365234
[TRAIN] Iter: 391300 Loss: 0.005239489022642374  PSNR: 27.79400062561035
[TRAIN] Iter: 391400 Loss: 0.0058057657442986965  PSNR: 26.61113929748535
[TRAIN] Iter: 391500 Loss: 0.0047550806775689125  PSNR: 28.14795684814453
[TRAIN] Iter: 391600 Loss: 0.004810696002095938  PSNR: 27.68845558166504
[TRAIN] Iter: 391700 Loss: 0.004167316015809774  PSNR: 28.86805534362793
[TRAIN] Iter: 391800 Loss: 0.004153381567448378  PSNR: 28.382478713989258
[TRAIN] Iter: 391900 Loss: 0.003931249026209116  PSNR: 30.180477142333984
[TRAIN] Iter: 392000 Loss: 0.004998790565878153  PSNR: 27.94570541381836
[TRAIN] Iter: 392100 Loss: 0.0056409090757369995  PSNR: 27.00690269470215
[TRAIN] Iter: 392200 Loss: 0.005145086441189051  PSNR: 27.65945053100586
[TRAIN] Iter: 392300 Loss: 0.0038673614617437124  PSNR: 30.141521453857422
[TRAIN] Iter: 392400 Loss: 0.003963958006352186  PSNR: 29.197982788085938
[TRAIN] Iter: 392500 Loss: 0.005276696756482124  PSNR: 26.325124740600586
[TRAIN] Iter: 392600 Loss: 0.005124978721141815  PSNR: 27.88648796081543
[TRAIN] Iter: 392700 Loss: 0.004049206152558327  PSNR: 29.33099937438965
[TRAIN] Iter: 392800 Loss: 0.004821817856281996  PSNR: 27.451719284057617
[TRAIN] Iter: 392900 Loss: 0.004205932375043631  PSNR: 29.569787979125977
[TRAIN] Iter: 393000 Loss: 0.003958899527788162  PSNR: 28.935035705566406
[TRAIN] Iter: 393100 Loss: 0.005858398042619228  PSNR: 26.515031814575195
[TRAIN] Iter: 393200 Loss: 0.005696939304471016  PSNR: 27.363821029663086
[TRAIN] Iter: 393300 Loss: 0.005268496926873922  PSNR: 27.20618438720703
[TRAIN] Iter: 393400 Loss: 0.0039939130656421185  PSNR: 29.411542892456055
[TRAIN] Iter: 393500 Loss: 0.006492968648672104  PSNR: 27.111974716186523
[TRAIN] Iter: 393600 Loss: 0.005520321428775787  PSNR: 26.471363067626953
[TRAIN] Iter: 393700 Loss: 0.005189785733819008  PSNR: 26.794687271118164
[TRAIN] Iter: 393800 Loss: 0.004605373367667198  PSNR: 28.61412811279297
[TRAIN] Iter: 393900 Loss: 0.005625611171126366  PSNR: 26.980199813842773
[TRAIN] Iter: 394000 Loss: 0.004327136091887951  PSNR: 28.617700576782227
[TRAIN] Iter: 394100 Loss: 0.004908965900540352  PSNR: 27.826702117919922
[TRAIN] Iter: 394200 Loss: 0.005761521868407726  PSNR: 26.23940658569336
[TRAIN] Iter: 394300 Loss: 0.004484676290303469  PSNR: 27.840667724609375
[TRAIN] Iter: 394400 Loss: 0.004496187902987003  PSNR: 28.052616119384766
[TRAIN] Iter: 394500 Loss: 0.005414855666458607  PSNR: 27.164377212524414
[TRAIN] Iter: 394600 Loss: 0.0039369696751236916  PSNR: 29.215829849243164
[TRAIN] Iter: 394700 Loss: 0.003534250892698765  PSNR: 30.009307861328125
[TRAIN] Iter: 394800 Loss: 0.005291098263114691  PSNR: 27.174833297729492
[TRAIN] Iter: 394900 Loss: 0.005019649397581816  PSNR: 27.67290496826172
[TRAIN] Iter: 395000 Loss: 0.003556294832378626  PSNR: 30.127893447875977
[TRAIN] Iter: 395100 Loss: 0.003689369186758995  PSNR: 29.519519805908203
[TRAIN] Iter: 395200 Loss: 0.004202277399599552  PSNR: 29.54024887084961
[TRAIN] Iter: 395300 Loss: 0.005927644204348326  PSNR: 26.70475196838379
[TRAIN] Iter: 395400 Loss: 0.0034417540300637484  PSNR: 29.838167190551758
[TRAIN] Iter: 395500 Loss: 0.004584169015288353  PSNR: 29.177854537963867
[TRAIN] Iter: 395600 Loss: 0.005338618997484446  PSNR: 26.936443328857422
[TRAIN] Iter: 395700 Loss: 0.004480582661926746  PSNR: 28.14267349243164
[TRAIN] Iter: 395800 Loss: 0.003494551172479987  PSNR: 30.57468032836914
[TRAIN] Iter: 395900 Loss: 0.0048973397351801395  PSNR: 27.785938262939453
[TRAIN] Iter: 396000 Loss: 0.003964556381106377  PSNR: 30.05744743347168
[TRAIN] Iter: 396100 Loss: 0.004177919588983059  PSNR: 29.44148826599121
[TRAIN] Iter: 396200 Loss: 0.005275215487927198  PSNR: 28.173322677612305
[TRAIN] Iter: 396300 Loss: 0.00434061698615551  PSNR: 28.03424072265625
[TRAIN] Iter: 396400 Loss: 0.003736377228051424  PSNR: 29.82855987548828
[TRAIN] Iter: 396500 Loss: 0.0037554893642663956  PSNR: 30.533567428588867
[TRAIN] Iter: 396600 Loss: 0.005561155267059803  PSNR: 27.82004737854004
[TRAIN] Iter: 396700 Loss: 0.005621088203042746  PSNR: 27.25226402282715
[TRAIN] Iter: 396800 Loss: 0.004509330727159977  PSNR: 28.137264251708984
[TRAIN] Iter: 396900 Loss: 0.004060989245772362  PSNR: 29.46517562866211
[TRAIN] Iter: 397000 Loss: 0.004459008574485779  PSNR: 28.52001190185547
[TRAIN] Iter: 397100 Loss: 0.004994869697839022  PSNR: 27.548255920410156
[TRAIN] Iter: 397200 Loss: 0.0037014614790678024  PSNR: 29.535367965698242
[TRAIN] Iter: 397300 Loss: 0.0038885907270014286  PSNR: 29.392822265625
[TRAIN] Iter: 397400 Loss: 0.004730580374598503  PSNR: 28.705127716064453
[TRAIN] Iter: 397500 Loss: 0.003376347478479147  PSNR: 30.72730255126953
[TRAIN] Iter: 397600 Loss: 0.005682494957000017  PSNR: 27.00114631652832
[TRAIN] Iter: 397700 Loss: 0.005558541044592857  PSNR: 27.826414108276367
[TRAIN] Iter: 397800 Loss: 0.005262383725494146  PSNR: 27.645278930664062
[TRAIN] Iter: 397900 Loss: 0.004078324884176254  PSNR: 29.066743850708008
[TRAIN] Iter: 398000 Loss: 0.004303726367652416  PSNR: 27.75275993347168
[TRAIN] Iter: 398100 Loss: 0.0036627426743507385  PSNR: 30.003822326660156
[TRAIN] Iter: 398200 Loss: 0.004085982218384743  PSNR: 30.052894592285156
[TRAIN] Iter: 398300 Loss: 0.005536089651286602  PSNR: 27.598201751708984
[TRAIN] Iter: 398400 Loss: 0.004549630917608738  PSNR: 28.394227981567383
[TRAIN] Iter: 398500 Loss: 0.004384614527225494  PSNR: 29.190067291259766
[TRAIN] Iter: 398600 Loss: 0.0055189477279782295  PSNR: 26.892820358276367
[TRAIN] Iter: 398700 Loss: 0.005753187462687492  PSNR: 27.425291061401367
[TRAIN] Iter: 398800 Loss: 0.004986971151083708  PSNR: 28.6164608001709
[TRAIN] Iter: 398900 Loss: 0.004409535322338343  PSNR: 28.463289260864258
[TRAIN] Iter: 399000 Loss: 0.004136636387556791  PSNR: 28.592004776000977
[TRAIN] Iter: 399100 Loss: 0.0058213938027620316  PSNR: 28.113439559936523
[TRAIN] Iter: 399200 Loss: 0.00489753857254982  PSNR: 27.313926696777344
[TRAIN] Iter: 399300 Loss: 0.004583051428198814  PSNR: 28.2939510345459
[TRAIN] Iter: 399400 Loss: 0.0036170529201626778  PSNR: 29.20277214050293
[TRAIN] Iter: 399500 Loss: 0.004700197838246822  PSNR: 27.51308822631836
[TRAIN] Iter: 399600 Loss: 0.005590362474322319  PSNR: 27.191028594970703
[TRAIN] Iter: 399700 Loss: 0.003746530506759882  PSNR: 29.853981018066406
[TRAIN] Iter: 399800 Loss: 0.00459130946546793  PSNR: 28.947811126708984
[TRAIN] Iter: 399900 Loss: 0.0050832065753638744  PSNR: 28.290929794311523
Saved checkpoints at ./logs/TUT-KE101-nerf/400000.tar
0 0.00038886070251464844
torch.Size([320, 640, 3]) torch.Size([320, 640])
1 12.866900205612183
2 16.23337197303772
3 12.851919651031494
4 12.76379108428955
5 16.135422945022583
6 12.819549798965454
7 12.75222373008728
8 16.37566614151001
9 12.773589849472046
10 16.294420957565308
11 12.775385618209839
12 12.76231074333191
13 16.30630397796631
14 12.755037784576416
15 16.285489320755005
16 12.796498537063599
17 12.759984254837036
18 16.37013077735901
19 12.777350902557373
20 16.343935251235962
21 12.693967580795288
22 12.7373685836792
23 16.320546627044678
24 12.78899621963501
25 16.32739520072937
26 12.679424285888672
27 12.754810571670532
28 16.338703632354736
29 12.783974647521973
30 16.34187650680542
31 12.763959407806396
32 12.77020788192749
33 16.35370373725891
34 12.746767520904541
35 16.346275329589844
36 12.781049489974976
37 12.749193668365479
38 16.34903597831726
39 12.76045274734497
40 16.351765871047974
41 12.76340389251709
42 12.746124982833862
43 16.344279527664185
44 12.802050352096558
45 16.341151475906372
46 12.757609605789185
47 12.653843879699707
48 16.253283977508545
49 12.799086570739746
50 12.774556159973145
51 16.30071449279785
52 12.77492904663086
53 16.346539974212646
54 12.783087730407715
55 12.771941184997559
56 16.52688503265381
57 12.824740171432495
58 15.991033554077148
59 12.742177486419678
60 12.737454652786255
61 16.557465076446533
62 12.970375299453735
63 15.854126453399658
64 12.755527257919312
65 12.755425930023193
66 16.46247363090515
67 12.74191403388977
68 16.177502393722534
69 12.767013788223267
70 12.73221492767334
71 16.324421644210815
72 12.871326923370361
73 16.09362292289734
74 12.702625751495361
75 12.78096890449524
76 16.367140769958496
77 12.939598083496094
78 16.134856939315796
79 12.731450319290161
80 12.772717475891113
81 16.42701482772827
82 12.952893018722534
83 16.12044358253479
84 12.958752870559692
85 12.957712411880493
86 15.982542753219604
87 12.827516794204712
88 16.102734327316284
89 12.794594764709473
90 12.70328688621521
91 16.334044933319092
92 12.77311110496521
93 16.205029726028442
94 12.79806637763977
95 12.71983551979065
96 16.248764514923096
97 12.764966487884521
98 16.197574138641357
99 12.732603788375854
100 12.738390922546387
101 16.34715700149536
102 12.731940031051636
103 16.311952590942383
104 12.756490707397461
105 12.749497652053833
106 16.335390090942383
107 12.751474618911743
108 16.323559284210205
109 12.717476606369019
110 12.750328302383423
111 16.308584690093994
112 12.73873496055603
113 12.779954671859741
114 16.320143461227417
115 12.707116603851318
116 16.31667923927307
117 12.725385189056396
118 12.734703779220581
119 16.342989683151245
Done, saving (120, 320, 640, 3) (120, 320, 640)
extras:{'raw': tensor([[[ 4.4566e-01,  1.0475e+00,  2.0024e+00, -6.1752e+01],
         [ 8.5721e-01,  6.5768e-01, -1.7420e-01, -4.8454e+01],
         [-1.2989e+00, -1.7262e+00, -2.4173e+00,  4.5682e+00],
         ...,
         [-1.8415e+01, -1.3545e+01, -1.0728e+01, -8.9845e+01],
         [-2.2232e+01, -1.6542e+01, -1.2910e+01, -5.4860e+01],
         [-1.9787e+01, -1.4703e+01, -1.1820e+01, -7.9908e+01]],

        [[-6.2693e-01,  1.6655e-02,  1.3415e+00, -6.3675e+01],
         [ 2.3483e+00,  2.5081e+00,  2.9077e+00, -1.5288e+01],
         [ 1.2098e+00,  1.2641e+00,  1.2718e+00, -1.8262e+01],
         ...,
         [ 1.0402e+00, -1.5232e+00, -7.9520e+00,  3.8504e+02],
         [ 1.1492e+00, -1.2994e+00, -7.3869e+00,  3.8657e+02],
         [ 1.1548e+00, -1.7034e+00, -8.4964e+00,  3.8603e+02]],

        [[ 4.4023e+00,  4.7329e+00,  6.7007e+00, -6.1445e+01],
         [ 1.1967e-01, -2.9282e-01, -1.1466e+00, -3.1214e+01],
         [ 8.4032e-01,  7.1511e-01,  6.5753e-01,  1.6716e+01],
         ...,
         [ 2.4377e-01, -1.6885e+00, -6.3898e+00, -8.0175e+01],
         [-1.9803e-02, -1.8228e+00, -5.8416e+00, -2.0969e+01],
         [ 1.5012e-01, -1.5950e+00, -5.5392e+00, -1.5693e+01]],

        ...,

        [[ 3.2485e+00,  4.1728e+00,  5.4881e+00, -6.3535e+01],
         [-4.3546e-03,  5.7841e-01,  1.5991e+00, -5.4841e+01],
         [-1.7330e-01,  3.5297e-01,  1.2165e+00, -5.3885e+01],
         ...,
         [-7.1595e+00, -7.0353e+00, -8.3497e+00, -4.1669e+01],
         [-8.1498e+00, -8.0425e+00, -9.1258e+00, -4.0156e+01],
         [-5.6389e+00, -5.5895e+00, -6.5628e+00, -1.1906e+02]],

        [[ 7.9646e+00,  9.2320e+00,  1.2730e+01, -6.8262e+01],
         [-3.2286e-01, -4.6915e-02,  7.9470e-01, -5.5090e+01],
         [ 3.4616e-01,  3.3033e-01,  4.2630e-01, -1.9237e+01],
         ...,
         [ 8.9584e-01, -8.0464e-01, -5.0904e+00,  4.6483e+01],
         [ 7.6460e-01, -1.0084e+00, -5.3244e+00,  1.0819e+01],
         [ 4.5572e-01, -1.3271e+00, -5.7303e+00,  9.7169e+01]],

        [[-1.4861e+00, -3.9914e-01,  1.8985e+00, -7.6872e+01],
         [-1.0010e-01, -2.0030e-02,  3.7785e-01, -1.4563e+01],
         [ 3.0175e-01,  3.6400e-01,  6.5764e-01, -2.8592e+01],
         ...,
         [ 4.5366e+00, -5.9433e-01, -1.8897e+01,  4.0769e+02],
         [ 6.1433e+00,  1.1473e+00, -1.5775e+01,  4.1153e+02],
         [ 5.0656e+00,  8.6473e-01, -1.4825e+01,  3.9217e+02]]],
       grad_fn=<ReshapeAliasBackward0>), 'rgb0': tensor([[0.1971, 0.1291, 0.0581],
        [0.7409, 0.7350, 0.7055],
        [0.5118, 0.5125, 0.5261],
        ...,
        [0.6287, 0.5750, 0.5118],
        [0.5421, 0.5646, 0.6680],
        [0.4749, 0.4235, 0.3482]], grad_fn=<ReshapeAliasBackward0>), 'disp0': tensor([ 67.8638,  65.3649,  51.6002,  ..., 104.3568,  66.5735, 395.9695],
       grad_fn=<ReshapeAliasBackward0>), 'acc0': tensor([1., 1., 1.,  ..., 1., 1., 1.], grad_fn=<ReshapeAliasBackward0>), 'z_std': tensor([0.0022, 0.0025, 0.0031,  ..., 0.0022, 0.0025, 0.3403])}
0 0.0005097389221191406
torch.Size([320, 640, 3]) torch.Size([320, 640])
1 16.251026391983032
2 12.79124927520752
3 12.743952989578247
4 16.322781562805176
5 12.763224601745605
6 16.32736611366272
7 12.71622347831726
8 12.724462509155273
9 16.320189952850342
10 12.762701749801636
11 16.324177980422974
12 12.80302357673645
13 12.953277587890625
14 18.14269781112671
15 14.455558776855469
16 18.088807344436646
17 14.506530046463013
18 18.192453384399414
19 14.827666759490967
20 18.06369400024414
21 15.224501371383667
22 17.089240074157715
23 15.259146451950073
24 17.01344585418701
25 15.430669069290161
26 16.87246012687683
27 15.429178953170776
28 15.7255277633667
29 16.982852935791016
30 16.052358627319336
31 16.939162492752075
32 16.082666873931885
33 16.97432780265808
34 15.673579931259155
35 16.75907611846924
36 15.710333824157715
37 16.69324517250061
38 15.671398162841797
39 16.59927201271057
40 15.616690635681152
41 16.65304446220398
42 15.619168758392334
43 16.658854722976685
44 15.713200569152832
45 16.569233655929565
46 15.633560419082642
47 16.626721143722534
48 15.6379075050354
49 16.61093497276306
50 15.614226818084717
51 16.686657667160034
52 15.666813135147095
53 16.639131546020508
54 15.698328495025635
55 16.77202844619751
56 15.778409957885742
57 16.37818932533264
58 15.782978296279907
59 16.575879096984863
60 15.75532341003418
61 16.620997190475464
62 15.740964412689209
63 16.627432107925415
64 15.655718803405762
65 15.68093752861023
66 16.63161849975586
67 15.699609756469727
68 16.62326169013977
69 15.715373277664185
70 16.608360290527344
71 15.636500597000122
72 16.580095291137695
73 15.697513580322266
74 16.94097352027893
75 15.808009147644043
76 16.310949087142944
77 15.882255554199219
78 16.46575355529785
79 16.15977716445923
80 16.12254047393799
81 16.046273946762085
82 16.20636224746704
83 16.106215000152588
84 16.201446294784546
85 16.1233127117157
86 16.23402762413025
87 16.111688137054443
88 16.251629114151
89 16.054854154586792
90 16.398010730743408
91 16.537807941436768
92 16.62928795814514
93 16.53117036819458
94 16.331629276275635
95 16.14047336578369
96 16.21430253982544
97 16.098060607910156
98 16.141275882720947
99 16.154442310333252
100 16.157505989074707
101 16.159547805786133
102 16.174298763275146
103 16.153942108154297
104 16.083468437194824
105 16.14091205596924
106 16.14056897163391
107 16.134819269180298
108 16.194693088531494
109 16.177273988723755
110 16.08235788345337
111 16.114561080932617
112 16.116963148117065
113 16.118962049484253
114 16.11184287071228
115 16.15797758102417
116 16.13092613220215
117 16.152236938476562
118 16.156490802764893
119 16.150325059890747
test poses shape torch.Size([4, 3, 4])
0 0.0006985664367675781
torch.Size([320, 640, 3]) torch.Size([320, 640])
1 16.216760635375977
2 16.16235899925232
3 16.180302381515503
Saved test set
[TRAIN] Iter: 400000 Loss: 0.00575699657201767  PSNR: 26.6051082611084
[TRAIN] Iter: 400100 Loss: 0.005503113381564617  PSNR: 27.505294799804688
[TRAIN] Iter: 400200 Loss: 0.004753303248435259  PSNR: 28.048309326171875
[TRAIN] Iter: 400300 Loss: 0.0036877933889627457  PSNR: 30.000293731689453
[TRAIN] Iter: 400400 Loss: 0.004975905641913414  PSNR: 27.201231002807617
[TRAIN] Iter: 400500 Loss: 0.004294398706406355  PSNR: 28.52242088317871
[TRAIN] Iter: 400600 Loss: 0.004792931955307722  PSNR: 27.466812133789062
[TRAIN] Iter: 400700 Loss: 0.004881103057414293  PSNR: 28.52127456665039
[TRAIN] Iter: 400800 Loss: 0.0041550323367118835  PSNR: 29.537975311279297
[TRAIN] Iter: 400900 Loss: 0.004971441812813282  PSNR: 27.081363677978516
[TRAIN] Iter: 401000 Loss: 0.0034046394284814596  PSNR: 30.258760452270508
[TRAIN] Iter: 401100 Loss: 0.005030184984207153  PSNR: 27.437776565551758
[TRAIN] Iter: 401200 Loss: 0.00483367033302784  PSNR: 27.898035049438477
[TRAIN] Iter: 401300 Loss: 0.004841483663767576  PSNR: 28.554285049438477
[TRAIN] Iter: 401400 Loss: 0.0051344893872737885  PSNR: 27.96394920349121
[TRAIN] Iter: 401500 Loss: 0.004571930970996618  PSNR: 28.723262786865234
[TRAIN] Iter: 401600 Loss: 0.003764052176848054  PSNR: 29.985280990600586
[TRAIN] Iter: 401700 Loss: 0.005021055229008198  PSNR: 28.049766540527344
[TRAIN] Iter: 401800 Loss: 0.004877799190580845  PSNR: 27.865848541259766
[TRAIN] Iter: 401900 Loss: 0.0049926601350307465  PSNR: 27.51176643371582
[TRAIN] Iter: 402000 Loss: 0.003343238029628992  PSNR: 30.84066390991211
[TRAIN] Iter: 402100 Loss: 0.003301878459751606  PSNR: 30.896020889282227
[TRAIN] Iter: 402200 Loss: 0.004192047286778688  PSNR: 27.78251838684082
[TRAIN] Iter: 402300 Loss: 0.004934053868055344  PSNR: 28.00224494934082
[TRAIN] Iter: 402400 Loss: 0.003779237624257803  PSNR: 30.699296951293945
[TRAIN] Iter: 402500 Loss: 0.0038859378546476364  PSNR: 29.84864044189453
[TRAIN] Iter: 402600 Loss: 0.0037978310137987137  PSNR: 28.49135398864746
[TRAIN] Iter: 402700 Loss: 0.00555990869179368  PSNR: 27.73383903503418
[TRAIN] Iter: 402800 Loss: 0.0049494532868266106  PSNR: 27.966421127319336
[TRAIN] Iter: 402900 Loss: 0.005514968186616898  PSNR: 27.83046531677246
[TRAIN] Iter: 403000 Loss: 0.0049599590711295605  PSNR: 28.07727813720703
[TRAIN] Iter: 403100 Loss: 0.004921918734908104  PSNR: 28.0650577545166
[TRAIN] Iter: 403200 Loss: 0.005242966581135988  PSNR: 27.64165687561035
[TRAIN] Iter: 403300 Loss: 0.004971377085894346  PSNR: 27.518766403198242
[TRAIN] Iter: 403400 Loss: 0.0053627463057637215  PSNR: 27.956878662109375
[TRAIN] Iter: 403500 Loss: 0.003953191451728344  PSNR: 29.589731216430664
[TRAIN] Iter: 403600 Loss: 0.004098951816558838  PSNR: 29.21839714050293
[TRAIN] Iter: 403700 Loss: 0.004517881199717522  PSNR: 28.274789810180664
[TRAIN] Iter: 403800 Loss: 0.004134505521506071  PSNR: 29.003934860229492
[TRAIN] Iter: 403900 Loss: 0.003705719718709588  PSNR: 29.84809684753418
[TRAIN] Iter: 404000 Loss: 0.004119281191378832  PSNR: 30.08692741394043
[TRAIN] Iter: 404100 Loss: 0.003670572768896818  PSNR: 30.08685302734375
[TRAIN] Iter: 404200 Loss: 0.004111911170184612  PSNR: 29.154071807861328
[TRAIN] Iter: 404300 Loss: 0.004689943976700306  PSNR: 27.991943359375
[TRAIN] Iter: 404400 Loss: 0.005074874963611364  PSNR: 27.092199325561523
[TRAIN] Iter: 404500 Loss: 0.004016959574073553  PSNR: 29.589494705200195
[TRAIN] Iter: 404600 Loss: 0.003669159486889839  PSNR: 30.225189208984375
[TRAIN] Iter: 404700 Loss: 0.003921831492334604  PSNR: 29.8125
[TRAIN] Iter: 404800 Loss: 0.003976726904511452  PSNR: 29.268373489379883
[TRAIN] Iter: 404900 Loss: 0.0042528072372078896  PSNR: 29.452482223510742
[TRAIN] Iter: 405000 Loss: 0.005028233863413334  PSNR: 27.894163131713867
[TRAIN] Iter: 405100 Loss: 0.004856630694121122  PSNR: 28.11714744567871
[TRAIN] Iter: 405200 Loss: 0.0046612867154181  PSNR: 27.65047836303711
[TRAIN] Iter: 405300 Loss: 0.0053307609632611275  PSNR: 27.76814079284668
[TRAIN] Iter: 405400 Loss: 0.00442362017929554  PSNR: 28.63149070739746
[TRAIN] Iter: 405500 Loss: 0.003874234389513731  PSNR: 30.10020637512207
[TRAIN] Iter: 405600 Loss: 0.0035740328021347523  PSNR: 30.062705993652344
[TRAIN] Iter: 405700 Loss: 0.004232275299727917  PSNR: 29.112689971923828
[TRAIN] Iter: 405800 Loss: 0.0050161755643785  PSNR: 27.516281127929688
[TRAIN] Iter: 405900 Loss: 0.005344073288142681  PSNR: 26.817434310913086
[TRAIN] Iter: 406000 Loss: 0.004973476752638817  PSNR: 28.033586502075195
[TRAIN] Iter: 406100 Loss: 0.0036771877203136683  PSNR: 30.135570526123047
[TRAIN] Iter: 406200 Loss: 0.004250572994351387  PSNR: 29.830936431884766
[TRAIN] Iter: 406300 Loss: 0.0030147540383040905  PSNR: 30.909069061279297
[TRAIN] Iter: 406400 Loss: 0.0035646131727844477  PSNR: 30.249820709228516
[TRAIN] Iter: 406500 Loss: 0.00514356279745698  PSNR: 27.572450637817383
[TRAIN] Iter: 406600 Loss: 0.0053431265987455845  PSNR: 27.971046447753906
[TRAIN] Iter: 406700 Loss: 0.005445299670100212  PSNR: 27.907846450805664
[TRAIN] Iter: 406800 Loss: 0.004656384699046612  PSNR: 28.588930130004883
[TRAIN] Iter: 406900 Loss: 0.0038185801822692156  PSNR: 28.667020797729492
[TRAIN] Iter: 407000 Loss: 0.0039557963609695435  PSNR: 28.97821617126465
[TRAIN] Iter: 407100 Loss: 0.005499498452991247  PSNR: 26.462291717529297
[TRAIN] Iter: 407200 Loss: 0.004630576819181442  PSNR: 28.222713470458984
[TRAIN] Iter: 407300 Loss: 0.004319948144257069  PSNR: 29.379070281982422
[TRAIN] Iter: 407400 Loss: 0.005009690299630165  PSNR: 28.06780433654785
[TRAIN] Iter: 407500 Loss: 0.004894264042377472  PSNR: 28.179916381835938
[TRAIN] Iter: 407600 Loss: 0.004854281898587942  PSNR: 27.687213897705078
[TRAIN] Iter: 407700 Loss: 0.005174449644982815  PSNR: 27.507488250732422
[TRAIN] Iter: 407800 Loss: 0.006435057148337364  PSNR: 26.017194747924805
[TRAIN] Iter: 407900 Loss: 0.004514126107096672  PSNR: 27.795578002929688
[TRAIN] Iter: 408000 Loss: 0.005706543102860451  PSNR: 26.36376953125
[TRAIN] Iter: 408100 Loss: 0.004054449498653412  PSNR: 29.83071517944336
[TRAIN] Iter: 408200 Loss: 0.005179961211979389  PSNR: 27.552656173706055
[TRAIN] Iter: 408300 Loss: 0.004887804388999939  PSNR: 28.014572143554688
[TRAIN] Iter: 408400 Loss: 0.005045048426836729  PSNR: 27.55893898010254
[TRAIN] Iter: 408500 Loss: 0.005190289579331875  PSNR: 27.268705368041992
[TRAIN] Iter: 408600 Loss: 0.005126392003148794  PSNR: 27.51675796508789
[TRAIN] Iter: 408700 Loss: 0.005407034419476986  PSNR: 27.00718879699707
[TRAIN] Iter: 408800 Loss: 0.0034683486446738243  PSNR: 29.374570846557617
[TRAIN] Iter: 408900 Loss: 0.0046930378302931786  PSNR: 28.243907928466797
[TRAIN] Iter: 409000 Loss: 0.00316664669662714  PSNR: 30.920297622680664
[TRAIN] Iter: 409100 Loss: 0.003173117060214281  PSNR: 30.533832550048828
[TRAIN] Iter: 409200 Loss: 0.004007795359939337  PSNR: 29.665836334228516
[TRAIN] Iter: 409300 Loss: 0.00449691666290164  PSNR: 28.283334732055664
[TRAIN] Iter: 409400 Loss: 0.003837483935058117  PSNR: 29.589948654174805
[TRAIN] Iter: 409500 Loss: 0.0036374926567077637  PSNR: 30.208749771118164
[TRAIN] Iter: 409600 Loss: 0.005070724990218878  PSNR: 28.02306365966797
[TRAIN] Iter: 409700 Loss: 0.004671352915465832  PSNR: 28.490190505981445
[TRAIN] Iter: 409800 Loss: 0.004951703827828169  PSNR: 27.840312957763672
[TRAIN] Iter: 409900 Loss: 0.003876553848385811  PSNR: 30.127197265625
Saved checkpoints at ./logs/TUT-KE101-nerf/410000.tar
[TRAIN] Iter: 410000 Loss: 0.004192115273326635  PSNR: 28.854774475097656
[TRAIN] Iter: 410100 Loss: 0.004272541031241417  PSNR: 28.352067947387695
[TRAIN] Iter: 410200 Loss: 0.004960622638463974  PSNR: 28.133220672607422
[TRAIN] Iter: 410300 Loss: 0.00396625604480505  PSNR: 29.777881622314453
[TRAIN] Iter: 410400 Loss: 0.0033558718860149384  PSNR: 30.90752410888672
[TRAIN] Iter: 410500 Loss: 0.0037569189444184303  PSNR: 29.81784439086914
[TRAIN] Iter: 410600 Loss: 0.003671979531645775  PSNR: 30.46385383605957
[TRAIN] Iter: 410700 Loss: 0.004279295913875103  PSNR: 29.24627685546875
[TRAIN] Iter: 410800 Loss: 0.0055117118172347546  PSNR: 27.92433738708496
[TRAIN] Iter: 410900 Loss: 0.0057968818582594395  PSNR: 27.676816940307617
[TRAIN] Iter: 411000 Loss: 0.005571948364377022  PSNR: 27.044883728027344
[TRAIN] Iter: 411100 Loss: 0.0038861765060573816  PSNR: 30.31367301940918
[TRAIN] Iter: 411200 Loss: 0.004230644553899765  PSNR: 27.879501342773438
[TRAIN] Iter: 411300 Loss: 0.0050456468015909195  PSNR: 27.274866104125977
[TRAIN] Iter: 411400 Loss: 0.005117122083902359  PSNR: 28.32832145690918
[TRAIN] Iter: 411500 Loss: 0.005505872890353203  PSNR: 26.969757080078125
[TRAIN] Iter: 411600 Loss: 0.004360683262348175  PSNR: 28.37978744506836
[TRAIN] Iter: 411700 Loss: 0.005481553263962269  PSNR: 27.40699005126953
[TRAIN] Iter: 411800 Loss: 0.004184860736131668  PSNR: 28.748069763183594
[TRAIN] Iter: 411900 Loss: 0.005269218236207962  PSNR: 27.779743194580078
[TRAIN] Iter: 412000 Loss: 0.005211531650274992  PSNR: 27.512149810791016
[TRAIN] Iter: 412100 Loss: 0.004071247763931751  PSNR: 29.75767707824707
[TRAIN] Iter: 412200 Loss: 0.003235223703086376  PSNR: 30.32286834716797
[TRAIN] Iter: 412300 Loss: 0.0035788281820714474  PSNR: 30.570941925048828
[TRAIN] Iter: 412400 Loss: 0.0060524167492985725  PSNR: 27.030508041381836
[TRAIN] Iter: 412500 Loss: 0.005270513705909252  PSNR: 27.451663970947266
[TRAIN] Iter: 412600 Loss: 0.003744275076314807  PSNR: 30.161909103393555
[TRAIN] Iter: 412700 Loss: 0.004362333565950394  PSNR: 28.185182571411133
[TRAIN] Iter: 412800 Loss: 0.005003758240491152  PSNR: 27.5345458984375
[TRAIN] Iter: 412900 Loss: 0.004716482944786549  PSNR: 27.942180633544922
[TRAIN] Iter: 413000 Loss: 0.004518571775406599  PSNR: 28.15826416015625
[TRAIN] Iter: 413100 Loss: 0.004791358020156622  PSNR: 28.03791046142578
[TRAIN] Iter: 413200 Loss: 0.005153759848326445  PSNR: 27.974565505981445
[TRAIN] Iter: 413300 Loss: 0.004921530839055777  PSNR: 27.169052124023438
[TRAIN] Iter: 413400 Loss: 0.0036940784193575382  PSNR: 29.785593032836914
[TRAIN] Iter: 413500 Loss: 0.00463353144004941  PSNR: 28.021028518676758
[TRAIN] Iter: 413600 Loss: 0.00474604032933712  PSNR: 29.36509132385254
[TRAIN] Iter: 413700 Loss: 0.0047387187369167805  PSNR: 28.080028533935547
[TRAIN] Iter: 413800 Loss: 0.004226003773510456  PSNR: 29.321657180786133
[TRAIN] Iter: 413900 Loss: 0.00377750676125288  PSNR: 29.73426055908203
[TRAIN] Iter: 414000 Loss: 0.003695165738463402  PSNR: 29.97090721130371
[TRAIN] Iter: 414100 Loss: 0.003982975147664547  PSNR: 28.45848846435547
[TRAIN] Iter: 414200 Loss: 0.00504936370998621  PSNR: 28.19055938720703
[TRAIN] Iter: 414300 Loss: 0.005112197250127792  PSNR: 27.934791564941406
[TRAIN] Iter: 414400 Loss: 0.005307169631123543  PSNR: 27.225587844848633
[TRAIN] Iter: 414500 Loss: 0.003594814334064722  PSNR: 29.609312057495117
[TRAIN] Iter: 414600 Loss: 0.003645343240350485  PSNR: 29.535749435424805
[TRAIN] Iter: 414700 Loss: 0.004459809977561235  PSNR: 27.615554809570312
[TRAIN] Iter: 414800 Loss: 0.004633620847016573  PSNR: 28.406997680664062
[TRAIN] Iter: 414900 Loss: 0.005083777941763401  PSNR: 28.24397850036621
[TRAIN] Iter: 415000 Loss: 0.005974414758384228  PSNR: 26.950075149536133
[TRAIN] Iter: 415100 Loss: 0.004629644099622965  PSNR: 27.944473266601562
[TRAIN] Iter: 415200 Loss: 0.004297703504562378  PSNR: 28.703222274780273
[TRAIN] Iter: 415300 Loss: 0.004563344642519951  PSNR: 28.406288146972656
[TRAIN] Iter: 415400 Loss: 0.004756790120154619  PSNR: 27.947425842285156
[TRAIN] Iter: 415500 Loss: 0.006055518984794617  PSNR: 26.306371688842773
[TRAIN] Iter: 415600 Loss: 0.005747862625867128  PSNR: 27.413516998291016
[TRAIN] Iter: 415700 Loss: 0.006218265742063522  PSNR: 27.127702713012695
[TRAIN] Iter: 415800 Loss: 0.0033876337110996246  PSNR: 29.585588455200195
[TRAIN] Iter: 415900 Loss: 0.005649582948535681  PSNR: 26.593708038330078
[TRAIN] Iter: 416000 Loss: 0.0047105299308896065  PSNR: 28.380212783813477
[TRAIN] Iter: 416100 Loss: 0.00451795756816864  PSNR: 29.232946395874023
[TRAIN] Iter: 416200 Loss: 0.003489544615149498  PSNR: 30.254621505737305
[TRAIN] Iter: 416300 Loss: 0.004537411034107208  PSNR: 28.242801666259766
[TRAIN] Iter: 416400 Loss: 0.005759182386100292  PSNR: 27.534250259399414
[TRAIN] Iter: 416500 Loss: 0.004595281556248665  PSNR: 29.27446937561035
[TRAIN] Iter: 416600 Loss: 0.0048804450780153275  PSNR: 27.8779296875
[TRAIN] Iter: 416700 Loss: 0.004290498793125153  PSNR: 28.23403549194336
[TRAIN] Iter: 416800 Loss: 0.0042806509882211685  PSNR: 29.354061126708984
[TRAIN] Iter: 416900 Loss: 0.005718034226447344  PSNR: 27.381145477294922
[TRAIN] Iter: 417000 Loss: 0.0038476584013551474  PSNR: 29.395471572875977
[TRAIN] Iter: 417100 Loss: 0.005735386162996292  PSNR: 27.121244430541992
[TRAIN] Iter: 417200 Loss: 0.00483379140496254  PSNR: 27.448274612426758
[TRAIN] Iter: 417300 Loss: 0.005059714429080486  PSNR: 27.285215377807617
[TRAIN] Iter: 417400 Loss: 0.0045349253341555595  PSNR: 28.629783630371094
[TRAIN] Iter: 417500 Loss: 0.004322088789194822  PSNR: 28.5870418548584
[TRAIN] Iter: 417600 Loss: 0.004082222003489733  PSNR: 28.530071258544922
[TRAIN] Iter: 417700 Loss: 0.005153028294444084  PSNR: 27.519054412841797
[TRAIN] Iter: 417800 Loss: 0.005861585959792137  PSNR: 26.121252059936523
[TRAIN] Iter: 417900 Loss: 0.0038189105689525604  PSNR: 29.80837631225586
[TRAIN] Iter: 418000 Loss: 0.004937806632369757  PSNR: 28.138423919677734
[TRAIN] Iter: 418100 Loss: 0.0038131242617964745  PSNR: 29.681163787841797
[TRAIN] Iter: 418200 Loss: 0.004190017469227314  PSNR: 28.217761993408203
[TRAIN] Iter: 418300 Loss: 0.0051882099360227585  PSNR: 27.993518829345703
[TRAIN] Iter: 418400 Loss: 0.004249098245054483  PSNR: 28.33902931213379
[TRAIN] Iter: 418500 Loss: 0.004120287951081991  PSNR: 29.95779037475586
[TRAIN] Iter: 418600 Loss: 0.005301499739289284  PSNR: 27.140926361083984
[TRAIN] Iter: 418700 Loss: 0.004527534823864698  PSNR: 28.601640701293945
[TRAIN] Iter: 418800 Loss: 0.006174370646476746  PSNR: 27.28794288635254
[TRAIN] Iter: 418900 Loss: 0.004523737356066704  PSNR: 27.180133819580078
[TRAIN] Iter: 419000 Loss: 0.0055184150114655495  PSNR: 27.450977325439453
[TRAIN] Iter: 419100 Loss: 0.0042530931532382965  PSNR: 28.294275283813477
[TRAIN] Iter: 419200 Loss: 0.004027219954878092  PSNR: 29.614505767822266
[TRAIN] Iter: 419300 Loss: 0.0032688104547560215  PSNR: 30.50873565673828
[TRAIN] Iter: 419400 Loss: 0.004751752130687237  PSNR: 27.886438369750977
[TRAIN] Iter: 419500 Loss: 0.0053436290472745895  PSNR: 27.702791213989258
[TRAIN] Iter: 419600 Loss: 0.0040786899626255035  PSNR: 29.8447208404541
[TRAIN] Iter: 419700 Loss: 0.004542290233075619  PSNR: 27.40389060974121
[TRAIN] Iter: 419800 Loss: 0.0037106440868228674  PSNR: 30.574199676513672
[TRAIN] Iter: 419900 Loss: 0.004170629195868969  PSNR: 28.57721710205078
Saved checkpoints at ./logs/TUT-KE101-nerf/420000.tar
[TRAIN] Iter: 420000 Loss: 0.004025611095130444  PSNR: 27.883655548095703
[TRAIN] Iter: 420100 Loss: 0.004150018561631441  PSNR: 28.1748046875
[TRAIN] Iter: 420200 Loss: 0.004586456343531609  PSNR: 28.03726577758789
[TRAIN] Iter: 420300 Loss: 0.005312295630574226  PSNR: 28.069229125976562
[TRAIN] Iter: 420400 Loss: 0.0034768879413604736  PSNR: 29.81437873840332
[TRAIN] Iter: 420500 Loss: 0.00447424128651619  PSNR: 29.422218322753906
[TRAIN] Iter: 420600 Loss: 0.003532693488523364  PSNR: 30.346830368041992
[TRAIN] Iter: 420700 Loss: 0.0039624739438295364  PSNR: 28.877208709716797
[TRAIN] Iter: 420800 Loss: 0.0039931535720825195  PSNR: 29.2706241607666
[TRAIN] Iter: 420900 Loss: 0.00539708137512207  PSNR: 27.418071746826172
[TRAIN] Iter: 421000 Loss: 0.005241498351097107  PSNR: 27.830942153930664
[TRAIN] Iter: 421100 Loss: 0.005829192232340574  PSNR: 26.455219268798828
[TRAIN] Iter: 421200 Loss: 0.004502483177930117  PSNR: 28.670217514038086
[TRAIN] Iter: 421300 Loss: 0.004724053665995598  PSNR: 28.737911224365234
[TRAIN] Iter: 421400 Loss: 0.004756688140332699  PSNR: 28.34766387939453
[TRAIN] Iter: 421500 Loss: 0.004838849417865276  PSNR: 27.99127769470215
[TRAIN] Iter: 421600 Loss: 0.00507638743147254  PSNR: 28.03120231628418
[TRAIN] Iter: 421700 Loss: 0.0034137549810111523  PSNR: 30.275297164916992
[TRAIN] Iter: 421800 Loss: 0.004925083369016647  PSNR: 28.21857452392578
[TRAIN] Iter: 421900 Loss: 0.004033747129142284  PSNR: 29.30950355529785
[TRAIN] Iter: 422000 Loss: 0.004831554368138313  PSNR: 27.848602294921875
[TRAIN] Iter: 422100 Loss: 0.003995261155068874  PSNR: 29.822925567626953
[TRAIN] Iter: 422200 Loss: 0.00401372741907835  PSNR: 29.77756690979004
[TRAIN] Iter: 422300 Loss: 0.003588925814256072  PSNR: 29.957630157470703
[TRAIN] Iter: 422400 Loss: 0.0038326801732182503  PSNR: 29.42650604248047
[TRAIN] Iter: 422500 Loss: 0.0049727787263691425  PSNR: 27.83968734741211
[TRAIN] Iter: 422600 Loss: 0.004135296680033207  PSNR: 29.609312057495117
[TRAIN] Iter: 422700 Loss: 0.00536974985152483  PSNR: 27.794719696044922
[TRAIN] Iter: 422800 Loss: 0.005029214080423117  PSNR: 27.89995002746582
[TRAIN] Iter: 422900 Loss: 0.005417856387794018  PSNR: 27.456829071044922
[TRAIN] Iter: 423000 Loss: 0.0037674650084227324  PSNR: 29.929555892944336
[TRAIN] Iter: 423100 Loss: 0.0054558333940804005  PSNR: 27.3455810546875
[TRAIN] Iter: 423200 Loss: 0.004856694955378771  PSNR: 27.389732360839844
[TRAIN] Iter: 423300 Loss: 0.004537477158010006  PSNR: 27.706464767456055
[TRAIN] Iter: 423400 Loss: 0.004846422933042049  PSNR: 27.74995994567871
[TRAIN] Iter: 423500 Loss: 0.005084584001451731  PSNR: 28.45033073425293
[TRAIN] Iter: 423600 Loss: 0.005410471465438604  PSNR: 27.685203552246094
[TRAIN] Iter: 423700 Loss: 0.004780984949320555  PSNR: 27.24616241455078
[TRAIN] Iter: 423800 Loss: 0.00661611370742321  PSNR: 26.603784561157227
[TRAIN] Iter: 423900 Loss: 0.004089822061359882  PSNR: 29.761978149414062
[TRAIN] Iter: 424000 Loss: 0.004790210165083408  PSNR: 27.150114059448242
[TRAIN] Iter: 424100 Loss: 0.003702485701069236  PSNR: 29.48215675354004
[TRAIN] Iter: 424200 Loss: 0.0044285510666668415  PSNR: 28.131391525268555
[TRAIN] Iter: 424300 Loss: 0.004015055950731039  PSNR: 29.052217483520508
[TRAIN] Iter: 424400 Loss: 0.00613168952986598  PSNR: 26.55794334411621
[TRAIN] Iter: 424500 Loss: 0.006164812948554754  PSNR: 27.093957901000977
[TRAIN] Iter: 424600 Loss: 0.004312537144869566  PSNR: 28.297576904296875
[TRAIN] Iter: 424700 Loss: 0.0033847508020699024  PSNR: 30.267364501953125
[TRAIN] Iter: 424800 Loss: 0.0038587143644690514  PSNR: 28.921340942382812
[TRAIN] Iter: 424900 Loss: 0.005098859779536724  PSNR: 28.108306884765625
[TRAIN] Iter: 425000 Loss: 0.004838939756155014  PSNR: 27.659029006958008
[TRAIN] Iter: 425100 Loss: 0.0046915328130126  PSNR: 27.662260055541992
[TRAIN] Iter: 425200 Loss: 0.005114311818033457  PSNR: 27.798381805419922
[TRAIN] Iter: 425300 Loss: 0.004769936203956604  PSNR: 27.99622917175293
[TRAIN] Iter: 425400 Loss: 0.004375068470835686  PSNR: 28.65475082397461
[TRAIN] Iter: 425500 Loss: 0.004620043095201254  PSNR: 28.45987892150879
[TRAIN] Iter: 425600 Loss: 0.0032849216368049383  PSNR: 30.918010711669922
[TRAIN] Iter: 425700 Loss: 0.004556115251034498  PSNR: 28.040393829345703
[TRAIN] Iter: 425800 Loss: 0.004003678448498249  PSNR: 28.318164825439453
[TRAIN] Iter: 425900 Loss: 0.004020509775727987  PSNR: 28.182159423828125
[TRAIN] Iter: 426000 Loss: 0.004408768378198147  PSNR: 30.268945693969727
[TRAIN] Iter: 426100 Loss: 0.0037764543667435646  PSNR: 29.883930206298828
[TRAIN] Iter: 426200 Loss: 0.004609811119735241  PSNR: 28.739835739135742
[TRAIN] Iter: 426300 Loss: 0.005916987545788288  PSNR: 27.047893524169922
[TRAIN] Iter: 426400 Loss: 0.005391303915530443  PSNR: 27.842784881591797
[TRAIN] Iter: 426500 Loss: 0.003951817285269499  PSNR: 30.449886322021484
[TRAIN] Iter: 426600 Loss: 0.004002015572041273  PSNR: 29.817569732666016
[TRAIN] Iter: 426700 Loss: 0.003359242808073759  PSNR: 30.338838577270508
[TRAIN] Iter: 426800 Loss: 0.005066574551165104  PSNR: 27.887493133544922
[TRAIN] Iter: 426900 Loss: 0.0038343286141753197  PSNR: 30.083093643188477
[TRAIN] Iter: 427000 Loss: 0.005264884792268276  PSNR: 27.658912658691406
[TRAIN] Iter: 427100 Loss: 0.003945312928408384  PSNR: 30.136978149414062
[TRAIN] Iter: 427200 Loss: 0.005070297047495842  PSNR: 27.88637351989746
[TRAIN] Iter: 427300 Loss: 0.005412150174379349  PSNR: 27.177932739257812
[TRAIN] Iter: 427400 Loss: 0.0038601707201451063  PSNR: 29.270715713500977
[TRAIN] Iter: 427500 Loss: 0.0038615770172327757  PSNR: 30.055959701538086
[TRAIN] Iter: 427600 Loss: 0.003562482073903084  PSNR: 29.53354835510254
[TRAIN] Iter: 427700 Loss: 0.004566555842757225  PSNR: 28.950111389160156
[TRAIN] Iter: 427800 Loss: 0.004674381576478481  PSNR: 28.447776794433594
[TRAIN] Iter: 427900 Loss: 0.006018444895744324  PSNR: 26.6047420501709
[TRAIN] Iter: 428000 Loss: 0.004538150504231453  PSNR: 28.128841400146484
[TRAIN] Iter: 428100 Loss: 0.0037236218340694904  PSNR: 29.693662643432617
[TRAIN] Iter: 428200 Loss: 0.004487060476094484  PSNR: 28.370630264282227
[TRAIN] Iter: 428300 Loss: 0.00483240932226181  PSNR: 28.488086700439453
[TRAIN] Iter: 428400 Loss: 0.005290434695780277  PSNR: 28.434728622436523
[TRAIN] Iter: 428500 Loss: 0.0037696335930377245  PSNR: 29.970029830932617
[TRAIN] Iter: 428600 Loss: 0.004582924302667379  PSNR: 27.931806564331055
[TRAIN] Iter: 428700 Loss: 0.004087128676474094  PSNR: 28.824026107788086
[TRAIN] Iter: 428800 Loss: 0.004953456576913595  PSNR: 27.884180068969727
[TRAIN] Iter: 428900 Loss: 0.0051246886141598225  PSNR: 27.87302017211914
[TRAIN] Iter: 429000 Loss: 0.004185015801340342  PSNR: 28.609493255615234
[TRAIN] Iter: 429100 Loss: 0.0038022627122700214  PSNR: 30.186073303222656
[TRAIN] Iter: 429200 Loss: 0.004410339519381523  PSNR: 28.525163650512695
[TRAIN] Iter: 429300 Loss: 0.004241590853780508  PSNR: 28.326648712158203
[TRAIN] Iter: 429400 Loss: 0.006504868157207966  PSNR: 26.88538360595703
[TRAIN] Iter: 429500 Loss: 0.005464034155011177  PSNR: 27.88588523864746
[TRAIN] Iter: 429600 Loss: 0.005276352632790804  PSNR: 27.303850173950195
[TRAIN] Iter: 429700 Loss: 0.005109678953886032  PSNR: 27.596445083618164
[TRAIN] Iter: 429800 Loss: 0.005314502865076065  PSNR: 27.878509521484375
[TRAIN] Iter: 429900 Loss: 0.005031193606555462  PSNR: 27.58294677734375
Saved checkpoints at ./logs/TUT-KE101-nerf/430000.tar
[TRAIN] Iter: 430000 Loss: 0.004570784047245979  PSNR: 29.303926467895508
[TRAIN] Iter: 430100 Loss: 0.00580889917910099  PSNR: 27.338054656982422
[TRAIN] Iter: 430200 Loss: 0.005089703015983105  PSNR: 27.72188377380371
[TRAIN] Iter: 430300 Loss: 0.005267555825412273  PSNR: 27.028841018676758
[TRAIN] Iter: 430400 Loss: 0.004689501598477364  PSNR: 28.164764404296875
[TRAIN] Iter: 430500 Loss: 0.0047934348694980145  PSNR: 27.906089782714844
[TRAIN] Iter: 430600 Loss: 0.004394591320306063  PSNR: 28.443458557128906
[TRAIN] Iter: 430700 Loss: 0.00504752341657877  PSNR: 28.134429931640625
[TRAIN] Iter: 430800 Loss: 0.00458989292383194  PSNR: 28.338960647583008
[TRAIN] Iter: 430900 Loss: 0.005027037113904953  PSNR: 28.036605834960938
[TRAIN] Iter: 431000 Loss: 0.0055779628455638885  PSNR: 27.602981567382812
[TRAIN] Iter: 431100 Loss: 0.004419390112161636  PSNR: 29.1428279876709
[TRAIN] Iter: 431200 Loss: 0.0047342199832201  PSNR: 29.30180549621582
[TRAIN] Iter: 431300 Loss: 0.0057038976810872555  PSNR: 26.6230411529541
[TRAIN] Iter: 431400 Loss: 0.004236388485878706  PSNR: 28.00694465637207
[TRAIN] Iter: 431500 Loss: 0.003787124063819647  PSNR: 29.554933547973633
[TRAIN] Iter: 431600 Loss: 0.004630920477211475  PSNR: 27.525867462158203
[TRAIN] Iter: 431700 Loss: 0.00481138750910759  PSNR: 28.515350341796875
[TRAIN] Iter: 431800 Loss: 0.004759758710861206  PSNR: 27.4935359954834
[TRAIN] Iter: 431900 Loss: 0.003952537663280964  PSNR: 28.299102783203125
[TRAIN] Iter: 432000 Loss: 0.003519359277561307  PSNR: 29.92525291442871
[TRAIN] Iter: 432100 Loss: 0.0037185288965702057  PSNR: 29.413633346557617
[TRAIN] Iter: 432200 Loss: 0.004167390987277031  PSNR: 28.917768478393555
[TRAIN] Iter: 432300 Loss: 0.004930395632982254  PSNR: 27.473934173583984
[TRAIN] Iter: 432400 Loss: 0.005033044144511223  PSNR: 27.646940231323242
[TRAIN] Iter: 432500 Loss: 0.004542618989944458  PSNR: 28.006805419921875
[TRAIN] Iter: 432600 Loss: 0.00492536835372448  PSNR: 27.722570419311523
[TRAIN] Iter: 432700 Loss: 0.004005793947726488  PSNR: 30.543214797973633
[TRAIN] Iter: 432800 Loss: 0.00578603520989418  PSNR: 27.28668212890625
[TRAIN] Iter: 432900 Loss: 0.004909321665763855  PSNR: 27.688880920410156
[TRAIN] Iter: 433000 Loss: 0.0036345277912914753  PSNR: 30.630632400512695
[TRAIN] Iter: 433100 Loss: 0.004970765206962824  PSNR: 27.871889114379883
[TRAIN] Iter: 433200 Loss: 0.004304529167711735  PSNR: 29.40700912475586
[TRAIN] Iter: 433300 Loss: 0.004761672578752041  PSNR: 28.009632110595703
[TRAIN] Iter: 433400 Loss: 0.005103635601699352  PSNR: 27.746044158935547
[TRAIN] Iter: 433500 Loss: 0.003688354277983308  PSNR: 30.460247039794922
[TRAIN] Iter: 433600 Loss: 0.00468172412365675  PSNR: 27.66181755065918
[TRAIN] Iter: 433700 Loss: 0.0028695003129541874  PSNR: 30.901315689086914
[TRAIN] Iter: 433800 Loss: 0.0037240826059132814  PSNR: 29.788434982299805
[TRAIN] Iter: 433900 Loss: 0.005574129056185484  PSNR: 27.68358612060547
[TRAIN] Iter: 434000 Loss: 0.0056741200387477875  PSNR: 27.669509887695312
[TRAIN] Iter: 434100 Loss: 0.003874432295560837  PSNR: 30.039289474487305
[TRAIN] Iter: 434200 Loss: 0.0052562919445335865  PSNR: 27.560636520385742
[TRAIN] Iter: 434300 Loss: 0.00361288757994771  PSNR: 30.45638084411621
[TRAIN] Iter: 434400 Loss: 0.005569471977651119  PSNR: 27.388813018798828
[TRAIN] Iter: 434500 Loss: 0.004655761178582907  PSNR: 27.848161697387695
[TRAIN] Iter: 434600 Loss: 0.0048506292514503  PSNR: 28.023086547851562
[TRAIN] Iter: 434700 Loss: 0.005059804767370224  PSNR: 27.55901527404785
[TRAIN] Iter: 434800 Loss: 0.004707404877990484  PSNR: 28.304468154907227
[TRAIN] Iter: 434900 Loss: 0.0047350432723760605  PSNR: 27.407682418823242
[TRAIN] Iter: 435000 Loss: 0.004600992891937494  PSNR: 28.837574005126953
[TRAIN] Iter: 435100 Loss: 0.005112392827868462  PSNR: 27.62265396118164
[TRAIN] Iter: 435200 Loss: 0.003585668746381998  PSNR: 30.08147621154785
[TRAIN] Iter: 435300 Loss: 0.004232226870954037  PSNR: 28.630565643310547
[TRAIN] Iter: 435400 Loss: 0.004205990117043257  PSNR: 28.092309951782227
[TRAIN] Iter: 435500 Loss: 0.005759879481047392  PSNR: 26.383821487426758
[TRAIN] Iter: 435600 Loss: 0.004955603741109371  PSNR: 28.522138595581055
[TRAIN] Iter: 435700 Loss: 0.0032779963221400976  PSNR: 30.49850082397461
[TRAIN] Iter: 435800 Loss: 0.0035957812797278166  PSNR: 30.206195831298828
[TRAIN] Iter: 435900 Loss: 0.005047071725130081  PSNR: 28.412723541259766
[TRAIN] Iter: 436000 Loss: 0.004627774003893137  PSNR: 27.83060073852539
[TRAIN] Iter: 436100 Loss: 0.004263990558683872  PSNR: 28.57724380493164
[TRAIN] Iter: 436200 Loss: 0.004410815425217152  PSNR: 29.05459213256836
[TRAIN] Iter: 436300 Loss: 0.004596505779772997  PSNR: 28.356237411499023
[TRAIN] Iter: 436400 Loss: 0.005006909370422363  PSNR: 27.943449020385742
[TRAIN] Iter: 436500 Loss: 0.0036886739544570446  PSNR: 29.680925369262695
[TRAIN] Iter: 436600 Loss: 0.0050564915873110294  PSNR: 27.52037811279297
[TRAIN] Iter: 436700 Loss: 0.0044966768473386765  PSNR: 29.168977737426758
[TRAIN] Iter: 436800 Loss: 0.003385326359421015  PSNR: 30.459257125854492
[TRAIN] Iter: 436900 Loss: 0.004704087041318417  PSNR: 28.906381607055664
[TRAIN] Iter: 437000 Loss: 0.004603982903063297  PSNR: 28.69696617126465
[TRAIN] Iter: 437100 Loss: 0.004908958449959755  PSNR: 27.770748138427734
[TRAIN] Iter: 437200 Loss: 0.0041792914271354675  PSNR: 28.28413963317871
[TRAIN] Iter: 437300 Loss: 0.004417601972818375  PSNR: 28.10181999206543
[TRAIN] Iter: 437400 Loss: 0.004275414161384106  PSNR: 29.306821823120117
[TRAIN] Iter: 437500 Loss: 0.004609772004187107  PSNR: 28.934852600097656
[TRAIN] Iter: 437600 Loss: 0.004637959413230419  PSNR: 28.4017391204834
[TRAIN] Iter: 437700 Loss: 0.0043025859631598  PSNR: 28.49286460876465
[TRAIN] Iter: 437800 Loss: 0.004272725433111191  PSNR: 27.998090744018555
[TRAIN] Iter: 437900 Loss: 0.005035259760916233  PSNR: 28.534692764282227
[TRAIN] Iter: 438000 Loss: 0.004212013445794582  PSNR: 28.696609497070312
[TRAIN] Iter: 438100 Loss: 0.0046975696459412575  PSNR: 28.10921859741211
[TRAIN] Iter: 438200 Loss: 0.004705473780632019  PSNR: 27.95657730102539
[TRAIN] Iter: 438300 Loss: 0.005037031602114439  PSNR: 27.54450798034668
[TRAIN] Iter: 438400 Loss: 0.004355347715318203  PSNR: 29.71781349182129
[TRAIN] Iter: 438500 Loss: 0.004603937268257141  PSNR: 27.675567626953125
[TRAIN] Iter: 438600 Loss: 0.00530504435300827  PSNR: 27.3102970123291
[TRAIN] Iter: 438700 Loss: 0.0038425978273153305  PSNR: 28.943689346313477
[TRAIN] Iter: 438800 Loss: 0.0049922652542591095  PSNR: 27.875429153442383
[TRAIN] Iter: 438900 Loss: 0.0037128524854779243  PSNR: 29.874868392944336
[TRAIN] Iter: 439000 Loss: 0.005395563319325447  PSNR: 27.713693618774414
[TRAIN] Iter: 439100 Loss: 0.004482433199882507  PSNR: 28.451595306396484
[TRAIN] Iter: 439200 Loss: 0.00531449168920517  PSNR: 28.152273178100586
[TRAIN] Iter: 439300 Loss: 0.005164974834769964  PSNR: 26.942790985107422
[TRAIN] Iter: 439400 Loss: 0.004279869608581066  PSNR: 29.409564971923828
[TRAIN] Iter: 439500 Loss: 0.004908007103949785  PSNR: 28.14429473876953
[TRAIN] Iter: 439600 Loss: 0.003988705575466156  PSNR: 28.895748138427734
[TRAIN] Iter: 439700 Loss: 0.0031369596254080534  PSNR: 30.687633514404297
[TRAIN] Iter: 439800 Loss: 0.006142657715827227  PSNR: 26.69796371459961
[TRAIN] Iter: 439900 Loss: 0.0038795177824795246  PSNR: 29.841827392578125
Saved checkpoints at ./logs/TUT-KE101-nerf/440000.tar
[TRAIN] Iter: 440000 Loss: 0.004291821271181107  PSNR: 29.44835090637207
[TRAIN] Iter: 440100 Loss: 0.004847836215049028  PSNR: 27.794694900512695
[TRAIN] Iter: 440200 Loss: 0.004143808502703905  PSNR: 29.4055118560791
[TRAIN] Iter: 440300 Loss: 0.0058486261405050755  PSNR: 27.388080596923828
[TRAIN] Iter: 440400 Loss: 0.0038470483850687742  PSNR: 30.00926399230957
[TRAIN] Iter: 440500 Loss: 0.0045390683226287365  PSNR: 28.1707820892334
[TRAIN] Iter: 440600 Loss: 0.005589755717664957  PSNR: 27.249183654785156
[TRAIN] Iter: 440700 Loss: 0.004648724105209112  PSNR: 28.80491828918457
[TRAIN] Iter: 440800 Loss: 0.0034924759529531  PSNR: 30.145660400390625
[TRAIN] Iter: 440900 Loss: 0.00400769617408514  PSNR: 29.69395637512207
[TRAIN] Iter: 441000 Loss: 0.004012752324342728  PSNR: 28.45969009399414
[TRAIN] Iter: 441100 Loss: 0.004066522233188152  PSNR: 29.448827743530273
[TRAIN] Iter: 441200 Loss: 0.005280840210616589  PSNR: 27.372690200805664
[TRAIN] Iter: 441300 Loss: 0.0066188969649374485  PSNR: 26.815799713134766
[TRAIN] Iter: 441400 Loss: 0.0038108457811176777  PSNR: 31.041471481323242
[TRAIN] Iter: 441500 Loss: 0.004307649563997984  PSNR: 28.79996681213379
[TRAIN] Iter: 441600 Loss: 0.004132288508117199  PSNR: 29.76880645751953
[TRAIN] Iter: 441700 Loss: 0.0036856126971542835  PSNR: 29.571331024169922
[TRAIN] Iter: 441800 Loss: 0.003904705634340644  PSNR: 29.600595474243164
[TRAIN] Iter: 441900 Loss: 0.004261502530425787  PSNR: 28.46409034729004
[TRAIN] Iter: 442000 Loss: 0.0044282423332333565  PSNR: 27.85108757019043
[TRAIN] Iter: 442100 Loss: 0.005250374786555767  PSNR: 27.294816970825195
[TRAIN] Iter: 442200 Loss: 0.005825690925121307  PSNR: 27.06509017944336
[TRAIN] Iter: 442300 Loss: 0.004838696680963039  PSNR: 27.681947708129883
[TRAIN] Iter: 442400 Loss: 0.003565405495464802  PSNR: 30.100326538085938
[TRAIN] Iter: 442500 Loss: 0.004594999365508556  PSNR: 27.77664566040039
[TRAIN] Iter: 442600 Loss: 0.00519151147454977  PSNR: 27.15522003173828
[TRAIN] Iter: 442700 Loss: 0.005396287888288498  PSNR: 27.576581954956055
[TRAIN] Iter: 442800 Loss: 0.004521374125033617  PSNR: 27.754230499267578
[TRAIN] Iter: 442900 Loss: 0.004338852129876614  PSNR: 28.224750518798828
[TRAIN] Iter: 443000 Loss: 0.005231013987213373  PSNR: 28.185043334960938
[TRAIN] Iter: 443100 Loss: 0.004992157686501741  PSNR: 28.0928897857666
[TRAIN] Iter: 443200 Loss: 0.0045821466483175755  PSNR: 28.194379806518555
[TRAIN] Iter: 443300 Loss: 0.004164958838373423  PSNR: 30.5079345703125
[TRAIN] Iter: 443400 Loss: 0.0056561799719929695  PSNR: 27.020553588867188
[TRAIN] Iter: 443500 Loss: 0.004095357842743397  PSNR: 29.288009643554688
[TRAIN] Iter: 443600 Loss: 0.005011370405554771  PSNR: 27.25326156616211
[TRAIN] Iter: 443700 Loss: 0.005455584265291691  PSNR: 27.208404541015625
[TRAIN] Iter: 443800 Loss: 0.004485001787543297  PSNR: 28.424453735351562
[TRAIN] Iter: 443900 Loss: 0.0050397999584674835  PSNR: 28.06496810913086
[TRAIN] Iter: 444000 Loss: 0.003594093257561326  PSNR: 29.823259353637695
[TRAIN] Iter: 444100 Loss: 0.00555947981774807  PSNR: 27.91257095336914
[TRAIN] Iter: 444200 Loss: 0.0036209439858794212  PSNR: 29.945478439331055
[TRAIN] Iter: 444300 Loss: 0.0038304240442812443  PSNR: 28.968914031982422
[TRAIN] Iter: 444400 Loss: 0.004092089831829071  PSNR: 28.685211181640625
[TRAIN] Iter: 444500 Loss: 0.004678488243371248  PSNR: 27.91097068786621
[TRAIN] Iter: 444600 Loss: 0.005873094312846661  PSNR: 27.19610023498535
[TRAIN] Iter: 444700 Loss: 0.003746938658878207  PSNR: 28.649980545043945
[TRAIN] Iter: 444800 Loss: 0.00506430771201849  PSNR: 27.332740783691406
[TRAIN] Iter: 444900 Loss: 0.005699045956134796  PSNR: 26.966819763183594
[TRAIN] Iter: 445000 Loss: 0.005760748405009508  PSNR: 26.792272567749023
[TRAIN] Iter: 445100 Loss: 0.005936026573181152  PSNR: 27.036218643188477
[TRAIN] Iter: 445200 Loss: 0.004008794669061899  PSNR: 28.74362564086914
[TRAIN] Iter: 445300 Loss: 0.005379510577768087  PSNR: 27.478134155273438
[TRAIN] Iter: 445400 Loss: 0.004361297469586134  PSNR: 29.915912628173828
[TRAIN] Iter: 445500 Loss: 0.004187581595033407  PSNR: 29.783527374267578
[TRAIN] Iter: 445600 Loss: 0.004385099746286869  PSNR: 29.49390411376953
[TRAIN] Iter: 445700 Loss: 0.005442567635327578  PSNR: 27.281654357910156
[TRAIN] Iter: 445800 Loss: 0.004362332634627819  PSNR: 28.56696319580078
[TRAIN] Iter: 445900 Loss: 0.0045018489472568035  PSNR: 28.26862335205078
[TRAIN] Iter: 446000 Loss: 0.005034155678004026  PSNR: 27.76372528076172
[TRAIN] Iter: 446100 Loss: 0.004950980190187693  PSNR: 27.795316696166992
[TRAIN] Iter: 446200 Loss: 0.004428866319358349  PSNR: 28.339414596557617
[TRAIN] Iter: 446300 Loss: 0.004039593040943146  PSNR: 28.968414306640625
[TRAIN] Iter: 446400 Loss: 0.005984648130834103  PSNR: 27.042964935302734
[TRAIN] Iter: 446500 Loss: 0.004188622813671827  PSNR: 28.67700958251953
[TRAIN] Iter: 446600 Loss: 0.005860809702426195  PSNR: 27.141176223754883
[TRAIN] Iter: 446700 Loss: 0.003939413465559483  PSNR: 28.63672637939453
[TRAIN] Iter: 446800 Loss: 0.004661403130739927  PSNR: 28.22675895690918
[TRAIN] Iter: 446900 Loss: 0.0044901203364133835  PSNR: 29.185203552246094
[TRAIN] Iter: 447000 Loss: 0.0046787443570792675  PSNR: 27.711505889892578
[TRAIN] Iter: 447100 Loss: 0.004297683015465736  PSNR: 28.602502822875977
[TRAIN] Iter: 447200 Loss: 0.004736939445137978  PSNR: 27.751590728759766
[TRAIN] Iter: 447300 Loss: 0.0037118818145245314  PSNR: 30.05674171447754
[TRAIN] Iter: 447400 Loss: 0.005615332629531622  PSNR: 27.491024017333984
[TRAIN] Iter: 447500 Loss: 0.005698993802070618  PSNR: 27.483781814575195
[TRAIN] Iter: 447600 Loss: 0.0049177962355315685  PSNR: 27.807781219482422
[TRAIN] Iter: 447700 Loss: 0.0038426569662988186  PSNR: 28.727954864501953
[TRAIN] Iter: 447800 Loss: 0.0037378808483481407  PSNR: 30.154918670654297
[TRAIN] Iter: 447900 Loss: 0.004812875296920538  PSNR: 27.914934158325195
[TRAIN] Iter: 448000 Loss: 0.0036799877416342497  PSNR: 29.562328338623047
[TRAIN] Iter: 448100 Loss: 0.006051697768270969  PSNR: 27.37456703186035
[TRAIN] Iter: 448200 Loss: 0.004059146158397198  PSNR: 28.378456115722656
[TRAIN] Iter: 448300 Loss: 0.003851818386465311  PSNR: 28.44162368774414
[TRAIN] Iter: 448400 Loss: 0.005355316214263439  PSNR: 27.87254524230957
[TRAIN] Iter: 448500 Loss: 0.004858161322772503  PSNR: 27.83202362060547
[TRAIN] Iter: 448600 Loss: 0.0044523607939481735  PSNR: 29.123733520507812
[TRAIN] Iter: 448700 Loss: 0.004790360573679209  PSNR: 28.047029495239258
[TRAIN] Iter: 448800 Loss: 0.00486453901976347  PSNR: 27.517044067382812
[TRAIN] Iter: 448900 Loss: 0.0044457269832491875  PSNR: 28.911840438842773
[TRAIN] Iter: 449000 Loss: 0.004495907574892044  PSNR: 27.833593368530273
[TRAIN] Iter: 449100 Loss: 0.0038058613426983356  PSNR: 29.183862686157227
[TRAIN] Iter: 449200 Loss: 0.0053597320802509785  PSNR: 27.59136962890625
[TRAIN] Iter: 449300 Loss: 0.004298805724829435  PSNR: 28.71757698059082
[TRAIN] Iter: 449400 Loss: 0.005019929725676775  PSNR: 27.4693546295166
[TRAIN] Iter: 449500 Loss: 0.00540132075548172  PSNR: 27.059444427490234
[TRAIN] Iter: 449600 Loss: 0.0031435946002602577  PSNR: 30.624019622802734
[TRAIN] Iter: 449700 Loss: 0.0040490031242370605  PSNR: 28.543739318847656
[TRAIN] Iter: 449800 Loss: 0.005153302568942308  PSNR: 27.710681915283203
[TRAIN] Iter: 449900 Loss: 0.005198586266487837  PSNR: 28.22024154663086
Saved checkpoints at ./logs/TUT-KE101-nerf/450000.tar
0 0.0004258155822753906
torch.Size([320, 640, 3]) torch.Size([320, 640])
1 14.12532377243042
2 14.140990257263184
3 14.226874351501465
4 14.16805386543274
5 14.170272588729858
6 14.352502822875977
7 14.19330644607544
8 13.976845979690552
9 14.182944774627686
10 14.117454528808594
11 14.176511526107788
12 14.151004314422607
13 14.353681325912476
14 14.078632593154907
15 14.07768201828003
16 14.159869194030762
17 14.149710893630981
18 14.132013320922852
19 14.239549398422241
20 14.261855363845825
21 14.044719696044922
22 14.213295936584473
23 14.106586933135986
24 14.194772243499756
25 14.127744197845459
26 14.207616329193115
27 14.218228340148926
28 14.099923372268677
29 14.228657722473145
30 14.097238063812256
31 14.245960474014282
32 14.172136068344116
33 14.17058801651001
34 14.12410283088684
35 14.169108867645264
36 14.23497223854065
37 14.032938003540039
38 14.170812129974365
39 14.097562789916992
40 14.215423345565796
41 14.12826132774353
42 14.140797138214111
43 14.168854475021362
44 14.076478242874146
45 14.205464839935303
46 14.125534057617188
47 14.249868869781494
48 14.131967782974243
49 14.202720642089844
50 14.12214708328247
51 14.169151067733765
52 14.261455297470093
53 14.096339702606201
54 14.237442016601562
55 14.095189332962036
56 14.237007141113281
57 14.11763620376587
58 14.219707727432251
59 14.168995141983032
60 14.076120615005493
61 14.075945377349854
62 14.243089199066162
63 14.12962532043457
64 14.216744422912598
65 14.303288698196411
66 14.166190385818481
67 14.081455945968628
68 14.201181411743164
69 14.097930669784546
70 14.143622159957886
71 14.215863943099976
72 14.351773500442505
73 14.105498790740967
74 14.06384825706482
75 14.16053056716919
76 14.219127178192139
77 14.181621551513672
78 14.175257444381714
79 14.203397989273071
80 14.167840957641602
81 14.273512840270996
82 14.20942211151123
83 14.170766830444336
84 13.839226484298706
85 14.193085670471191
86 14.117800951004028
87 14.146711587905884
88 14.285550117492676
89 14.126211881637573
90 14.083949089050293
91 14.199776649475098
92 14.124292373657227
93 14.136792182922363
94 14.197770833969116
95 14.322289943695068
96 14.072397470474243
97 14.136323928833008
98 14.144645690917969
99 14.274629592895508
100 14.183070421218872
101 14.082214832305908
102 14.127742052078247
103 14.250556468963623
104 14.190712690353394
105 14.183044195175171
106 14.31895637512207
107 14.07954716682434
108 14.138504028320312
109 14.180728912353516
110 14.141157627105713
111 14.282694816589355
112 14.065783739089966
113 14.233372926712036
114 14.03376317024231
115 14.20189356803894
116 14.161924600601196
117 14.114813566207886
118 14.171131610870361
119 14.077171564102173
Done, saving (120, 320, 640, 3) (120, 320, 640)
extras:{'raw': tensor([[[ 2.1705e+00,  2.9488e+00,  6.0432e+00, -5.7102e+01],
         [ 3.3000e-01,  1.3301e-01, -1.2657e-01, -4.5920e+01],
         [ 3.4776e-01,  2.1671e-01,  7.2083e-02, -4.9036e+01],
         ...,
         [-2.3003e+00, -2.0081e+00, -9.5509e+00,  6.8667e+02],
         [-2.8361e+00, -2.1173e+00, -9.4827e+00,  6.7710e+02],
         [-3.5981e+00, -2.1842e+00, -9.1080e+00,  6.6645e+02]],

        [[-1.0973e+00, -4.3837e-01,  7.3055e-01, -4.3949e+01],
         [-1.0471e+00, -7.0556e-01,  4.2205e-02, -4.1022e+01],
         [-1.0663e+00, -8.8609e-01, -3.9333e-01, -2.0841e+01],
         ...,
         [-2.6568e+01, -2.5477e+01, -2.4974e+01, -3.8326e+02],
         [-2.6395e+01, -2.5517e+01, -2.5319e+01, -3.9684e+02],
         [-2.2729e+01, -2.2507e+01, -2.3196e+01, -4.4383e+02]],

        [[-1.2568e-01,  6.5476e-01,  2.2644e+00, -6.2406e+01],
         [-1.9889e-01, -1.6504e-01, -3.2610e-01, -3.1307e+01],
         [-1.8315e+00, -1.8278e+00, -1.8080e+00, -3.9228e+01],
         ...,
         [-3.2995e+01, -3.3335e+01, -3.4701e+01, -3.1668e+02],
         [-3.2031e+01, -3.1703e+01, -3.2475e+01, -2.7924e+02],
         [-3.2960e+01, -3.2835e+01, -3.3272e+01, -2.9123e+02]],

        ...,

        [[-4.2552e-02,  3.9617e-03,  5.5308e-02, -6.0411e+01],
         [ 2.4544e-01, -1.9251e-01, -1.0407e+00, -1.4773e+01],
         [ 8.1139e-02, -2.2994e-01, -8.3013e-01, -8.5188e+00],
         ...,
         [ 4.8612e-01, -3.3745e+00, -1.2284e+01, -2.5537e+02],
         [-1.1809e+00, -4.6851e+00, -1.2846e+01, -2.7941e+02],
         [-7.8754e+00, -1.0573e+01, -1.5980e+01, -1.4580e+02]],

        [[-5.0847e-01,  5.6893e-01,  2.9260e+00, -6.5301e+01],
         [-3.1024e+00, -3.4908e+00, -4.6922e+00, -3.5650e+01],
         [-8.7976e-01, -1.3695e+00, -2.3910e+00, -2.0132e+01],
         ...,
         [-1.2207e+01, -1.4893e+01, -2.0997e+01, -4.4707e+02],
         [-1.2691e+01, -1.4664e+01, -1.9609e+01, -4.2390e+02],
         [-1.2328e+01, -1.4245e+01, -1.8827e+01, -3.4396e+02]],

        [[-3.8793e+00, -4.3556e+00, -5.4144e+00, -6.7176e+01],
         [-6.2275e-01, -5.2404e-01, -8.7462e-01, -1.9257e+01],
         [-5.1191e-01, -5.2217e-01, -5.9781e-01,  2.3688e+01],
         ...,
         [-4.3674e+00, -5.4811e+00, -7.3095e+00,  6.6928e+01],
         [-4.7221e+00, -6.0345e+00, -8.1123e+00,  6.4623e+01],
         [-2.6811e+00, -3.8479e+00, -5.8092e+00,  6.4466e+01]]],
       grad_fn=<ReshapeAliasBackward0>), 'rgb0': tensor([[0.4756, 0.4203, 0.3413],
        [0.4142, 0.3905, 0.3589],
        [0.1000, 0.0970, 0.0931],
        ...,
        [0.5337, 0.4249, 0.2152],
        [0.4287, 0.2972, 0.1181],
        [0.3385, 0.3366, 0.3090]], grad_fn=<ReshapeAliasBackward0>), 'disp0': tensor([132.1860,  43.6927,  32.0575,  ..., 367.2824,  67.6151,  51.8888],
       grad_fn=<ReshapeAliasBackward0>), 'acc0': tensor([1., 1., 1.,  ..., 1., 1., 1.], grad_fn=<ReshapeAliasBackward0>), 'z_std': tensor([0.0022, 0.0036, 0.0022,  ..., 0.2734, 0.0925, 0.0022])}
0 0.0004696846008300781
torch.Size([320, 640, 3]) torch.Size([320, 640])
1 14.083827257156372
2 14.294838190078735
3 14.133486986160278
4 14.240103244781494
5 14.166029214859009
6 14.165022850036621
7 14.183077335357666
8 14.077414512634277
9 14.28318452835083
10 14.095576524734497
11 14.269463062286377
12 14.132569313049316
13 14.208586931228638
14 14.166120767593384
15 14.173209190368652
16 14.237832069396973
17 14.096168518066406
18 14.23270320892334
19 14.058056354522705
20 14.221986293792725
21 14.084053993225098
22 14.232688665390015
23 14.215129137039185
24 14.078070163726807
25 14.288365125656128
26 14.090999364852905
27 14.867603540420532
28 16.06173014640808
29 16.251697540283203
30 16.037039756774902
31 16.25842523574829
32 16.03895592689514
33 16.25723671913147
34 16.036017179489136
35 16.258325576782227
36 15.965627431869507
37 16.23479676246643
38 16.01788830757141
39 16.23958969116211
40 16.082441091537476
41 16.233375549316406
42 16.03191590309143
43 16.25177001953125
44 15.98342752456665
45 16.202483415603638
46 16.042162656784058
47 16.263394594192505
48 16.039632558822632
49 16.218483686447144
50 16.03260040283203
51 16.235613584518433
52 16.02902388572693
53 16.46327543258667
54 16.435702800750732
55 16.728227615356445
56 16.461373805999756
57 16.47153115272522
58 16.049946784973145
59 16.249427318572998
60 16.070618867874146
61 16.26541829109192
62 16.067984580993652
63 16.288981914520264
64 15.963602304458618
65 16.21588373184204
66 16.02352285385132
67 16.22534418106079
68 16.029690265655518
69 16.219741106033325
70 16.037357807159424
71 16.250199556350708
72 16.076282739639282
73 16.277941942214966
74 16.054134368896484
75 16.171961069107056
76 16.00895071029663
77 16.21391248703003
78 16.021535634994507
79 16.30326008796692
80 16.046412706375122
81 16.26727533340454
82 16.05595850944519
83 16.165215015411377
84 16.00456666946411
85 16.21197199821472
86 16.057339906692505
87 16.249915838241577
88 16.022530794143677
89 16.28154969215393
90 16.023401021957397
91 16.234692096710205
92 16.05180287361145
93 16.28392481803894
94 15.940473079681396
95 16.264745473861694
96 16.009499073028564
97 16.262122631072998
98 16.02505087852478
99 16.27611994743347
100 16.01152539253235
101 16.280317783355713
102 15.925959348678589
103 16.225876331329346
104 16.022518634796143
105 16.26387643814087
106 15.991904497146606
107 16.269819498062134
108 16.006626844406128
109 16.243069648742676
110 15.976802110671997
111 16.281962156295776
112 16.047475576400757
113 16.362249851226807
114 16.395830631256104
115 16.730732440948486
116 16.299152851104736
117 16.315858364105225
118 16.0238356590271
119 16.318150758743286
test poses shape torch.Size([4, 3, 4])
0 0.0006685256958007812
torch.Size([320, 640, 3]) torch.Size([320, 640])
1 16.26324152946472
2 16.03591823577881
3 16.31816554069519
Saved test set
[TRAIN] Iter: 450000 Loss: 0.004629339091479778  PSNR: 28.46874237060547
[TRAIN] Iter: 450100 Loss: 0.005428145173937082  PSNR: 27.0279598236084
[TRAIN] Iter: 450200 Loss: 0.0059449034743011  PSNR: 27.26984405517578
[TRAIN] Iter: 450300 Loss: 0.005020280368626118  PSNR: 27.778535842895508
[TRAIN] Iter: 450400 Loss: 0.004312325268983841  PSNR: 28.25774574279785
[TRAIN] Iter: 450500 Loss: 0.004960180260241032  PSNR: 27.31423568725586
[TRAIN] Iter: 450600 Loss: 0.005561559461057186  PSNR: 27.591718673706055
[TRAIN] Iter: 450700 Loss: 0.005301604978740215  PSNR: 27.31764793395996
[TRAIN] Iter: 450800 Loss: 0.004930245690047741  PSNR: 27.472976684570312
[TRAIN] Iter: 450900 Loss: 0.00516206631436944  PSNR: 27.943466186523438
[TRAIN] Iter: 451000 Loss: 0.004477451555430889  PSNR: 28.06119155883789
[TRAIN] Iter: 451100 Loss: 0.004415139090269804  PSNR: 28.250505447387695
[TRAIN] Iter: 451200 Loss: 0.003625414567068219  PSNR: 29.63786506652832
[TRAIN] Iter: 451300 Loss: 0.004666321910917759  PSNR: 28.207427978515625
[TRAIN] Iter: 451400 Loss: 0.0031419810838997364  PSNR: 29.8302001953125
[TRAIN] Iter: 451500 Loss: 0.003933289088308811  PSNR: 29.563905715942383
[TRAIN] Iter: 451600 Loss: 0.0045337313786149025  PSNR: 28.632915496826172
[TRAIN] Iter: 451700 Loss: 0.005255601834505796  PSNR: 28.087045669555664
[TRAIN] Iter: 451800 Loss: 0.005416531581431627  PSNR: 27.455852508544922
[TRAIN] Iter: 451900 Loss: 0.003770815674215555  PSNR: 29.684009552001953
[TRAIN] Iter: 452000 Loss: 0.0035354297142475843  PSNR: 29.62720489501953
[TRAIN] Iter: 452100 Loss: 0.004943437874317169  PSNR: 28.052396774291992
[TRAIN] Iter: 452200 Loss: 0.0047638388350605965  PSNR: 28.1545467376709
[TRAIN] Iter: 452300 Loss: 0.004716499242931604  PSNR: 27.69678497314453
[TRAIN] Iter: 452400 Loss: 0.003872128203511238  PSNR: 29.71378517150879
[TRAIN] Iter: 452500 Loss: 0.005646893754601479  PSNR: 27.146053314208984
[TRAIN] Iter: 452600 Loss: 0.004676369018852711  PSNR: 27.984342575073242
[TRAIN] Iter: 452700 Loss: 0.005260406993329525  PSNR: 27.44894027709961
[TRAIN] Iter: 452800 Loss: 0.0047455839812755585  PSNR: 28.313003540039062
[TRAIN] Iter: 452900 Loss: 0.003994817845523357  PSNR: 29.283926010131836
[TRAIN] Iter: 453000 Loss: 0.005241187289357185  PSNR: 27.644521713256836
[TRAIN] Iter: 453100 Loss: 0.004885110538452864  PSNR: 27.86855697631836
[TRAIN] Iter: 453200 Loss: 0.003765965811908245  PSNR: 29.408872604370117
[TRAIN] Iter: 453300 Loss: 0.0034641441889107227  PSNR: 30.162343978881836
[TRAIN] Iter: 453400 Loss: 0.004165686666965485  PSNR: 28.455549240112305
[TRAIN] Iter: 453500 Loss: 0.004659917205572128  PSNR: 28.485193252563477
[TRAIN] Iter: 453600 Loss: 0.004856145940721035  PSNR: 27.873987197875977
[TRAIN] Iter: 453700 Loss: 0.003216983051970601  PSNR: 29.786062240600586
[TRAIN] Iter: 453800 Loss: 0.004047311842441559  PSNR: 29.25088119506836
[TRAIN] Iter: 453900 Loss: 0.004587700590491295  PSNR: 27.4462890625
[TRAIN] Iter: 454000 Loss: 0.005018746480345726  PSNR: 27.784345626831055
[TRAIN] Iter: 454100 Loss: 0.005041994620114565  PSNR: 28.155620574951172
[TRAIN] Iter: 454200 Loss: 0.004490408580750227  PSNR: 27.684925079345703
[TRAIN] Iter: 454300 Loss: 0.004265766590833664  PSNR: 28.65813636779785
[TRAIN] Iter: 454400 Loss: 0.004622107371687889  PSNR: 27.90414810180664
[TRAIN] Iter: 454500 Loss: 0.004995543044060469  PSNR: 27.747291564941406
[TRAIN] Iter: 454600 Loss: 0.00429640943184495  PSNR: 28.627471923828125
[TRAIN] Iter: 454700 Loss: 0.003937808331102133  PSNR: 29.196413040161133
[TRAIN] Iter: 454800 Loss: 0.004517885856330395  PSNR: 28.517858505249023
[TRAIN] Iter: 454900 Loss: 0.004554195329546928  PSNR: 28.737619400024414
[TRAIN] Iter: 455000 Loss: 0.0047822995111346245  PSNR: 27.643901824951172
[TRAIN] Iter: 455100 Loss: 0.005255346652120352  PSNR: 27.059276580810547
[TRAIN] Iter: 455200 Loss: 0.004348662216216326  PSNR: 28.916765213012695
[TRAIN] Iter: 455300 Loss: 0.003944910131394863  PSNR: 29.716243743896484
[TRAIN] Iter: 455400 Loss: 0.005248130764812231  PSNR: 27.80364418029785
[TRAIN] Iter: 455500 Loss: 0.0039621563628315926  PSNR: 29.816387176513672
[TRAIN] Iter: 455600 Loss: 0.005334213376045227  PSNR: 27.53815460205078
[TRAIN] Iter: 455700 Loss: 0.004476240370422602  PSNR: 28.403953552246094
[TRAIN] Iter: 455800 Loss: 0.004686952102929354  PSNR: 28.311697006225586
[TRAIN] Iter: 455900 Loss: 0.0033829975873231888  PSNR: 30.036165237426758
[TRAIN] Iter: 456000 Loss: 0.00411277124658227  PSNR: 29.26625633239746
[TRAIN] Iter: 456100 Loss: 0.003882333869114518  PSNR: 30.12156105041504
[TRAIN] Iter: 456200 Loss: 0.004045510198920965  PSNR: 30.329660415649414
[TRAIN] Iter: 456300 Loss: 0.003845960833132267  PSNR: 29.915206909179688
[TRAIN] Iter: 456400 Loss: 0.003421326167881489  PSNR: 30.100242614746094
[TRAIN] Iter: 456500 Loss: 0.003879929892718792  PSNR: 28.795982360839844
[TRAIN] Iter: 456600 Loss: 0.0039606899954378605  PSNR: 28.805795669555664
[TRAIN] Iter: 456700 Loss: 0.005270061548799276  PSNR: 27.755483627319336
[TRAIN] Iter: 456800 Loss: 0.004424005281180143  PSNR: 27.751604080200195
[TRAIN] Iter: 456900 Loss: 0.00464090658351779  PSNR: 27.835750579833984
[TRAIN] Iter: 457000 Loss: 0.004933720454573631  PSNR: 28.0440673828125
[TRAIN] Iter: 457100 Loss: 0.004444317892193794  PSNR: 27.709205627441406
[TRAIN] Iter: 457200 Loss: 0.004419085569679737  PSNR: 27.328067779541016
[TRAIN] Iter: 457300 Loss: 0.005581261124461889  PSNR: 27.770721435546875
[TRAIN] Iter: 457400 Loss: 0.00468407291918993  PSNR: 27.825088500976562
[TRAIN] Iter: 457500 Loss: 0.003541515441611409  PSNR: 30.318941116333008
[TRAIN] Iter: 457600 Loss: 0.0046423436142504215  PSNR: 28.321435928344727
[TRAIN] Iter: 457700 Loss: 0.003614309709519148  PSNR: 30.222789764404297
[TRAIN] Iter: 457800 Loss: 0.005293707363307476  PSNR: 27.53203773498535
[TRAIN] Iter: 457900 Loss: 0.0035057514905929565  PSNR: 29.421958923339844
[TRAIN] Iter: 458000 Loss: 0.00621821777895093  PSNR: 27.221145629882812
[TRAIN] Iter: 458100 Loss: 0.003478893544524908  PSNR: 29.846195220947266
[TRAIN] Iter: 458200 Loss: 0.0057636285200715065  PSNR: 27.677532196044922
[TRAIN] Iter: 458300 Loss: 0.00498944241553545  PSNR: 28.061092376708984
[TRAIN] Iter: 458400 Loss: 0.0045786150731146336  PSNR: 27.547761917114258
[TRAIN] Iter: 458500 Loss: 0.003940018825232983  PSNR: 30.057506561279297
[TRAIN] Iter: 458600 Loss: 0.0048149218782782555  PSNR: 27.79146385192871
[TRAIN] Iter: 458700 Loss: 0.004215487744659185  PSNR: 28.35612678527832
[TRAIN] Iter: 458800 Loss: 0.0043975128792226315  PSNR: 28.518245697021484
[TRAIN] Iter: 458900 Loss: 0.00535330455750227  PSNR: 27.552391052246094
[TRAIN] Iter: 459000 Loss: 0.004349792841821909  PSNR: 27.841243743896484
[TRAIN] Iter: 459100 Loss: 0.004835756029933691  PSNR: 27.17493438720703
[TRAIN] Iter: 459200 Loss: 0.00610449630767107  PSNR: 26.356447219848633
[TRAIN] Iter: 459300 Loss: 0.005251234397292137  PSNR: 26.867626190185547
[TRAIN] Iter: 459400 Loss: 0.005403444636613131  PSNR: 27.57081413269043
[TRAIN] Iter: 459500 Loss: 0.003932883031666279  PSNR: 29.4796199798584
[TRAIN] Iter: 459600 Loss: 0.005506371147930622  PSNR: 27.428260803222656
[TRAIN] Iter: 459700 Loss: 0.004523176699876785  PSNR: 27.756181716918945
[TRAIN] Iter: 459800 Loss: 0.0037867152132093906  PSNR: 29.42119026184082
[TRAIN] Iter: 459900 Loss: 0.004406169522553682  PSNR: 28.375045776367188
Saved checkpoints at ./logs/TUT-KE101-nerf/460000.tar
[TRAIN] Iter: 460000 Loss: 0.005144442897289991  PSNR: 27.26432991027832
[TRAIN] Iter: 460100 Loss: 0.003701335284858942  PSNR: 30.111047744750977
[TRAIN] Iter: 460200 Loss: 0.005174776539206505  PSNR: 27.373546600341797
[TRAIN] Iter: 460300 Loss: 0.005400475114583969  PSNR: 27.180479049682617
[TRAIN] Iter: 460400 Loss: 0.004473872482776642  PSNR: 29.25442123413086
[TRAIN] Iter: 460500 Loss: 0.006287090480327606  PSNR: 26.924673080444336
[TRAIN] Iter: 460600 Loss: 0.005784297827631235  PSNR: 27.21575355529785
[TRAIN] Iter: 460700 Loss: 0.0050556897185742855  PSNR: 27.186925888061523
[TRAIN] Iter: 460800 Loss: 0.005553589202463627  PSNR: 27.768949508666992
[TRAIN] Iter: 460900 Loss: 0.005153981037437916  PSNR: 27.424453735351562
[TRAIN] Iter: 461000 Loss: 0.004697637166827917  PSNR: 27.563371658325195
[TRAIN] Iter: 461100 Loss: 0.005575436633080244  PSNR: 27.07611656188965
[TRAIN] Iter: 461200 Loss: 0.0033825498539954424  PSNR: 30.669788360595703
[TRAIN] Iter: 461300 Loss: 0.005842610727995634  PSNR: 26.796409606933594
[TRAIN] Iter: 461400 Loss: 0.005089929793030024  PSNR: 27.540037155151367
[TRAIN] Iter: 461500 Loss: 0.004095260985195637  PSNR: 30.127614974975586
[TRAIN] Iter: 461600 Loss: 0.003786236047744751  PSNR: 29.721912384033203
[TRAIN] Iter: 461700 Loss: 0.00487878080457449  PSNR: 27.96196174621582
[TRAIN] Iter: 461800 Loss: 0.003978374879807234  PSNR: 29.74448013305664
[TRAIN] Iter: 461900 Loss: 0.0044072470627725124  PSNR: 28.547685623168945
[TRAIN] Iter: 462000 Loss: 0.005224697291851044  PSNR: 27.79185676574707
[TRAIN] Iter: 462100 Loss: 0.003953260835260153  PSNR: 28.821815490722656
[TRAIN] Iter: 462200 Loss: 0.003940021153539419  PSNR: 29.02197265625
[TRAIN] Iter: 462300 Loss: 0.0057205804623663425  PSNR: 27.3280086517334
[TRAIN] Iter: 462400 Loss: 0.005381779745221138  PSNR: 27.58557891845703
[TRAIN] Iter: 462500 Loss: 0.005649255588650703  PSNR: 27.227434158325195
[TRAIN] Iter: 462600 Loss: 0.0035359193570911884  PSNR: 30.12348747253418
[TRAIN] Iter: 462700 Loss: 0.0036705355159938335  PSNR: 29.91191291809082
[TRAIN] Iter: 462800 Loss: 0.004904306028038263  PSNR: 28.3391170501709
[TRAIN] Iter: 462900 Loss: 0.003991156816482544  PSNR: 29.565792083740234
[TRAIN] Iter: 463000 Loss: 0.0039735957980155945  PSNR: 29.42822265625
[TRAIN] Iter: 463100 Loss: 0.004822214599698782  PSNR: 27.929035186767578
[TRAIN] Iter: 463200 Loss: 0.0047157201915979385  PSNR: 28.487991333007812
[TRAIN] Iter: 463300 Loss: 0.003521539503708482  PSNR: 30.25504493713379
[TRAIN] Iter: 463400 Loss: 0.0038366341032087803  PSNR: 29.754783630371094
[TRAIN] Iter: 463500 Loss: 0.003949258476495743  PSNR: 30.274938583374023
[TRAIN] Iter: 463600 Loss: 0.003247395623475313  PSNR: 29.7801513671875
[TRAIN] Iter: 463700 Loss: 0.004230325110256672  PSNR: 29.1776065826416
[TRAIN] Iter: 463800 Loss: 0.003944676369428635  PSNR: 29.990314483642578
[TRAIN] Iter: 463900 Loss: 0.00633879192173481  PSNR: 26.49674415588379
[TRAIN] Iter: 464000 Loss: 0.00321969878859818  PSNR: 30.666826248168945
[TRAIN] Iter: 464100 Loss: 0.004778163507580757  PSNR: 27.59516143798828
[TRAIN] Iter: 464200 Loss: 0.0036784615367650986  PSNR: 30.308364868164062
[TRAIN] Iter: 464300 Loss: 0.00445243576541543  PSNR: 28.986433029174805
[TRAIN] Iter: 464400 Loss: 0.003663493786007166  PSNR: 29.951852798461914
[TRAIN] Iter: 464500 Loss: 0.004165018908679485  PSNR: 29.517826080322266
[TRAIN] Iter: 464600 Loss: 0.005360131151974201  PSNR: 27.30949592590332
[TRAIN] Iter: 464700 Loss: 0.004633666947484016  PSNR: 28.856779098510742
[TRAIN] Iter: 464800 Loss: 0.005407570395618677  PSNR: 27.46607208251953
[TRAIN] Iter: 464900 Loss: 0.004521156195551157  PSNR: 28.26225471496582
[TRAIN] Iter: 465000 Loss: 0.0050884573720395565  PSNR: 27.85677719116211
[TRAIN] Iter: 465100 Loss: 0.003635320346802473  PSNR: 29.329326629638672
[TRAIN] Iter: 465200 Loss: 0.004276576451957226  PSNR: 28.368959426879883
[TRAIN] Iter: 465300 Loss: 0.006317842751741409  PSNR: 27.027437210083008
[TRAIN] Iter: 465400 Loss: 0.004831676371395588  PSNR: 28.132694244384766
[TRAIN] Iter: 465500 Loss: 0.004802213981747627  PSNR: 28.15064239501953
[TRAIN] Iter: 465600 Loss: 0.004497995600104332  PSNR: 27.90507698059082
[TRAIN] Iter: 465700 Loss: 0.004239383153617382  PSNR: 29.705259323120117
[TRAIN] Iter: 465800 Loss: 0.004245876334607601  PSNR: 28.89646339416504
[TRAIN] Iter: 465900 Loss: 0.005221912171691656  PSNR: 28.553321838378906
[TRAIN] Iter: 466000 Loss: 0.005244664382189512  PSNR: 27.131744384765625
[TRAIN] Iter: 466100 Loss: 0.003751048818230629  PSNR: 29.43834114074707
[TRAIN] Iter: 466200 Loss: 0.0037588649429380894  PSNR: 29.80190086364746
[TRAIN] Iter: 466300 Loss: 0.0035685496404767036  PSNR: 30.315597534179688
[TRAIN] Iter: 466400 Loss: 0.004598235245794058  PSNR: 27.90829086303711
[TRAIN] Iter: 466500 Loss: 0.005483095999807119  PSNR: 27.62449073791504
[TRAIN] Iter: 466600 Loss: 0.0057651204988360405  PSNR: 26.75922393798828
[TRAIN] Iter: 466700 Loss: 0.004490579944103956  PSNR: 27.847761154174805
[TRAIN] Iter: 466800 Loss: 0.0052101220935583115  PSNR: 27.712053298950195
[TRAIN] Iter: 466900 Loss: 0.005872660782188177  PSNR: 26.773752212524414
[TRAIN] Iter: 467000 Loss: 0.004613031167536974  PSNR: 28.54214096069336
[TRAIN] Iter: 467100 Loss: 0.005686599761247635  PSNR: 26.807863235473633
[TRAIN] Iter: 467200 Loss: 0.0036412214394658804  PSNR: 29.582006454467773
[TRAIN] Iter: 467300 Loss: 0.0050266501493752  PSNR: 28.1359806060791
[TRAIN] Iter: 467400 Loss: 0.00474624615162611  PSNR: 28.154645919799805
[TRAIN] Iter: 467500 Loss: 0.0063826232217252254  PSNR: 26.776958465576172
[TRAIN] Iter: 467600 Loss: 0.004847048781812191  PSNR: 28.005407333374023
[TRAIN] Iter: 467700 Loss: 0.005417285021394491  PSNR: 27.555601119995117
[TRAIN] Iter: 467800 Loss: 0.004207512829452753  PSNR: 28.82292938232422
[TRAIN] Iter: 467900 Loss: 0.0036062898579984903  PSNR: 29.818647384643555
[TRAIN] Iter: 468000 Loss: 0.005605163984000683  PSNR: 27.28065299987793
[TRAIN] Iter: 468100 Loss: 0.004342051222920418  PSNR: 28.283594131469727
[TRAIN] Iter: 468200 Loss: 0.005314997397363186  PSNR: 27.831859588623047
[TRAIN] Iter: 468300 Loss: 0.004072858951985836  PSNR: 28.90060806274414
[TRAIN] Iter: 468400 Loss: 0.004050110466778278  PSNR: 28.798940658569336
[TRAIN] Iter: 468500 Loss: 0.004121961072087288  PSNR: 28.787416458129883
[TRAIN] Iter: 468600 Loss: 0.0051359329372644424  PSNR: 28.540035247802734
[TRAIN] Iter: 468700 Loss: 0.005345466546714306  PSNR: 27.214885711669922
[TRAIN] Iter: 468800 Loss: 0.004779502283781767  PSNR: 28.36497688293457
[TRAIN] Iter: 468900 Loss: 0.003909043502062559  PSNR: 29.794206619262695
[TRAIN] Iter: 469000 Loss: 0.0049051144160330296  PSNR: 27.40756607055664
[TRAIN] Iter: 469100 Loss: 0.005705082789063454  PSNR: 26.790760040283203
[TRAIN] Iter: 469200 Loss: 0.005716051906347275  PSNR: 27.859621047973633
[TRAIN] Iter: 469300 Loss: 0.0045494805090129375  PSNR: 28.809598922729492
[TRAIN] Iter: 469400 Loss: 0.00487176701426506  PSNR: 28.660154342651367
[TRAIN] Iter: 469500 Loss: 0.004164339043200016  PSNR: 28.97542953491211
[TRAIN] Iter: 469600 Loss: 0.005308953113853931  PSNR: 27.50473976135254
[TRAIN] Iter: 469700 Loss: 0.004453505389392376  PSNR: 30.08951187133789
[TRAIN] Iter: 469800 Loss: 0.005439733155071735  PSNR: 27.42156410217285
[TRAIN] Iter: 469900 Loss: 0.005359271541237831  PSNR: 28.157499313354492
Saved checkpoints at ./logs/TUT-KE101-nerf/470000.tar
[TRAIN] Iter: 470000 Loss: 0.004889760632067919  PSNR: 27.862215042114258
[TRAIN] Iter: 470100 Loss: 0.00426457030698657  PSNR: 28.273725509643555
[TRAIN] Iter: 470200 Loss: 0.004260588437318802  PSNR: 29.183971405029297
[TRAIN] Iter: 470300 Loss: 0.005892478860914707  PSNR: 26.744192123413086
[TRAIN] Iter: 470400 Loss: 0.004513739142566919  PSNR: 28.28143310546875
[TRAIN] Iter: 470500 Loss: 0.005383437965065241  PSNR: 26.912216186523438
[TRAIN] Iter: 470600 Loss: 0.004290612414479256  PSNR: 29.281341552734375
[TRAIN] Iter: 470700 Loss: 0.0037504786159843206  PSNR: 30.4832763671875
[TRAIN] Iter: 470800 Loss: 0.0035581844858825207  PSNR: 30.387535095214844
[TRAIN] Iter: 470900 Loss: 0.005053674802184105  PSNR: 27.817487716674805
[TRAIN] Iter: 471000 Loss: 0.0034201093949377537  PSNR: 30.031585693359375
[TRAIN] Iter: 471100 Loss: 0.004852559883147478  PSNR: 27.715496063232422
[TRAIN] Iter: 471200 Loss: 0.0045883795246481895  PSNR: 28.425294876098633
[TRAIN] Iter: 471300 Loss: 0.005316638853400946  PSNR: 27.41376495361328
[TRAIN] Iter: 471400 Loss: 0.003435468068346381  PSNR: 29.536331176757812
[TRAIN] Iter: 471500 Loss: 0.004285701550543308  PSNR: 28.32752227783203
[TRAIN] Iter: 471600 Loss: 0.005028854124248028  PSNR: 27.92955207824707
[TRAIN] Iter: 471700 Loss: 0.005356821231544018  PSNR: 27.80278778076172
[TRAIN] Iter: 471800 Loss: 0.0043354760855436325  PSNR: 29.210601806640625
[TRAIN] Iter: 471900 Loss: 0.004094887059181929  PSNR: 28.910207748413086
[TRAIN] Iter: 472000 Loss: 0.00528312660753727  PSNR: 27.978687286376953
[TRAIN] Iter: 472100 Loss: 0.004828879609704018  PSNR: 27.563793182373047
[TRAIN] Iter: 472200 Loss: 0.0051251985132694244  PSNR: 27.778074264526367
[TRAIN] Iter: 472300 Loss: 0.003848697990179062  PSNR: 28.609498977661133
[TRAIN] Iter: 472400 Loss: 0.0041498830541968346  PSNR: 28.731924057006836
[TRAIN] Iter: 472500 Loss: 0.004768743179738522  PSNR: 27.260494232177734
[TRAIN] Iter: 472600 Loss: 0.004476912319660187  PSNR: 28.231910705566406
[TRAIN] Iter: 472700 Loss: 0.0051927389577031136  PSNR: 27.475093841552734
[TRAIN] Iter: 472800 Loss: 0.0053866044618189335  PSNR: 27.97025489807129
[TRAIN] Iter: 472900 Loss: 0.004574492108076811  PSNR: 27.979900360107422
[TRAIN] Iter: 473000 Loss: 0.005187122616916895  PSNR: 27.30133628845215
[TRAIN] Iter: 473100 Loss: 0.004883386194705963  PSNR: 28.49563217163086
[TRAIN] Iter: 473200 Loss: 0.0038494225591421127  PSNR: 29.88652229309082
[TRAIN] Iter: 473300 Loss: 0.0036257486790418625  PSNR: 29.825204849243164
[TRAIN] Iter: 473400 Loss: 0.005255920812487602  PSNR: 27.716110229492188
[TRAIN] Iter: 473500 Loss: 0.004166273400187492  PSNR: 28.771684646606445
[TRAIN] Iter: 473600 Loss: 0.005661186762154102  PSNR: 26.9935302734375
[TRAIN] Iter: 473700 Loss: 0.0048757754266262054  PSNR: 27.96918487548828
[TRAIN] Iter: 473800 Loss: 0.004108239896595478  PSNR: 29.61648941040039
[TRAIN] Iter: 473900 Loss: 0.0047048404812812805  PSNR: 28.645530700683594
[TRAIN] Iter: 474000 Loss: 0.005241934210062027  PSNR: 27.955547332763672
[TRAIN] Iter: 474100 Loss: 0.00422030920162797  PSNR: 29.04877471923828
[TRAIN] Iter: 474200 Loss: 0.0036376281641423702  PSNR: 29.53743553161621
[TRAIN] Iter: 474300 Loss: 0.0039054080843925476  PSNR: 29.545122146606445
[TRAIN] Iter: 474400 Loss: 0.004819526337087154  PSNR: 28.086219787597656
[TRAIN] Iter: 474500 Loss: 0.0046942648477852345  PSNR: 28.188356399536133
[TRAIN] Iter: 474600 Loss: 0.0053745838813483715  PSNR: 27.202804565429688
[TRAIN] Iter: 474700 Loss: 0.005314001813530922  PSNR: 27.283794403076172
[TRAIN] Iter: 474800 Loss: 0.004580736625939608  PSNR: 27.71965789794922
[TRAIN] Iter: 474900 Loss: 0.0039013719651848078  PSNR: 29.414226531982422
[TRAIN] Iter: 475000 Loss: 0.0049295141361653805  PSNR: 28.061168670654297
[TRAIN] Iter: 475100 Loss: 0.00487486319616437  PSNR: 28.446056365966797
[TRAIN] Iter: 475200 Loss: 0.003555100876837969  PSNR: 29.717021942138672
[TRAIN] Iter: 475300 Loss: 0.0036556886043399572  PSNR: 30.194604873657227
[TRAIN] Iter: 475400 Loss: 0.004676294047385454  PSNR: 27.240999221801758
[TRAIN] Iter: 475500 Loss: 0.004910658113658428  PSNR: 27.796396255493164
[TRAIN] Iter: 475600 Loss: 0.003861241275444627  PSNR: 30.134244918823242
[TRAIN] Iter: 475700 Loss: 0.005604232661426067  PSNR: 27.423633575439453
[TRAIN] Iter: 475800 Loss: 0.004714466631412506  PSNR: 28.42807388305664
[TRAIN] Iter: 475900 Loss: 0.004298232961446047  PSNR: 28.561979293823242
[TRAIN] Iter: 476000 Loss: 0.0037056265864521265  PSNR: 28.986068725585938
[TRAIN] Iter: 476100 Loss: 0.0036749402061104774  PSNR: 29.796951293945312
[TRAIN] Iter: 476200 Loss: 0.005671704653650522  PSNR: 26.338911056518555
[TRAIN] Iter: 476300 Loss: 0.005171644501388073  PSNR: 28.039644241333008
[TRAIN] Iter: 476400 Loss: 0.004138599615544081  PSNR: 28.535295486450195
[TRAIN] Iter: 476500 Loss: 0.00464117294177413  PSNR: 28.161348342895508
[TRAIN] Iter: 476600 Loss: 0.0058491406962275505  PSNR: 26.87371253967285
[TRAIN] Iter: 476700 Loss: 0.005006910301744938  PSNR: 27.732818603515625
[TRAIN] Iter: 476800 Loss: 0.005053362809121609  PSNR: 27.231372833251953
[TRAIN] Iter: 476900 Loss: 0.005250046961009502  PSNR: 27.687223434448242
[TRAIN] Iter: 477000 Loss: 0.005399701651185751  PSNR: 27.979074478149414
[TRAIN] Iter: 477100 Loss: 0.005031323526054621  PSNR: 27.509326934814453
[TRAIN] Iter: 477200 Loss: 0.004037398844957352  PSNR: 28.735191345214844
[TRAIN] Iter: 477300 Loss: 0.0039938874542713165  PSNR: 28.416851043701172
[TRAIN] Iter: 477400 Loss: 0.003975560888648033  PSNR: 28.875431060791016
[TRAIN] Iter: 477500 Loss: 0.004790004342794418  PSNR: 27.945133209228516
[TRAIN] Iter: 477600 Loss: 0.005511294584721327  PSNR: 28.037343978881836
[TRAIN] Iter: 477700 Loss: 0.004258268047124147  PSNR: 29.106002807617188
[TRAIN] Iter: 477800 Loss: 0.005007194355130196  PSNR: 27.757022857666016
[TRAIN] Iter: 477900 Loss: 0.004992392845451832  PSNR: 27.231538772583008
[TRAIN] Iter: 478000 Loss: 0.003736928105354309  PSNR: 29.774539947509766
[TRAIN] Iter: 478100 Loss: 0.004713290371000767  PSNR: 28.232723236083984
[TRAIN] Iter: 478200 Loss: 0.005195115227252245  PSNR: 27.79570770263672
[TRAIN] Iter: 478300 Loss: 0.005031171254813671  PSNR: 27.97271728515625
[TRAIN] Iter: 478400 Loss: 0.004457717761397362  PSNR: 28.31047821044922
[TRAIN] Iter: 478500 Loss: 0.004090838134288788  PSNR: 28.62598419189453
[TRAIN] Iter: 478600 Loss: 0.004709766246378422  PSNR: 27.85405731201172
[TRAIN] Iter: 478700 Loss: 0.005007808096706867  PSNR: 27.986413955688477
[TRAIN] Iter: 478800 Loss: 0.004456104710698128  PSNR: 28.285951614379883
[TRAIN] Iter: 478900 Loss: 0.005320360884070396  PSNR: 26.998241424560547
[TRAIN] Iter: 479000 Loss: 0.0048533473163843155  PSNR: 27.91160774230957
[TRAIN] Iter: 479100 Loss: 0.0040932754054665565  PSNR: 29.147218704223633
[TRAIN] Iter: 479200 Loss: 0.004859100095927715  PSNR: 28.78635025024414
[TRAIN] Iter: 479300 Loss: 0.004910571034997702  PSNR: 28.504735946655273
[TRAIN] Iter: 479400 Loss: 0.004269051365554333  PSNR: 28.206817626953125
[TRAIN] Iter: 479500 Loss: 0.005123034119606018  PSNR: 27.72259521484375
[TRAIN] Iter: 479600 Loss: 0.0060282545164227486  PSNR: 26.574085235595703
[TRAIN] Iter: 479700 Loss: 0.003921030089259148  PSNR: 28.91493034362793
[TRAIN] Iter: 479800 Loss: 0.004797482863068581  PSNR: 27.84271240234375
[TRAIN] Iter: 479900 Loss: 0.003732820274308324  PSNR: 29.715198516845703
Saved checkpoints at ./logs/TUT-KE101-nerf/480000.tar
[TRAIN] Iter: 480000 Loss: 0.004903361666947603  PSNR: 28.019628524780273
[TRAIN] Iter: 480100 Loss: 0.004141468089073896  PSNR: 29.406213760375977
[TRAIN] Iter: 480200 Loss: 0.0038761855103075504  PSNR: 29.6104793548584
[TRAIN] Iter: 480300 Loss: 0.003992081619799137  PSNR: 28.970319747924805
[TRAIN] Iter: 480400 Loss: 0.004581416957080364  PSNR: 28.17655372619629
[TRAIN] Iter: 480500 Loss: 0.0037272335030138493  PSNR: 29.621631622314453
[TRAIN] Iter: 480600 Loss: 0.0053155370987951756  PSNR: 27.453935623168945
[TRAIN] Iter: 480700 Loss: 0.004854024387896061  PSNR: 27.968420028686523
[TRAIN] Iter: 480800 Loss: 0.004920405335724354  PSNR: 28.371763229370117
[TRAIN] Iter: 480900 Loss: 0.004145377781242132  PSNR: 29.138072967529297
[TRAIN] Iter: 481000 Loss: 0.003716961946338415  PSNR: 30.13620948791504
[TRAIN] Iter: 481100 Loss: 0.004841816611588001  PSNR: 27.947696685791016
[TRAIN] Iter: 481200 Loss: 0.0035238750278949738  PSNR: 30.761837005615234
[TRAIN] Iter: 481300 Loss: 0.003068353747949004  PSNR: 29.888994216918945
[TRAIN] Iter: 481400 Loss: 0.003881497774273157  PSNR: 29.833086013793945
[TRAIN] Iter: 481500 Loss: 0.0035270133521407843  PSNR: 29.907194137573242
[TRAIN] Iter: 481600 Loss: 0.005504177883267403  PSNR: 27.497373580932617
[TRAIN] Iter: 481700 Loss: 0.004124585073441267  PSNR: 29.174402236938477
[TRAIN] Iter: 481800 Loss: 0.004087525419890881  PSNR: 28.614036560058594
[TRAIN] Iter: 481900 Loss: 0.004086610395461321  PSNR: 28.848243713378906
[TRAIN] Iter: 482000 Loss: 0.004616447724401951  PSNR: 27.706802368164062
[TRAIN] Iter: 482100 Loss: 0.0039680637419223785  PSNR: 29.494991302490234
[TRAIN] Iter: 482200 Loss: 0.005345165729522705  PSNR: 27.647747039794922
[TRAIN] Iter: 482300 Loss: 0.004740357398986816  PSNR: 27.478857040405273
[TRAIN] Iter: 482400 Loss: 0.004611963406205177  PSNR: 27.783802032470703
[TRAIN] Iter: 482500 Loss: 0.003623869037255645  PSNR: 30.464168548583984
[TRAIN] Iter: 482600 Loss: 0.004722407087683678  PSNR: 28.406984329223633
[TRAIN] Iter: 482700 Loss: 0.005221921484917402  PSNR: 27.62015151977539
[TRAIN] Iter: 482800 Loss: 0.0039021880365908146  PSNR: 29.789525985717773
[TRAIN] Iter: 482900 Loss: 0.004045691341161728  PSNR: 29.89230728149414
[TRAIN] Iter: 483000 Loss: 0.005189301446080208  PSNR: 27.573209762573242
[TRAIN] Iter: 483100 Loss: 0.004883235320448875  PSNR: 28.217388153076172
[TRAIN] Iter: 483200 Loss: 0.0030096638947725296  PSNR: 30.80914306640625
[TRAIN] Iter: 483300 Loss: 0.005970986559987068  PSNR: 27.386394500732422
[TRAIN] Iter: 483400 Loss: 0.004149521701037884  PSNR: 29.885848999023438
[TRAIN] Iter: 483500 Loss: 0.0036652053240686655  PSNR: 30.141382217407227
[TRAIN] Iter: 483600 Loss: 0.005209061782807112  PSNR: 27.69764518737793
[TRAIN] Iter: 483700 Loss: 0.0034958452451974154  PSNR: 30.312191009521484
[TRAIN] Iter: 483800 Loss: 0.005369592923671007  PSNR: 27.63372802734375
[TRAIN] Iter: 483900 Loss: 0.0036433013156056404  PSNR: 29.862924575805664
[TRAIN] Iter: 484000 Loss: 0.004071069415658712  PSNR: 28.56275177001953
[TRAIN] Iter: 484100 Loss: 0.004879135172814131  PSNR: 28.069637298583984
[TRAIN] Iter: 484200 Loss: 0.004234082996845245  PSNR: 28.752105712890625
[TRAIN] Iter: 484300 Loss: 0.005265164189040661  PSNR: 27.348302841186523
[TRAIN] Iter: 484400 Loss: 0.005271683447062969  PSNR: 27.30609130859375
[TRAIN] Iter: 484500 Loss: 0.0038466311525553465  PSNR: 30.100116729736328
[TRAIN] Iter: 484600 Loss: 0.005426954478025436  PSNR: 26.935958862304688
[TRAIN] Iter: 484700 Loss: 0.003810117021203041  PSNR: 29.83359718322754
[TRAIN] Iter: 484800 Loss: 0.0037601375952363014  PSNR: 28.63125228881836
[TRAIN] Iter: 484900 Loss: 0.004271725192666054  PSNR: 28.760080337524414
[TRAIN] Iter: 485000 Loss: 0.00542581919580698  PSNR: 27.276884078979492
[TRAIN] Iter: 485100 Loss: 0.004697543568909168  PSNR: 27.882787704467773
[TRAIN] Iter: 485200 Loss: 0.003921929281204939  PSNR: 28.84583854675293
[TRAIN] Iter: 485300 Loss: 0.004194951616227627  PSNR: 29.059921264648438
[TRAIN] Iter: 485400 Loss: 0.005004026927053928  PSNR: 27.0798397064209
[TRAIN] Iter: 485500 Loss: 0.003848268883302808  PSNR: 29.846466064453125
[TRAIN] Iter: 485600 Loss: 0.004083629231899977  PSNR: 29.789945602416992
[TRAIN] Iter: 485700 Loss: 0.003871842287480831  PSNR: 29.879653930664062
[TRAIN] Iter: 485800 Loss: 0.005701473914086819  PSNR: 27.244155883789062
[TRAIN] Iter: 485900 Loss: 0.004882490262389183  PSNR: 27.99863052368164
[TRAIN] Iter: 486000 Loss: 0.003909214865416288  PSNR: 29.962419509887695
[TRAIN] Iter: 486100 Loss: 0.0042432136833667755  PSNR: 28.011709213256836
[TRAIN] Iter: 486200 Loss: 0.004041209816932678  PSNR: 29.88692283630371
[TRAIN] Iter: 486300 Loss: 0.00550682470202446  PSNR: 27.579748153686523
[TRAIN] Iter: 486400 Loss: 0.004140124656260014  PSNR: 28.30134391784668
[TRAIN] Iter: 486500 Loss: 0.004836436361074448  PSNR: 27.66886329650879
[TRAIN] Iter: 486600 Loss: 0.0044349790550768375  PSNR: 28.40522575378418
[TRAIN] Iter: 486700 Loss: 0.0042727431282401085  PSNR: 29.08928108215332
[TRAIN] Iter: 486800 Loss: 0.005304768215864897  PSNR: 27.37019157409668
[TRAIN] Iter: 486900 Loss: 0.005142240785062313  PSNR: 27.35245132446289
[TRAIN] Iter: 487000 Loss: 0.0050521004013717175  PSNR: 27.291942596435547
[TRAIN] Iter: 487100 Loss: 0.0056100632064044476  PSNR: 27.063858032226562
[TRAIN] Iter: 487200 Loss: 0.005729973781853914  PSNR: 26.903759002685547
[TRAIN] Iter: 487300 Loss: 0.0040852343663573265  PSNR: 28.68052864074707
[TRAIN] Iter: 487400 Loss: 0.005437924526631832  PSNR: 26.906415939331055
[TRAIN] Iter: 487500 Loss: 0.005011343862861395  PSNR: 27.2695255279541
[TRAIN] Iter: 487600 Loss: 0.004691833630204201  PSNR: 27.822872161865234
[TRAIN] Iter: 487700 Loss: 0.004125518724322319  PSNR: 28.50470733642578
[TRAIN] Iter: 487800 Loss: 0.004520981572568417  PSNR: 27.889278411865234
[TRAIN] Iter: 487900 Loss: 0.004400372505187988  PSNR: 27.774477005004883
[TRAIN] Iter: 488000 Loss: 0.004848904442042112  PSNR: 27.34722137451172
[TRAIN] Iter: 488100 Loss: 0.005669287871569395  PSNR: 27.1234073638916
[TRAIN] Iter: 488200 Loss: 0.004735709633678198  PSNR: 28.173372268676758
[TRAIN] Iter: 488300 Loss: 0.004973982460796833  PSNR: 28.151472091674805
[TRAIN] Iter: 488400 Loss: 0.005024519748985767  PSNR: 27.652259826660156
[TRAIN] Iter: 488500 Loss: 0.003761498723179102  PSNR: 29.721418380737305
[TRAIN] Iter: 488600 Loss: 0.004603956826031208  PSNR: 28.363407135009766
[TRAIN] Iter: 488700 Loss: 0.006136286072432995  PSNR: 27.607181549072266
[TRAIN] Iter: 488800 Loss: 0.00568009540438652  PSNR: 27.466087341308594
[TRAIN] Iter: 488900 Loss: 0.004769462626427412  PSNR: 28.29273796081543
[TRAIN] Iter: 489000 Loss: 0.006486819125711918  PSNR: 26.017210006713867
[TRAIN] Iter: 489100 Loss: 0.004452292341738939  PSNR: 28.091297149658203
[TRAIN] Iter: 489200 Loss: 0.004988100379705429  PSNR: 27.661752700805664
[TRAIN] Iter: 489300 Loss: 0.005324320401996374  PSNR: 27.28213119506836
[TRAIN] Iter: 489400 Loss: 0.0036931512877345085  PSNR: 30.199317932128906
[TRAIN] Iter: 489500 Loss: 0.005225952249020338  PSNR: 27.239431381225586
[TRAIN] Iter: 489600 Loss: 0.004865702707320452  PSNR: 28.33613395690918
[TRAIN] Iter: 489700 Loss: 0.0034370264038443565  PSNR: 30.09786033630371
[TRAIN] Iter: 489800 Loss: 0.0033894749358296394  PSNR: 30.403940200805664
[TRAIN] Iter: 489900 Loss: 0.0043316800147295  PSNR: 28.721790313720703
Saved checkpoints at ./logs/TUT-KE101-nerf/490000.tar
[TRAIN] Iter: 490000 Loss: 0.0050223516300320625  PSNR: 27.498205184936523
[TRAIN] Iter: 490100 Loss: 0.005224546417593956  PSNR: 27.715394973754883
[TRAIN] Iter: 490200 Loss: 0.0038120043464004993  PSNR: 29.66252899169922
[TRAIN] Iter: 490300 Loss: 0.005544658750295639  PSNR: 27.005949020385742
[TRAIN] Iter: 490400 Loss: 0.00493169529363513  PSNR: 28.223787307739258
[TRAIN] Iter: 490500 Loss: 0.00429330300539732  PSNR: 29.898906707763672
[TRAIN] Iter: 490600 Loss: 0.004836536478251219  PSNR: 27.652294158935547
[TRAIN] Iter: 490700 Loss: 0.00676667271181941  PSNR: 26.799636840820312
[TRAIN] Iter: 490800 Loss: 0.004210283979773521  PSNR: 28.20787239074707
[TRAIN] Iter: 490900 Loss: 0.003807466011494398  PSNR: 29.649866104125977
[TRAIN] Iter: 491000 Loss: 0.00524335540831089  PSNR: 27.625137329101562
[TRAIN] Iter: 491100 Loss: 0.005884105805307627  PSNR: 26.825841903686523
[TRAIN] Iter: 491200 Loss: 0.0037418841384351254  PSNR: 29.863046646118164
[TRAIN] Iter: 491300 Loss: 0.005416402593255043  PSNR: 27.274869918823242
[TRAIN] Iter: 491400 Loss: 0.005658656358718872  PSNR: 27.490245819091797
[TRAIN] Iter: 491500 Loss: 0.0042976983822882175  PSNR: 28.399099349975586
[TRAIN] Iter: 491600 Loss: 0.005275582429021597  PSNR: 27.102697372436523
[TRAIN] Iter: 491700 Loss: 0.006232777144759893  PSNR: 27.496522903442383
[TRAIN] Iter: 491800 Loss: 0.006279990542680025  PSNR: 26.408916473388672
[TRAIN] Iter: 491900 Loss: 0.004662822932004929  PSNR: 27.722026824951172
[TRAIN] Iter: 492000 Loss: 0.0057767280377447605  PSNR: 27.112077713012695
[TRAIN] Iter: 492100 Loss: 0.003421675181016326  PSNR: 30.143320083618164
[TRAIN] Iter: 492200 Loss: 0.0034745391458272934  PSNR: 30.05913734436035
[TRAIN] Iter: 492300 Loss: 0.004114086739718914  PSNR: 28.31462287902832
[TRAIN] Iter: 492400 Loss: 0.003540266305208206  PSNR: 29.97924041748047
[TRAIN] Iter: 492500 Loss: 0.0033636754378676414  PSNR: 29.75350570678711
[TRAIN] Iter: 492600 Loss: 0.004466915503144264  PSNR: 27.653467178344727
[TRAIN] Iter: 492700 Loss: 0.0034898915328085423  PSNR: 29.53265380859375
[TRAIN] Iter: 492800 Loss: 0.0037122503854334354  PSNR: 29.63982582092285
[TRAIN] Iter: 492900 Loss: 0.0049246810376644135  PSNR: 27.34707260131836
[TRAIN] Iter: 493000 Loss: 0.004227204713970423  PSNR: 28.571468353271484
[TRAIN] Iter: 493100 Loss: 0.0031465471256524324  PSNR: 30.508573532104492
[TRAIN] Iter: 493200 Loss: 0.003428108524531126  PSNR: 30.200963973999023
[TRAIN] Iter: 493300 Loss: 0.005033095367252827  PSNR: 27.87942123413086
[TRAIN] Iter: 493400 Loss: 0.003737587481737137  PSNR: 29.413965225219727
[TRAIN] Iter: 493500 Loss: 0.003855899441987276  PSNR: 29.95792579650879
[TRAIN] Iter: 493600 Loss: 0.00489035015925765  PSNR: 28.12692642211914
[TRAIN] Iter: 493700 Loss: 0.004061776213347912  PSNR: 28.619234085083008
[TRAIN] Iter: 493800 Loss: 0.004649381153285503  PSNR: 27.833662033081055
[TRAIN] Iter: 493900 Loss: 0.004905955865979195  PSNR: 27.857311248779297
[TRAIN] Iter: 494000 Loss: 0.0046176603063941  PSNR: 28.728809356689453
[TRAIN] Iter: 494100 Loss: 0.004676125943660736  PSNR: 27.736770629882812
[TRAIN] Iter: 494200 Loss: 0.004986660089343786  PSNR: 27.739046096801758
[TRAIN] Iter: 494300 Loss: 0.004129916895180941  PSNR: 28.814590454101562
[TRAIN] Iter: 494400 Loss: 0.004360300954431295  PSNR: 29.211923599243164
[TRAIN] Iter: 494500 Loss: 0.005440017208456993  PSNR: 27.046140670776367
[TRAIN] Iter: 494600 Loss: 0.004579675383865833  PSNR: 28.238040924072266
[TRAIN] Iter: 494700 Loss: 0.004125155508518219  PSNR: 28.85991096496582
[TRAIN] Iter: 494800 Loss: 0.0036212815903127193  PSNR: 29.719745635986328
[TRAIN] Iter: 494900 Loss: 0.005721339490264654  PSNR: 27.381439208984375
[TRAIN] Iter: 495000 Loss: 0.005962884984910488  PSNR: 26.469100952148438
[TRAIN] Iter: 495100 Loss: 0.003735209349542856  PSNR: 29.751304626464844
[TRAIN] Iter: 495200 Loss: 0.003539562691003084  PSNR: 29.722253799438477
[TRAIN] Iter: 495300 Loss: 0.005598603747785091  PSNR: 27.715309143066406
[TRAIN] Iter: 495400 Loss: 0.004984736442565918  PSNR: 27.416080474853516
[TRAIN] Iter: 495500 Loss: 0.005621453747153282  PSNR: 26.864198684692383
[TRAIN] Iter: 495600 Loss: 0.005588746629655361  PSNR: 27.78573226928711
[TRAIN] Iter: 495700 Loss: 0.0037980503402650356  PSNR: 30.155258178710938
[TRAIN] Iter: 495800 Loss: 0.0037310183979570866  PSNR: 30.407386779785156
[TRAIN] Iter: 495900 Loss: 0.005821293219923973  PSNR: 27.11516571044922
[TRAIN] Iter: 496000 Loss: 0.00337689439766109  PSNR: 30.756343841552734
[TRAIN] Iter: 496100 Loss: 0.005538864992558956  PSNR: 27.23808479309082
[TRAIN] Iter: 496200 Loss: 0.005017301067709923  PSNR: 28.09979820251465
[TRAIN] Iter: 496300 Loss: 0.0040961699560284615  PSNR: 28.97026252746582
[TRAIN] Iter: 496400 Loss: 0.004133835434913635  PSNR: 28.453786849975586
[TRAIN] Iter: 496500 Loss: 0.004826565273106098  PSNR: 27.955808639526367
[TRAIN] Iter: 496600 Loss: 0.0035738195292651653  PSNR: 30.405521392822266
[TRAIN] Iter: 496700 Loss: 0.0045477719977498055  PSNR: 27.917482376098633
[TRAIN] Iter: 496800 Loss: 0.003919166978448629  PSNR: 29.990718841552734
[TRAIN] Iter: 496900 Loss: 0.004681712947785854  PSNR: 27.817188262939453
[TRAIN] Iter: 497000 Loss: 0.003998827654868364  PSNR: 29.8350772857666
[TRAIN] Iter: 497100 Loss: 0.004000690765678883  PSNR: 29.586227416992188
[TRAIN] Iter: 497200 Loss: 0.005237256642431021  PSNR: 27.89694595336914
[TRAIN] Iter: 497300 Loss: 0.004555589519441128  PSNR: 28.932573318481445
[TRAIN] Iter: 497400 Loss: 0.0037581375800073147  PSNR: 29.794870376586914
[TRAIN] Iter: 497500 Loss: 0.0039017607923597097  PSNR: 29.902286529541016
[TRAIN] Iter: 497600 Loss: 0.004276586230844259  PSNR: 29.21187400817871
[TRAIN] Iter: 497700 Loss: 0.004158414900302887  PSNR: 29.390336990356445
[TRAIN] Iter: 497800 Loss: 0.005654626525938511  PSNR: 27.170490264892578
[TRAIN] Iter: 497900 Loss: 0.0037873692344874144  PSNR: 30.5949649810791
[TRAIN] Iter: 498000 Loss: 0.005480273626744747  PSNR: 27.488481521606445
[TRAIN] Iter: 498100 Loss: 0.0038793517742305994  PSNR: 29.257585525512695
[TRAIN] Iter: 498200 Loss: 0.004514164756983519  PSNR: 29.63741683959961
[TRAIN] Iter: 498300 Loss: 0.005333282984793186  PSNR: 27.149747848510742
[TRAIN] Iter: 498400 Loss: 0.004702529404312372  PSNR: 29.01386070251465
[TRAIN] Iter: 498500 Loss: 0.005359834060072899  PSNR: 27.70052719116211
[TRAIN] Iter: 498600 Loss: 0.004166409373283386  PSNR: 29.258115768432617
[TRAIN] Iter: 498700 Loss: 0.004991506692022085  PSNR: 27.28597640991211
[TRAIN] Iter: 498800 Loss: 0.0035323631018400192  PSNR: 29.846128463745117
[TRAIN] Iter: 498900 Loss: 0.005019070580601692  PSNR: 27.325252532958984
[TRAIN] Iter: 499000 Loss: 0.004731308203190565  PSNR: 28.279186248779297
[TRAIN] Iter: 499100 Loss: 0.003539147786796093  PSNR: 30.1547794342041
[TRAIN] Iter: 499200 Loss: 0.005666246172040701  PSNR: 27.41580581665039
[TRAIN] Iter: 499300 Loss: 0.004667521454393864  PSNR: 28.59842300415039
[TRAIN] Iter: 499400 Loss: 0.005836016032844782  PSNR: 27.30902862548828
[TRAIN] Iter: 499500 Loss: 0.0039194305427372456  PSNR: 29.99959945678711
[TRAIN] Iter: 499600 Loss: 0.004868816584348679  PSNR: 27.639638900756836
[TRAIN] Iter: 499700 Loss: 0.003968905657529831  PSNR: 30.015583038330078
[TRAIN] Iter: 499800 Loss: 0.005013675894588232  PSNR: 27.66735076904297
[TRAIN] Iter: 499900 Loss: 0.004112766124308109  PSNR: 28.882762908935547
Saved checkpoints at ./logs/TUT-KE101-nerf/500000.tar
0 0.00043201446533203125
torch.Size([320, 640, 3]) torch.Size([320, 640])
1 15.170031070709229
2 14.212543487548828
3 15.34071135520935
4 14.005803346633911
5 14.786375999450684
6 14.83982253074646
7 14.668157815933228
8 15.058775424957275
9 14.242109060287476
10 15.410606145858765
11 14.337049961090088
12 15.292346715927124
13 14.025413036346436
14 14.979923963546753
15 14.835044384002686
16 14.809446334838867
17 15.015743255615234
18 14.265686273574829
19 15.31553316116333
20 14.120339393615723
21 15.349940538406372
22 14.008359909057617
23 14.881191492080688
24 14.687623739242554
25 14.632476568222046
26 15.009971857070923
27 14.348056316375732
28 15.322567224502563
29 14.084421396255493
30 15.223730564117432
31 14.143981218338013
32 15.055585622787476
33 14.781732559204102
34 14.874465465545654
35 15.007570505142212
36 14.50386667251587
37 15.33414101600647
38 14.190321207046509
39 15.205310344696045
40 14.005847930908203
41 14.824925422668457
42 14.570473670959473
43 14.755947589874268
44 14.886974811553955
45 14.479755878448486
46 15.207983493804932
47 14.17416262626648
48 15.25520133972168
49 14.123991012573242
50 14.676657438278198
51 14.634639263153076
52 14.780543565750122
53 14.757046461105347
54 14.449785947799683
55 15.13221526145935
56 14.176193714141846
57 15.212671995162964
58 14.006268739700317
59 14.695505619049072
60 14.570966482162476
61 14.785314559936523
62 14.861178398132324
63 14.488975763320923
64 15.089861392974854
65 14.157829761505127
66 15.224311351776123
67 14.066420793533325
68 14.657231330871582
69 14.67276668548584
70 14.714917421340942
71 14.873772859573364
72 14.438010454177856
73 15.253593683242798
74 14.252185821533203
75 15.231501340866089
76 14.102230310440063
77 14.566128253936768
78 14.586451530456543
79 14.789480447769165
80 14.783467292785645
81 14.567718267440796
82 15.090101718902588
83 14.187631368637085
84 15.275371313095093
85 14.166508436203003
86 14.686740636825562
87 14.610219717025757
88 88.424729347229
89 14.10178828239441
90 14.613611459732056
91 14.516492366790771
92 14.78676438331604
93 14.764079570770264
94 19.64299464225769
95 14.705540657043457
96 14.765082836151123
97 15.135774850845337
98 14.970629930496216
99 14.723665952682495
100 14.16194462776184
101 15.46452021598816
102 14.693270206451416
103 14.636479616165161
104 14.946536779403687
105 14.642342329025269
106 15.204036712646484
107 14.651177883148193
108 15.037185668945312
109 14.562347888946533
110 15.123388051986694
111 14.74853253364563
112 15.043754816055298
113 14.635440349578857
114 14.842976570129395
115 14.961767435073853
116 14.500145196914673
117 14.991504192352295
118 14.400264501571655
119 15.059765577316284
Done, saving (120, 320, 640, 3) (120, 320, 640)
extras:{'raw': tensor([[[-5.2594e+00, -4.7897e+00, -3.7347e+00, -5.4667e+01],
         [-5.6191e+00, -5.0973e+00, -4.6252e+00, -6.9123e+01],
         [-4.8502e-01, -5.4174e-01, -6.8615e-01,  4.3952e+00],
         ...,
         [-2.0304e+01, -2.0155e+01, -2.3478e+01, -2.4738e+02],
         [-2.1089e+01, -2.0927e+01, -2.4228e+01, -2.5993e+02],
         [-2.1436e+01, -2.0822e+01, -2.2677e+01, -2.1961e+02]],

        [[ 2.0854e+00,  2.2192e+00,  2.9327e+00, -3.6191e+01],
         [-1.3125e-01,  8.2487e-01,  2.2542e+00, -4.9498e+01],
         [-6.1200e-01, -2.9636e-01,  4.3060e-02, -1.7998e+01],
         ...,
         [-6.8211e-01, -8.6992e-01, -1.3289e+00, -2.5232e+01],
         [-1.8499e+00, -2.1800e+00, -3.0516e+00, -7.3645e+01],
         [-2.9063e+00, -2.8198e+00, -3.0316e+00,  2.0502e+01]],

        [[-2.2009e+00, -2.2258e+00, -2.0543e+00, -4.7827e+01],
         [-1.8810e+00, -6.7341e-01,  1.7803e+00, -7.7728e+01],
         [ 7.3515e+00,  6.9883e+00,  6.2460e+00, -3.1709e+01],
         ...,
         [-6.5284e+00, -7.1450e+00, -9.4405e+00, -2.9554e+02],
         [-7.5039e+00, -9.4870e+00, -1.4138e+01, -2.7254e+02],
         [-8.0415e+00, -9.3308e+00, -1.3084e+01, -2.5236e+02]],

        ...,

        [[-1.9626e+00, -1.0876e+00,  1.0703e+00, -2.8592e+01],
         [-5.4506e-01, -2.7870e-01,  3.7500e-01, -4.9974e+01],
         [-2.5698e+00, -3.2106e+00, -4.2165e+00, -5.8484e+01],
         ...,
         [-2.0434e+01, -1.9471e+01, -2.2042e+01, -3.6711e+02],
         [-2.4271e+01, -2.2403e+01, -2.4100e+01, -3.3667e+02],
         [-2.1525e+01, -2.0959e+01, -2.3903e+01, -2.0992e+02]],

        [[ 4.1923e+00,  4.3518e+00,  5.1745e+00, -2.8017e+01],
         [-6.2361e-01, -3.0937e-02,  1.2883e+00, -7.5254e+01],
         [-9.0153e-01, -3.2897e-01,  1.0071e+00, -7.7351e+01],
         ...,
         [-1.4125e+00, -8.9821e-01, -3.0076e+00,  1.8708e+02],
         [-1.4947e+00, -7.3198e-01, -2.3142e+00,  1.9856e+02],
         [-1.4191e+00, -8.9424e-01, -2.9428e+00,  2.3286e+02]],

        [[ 5.7209e+00,  5.6805e+00,  5.6925e+00, -3.7686e+01],
         [-9.3818e-03, -2.3871e-01, -7.5326e-01, -1.3936e+01],
         [-9.3161e-02, -1.1944e-01, -2.7492e-01, -1.4922e+00],
         ...,
         [ 4.1297e-01, -4.8642e+00, -1.8645e+01,  3.8501e+02],
         [ 7.6180e-01, -4.3725e+00, -1.8019e+01,  4.1400e+02],
         [ 8.6115e-01, -3.9183e+00, -1.7076e+01,  4.7729e+02]]],
       grad_fn=<ReshapeAliasBackward0>), 'rgb0': tensor([[0.3475, 0.3397, 0.3001],
        [0.3848, 0.3045, 0.2055],
        [0.3429, 0.3078, 0.2282],
        ...,
        [0.0697, 0.0836, 0.1170],
        [0.4582, 0.4025, 0.3135],
        [0.4861, 0.4586, 0.3890]], grad_fn=<ReshapeAliasBackward0>), 'disp0': tensor([ 43.2019,  49.5257,  57.3136,  ...,  33.3580, 129.2538,  51.3857],
       grad_fn=<ReshapeAliasBackward0>), 'acc0': tensor([1., 1., 1.,  ..., 1., 1., 1.], grad_fn=<ReshapeAliasBackward0>), 'z_std': tensor([0.0022, 0.0034, 0.0624,  ..., 0.0025, 0.0023, 0.0025])}
0 0.0005636215209960938
torch.Size([320, 640, 3]) torch.Size([320, 640])
1 15.162439107894897
2 14.452131271362305
3 15.010045289993286
4 14.454753398895264
5 14.419396162033081
6 15.020378351211548
7 14.511405229568481
8 15.107280254364014
9 14.328601837158203
10 15.34899115562439
11 14.340540409088135
12 15.12665867805481
13 14.199676036834717
14 14.471731424331665
15 15.046945810317993
16 14.42966103553772
17 15.245274305343628
18 14.312195539474487
19 15.317139148712158
20 14.23880386352539
21 15.27907109260559
22 14.292072772979736
23 14.475788593292236
24 15.069129228591919
25 14.467592239379883
26 15.152714967727661
27 14.411787271499634
28 15.261156558990479
29 14.209690809249878
30 15.330665111541748
31 14.234172344207764
32 15.259157180786133
33 14.261794328689575
34 14.383499145507812
35 15.07684850692749
36 14.416179180145264
37 15.240846157073975
38 14.22161078453064
39 15.28889274597168
40 16.740572452545166
41 17.804062604904175
42 16.560465574264526
43 17.51090121269226
44 16.29676604270935
45 17.393154621124268
46 16.38644027709961
47 16.964480876922607
48 16.408655881881714
49 16.88296937942505
50 16.541640996932983
51 16.795827865600586
52 16.4455783367157
53 16.73942732810974
54 16.559611320495605
55 16.698659896850586
56 16.650102853775024
57 16.67600727081299
58 16.649415731430054
59 16.664666414260864
60 16.602355480194092
61 16.59381103515625
62 16.734151601791382
63 16.602209091186523
64 16.72215962409973
65 16.61932682991028
66 16.738243579864502
67 16.624284744262695
68 16.743491888046265
69 16.60515069961548
70 16.748408794403076
71 16.539626836776733
72 16.71966290473938
73 16.531296730041504
74 16.785783767700195
75 16.58670449256897
76 16.74993109703064
77 16.58539605140686
78 16.78645944595337
79 16.4998517036438
80 16.701131343841553
81 16.64809226989746
82 16.738981008529663
83 16.622401237487793
84 17.247304439544678
85 16.067634105682373
86 17.233935594558716
87 16.05852460861206
88 17.28074336051941
89 16.132428407669067
90 17.1241672039032
91 16.106714487075806
92 17.233814239501953
93 16.0658175945282
94 17.20469355583191
95 16.132949352264404
96 17.262860774993896
97 16.117517232894897
98 17.14662194252014
99 16.116294384002686
100 17.208275079727173
101 16.095391035079956
102 17.212562084197998
103 16.10559868812561
104 17.19435167312622
105 16.094884395599365
106 17.251513481140137
107 16.13630437850952
108 17.20989179611206
109 15.977818965911865
110 17.21206521987915
111 16.081135749816895
112 17.1704158782959
113 16.12638783454895
114 17.2097806930542
115 16.183990955352783
116 17.16521978378296
117 16.151418924331665
118 17.04317808151245
119 16.247825384140015
test poses shape torch.Size([4, 3, 4])
0 0.0007598400115966797
torch.Size([320, 640, 3]) torch.Size([320, 640])
1 16.551411867141724
2 16.809498071670532
3 16.5648033618927
Saved test set
[TRAIN] Iter: 500000 Loss: 0.004082900006324053  PSNR: 30.020492553710938
[TRAIN] Iter: 500100 Loss: 0.005419819615781307  PSNR: 27.30933380126953
[TRAIN] Iter: 500200 Loss: 0.006349064409732819  PSNR: 27.11112403869629
[TRAIN] Iter: 500300 Loss: 0.005823260173201561  PSNR: 27.197797775268555
[TRAIN] Iter: 500400 Loss: 0.005471345037221909  PSNR: 28.26677703857422
[TRAIN] Iter: 500500 Loss: 0.0043669757433235645  PSNR: 28.1856632232666
[TRAIN] Iter: 500600 Loss: 0.0040318588726222515  PSNR: 28.86903953552246
[TRAIN] Iter: 500700 Loss: 0.0046414160169661045  PSNR: 28.849403381347656
[TRAIN] Iter: 500800 Loss: 0.005223143380135298  PSNR: 28.075666427612305
[TRAIN] Iter: 500900 Loss: 0.004553998354822397  PSNR: 27.848304748535156
[TRAIN] Iter: 501000 Loss: 0.004017471801489592  PSNR: 28.670366287231445
[TRAIN] Iter: 501100 Loss: 0.004673306830227375  PSNR: 28.813827514648438
[TRAIN] Iter: 501200 Loss: 0.00508266594260931  PSNR: 27.255495071411133
[TRAIN] Iter: 501300 Loss: 0.003748338669538498  PSNR: 29.87663459777832
[TRAIN] Iter: 501400 Loss: 0.005194491241127253  PSNR: 28.292619705200195
[TRAIN] Iter: 501500 Loss: 0.003362842835485935  PSNR: 30.02216911315918
[TRAIN] Iter: 501600 Loss: 0.005276068579405546  PSNR: 27.948514938354492
[TRAIN] Iter: 501700 Loss: 0.004547477699816227  PSNR: 28.831344604492188
[TRAIN] Iter: 501800 Loss: 0.003328906139358878  PSNR: 30.33818244934082
[TRAIN] Iter: 501900 Loss: 0.004285826347768307  PSNR: 28.698057174682617
[TRAIN] Iter: 502000 Loss: 0.0043157292529940605  PSNR: 29.125446319580078
[TRAIN] Iter: 502100 Loss: 0.004093030467629433  PSNR: 28.68913459777832
[TRAIN] Iter: 502200 Loss: 0.00534279178828001  PSNR: 28.160058975219727
[TRAIN] Iter: 502300 Loss: 0.004629905801266432  PSNR: 28.307151794433594
[TRAIN] Iter: 502400 Loss: 0.0036879489198327065  PSNR: 29.689603805541992
[TRAIN] Iter: 502500 Loss: 0.00479064742103219  PSNR: 27.448291778564453
[TRAIN] Iter: 502600 Loss: 0.005466249771416187  PSNR: 27.21706771850586
[TRAIN] Iter: 502700 Loss: 0.005650791805237532  PSNR: 28.085351943969727
[TRAIN] Iter: 502800 Loss: 0.003962656017392874  PSNR: 30.143089294433594
[TRAIN] Iter: 502900 Loss: 0.0039019971154630184  PSNR: 29.41468048095703
[TRAIN] Iter: 503000 Loss: 0.004860215820372105  PSNR: 27.505767822265625
[TRAIN] Iter: 503100 Loss: 0.0033663930371403694  PSNR: 30.147687911987305
[TRAIN] Iter: 503200 Loss: 0.005046318285167217  PSNR: 28.193660736083984
[TRAIN] Iter: 503300 Loss: 0.004986126907169819  PSNR: 27.73299217224121
[TRAIN] Iter: 503400 Loss: 0.005738589912652969  PSNR: 27.730562210083008
[TRAIN] Iter: 503500 Loss: 0.005182499997317791  PSNR: 27.514524459838867
[TRAIN] Iter: 503600 Loss: 0.003547781379893422  PSNR: 30.61227035522461
[TRAIN] Iter: 503700 Loss: 0.005882982164621353  PSNR: 26.576051712036133
[TRAIN] Iter: 503800 Loss: 0.006236261688172817  PSNR: 27.493303298950195
[TRAIN] Iter: 503900 Loss: 0.005907783284783363  PSNR: 26.665498733520508
[TRAIN] Iter: 504000 Loss: 0.0033686989918351173  PSNR: 30.10225486755371
[TRAIN] Iter: 504100 Loss: 0.004662989638745785  PSNR: 27.671293258666992
[TRAIN] Iter: 504200 Loss: 0.0044979676604270935  PSNR: 27.737497329711914
[TRAIN] Iter: 504300 Loss: 0.0035707582719624043  PSNR: 30.049640655517578
[TRAIN] Iter: 504400 Loss: 0.003783874213695526  PSNR: 30.197587966918945
[TRAIN] Iter: 504500 Loss: 0.003494884818792343  PSNR: 28.829212188720703
[TRAIN] Iter: 504600 Loss: 0.0037186455447226763  PSNR: 30.219558715820312
[TRAIN] Iter: 504700 Loss: 0.004776828456670046  PSNR: 28.284282684326172
[TRAIN] Iter: 504800 Loss: 0.00554180983453989  PSNR: 28.09923553466797
[TRAIN] Iter: 504900 Loss: 0.00407273368909955  PSNR: 28.602094650268555
[TRAIN] Iter: 505000 Loss: 0.005295716226100922  PSNR: 28.108768463134766
[TRAIN] Iter: 505100 Loss: 0.0041125621646642685  PSNR: 28.863035202026367
[TRAIN] Iter: 505200 Loss: 0.005179889965802431  PSNR: 27.45038414001465
[TRAIN] Iter: 505300 Loss: 0.0044506611302495  PSNR: 28.557601928710938
[TRAIN] Iter: 505400 Loss: 0.0041388412937521935  PSNR: 29.837242126464844
[TRAIN] Iter: 505500 Loss: 0.005349947139620781  PSNR: 28.335567474365234
[TRAIN] Iter: 505600 Loss: 0.004945321008563042  PSNR: 28.160337448120117
[TRAIN] Iter: 505700 Loss: 0.004663187079131603  PSNR: 28.775163650512695
[TRAIN] Iter: 505800 Loss: 0.00467118714004755  PSNR: 27.567171096801758
[TRAIN] Iter: 505900 Loss: 0.004802219104021788  PSNR: 28.08173370361328
[TRAIN] Iter: 506000 Loss: 0.00439576106145978  PSNR: 28.29828453063965
[TRAIN] Iter: 506100 Loss: 0.0033083450980484486  PSNR: 30.07876205444336
[TRAIN] Iter: 506200 Loss: 0.004338536877185106  PSNR: 28.418554306030273
[TRAIN] Iter: 506300 Loss: 0.004100113175809383  PSNR: 28.69977378845215
[TRAIN] Iter: 506400 Loss: 0.005226378329098225  PSNR: 27.968564987182617
[TRAIN] Iter: 506500 Loss: 0.005196029786020517  PSNR: 27.68073844909668
[TRAIN] Iter: 506600 Loss: 0.004635130986571312  PSNR: 27.977752685546875
[TRAIN] Iter: 506700 Loss: 0.004426619503647089  PSNR: 29.460674285888672
[TRAIN] Iter: 506800 Loss: 0.00470888614654541  PSNR: 28.103025436401367
[TRAIN] Iter: 506900 Loss: 0.004698270931839943  PSNR: 28.482635498046875
[TRAIN] Iter: 507000 Loss: 0.004802502691745758  PSNR: 27.813587188720703
[TRAIN] Iter: 507100 Loss: 0.003491869429126382  PSNR: 29.51433753967285
[TRAIN] Iter: 507200 Loss: 0.005964952986687422  PSNR: 27.025480270385742
[TRAIN] Iter: 507300 Loss: 0.005153407342731953  PSNR: 27.273584365844727
[TRAIN] Iter: 507400 Loss: 0.005437767133116722  PSNR: 27.377233505249023
[TRAIN] Iter: 507500 Loss: 0.005253958981484175  PSNR: 27.411964416503906
[TRAIN] Iter: 507600 Loss: 0.003952923230826855  PSNR: 29.157426834106445
[TRAIN] Iter: 507700 Loss: 0.00444587878882885  PSNR: 28.094921112060547
[TRAIN] Iter: 507800 Loss: 0.004081082530319691  PSNR: 29.315013885498047
[TRAIN] Iter: 507900 Loss: 0.003380953101441264  PSNR: 29.770540237426758
[TRAIN] Iter: 508000 Loss: 0.0034026382490992546  PSNR: 30.215940475463867
[TRAIN] Iter: 508100 Loss: 0.0043299500830471516  PSNR: 28.67194175720215
[TRAIN] Iter: 508200 Loss: 0.005288525950163603  PSNR: 27.770366668701172
[TRAIN] Iter: 508300 Loss: 0.0047258734703063965  PSNR: 27.93663787841797
[TRAIN] Iter: 508400 Loss: 0.005172920413315296  PSNR: 27.52669334411621
[TRAIN] Iter: 508500 Loss: 0.0035951302852481604  PSNR: 29.973594665527344
[TRAIN] Iter: 508600 Loss: 0.0036000069230794907  PSNR: 30.523509979248047
[TRAIN] Iter: 508700 Loss: 0.004350044298917055  PSNR: 28.779563903808594
[TRAIN] Iter: 508800 Loss: 0.005229666829109192  PSNR: 27.1718692779541
[TRAIN] Iter: 508900 Loss: 0.004260341636836529  PSNR: 27.916561126708984
[TRAIN] Iter: 509000 Loss: 0.004168945364654064  PSNR: 28.432514190673828
[TRAIN] Iter: 509100 Loss: 0.005209294613450766  PSNR: 27.702974319458008
[TRAIN] Iter: 509200 Loss: 0.003338060574606061  PSNR: 30.237632751464844
[TRAIN] Iter: 509300 Loss: 0.0031217276118695736  PSNR: 29.984899520874023
[TRAIN] Iter: 509400 Loss: 0.004338354803621769  PSNR: 28.42796516418457
[TRAIN] Iter: 509500 Loss: 0.0035966201685369015  PSNR: 29.721261978149414
[TRAIN] Iter: 509600 Loss: 0.0033355006016790867  PSNR: 30.254234313964844
[TRAIN] Iter: 509700 Loss: 0.004186629317700863  PSNR: 28.805301666259766
[TRAIN] Iter: 509800 Loss: 0.004315214231610298  PSNR: 28.339033126831055
[TRAIN] Iter: 509900 Loss: 0.005162511020898819  PSNR: 28.22545623779297
Saved checkpoints at ./logs/TUT-KE101-nerf/510000.tar
[TRAIN] Iter: 510000 Loss: 0.005365803837776184  PSNR: 26.889780044555664
[TRAIN] Iter: 510100 Loss: 0.003601568751037121  PSNR: 29.96901512145996
[TRAIN] Iter: 510200 Loss: 0.004202875308692455  PSNR: 29.176668167114258
[TRAIN] Iter: 510300 Loss: 0.005238055717200041  PSNR: 28.227218627929688
[TRAIN] Iter: 510400 Loss: 0.003944803960621357  PSNR: 28.44802474975586
[TRAIN] Iter: 510500 Loss: 0.0029815626330673695  PSNR: 30.584476470947266
[TRAIN] Iter: 510600 Loss: 0.0037533468566834927  PSNR: 29.586139678955078
[TRAIN] Iter: 510700 Loss: 0.004841840825974941  PSNR: 27.497278213500977
[TRAIN] Iter: 510800 Loss: 0.004575793631374836  PSNR: 28.360275268554688
[TRAIN] Iter: 510900 Loss: 0.005350835621356964  PSNR: 27.50665283203125
[TRAIN] Iter: 511000 Loss: 0.005877691321074963  PSNR: 27.487159729003906
[TRAIN] Iter: 511100 Loss: 0.003374666441231966  PSNR: 30.54245376586914
[TRAIN] Iter: 511200 Loss: 0.003340945579111576  PSNR: 30.142501831054688
[TRAIN] Iter: 511300 Loss: 0.004672865848988295  PSNR: 28.334653854370117
[TRAIN] Iter: 511400 Loss: 0.004033886361867189  PSNR: 29.664165496826172
[TRAIN] Iter: 511500 Loss: 0.004119953606277704  PSNR: 28.050085067749023
[TRAIN] Iter: 511600 Loss: 0.005109954625368118  PSNR: 26.914888381958008
[TRAIN] Iter: 511700 Loss: 0.0058448100462555885  PSNR: 26.725717544555664
[TRAIN] Iter: 511800 Loss: 0.004139238968491554  PSNR: 29.314308166503906
[TRAIN] Iter: 511900 Loss: 0.0064897723495960236  PSNR: 26.442935943603516
[TRAIN] Iter: 512000 Loss: 0.0040112207643687725  PSNR: 28.81014060974121
[TRAIN] Iter: 512100 Loss: 0.004063441418111324  PSNR: 28.371746063232422
[TRAIN] Iter: 512200 Loss: 0.0036541426088660955  PSNR: 30.17188262939453
[TRAIN] Iter: 512300 Loss: 0.004675672389566898  PSNR: 27.58401107788086
[TRAIN] Iter: 512400 Loss: 0.0036631543189287186  PSNR: 29.900150299072266
[TRAIN] Iter: 512500 Loss: 0.004391063936054707  PSNR: 27.95795249938965
[TRAIN] Iter: 512600 Loss: 0.003862331621348858  PSNR: 29.354299545288086
[TRAIN] Iter: 512700 Loss: 0.004046685062348843  PSNR: 29.22925567626953
[TRAIN] Iter: 512800 Loss: 0.003982327878475189  PSNR: 30.418807983398438
[TRAIN] Iter: 512900 Loss: 0.0036675746086984873  PSNR: 29.867074966430664
[TRAIN] Iter: 513000 Loss: 0.00547201419249177  PSNR: 27.998485565185547
[TRAIN] Iter: 513100 Loss: 0.0043381983414292336  PSNR: 28.192888259887695
[TRAIN] Iter: 513200 Loss: 0.004845145158469677  PSNR: 27.800180435180664
[TRAIN] Iter: 513300 Loss: 0.0045025525614619255  PSNR: 28.276880264282227
[TRAIN] Iter: 513400 Loss: 0.004964709281921387  PSNR: 27.799015045166016
[TRAIN] Iter: 513500 Loss: 0.004907640628516674  PSNR: 27.923913955688477
[TRAIN] Iter: 513600 Loss: 0.004372971132397652  PSNR: 28.294570922851562
[TRAIN] Iter: 513700 Loss: 0.005312965717166662  PSNR: 27.631595611572266
[TRAIN] Iter: 513800 Loss: 0.005868667736649513  PSNR: 27.561857223510742
[TRAIN] Iter: 513900 Loss: 0.005778911989182234  PSNR: 27.172340393066406
[TRAIN] Iter: 514000 Loss: 0.005250944755971432  PSNR: 27.86733627319336
[TRAIN] Iter: 514100 Loss: 0.00444658100605011  PSNR: 27.90652084350586
[TRAIN] Iter: 514200 Loss: 0.004542828071862459  PSNR: 27.700223922729492
[TRAIN] Iter: 514300 Loss: 0.004082028288394213  PSNR: 29.948917388916016
[TRAIN] Iter: 514400 Loss: 0.0035644667223095894  PSNR: 29.659543991088867
[TRAIN] Iter: 514500 Loss: 0.004523125477135181  PSNR: 28.968490600585938
[TRAIN] Iter: 514600 Loss: 0.005230495240539312  PSNR: 27.856143951416016
[TRAIN] Iter: 514700 Loss: 0.0034728986211121082  PSNR: 30.67608642578125
[TRAIN] Iter: 514800 Loss: 0.004006029572337866  PSNR: 28.949676513671875
[TRAIN] Iter: 514900 Loss: 0.0042288294062018394  PSNR: 28.45244789123535
[TRAIN] Iter: 515000 Loss: 0.004556604661047459  PSNR: 27.870235443115234
[TRAIN] Iter: 515100 Loss: 0.0033685925882309675  PSNR: 30.692163467407227
[TRAIN] Iter: 515200 Loss: 0.005587077233940363  PSNR: 27.69428253173828
[TRAIN] Iter: 515300 Loss: 0.005259535741060972  PSNR: 27.693700790405273
[TRAIN] Iter: 515400 Loss: 0.0044934931211173534  PSNR: 28.92535972595215
[TRAIN] Iter: 515500 Loss: 0.004321501590311527  PSNR: 28.196407318115234
[TRAIN] Iter: 515600 Loss: 0.005517867393791676  PSNR: 27.746850967407227
[TRAIN] Iter: 515700 Loss: 0.004298000130802393  PSNR: 28.685102462768555
[TRAIN] Iter: 515800 Loss: 0.004806290380656719  PSNR: 28.27375602722168
[TRAIN] Iter: 515900 Loss: 0.0047579980455338955  PSNR: 28.026132583618164
[TRAIN] Iter: 516000 Loss: 0.005443788133561611  PSNR: 28.045249938964844
[TRAIN] Iter: 516100 Loss: 0.004854562226682901  PSNR: 27.282663345336914
[TRAIN] Iter: 516200 Loss: 0.005651689134538174  PSNR: 26.463396072387695
[TRAIN] Iter: 516300 Loss: 0.006178692914545536  PSNR: 26.59583854675293
[TRAIN] Iter: 516400 Loss: 0.005930865183472633  PSNR: 26.98945426940918
[TRAIN] Iter: 516500 Loss: 0.003061048686504364  PSNR: 30.775726318359375
[TRAIN] Iter: 516600 Loss: 0.005282923579216003  PSNR: 26.441808700561523
[TRAIN] Iter: 516700 Loss: 0.005606865976005793  PSNR: 27.734336853027344
[TRAIN] Iter: 516800 Loss: 0.0053942278027534485  PSNR: 27.5609130859375
[TRAIN] Iter: 516900 Loss: 0.003815803211182356  PSNR: 29.468252182006836
[TRAIN] Iter: 517000 Loss: 0.004764450713992119  PSNR: 27.99644660949707
[TRAIN] Iter: 517100 Loss: 0.003948898520320654  PSNR: 28.594907760620117
[TRAIN] Iter: 517200 Loss: 0.004730551037937403  PSNR: 27.841035842895508
[TRAIN] Iter: 517300 Loss: 0.005578823387622833  PSNR: 27.449615478515625
[TRAIN] Iter: 517400 Loss: 0.0040465593338012695  PSNR: 28.57786750793457
[TRAIN] Iter: 517500 Loss: 0.003383215516805649  PSNR: 29.619203567504883
[TRAIN] Iter: 517600 Loss: 0.004672526381909847  PSNR: 28.236112594604492
[TRAIN] Iter: 517700 Loss: 0.004825622774660587  PSNR: 27.769081115722656
[TRAIN] Iter: 517800 Loss: 0.004242659546434879  PSNR: 28.791744232177734
[TRAIN] Iter: 517900 Loss: 0.004438017960637808  PSNR: 28.495107650756836
[TRAIN] Iter: 518000 Loss: 0.004900865722447634  PSNR: 27.79713249206543
[TRAIN] Iter: 518100 Loss: 0.004998383577913046  PSNR: 27.85508155822754
[TRAIN] Iter: 518200 Loss: 0.004959776997566223  PSNR: 27.104028701782227
[TRAIN] Iter: 518300 Loss: 0.00355916447006166  PSNR: 30.35959243774414
[TRAIN] Iter: 518400 Loss: 0.005739562213420868  PSNR: 27.6866397857666
[TRAIN] Iter: 518500 Loss: 0.005738548003137112  PSNR: 27.388418197631836
[TRAIN] Iter: 518600 Loss: 0.004577097482979298  PSNR: 29.442489624023438
[TRAIN] Iter: 518700 Loss: 0.003596567315980792  PSNR: 31.013784408569336
[TRAIN] Iter: 518800 Loss: 0.004483968950808048  PSNR: 28.213207244873047
[TRAIN] Iter: 518900 Loss: 0.003891563042998314  PSNR: 29.982452392578125
[TRAIN] Iter: 519000 Loss: 0.004724002443253994  PSNR: 27.835018157958984
[TRAIN] Iter: 519100 Loss: 0.004316088277846575  PSNR: 28.52716827392578
[TRAIN] Iter: 519200 Loss: 0.004253057762980461  PSNR: 28.07796287536621
[TRAIN] Iter: 519300 Loss: 0.004602017812430859  PSNR: 28.087005615234375
[TRAIN] Iter: 519400 Loss: 0.00536250788718462  PSNR: 27.59625816345215
[TRAIN] Iter: 519500 Loss: 0.003910204395651817  PSNR: 29.39891815185547
[TRAIN] Iter: 519600 Loss: 0.004690570756793022  PSNR: 28.27964973449707
[TRAIN] Iter: 519700 Loss: 0.006814843509346247  PSNR: 26.97825813293457
[TRAIN] Iter: 519800 Loss: 0.00430464930832386  PSNR: 28.40372085571289
[TRAIN] Iter: 519900 Loss: 0.004906968213617802  PSNR: 28.81880760192871
Saved checkpoints at ./logs/TUT-KE101-nerf/520000.tar
[TRAIN] Iter: 520000 Loss: 0.004811687394976616  PSNR: 28.050893783569336
[TRAIN] Iter: 520100 Loss: 0.0037489519454538822  PSNR: 29.62307357788086
[TRAIN] Iter: 520200 Loss: 0.00402756268158555  PSNR: 29.641565322875977
[TRAIN] Iter: 520300 Loss: 0.00468025729060173  PSNR: 28.46453094482422
[TRAIN] Iter: 520400 Loss: 0.004929283633828163  PSNR: 27.746082305908203
[TRAIN] Iter: 520500 Loss: 0.004120822064578533  PSNR: 28.585119247436523
[TRAIN] Iter: 520600 Loss: 0.004704867489635944  PSNR: 28.612199783325195
[TRAIN] Iter: 520700 Loss: 0.004287827294319868  PSNR: 28.36383819580078
[TRAIN] Iter: 520800 Loss: 0.0045806290581822395  PSNR: 28.878231048583984
[TRAIN] Iter: 520900 Loss: 0.004882279317826033  PSNR: 27.654626846313477
[TRAIN] Iter: 521000 Loss: 0.004699357319623232  PSNR: 28.213581085205078
[TRAIN] Iter: 521100 Loss: 0.004655384458601475  PSNR: 27.314775466918945
[TRAIN] Iter: 521200 Loss: 0.0035909442231059074  PSNR: 30.083715438842773
[TRAIN] Iter: 521300 Loss: 0.003762434236705303  PSNR: 29.930513381958008
[TRAIN] Iter: 521400 Loss: 0.004082783591002226  PSNR: 29.466819763183594
[TRAIN] Iter: 521500 Loss: 0.0036951927468180656  PSNR: 30.049348831176758
[TRAIN] Iter: 521600 Loss: 0.004181337542831898  PSNR: 29.505868911743164
[TRAIN] Iter: 521700 Loss: 0.004436458460986614  PSNR: 27.945520401000977
[TRAIN] Iter: 521800 Loss: 0.005090252496302128  PSNR: 27.788484573364258
[TRAIN] Iter: 521900 Loss: 0.003832641988992691  PSNR: 29.70768165588379
[TRAIN] Iter: 522000 Loss: 0.00442935386672616  PSNR: 28.58363151550293
[TRAIN] Iter: 522100 Loss: 0.005050469655543566  PSNR: 28.176719665527344
[TRAIN] Iter: 522200 Loss: 0.005062676966190338  PSNR: 27.895030975341797
[TRAIN] Iter: 522300 Loss: 0.0037671164609491825  PSNR: 29.59393882751465
[TRAIN] Iter: 522400 Loss: 0.004793764092028141  PSNR: 28.548503875732422
[TRAIN] Iter: 522500 Loss: 0.003728853538632393  PSNR: 29.38221549987793
[TRAIN] Iter: 522600 Loss: 0.005522642284631729  PSNR: 27.991802215576172
[TRAIN] Iter: 522700 Loss: 0.00402688467875123  PSNR: 29.685020446777344
[TRAIN] Iter: 522800 Loss: 0.004211263731122017  PSNR: 29.472352981567383
[TRAIN] Iter: 522900 Loss: 0.0045521738938987255  PSNR: 28.8885440826416
[TRAIN] Iter: 523000 Loss: 0.004560012370347977  PSNR: 28.448734283447266
[TRAIN] Iter: 523100 Loss: 0.005552258808165789  PSNR: 27.20352554321289
[TRAIN] Iter: 523200 Loss: 0.004853719845414162  PSNR: 27.41840362548828
[TRAIN] Iter: 523300 Loss: 0.0038727112114429474  PSNR: 29.46520233154297
[TRAIN] Iter: 523400 Loss: 0.004830378573387861  PSNR: 28.01401138305664
[TRAIN] Iter: 523500 Loss: 0.00561768002808094  PSNR: 27.382572174072266
[TRAIN] Iter: 523600 Loss: 0.005325910169631243  PSNR: 27.986629486083984
[TRAIN] Iter: 523700 Loss: 0.0032401024363934994  PSNR: 30.22500991821289
[TRAIN] Iter: 523800 Loss: 0.003824670100584626  PSNR: 30.07647705078125
[TRAIN] Iter: 523900 Loss: 0.0036376584321260452  PSNR: 30.051311492919922
[TRAIN] Iter: 524000 Loss: 0.004494714550673962  PSNR: 28.669645309448242
[TRAIN] Iter: 524100 Loss: 0.0046908254735171795  PSNR: 28.4349365234375
[TRAIN] Iter: 524200 Loss: 0.004614796489477158  PSNR: 28.223846435546875
[TRAIN] Iter: 524300 Loss: 0.004558950662612915  PSNR: 28.605012893676758
[TRAIN] Iter: 524400 Loss: 0.004487834870815277  PSNR: 28.37174415588379
[TRAIN] Iter: 524500 Loss: 0.00482210423797369  PSNR: 28.17611312866211
[TRAIN] Iter: 524600 Loss: 0.004048454109579325  PSNR: 28.721824645996094
[TRAIN] Iter: 524700 Loss: 0.005746749229729176  PSNR: 27.013263702392578
[TRAIN] Iter: 524800 Loss: 0.005207453854382038  PSNR: 27.319276809692383
[TRAIN] Iter: 524900 Loss: 0.005016719922423363  PSNR: 28.356189727783203
[TRAIN] Iter: 525000 Loss: 0.005630695726722479  PSNR: 28.09127426147461
[TRAIN] Iter: 525100 Loss: 0.004077566787600517  PSNR: 28.51194953918457
[TRAIN] Iter: 525200 Loss: 0.005060710944235325  PSNR: 27.406661987304688
[TRAIN] Iter: 525300 Loss: 0.0038678571581840515  PSNR: 28.566146850585938
[TRAIN] Iter: 525400 Loss: 0.006313328631222248  PSNR: 26.46321678161621
[TRAIN] Iter: 525500 Loss: 0.005351814441382885  PSNR: 28.24659538269043
[TRAIN] Iter: 525600 Loss: 0.0043060053139925  PSNR: 28.186370849609375
[TRAIN] Iter: 525700 Loss: 0.005289913155138493  PSNR: 27.430776596069336
[TRAIN] Iter: 525800 Loss: 0.004750129301100969  PSNR: 27.555551528930664
[TRAIN] Iter: 525900 Loss: 0.005731501150876284  PSNR: 27.297508239746094
[TRAIN] Iter: 526000 Loss: 0.004422183148562908  PSNR: 27.96051597595215
[TRAIN] Iter: 526100 Loss: 0.0031590128783136606  PSNR: 30.773832321166992
[TRAIN] Iter: 526200 Loss: 0.004835312254726887  PSNR: 27.7354679107666
[TRAIN] Iter: 526300 Loss: 0.004951002076268196  PSNR: 28.00067138671875
[TRAIN] Iter: 526400 Loss: 0.004368170630186796  PSNR: 28.54253578186035
[TRAIN] Iter: 526500 Loss: 0.0035891924053430557  PSNR: 30.263710021972656
[TRAIN] Iter: 526600 Loss: 0.004068121314048767  PSNR: 29.49576187133789
[TRAIN] Iter: 526700 Loss: 0.005356119014322758  PSNR: 28.026552200317383
[TRAIN] Iter: 526800 Loss: 0.005508405156433582  PSNR: 27.632644653320312
[TRAIN] Iter: 526900 Loss: 0.003843217622488737  PSNR: 30.110610961914062
[TRAIN] Iter: 527000 Loss: 0.0048363530077040195  PSNR: 28.581296920776367
[TRAIN] Iter: 527100 Loss: 0.0040595559403300285  PSNR: 29.067411422729492
[TRAIN] Iter: 527200 Loss: 0.005022284109145403  PSNR: 28.10297966003418
[TRAIN] Iter: 527300 Loss: 0.004888040013611317  PSNR: 27.50994110107422
[TRAIN] Iter: 527400 Loss: 0.003224046900868416  PSNR: 30.528656005859375
[TRAIN] Iter: 527500 Loss: 0.004793337546288967  PSNR: 28.449525833129883
[TRAIN] Iter: 527600 Loss: 0.004363031126558781  PSNR: 29.375028610229492
[TRAIN] Iter: 527700 Loss: 0.004409572575241327  PSNR: 28.421506881713867
[TRAIN] Iter: 527800 Loss: 0.005709945224225521  PSNR: 27.541650772094727
[TRAIN] Iter: 527900 Loss: 0.0051423655822873116  PSNR: 27.700950622558594
[TRAIN] Iter: 528000 Loss: 0.003611407708376646  PSNR: 29.871185302734375
[TRAIN] Iter: 528100 Loss: 0.0046818372793495655  PSNR: 28.411483764648438
[TRAIN] Iter: 528200 Loss: 0.005313959904015064  PSNR: 27.002016067504883
[TRAIN] Iter: 528300 Loss: 0.003950017038732767  PSNR: 30.08277130126953
[TRAIN] Iter: 528400 Loss: 0.004450532607734203  PSNR: 28.850099563598633
[TRAIN] Iter: 528500 Loss: 0.004703155253082514  PSNR: 27.42062759399414
[TRAIN] Iter: 528600 Loss: 0.004590856842696667  PSNR: 28.19196128845215
[TRAIN] Iter: 528700 Loss: 0.004030962474644184  PSNR: 28.850257873535156
[TRAIN] Iter: 528800 Loss: 0.003682444104924798  PSNR: 30.466323852539062
[TRAIN] Iter: 528900 Loss: 0.004860371351242065  PSNR: 27.9743595123291
[TRAIN] Iter: 529000 Loss: 0.004561401903629303  PSNR: 28.69984245300293
[TRAIN] Iter: 529100 Loss: 0.0034579760394990444  PSNR: 30.707172393798828
[TRAIN] Iter: 529200 Loss: 0.003762725042179227  PSNR: 28.978837966918945
[TRAIN] Iter: 529300 Loss: 0.003358806949108839  PSNR: 30.08576774597168
[TRAIN] Iter: 529400 Loss: 0.0046310522593557835  PSNR: 28.57652473449707
[TRAIN] Iter: 529500 Loss: 0.00522315688431263  PSNR: 27.71898651123047
[TRAIN] Iter: 529600 Loss: 0.005171533674001694  PSNR: 27.962141036987305
[TRAIN] Iter: 529700 Loss: 0.004247278906404972  PSNR: 28.05221176147461
[TRAIN] Iter: 529800 Loss: 0.005655595101416111  PSNR: 26.969226837158203
[TRAIN] Iter: 529900 Loss: 0.0048485733568668365  PSNR: 28.50539207458496
Saved checkpoints at ./logs/TUT-KE101-nerf/530000.tar
[TRAIN] Iter: 530000 Loss: 0.004508748184889555  PSNR: 28.36357879638672
[TRAIN] Iter: 530100 Loss: 0.0057007623836398125  PSNR: 26.770151138305664
[TRAIN] Iter: 530200 Loss: 0.0034568491391837597  PSNR: 29.645395278930664
[TRAIN] Iter: 530300 Loss: 0.0048145423643291  PSNR: 27.891067504882812
[TRAIN] Iter: 530400 Loss: 0.0031270442996174097  PSNR: 30.049299240112305
[TRAIN] Iter: 530500 Loss: 0.004958190023899078  PSNR: 27.665428161621094
[TRAIN] Iter: 530600 Loss: 0.00601897481828928  PSNR: 26.673704147338867
[TRAIN] Iter: 530700 Loss: 0.004706062376499176  PSNR: 27.76900863647461
[TRAIN] Iter: 530800 Loss: 0.004094284027814865  PSNR: 29.4898738861084
[TRAIN] Iter: 530900 Loss: 0.005010521970689297  PSNR: 27.982053756713867
[TRAIN] Iter: 531000 Loss: 0.003521108999848366  PSNR: 30.113605499267578
[TRAIN] Iter: 531100 Loss: 0.0031391368247568607  PSNR: 30.766056060791016
[TRAIN] Iter: 531200 Loss: 0.0038165347650647163  PSNR: 29.791885375976562
[TRAIN] Iter: 531300 Loss: 0.005151520017534494  PSNR: 27.283287048339844
[TRAIN] Iter: 531400 Loss: 0.004136262461543083  PSNR: 28.41021156311035
[TRAIN] Iter: 531500 Loss: 0.0037394464015960693  PSNR: 30.129961013793945
[TRAIN] Iter: 531600 Loss: 0.004124010913074017  PSNR: 29.405445098876953
[TRAIN] Iter: 531700 Loss: 0.004959622398018837  PSNR: 27.326509475708008
[TRAIN] Iter: 531800 Loss: 0.004517270252108574  PSNR: 28.367137908935547
[TRAIN] Iter: 531900 Loss: 0.004975170362740755  PSNR: 27.27625846862793
[TRAIN] Iter: 532000 Loss: 0.0038910829462110996  PSNR: 28.973291397094727
[TRAIN] Iter: 532100 Loss: 0.0034653146285563707  PSNR: 30.4173526763916
[TRAIN] Iter: 532200 Loss: 0.003909890074282885  PSNR: 30.03652572631836
[TRAIN] Iter: 532300 Loss: 0.004387194756418467  PSNR: 27.996885299682617
[TRAIN] Iter: 532400 Loss: 0.005489584989845753  PSNR: 27.63973617553711
[TRAIN] Iter: 532500 Loss: 0.005117930471897125  PSNR: 28.547286987304688
[TRAIN] Iter: 532600 Loss: 0.0035432891454547644  PSNR: 30.425424575805664
[TRAIN] Iter: 532700 Loss: 0.004613935016095638  PSNR: 28.402807235717773
[TRAIN] Iter: 532800 Loss: 0.005442801862955093  PSNR: 27.258920669555664
[TRAIN] Iter: 532900 Loss: 0.004497608635574579  PSNR: 28.399005889892578
[TRAIN] Iter: 533000 Loss: 0.006303017493337393  PSNR: 26.843067169189453
[TRAIN] Iter: 533100 Loss: 0.005460164975374937  PSNR: 27.66378402709961
[TRAIN] Iter: 533200 Loss: 0.005273689515888691  PSNR: 27.665014266967773
[TRAIN] Iter: 533300 Loss: 0.005050805397331715  PSNR: 28.460708618164062
[TRAIN] Iter: 533400 Loss: 0.005266809370368719  PSNR: 27.246061325073242
[TRAIN] Iter: 533500 Loss: 0.004258619621396065  PSNR: 28.980064392089844
[TRAIN] Iter: 533600 Loss: 0.003552211681380868  PSNR: 30.802906036376953
[TRAIN] Iter: 533700 Loss: 0.004297063685953617  PSNR: 28.61101722717285
[TRAIN] Iter: 533800 Loss: 0.0038778004236519337  PSNR: 29.667177200317383
[TRAIN] Iter: 533900 Loss: 0.0037758639082312584  PSNR: 29.60468101501465
[TRAIN] Iter: 534000 Loss: 0.0035516717471182346  PSNR: 30.265670776367188
[TRAIN] Iter: 534100 Loss: 0.004182191099971533  PSNR: 29.416959762573242
[TRAIN] Iter: 534200 Loss: 0.003654557978734374  PSNR: 30.075157165527344
[TRAIN] Iter: 534300 Loss: 0.004371313843876123  PSNR: 28.86715316772461
[TRAIN] Iter: 534400 Loss: 0.0033647397067397833  PSNR: 30.43025779724121
[TRAIN] Iter: 534500 Loss: 0.004372597672045231  PSNR: 28.272357940673828
[TRAIN] Iter: 534600 Loss: 0.004539027810096741  PSNR: 28.44290542602539
[TRAIN] Iter: 534700 Loss: 0.003985305316746235  PSNR: 29.963180541992188
[TRAIN] Iter: 534800 Loss: 0.0041166567243635654  PSNR: 29.60431671142578
[TRAIN] Iter: 534900 Loss: 0.005264267325401306  PSNR: 27.362916946411133
[TRAIN] Iter: 535000 Loss: 0.0038569816388189793  PSNR: 29.287769317626953
[TRAIN] Iter: 535100 Loss: 0.004385262727737427  PSNR: 28.42706298828125
[TRAIN] Iter: 535200 Loss: 0.005266733001917601  PSNR: 27.61860466003418
[TRAIN] Iter: 535300 Loss: 0.005475567653775215  PSNR: 28.196016311645508
[TRAIN] Iter: 535400 Loss: 0.004860366694629192  PSNR: 28.1954345703125
[TRAIN] Iter: 535500 Loss: 0.0041687944903969765  PSNR: 29.553234100341797
[TRAIN] Iter: 535600 Loss: 0.004532952327281237  PSNR: 28.879436492919922
[TRAIN] Iter: 535700 Loss: 0.004348542541265488  PSNR: 28.67768669128418
[TRAIN] Iter: 535800 Loss: 0.004381279461085796  PSNR: 28.40143394470215
[TRAIN] Iter: 535900 Loss: 0.00465066684409976  PSNR: 28.218219757080078
[TRAIN] Iter: 536000 Loss: 0.004843904636800289  PSNR: 28.112836837768555
[TRAIN] Iter: 536100 Loss: 0.005071919411420822  PSNR: 27.728055953979492
[TRAIN] Iter: 536200 Loss: 0.004047343507409096  PSNR: 30.458759307861328
[TRAIN] Iter: 536300 Loss: 0.004583824425935745  PSNR: 27.627681732177734
[TRAIN] Iter: 536400 Loss: 0.005335170775651932  PSNR: 27.23475456237793
[TRAIN] Iter: 536500 Loss: 0.0042776623740792274  PSNR: 29.090023040771484
[TRAIN] Iter: 536600 Loss: 0.00433953246101737  PSNR: 28.920896530151367
[TRAIN] Iter: 536700 Loss: 0.005123753100633621  PSNR: 28.316810607910156
[TRAIN] Iter: 536800 Loss: 0.006201812066137791  PSNR: 27.27042007446289
[TRAIN] Iter: 536900 Loss: 0.004157197196036577  PSNR: 28.078327178955078
[TRAIN] Iter: 537000 Loss: 0.004938783589750528  PSNR: 27.075374603271484
[TRAIN] Iter: 537100 Loss: 0.004634438548237085  PSNR: 28.43583106994629
[TRAIN] Iter: 537200 Loss: 0.004173118621110916  PSNR: 28.021406173706055
[TRAIN] Iter: 537300 Loss: 0.0052907210774719715  PSNR: 27.030698776245117
[TRAIN] Iter: 537400 Loss: 0.003897974733263254  PSNR: 28.982194900512695
[TRAIN] Iter: 537500 Loss: 0.004935157019644976  PSNR: 28.374536514282227
[TRAIN] Iter: 537600 Loss: 0.0049049886874854565  PSNR: 27.29764747619629
[TRAIN] Iter: 537700 Loss: 0.0040018269792199135  PSNR: 29.251081466674805
[TRAIN] Iter: 537800 Loss: 0.0038211753126233816  PSNR: 28.740108489990234
[TRAIN] Iter: 537900 Loss: 0.004898764658719301  PSNR: 27.706649780273438
[TRAIN] Iter: 538000 Loss: 0.005097305867820978  PSNR: 27.86553192138672
[TRAIN] Iter: 538100 Loss: 0.004802031442523003  PSNR: 28.44785499572754
[TRAIN] Iter: 538200 Loss: 0.0034449738450348377  PSNR: 30.516820907592773
[TRAIN] Iter: 538300 Loss: 0.004251543432474136  PSNR: 29.239418029785156
[TRAIN] Iter: 538400 Loss: 0.0036212000995874405  PSNR: 29.674097061157227
[TRAIN] Iter: 538500 Loss: 0.006571026984602213  PSNR: 26.41834831237793
[TRAIN] Iter: 538600 Loss: 0.0039273579604923725  PSNR: 29.80315399169922
[TRAIN] Iter: 538700 Loss: 0.005624821875244379  PSNR: 26.932044982910156
[TRAIN] Iter: 538800 Loss: 0.00575648620724678  PSNR: 27.647058486938477
[TRAIN] Iter: 538900 Loss: 0.005131280515342951  PSNR: 27.24750328063965
[TRAIN] Iter: 539000 Loss: 0.004918600432574749  PSNR: 27.429407119750977
[TRAIN] Iter: 539100 Loss: 0.0035132979974150658  PSNR: 29.35602378845215
[TRAIN] Iter: 539200 Loss: 0.004509176593273878  PSNR: 28.353132247924805
[TRAIN] Iter: 539300 Loss: 0.004981128964573145  PSNR: 27.576810836791992
[TRAIN] Iter: 539400 Loss: 0.005072560161352158  PSNR: 27.526386260986328
[TRAIN] Iter: 539500 Loss: 0.003944532945752144  PSNR: 29.67998504638672
[TRAIN] Iter: 539600 Loss: 0.004876831546425819  PSNR: 27.552291870117188
[TRAIN] Iter: 539700 Loss: 0.0034814998507499695  PSNR: 30.227949142456055
[TRAIN] Iter: 539800 Loss: 0.005517562385648489  PSNR: 27.61626434326172
[TRAIN] Iter: 539900 Loss: 0.004038853570818901  PSNR: 29.527803421020508
Saved checkpoints at ./logs/TUT-KE101-nerf/540000.tar
[TRAIN] Iter: 540000 Loss: 0.00417039031162858  PSNR: 29.694473266601562
[TRAIN] Iter: 540100 Loss: 0.004301320295780897  PSNR: 28.734580993652344
[TRAIN] Iter: 540200 Loss: 0.003968516830354929  PSNR: 28.637954711914062
[TRAIN] Iter: 540300 Loss: 0.003591495333239436  PSNR: 29.857683181762695
[TRAIN] Iter: 540400 Loss: 0.003971979487687349  PSNR: 30.05402374267578
[TRAIN] Iter: 540500 Loss: 0.004214180633425713  PSNR: 27.87456703186035
[TRAIN] Iter: 540600 Loss: 0.0037314430810511112  PSNR: 29.848825454711914
[TRAIN] Iter: 540700 Loss: 0.0055510541424155235  PSNR: 27.568117141723633
[TRAIN] Iter: 540800 Loss: 0.0036803395487368107  PSNR: 29.78168296813965
[TRAIN] Iter: 540900 Loss: 0.004263470880687237  PSNR: 28.5687255859375
[TRAIN] Iter: 541000 Loss: 0.0036839363165199757  PSNR: 30.38833999633789
[TRAIN] Iter: 541100 Loss: 0.004625218920409679  PSNR: 27.996112823486328
[TRAIN] Iter: 541200 Loss: 0.0039741951040923595  PSNR: 29.948074340820312
[TRAIN] Iter: 541300 Loss: 0.004314490593969822  PSNR: 28.23074722290039
[TRAIN] Iter: 541400 Loss: 0.003349470905959606  PSNR: 29.98300552368164
[TRAIN] Iter: 541500 Loss: 0.004562725313007832  PSNR: 28.413803100585938
[TRAIN] Iter: 541600 Loss: 0.004870771896094084  PSNR: 28.176891326904297
[TRAIN] Iter: 541700 Loss: 0.0046768104657530785  PSNR: 28.262069702148438
[TRAIN] Iter: 541800 Loss: 0.00455872155725956  PSNR: 27.906461715698242
[TRAIN] Iter: 541900 Loss: 0.005375505890697241  PSNR: 27.527746200561523
[TRAIN] Iter: 542000 Loss: 0.00369099248200655  PSNR: 30.204389572143555
[TRAIN] Iter: 542100 Loss: 0.0047514913603663445  PSNR: 27.685062408447266
[TRAIN] Iter: 542200 Loss: 0.0035948320291936398  PSNR: 29.850452423095703
[TRAIN] Iter: 542300 Loss: 0.004317993763834238  PSNR: 28.521347045898438
[TRAIN] Iter: 542400 Loss: 0.0035823271609842777  PSNR: 29.918102264404297
[TRAIN] Iter: 542500 Loss: 0.004384471103549004  PSNR: 28.30133628845215
[TRAIN] Iter: 542600 Loss: 0.004018407315015793  PSNR: 29.195087432861328
[TRAIN] Iter: 542700 Loss: 0.0034273455385118723  PSNR: 30.530120849609375
[TRAIN] Iter: 542800 Loss: 0.005378891713917255  PSNR: 27.158885955810547
[TRAIN] Iter: 542900 Loss: 0.0037003536708652973  PSNR: 30.075992584228516
[TRAIN] Iter: 543000 Loss: 0.0038318608421832323  PSNR: 28.863229751586914
[TRAIN] Iter: 543100 Loss: 0.004207060672342777  PSNR: 27.8331356048584
[TRAIN] Iter: 543200 Loss: 0.004611327312886715  PSNR: 28.541624069213867
[TRAIN] Iter: 543300 Loss: 0.004016404040157795  PSNR: 28.98106575012207
[TRAIN] Iter: 543400 Loss: 0.00526627479121089  PSNR: 27.848907470703125
[TRAIN] Iter: 543500 Loss: 0.003980104811489582  PSNR: 29.92694854736328
[TRAIN] Iter: 543600 Loss: 0.005644128192216158  PSNR: 27.517555236816406
[TRAIN] Iter: 543700 Loss: 0.0035035242326557636  PSNR: 30.355106353759766
[TRAIN] Iter: 543800 Loss: 0.003970948047935963  PSNR: 29.861284255981445
[TRAIN] Iter: 543900 Loss: 0.00452036876231432  PSNR: 27.530433654785156
[TRAIN] Iter: 544000 Loss: 0.00407234625890851  PSNR: 30.31087875366211
[TRAIN] Iter: 544100 Loss: 0.004060185048729181  PSNR: 29.066390991210938
[TRAIN] Iter: 544200 Loss: 0.0036773793399333954  PSNR: 30.00447654724121
[TRAIN] Iter: 544300 Loss: 0.004880440887063742  PSNR: 28.060779571533203
[TRAIN] Iter: 544400 Loss: 0.004035591147840023  PSNR: 28.742403030395508
[TRAIN] Iter: 544500 Loss: 0.00586860254406929  PSNR: 27.259275436401367
[TRAIN] Iter: 544600 Loss: 0.0045503657311201096  PSNR: 27.8647518157959
[TRAIN] Iter: 544700 Loss: 0.003971962258219719  PSNR: 30.25105857849121
[TRAIN] Iter: 544800 Loss: 0.003553215181455016  PSNR: 29.19133758544922
[TRAIN] Iter: 544900 Loss: 0.004429057706147432  PSNR: 28.710647583007812
[TRAIN] Iter: 545000 Loss: 0.004144055303186178  PSNR: 28.403270721435547
[TRAIN] Iter: 545100 Loss: 0.00516430102288723  PSNR: 26.849483489990234
[TRAIN] Iter: 545200 Loss: 0.00602544192224741  PSNR: 26.757078170776367
[TRAIN] Iter: 545300 Loss: 0.0038509350270032883  PSNR: 29.291311264038086
[TRAIN] Iter: 545400 Loss: 0.004751425236463547  PSNR: 28.893640518188477
[TRAIN] Iter: 545500 Loss: 0.006020093336701393  PSNR: 27.278217315673828
[TRAIN] Iter: 545600 Loss: 0.004966001491993666  PSNR: 27.455398559570312
[TRAIN] Iter: 545700 Loss: 0.004319330211728811  PSNR: 29.285274505615234
[TRAIN] Iter: 545800 Loss: 0.004881813190877438  PSNR: 27.599193572998047
[TRAIN] Iter: 545900 Loss: 0.004151835106313229  PSNR: 28.95560073852539
[TRAIN] Iter: 546000 Loss: 0.005430320277810097  PSNR: 27.363496780395508
[TRAIN] Iter: 546100 Loss: 0.003997676074504852  PSNR: 29.286954879760742
[TRAIN] Iter: 546200 Loss: 0.002951154951006174  PSNR: 31.217639923095703
[TRAIN] Iter: 546300 Loss: 0.004141559824347496  PSNR: 28.467500686645508
[TRAIN] Iter: 546400 Loss: 0.004361992236226797  PSNR: 28.941181182861328
[TRAIN] Iter: 546500 Loss: 0.004686640575528145  PSNR: 28.804218292236328
[TRAIN] Iter: 546600 Loss: 0.005879078060388565  PSNR: 26.77764892578125
[TRAIN] Iter: 546700 Loss: 0.005900091957300901  PSNR: 26.79553985595703
[TRAIN] Iter: 546800 Loss: 0.004306569695472717  PSNR: 28.617996215820312
[TRAIN] Iter: 546900 Loss: 0.0033197940792888403  PSNR: 30.56455421447754
[TRAIN] Iter: 547000 Loss: 0.005412890575826168  PSNR: 27.494577407836914
[TRAIN] Iter: 547100 Loss: 0.005357789807021618  PSNR: 27.297651290893555
[TRAIN] Iter: 547200 Loss: 0.004099716432392597  PSNR: 28.399513244628906
[TRAIN] Iter: 547300 Loss: 0.005357357673346996  PSNR: 27.205392837524414
[TRAIN] Iter: 547400 Loss: 0.004449890926480293  PSNR: 28.083621978759766
[TRAIN] Iter: 547500 Loss: 0.004244163166731596  PSNR: 30.460460662841797
[TRAIN] Iter: 547600 Loss: 0.0038685465697199106  PSNR: 29.506715774536133
[TRAIN] Iter: 547700 Loss: 0.00370870903134346  PSNR: 30.75735092163086
[TRAIN] Iter: 547800 Loss: 0.004015462473034859  PSNR: 29.88365936279297
[TRAIN] Iter: 547900 Loss: 0.003435961902141571  PSNR: 30.188432693481445
[TRAIN] Iter: 548000 Loss: 0.004555007442831993  PSNR: 28.43671989440918
[TRAIN] Iter: 548100 Loss: 0.005289293825626373  PSNR: 27.458263397216797
[TRAIN] Iter: 548200 Loss: 0.005706203170120716  PSNR: 27.134946823120117
[TRAIN] Iter: 548300 Loss: 0.0039129965007305145  PSNR: 29.332536697387695
[TRAIN] Iter: 548400 Loss: 0.006101494189351797  PSNR: 26.68392562866211
[TRAIN] Iter: 548500 Loss: 0.003707708790898323  PSNR: 29.957355499267578
[TRAIN] Iter: 548600 Loss: 0.003908995538949966  PSNR: 29.887725830078125
[TRAIN] Iter: 548700 Loss: 0.004949619527906179  PSNR: 27.75640296936035
[TRAIN] Iter: 548800 Loss: 0.004104785155504942  PSNR: 28.912723541259766
[TRAIN] Iter: 548900 Loss: 0.005244800355285406  PSNR: 28.28041648864746
[TRAIN] Iter: 549000 Loss: 0.0035407627001404762  PSNR: 30.210691452026367
[TRAIN] Iter: 549100 Loss: 0.00536300428211689  PSNR: 27.510095596313477
[TRAIN] Iter: 549200 Loss: 0.00465967133641243  PSNR: 27.94676971435547
[TRAIN] Iter: 549300 Loss: 0.005752323195338249  PSNR: 27.40070343017578
[TRAIN] Iter: 549400 Loss: 0.0057365503162145615  PSNR: 26.561004638671875
[TRAIN] Iter: 549500 Loss: 0.004040461964905262  PSNR: 29.130393981933594
[TRAIN] Iter: 549600 Loss: 0.003666906850412488  PSNR: 29.472564697265625
[TRAIN] Iter: 549700 Loss: 0.0033813139889389277  PSNR: 30.496797561645508
[TRAIN] Iter: 549800 Loss: 0.00551945436745882  PSNR: 27.36358642578125
[TRAIN] Iter: 549900 Loss: 0.0035614855587482452  PSNR: 29.551054000854492
Saved checkpoints at ./logs/TUT-KE101-nerf/550000.tar
0 0.0003838539123535156
torch.Size([320, 640, 3]) torch.Size([320, 640])
1 13.692906618118286
2 14.89528226852417
3 13.585057735443115
4 13.519545078277588
5 15.143307447433472
6 13.705270528793335
7 14.866651773452759
8 13.740977764129639
9 14.879919528961182
10 13.689312934875488
11 13.695139169692993
12 14.939627170562744
13 13.690816640853882
14 14.950798511505127
15 13.668577432632446
16 14.963566780090332
17 13.676478385925293
18 13.65433120727539
19 15.060254573822021
20 13.569410562515259
21 15.087368249893188
22 13.628922700881958
23 14.76271677017212
24 13.771506071090698
25 13.543586254119873
26 15.0770263671875
27 13.589471101760864
28 14.942182302474976
29 13.622673988342285
30 13.676856279373169
31 15.033918142318726
32 13.486825466156006
33 15.113841772079468
34 13.478135824203491
35 15.107642412185669
36 13.565190076828003
37 13.634104490280151
38 15.020959854125977
39 13.53439450263977
40 15.185340881347656
41 13.42138147354126
42 15.144501209259033
43 13.525408506393433
44 13.586647272109985
45 15.059566736221313
46 13.504643678665161
47 15.207092761993408
48 13.43691110610962
49 15.067505598068237
50 13.662482500076294
51 13.665027141571045
52 14.933889150619507
53 13.682562589645386
54 14.939675092697144
55 13.715512752532959
56 14.93709683418274
57 13.650846004486084
58 13.671923398971558
59 14.911091089248657
60 13.678201913833618
61 14.955549001693726
62 13.70459794998169
63 13.700365543365479
64 14.940168142318726
65 13.691530466079712
66 14.939961194992065
67 13.595711469650269
68 14.93667221069336
69 13.681185483932495
70 13.707411766052246
71 14.918421030044556
72 13.693838596343994
73 14.962107419967651
74 13.644744396209717
75 14.985234260559082
76 13.704746961593628
77 13.677881717681885
78 14.939127922058105
79 13.657889366149902
80 14.990433931350708
81 13.652045488357544
82 15.044060707092285
83 13.7581148147583
84 13.754006147384644
85 14.807705163955688
86 13.664708614349365
87 14.934343576431274
88 13.621416568756104
89 14.90435004234314
90 13.673585891723633
91 13.643674373626709
92 14.953042268753052
93 13.676193714141846
94 14.986416101455688
95 13.676378965377808
96 13.682447910308838
97 14.921741247177124
98 13.657529354095459
99 14.964551210403442
100 13.699169158935547
101 14.941909074783325
102 13.664918184280396
103 13.647186517715454
104 15.011255741119385
105 13.572603225708008
106 15.09150505065918
107 13.637303113937378
108 14.971555948257446
109 13.68297791481018
110 13.597285509109497
111 15.091643333435059
112 13.58167052268982
113 15.079538583755493
114 13.57915210723877
115 14.955896615982056
116 13.78681755065918
117 13.444214820861816
118 15.088173866271973
119 13.501763343811035
Done, saving (120, 320, 640, 3) (120, 320, 640)
extras:{'raw': tensor([[[-4.5917e+00, -4.2315e+00, -3.6278e+00, -6.7997e+01],
         [ 4.1305e-01,  4.5800e-01,  7.1215e-01, -5.8450e+01],
         [ 4.5988e-01,  4.8404e-01,  7.0154e-01, -5.7486e+01],
         ...,
         [-3.0630e-01, -1.7114e+00, -3.5914e+00,  8.0034e+01],
         [-1.0673e+00, -2.4432e+00, -4.4385e+00,  7.6408e+01],
         [-5.1172e-01, -1.6367e+00, -3.1207e+00,  1.1027e+02]],

        [[ 1.6328e-01,  1.7919e-01,  5.2394e-01, -5.8991e+01],
         [-1.0761e-01, -2.3946e-01, -4.8058e-01,  6.1134e-01],
         [-8.8068e-02, -2.0779e-01, -4.2694e-01,  3.8524e+01],
         ...,
         [-2.7662e+00, -2.8557e+00, -1.2669e+01,  9.1634e+02],
         [-2.7258e+00, -2.8438e+00, -1.2794e+01,  9.1913e+02],
         [-3.1166e+00, -2.8621e+00, -1.2905e+01,  9.6625e+02]],

        [[-5.5804e-02,  2.9674e-01,  2.5731e+00, -6.0872e+01],
         [-9.9545e-01, -5.5752e-01,  3.4284e-01, -5.5374e+01],
         [ 2.9325e-01,  2.4216e-01,  2.8039e-01,  2.9128e+01],
         ...,
         [-3.0238e-01, -3.6384e+00, -1.1372e+01, -1.6646e+02],
         [-5.4631e-01, -3.7581e+00, -1.1016e+01, -1.2015e+02],
         [ 1.3061e-01, -2.7415e+00, -9.0858e+00, -8.6100e+01]],

        ...,

        [[ 4.6196e-01,  7.1499e-01,  1.1900e+00, -4.9583e+01],
         [-1.5049e-01, -3.1863e-01, -1.0939e+00, -4.7920e+01],
         [-1.1908e-01, -2.8717e-01, -1.0702e+00, -4.7282e+01],
         ...,
         [-9.4972e+00, -8.9325e+00, -1.1291e+01, -2.5291e+02],
         [-1.5600e+01, -1.2163e+01, -1.0888e+01, -1.4477e+02],
         [-1.4128e+01, -1.2334e+01, -1.2736e+01, -1.5496e+02]],

        [[ 2.8123e+00,  4.2051e+00,  7.5407e+00, -7.6915e+01],
         [-4.9157e-01, -3.5671e-01, -9.7182e-02, -3.3591e+01],
         [-5.8631e-01, -6.7812e-01, -8.2677e-01, -2.0712e+01],
         ...,
         [ 1.1291e-02, -1.3658e+00, -5.1611e+00,  2.8166e+02],
         [ 4.0486e-01, -1.1546e+00, -5.4099e+00,  2.7551e+02],
         [-5.4484e-02, -1.5660e+00, -5.7567e+00,  2.9204e+02]],

        [[ 1.2315e+00,  1.7958e+00,  3.0699e+00, -6.5500e+01],
         [-1.4511e-01, -2.8545e-01, -4.6910e-01, -9.9849e+00],
         [-1.1877e-01, -2.1130e-01, -3.1782e-01, -1.5664e+01],
         ...,
         [ 4.7554e+01,  4.2677e+01,  3.4898e+01,  1.0428e+02],
         [ 4.6999e+01,  4.1690e+01,  3.3357e+01,  8.8462e+01],
         [ 4.0372e+01,  3.7947e+01,  3.5442e+01,  1.1929e+02]]],
       grad_fn=<ReshapeAliasBackward0>), 'rgb0': tensor([[0.4459, 0.3336, 0.1629],
        [0.4796, 0.4484, 0.4072],
        [0.5073, 0.5039, 0.5189],
        ...,
        [0.5476, 0.4310, 0.2189],
        [0.4739, 0.4254, 0.3383],
        [0.8615, 0.8663, 0.9040]], grad_fn=<ReshapeAliasBackward0>), 'disp0': tensor([111.3394, 428.1859,  51.4142,  ..., 138.4043, 290.5690, 176.7167],
       grad_fn=<ReshapeAliasBackward0>), 'acc0': tensor([1., 1., 1.,  ..., 1., 1., 1.], grad_fn=<ReshapeAliasBackward0>), 'z_std': tensor([0.0785, 0.2918, 0.0436,  ..., 0.0018, 0.0114, 0.0020])}
0 0.000530242919921875
torch.Size([320, 640, 3]) torch.Size([320, 640])
1 13.442469596862793
2 15.166028499603271
3 13.483306884765625
4 13.678341388702393
5 15.093680381774902
6 13.472789525985718
7 15.193405151367188
8 13.489934206008911
9 14.996203899383545
10 13.714542627334595
11 13.634392976760864
12 14.865646600723267
13 13.555561542510986
14 15.113028526306152
15 13.550003290176392
16 14.99431562423706
17 13.638634204864502
18 13.51063346862793
19 15.158173322677612
20 13.627955198287964
21 14.936789512634277
22 13.652027130126953
23 13.644839525222778
24 14.911906480789185
25 13.614904642105103
26 14.954183101654053
27 13.650126695632935
28 14.938912868499756
29 13.643386363983154
30 13.654077529907227
31 14.896447896957397
32 13.692749977111816
33 14.987817287445068
34 13.63456392288208
35 14.97381329536438
36 13.674491167068481
37 13.640440940856934
38 14.955024242401123
39 13.630603551864624
40 14.99808120727539
41 13.599064111709595
42 13.63776183128357
43 14.934719324111938
44 13.70859670639038
45 14.943609237670898
46 13.657813549041748
47 14.962645530700684
48 13.635937213897705
49 13.61864185333252
50 14.965691804885864
51 14.968520402908325
52 16.858617067337036
53 15.559485912322998
54 16.89840054512024
55 15.587997674942017
56 16.883813619613647
57 15.472899675369263
58 16.891958951950073
59 15.541422367095947
60 16.862841367721558
61 15.540011882781982
62 16.87410044670105
63 15.514732122421265
64 16.990681409835815
65 15.513962745666504
66 16.994855403900146
67 15.62009572982788
68 16.87844705581665
69 15.75640320777893
70 17.417115688323975
71 16.10452628135681
72 17.38070845603943
73 16.061893224716187
74 17.423415660858154
75 24.25168514251709
76 17.463768482208252
77 15.888332843780518
78 17.583536863327026
79 21.78379988670349
80 22.87693977355957
81 17.650128841400146
82 15.871865034103394
83 17.554580211639404
84 20.40356969833374
85 17.572574615478516
86 15.881539344787598
87 17.808141946792603
88 15.835363626480103
89 27.023386240005493
90 17.55732226371765
91 15.944000720977783
92 430.17827701568604
93 253.65960884094238
94 15.934395551681519
95 17.12273406982422
96 15.955450057983398
97 17.07030439376831
98 15.854644298553467
99 17.13449454307556
100 15.858266592025757
101 17.160290241241455
102 15.891196727752686
103 17.06235432624817
104 15.826889991760254
105 17.10527276992798
106 15.807421922683716
107 17.181489944458008
108 15.792455196380615
109 17.344587087631226
110 15.711421489715576
111 17.274158239364624
112 15.636454105377197
113 17.476348161697388
114 15.582524299621582
115 17.394081830978394
116 15.511921405792236
117 17.532400131225586
118 15.430025339126587
119 17.523370265960693
test poses shape torch.Size([4, 3, 4])
0 0.0007350444793701172
torch.Size([320, 640, 3]) torch.Size([320, 640])
1 17.56295657157898
2 15.552161931991577
3 17.534221172332764
Saved test set
[TRAIN] Iter: 550000 Loss: 0.005771918222308159  PSNR: 27.03597068786621
[TRAIN] Iter: 550100 Loss: 0.0038371977861970663  PSNR: 29.67135238647461
[TRAIN] Iter: 550200 Loss: 0.005895337089896202  PSNR: 27.430570602416992
[TRAIN] Iter: 550300 Loss: 0.005607855040580034  PSNR: 27.51308822631836
[TRAIN] Iter: 550400 Loss: 0.004823053255677223  PSNR: 27.400047302246094
[TRAIN] Iter: 550500 Loss: 0.003679742105305195  PSNR: 30.031639099121094
[TRAIN] Iter: 550600 Loss: 0.004640354309231043  PSNR: 27.318477630615234
[TRAIN] Iter: 550700 Loss: 0.003715185448527336  PSNR: 29.81189727783203
[TRAIN] Iter: 550800 Loss: 0.005530606955289841  PSNR: 27.26060676574707
[TRAIN] Iter: 550900 Loss: 0.004636578261852264  PSNR: 28.14854621887207
[TRAIN] Iter: 551000 Loss: 0.005306273698806763  PSNR: 27.796350479125977
[TRAIN] Iter: 551100 Loss: 0.004059570841491222  PSNR: 29.36023712158203
[TRAIN] Iter: 551200 Loss: 0.004600026179105043  PSNR: 28.274465560913086
[TRAIN] Iter: 551300 Loss: 0.003903517033904791  PSNR: 29.490053176879883
[TRAIN] Iter: 551400 Loss: 0.004865854512900114  PSNR: 27.965669631958008
[TRAIN] Iter: 551500 Loss: 0.0035662921145558357  PSNR: 30.631710052490234
[TRAIN] Iter: 551600 Loss: 0.004841671325266361  PSNR: 27.868741989135742
[TRAIN] Iter: 551700 Loss: 0.005115943029522896  PSNR: 27.50912857055664
[TRAIN] Iter: 551800 Loss: 0.0035234468523412943  PSNR: 29.97519302368164
[TRAIN] Iter: 551900 Loss: 0.005194785539060831  PSNR: 28.067119598388672
[TRAIN] Iter: 552000 Loss: 0.004430470056831837  PSNR: 28.526269912719727
[TRAIN] Iter: 552100 Loss: 0.005408010445535183  PSNR: 27.85520362854004
[TRAIN] Iter: 552200 Loss: 0.003338782349601388  PSNR: 30.598947525024414
[TRAIN] Iter: 552300 Loss: 0.003668446559458971  PSNR: 29.43453025817871
[TRAIN] Iter: 552400 Loss: 0.00488424813374877  PSNR: 28.06473731994629
[TRAIN] Iter: 552500 Loss: 0.004349197261035442  PSNR: 28.34440040588379
[TRAIN] Iter: 552600 Loss: 0.004293256904929876  PSNR: 28.26398468017578
[TRAIN] Iter: 552700 Loss: 0.0031846272759139538  PSNR: 30.57071304321289
[TRAIN] Iter: 552800 Loss: 0.005697980057448149  PSNR: 27.74240493774414
[TRAIN] Iter: 552900 Loss: 0.0035587737802416086  PSNR: 29.43531608581543
[TRAIN] Iter: 553000 Loss: 0.003923483192920685  PSNR: 28.919246673583984
[TRAIN] Iter: 553100 Loss: 0.00547393225133419  PSNR: 27.69713020324707
[TRAIN] Iter: 553200 Loss: 0.005164202302694321  PSNR: 28.258302688598633
[TRAIN] Iter: 553300 Loss: 0.003558647818863392  PSNR: 29.31344223022461
[TRAIN] Iter: 553400 Loss: 0.005525745451450348  PSNR: 27.180644989013672
[TRAIN] Iter: 553500 Loss: 0.005788367707282305  PSNR: 27.073896408081055
[TRAIN] Iter: 553600 Loss: 0.004096959717571735  PSNR: 28.40265655517578
[TRAIN] Iter: 553700 Loss: 0.003826064057648182  PSNR: 29.794496536254883
[TRAIN] Iter: 553800 Loss: 0.004129163920879364  PSNR: 28.663061141967773
[TRAIN] Iter: 553900 Loss: 0.0047356123104691505  PSNR: 27.8220157623291
[TRAIN] Iter: 554000 Loss: 0.004164827987551689  PSNR: 29.03577995300293
[TRAIN] Iter: 554100 Loss: 0.003873982233926654  PSNR: 29.27044677734375
[TRAIN] Iter: 554200 Loss: 0.0055124033242464066  PSNR: 27.209854125976562
[TRAIN] Iter: 554300 Loss: 0.0037902246695011854  PSNR: 30.140216827392578
[TRAIN] Iter: 554400 Loss: 0.0034000216983258724  PSNR: 30.204605102539062
[TRAIN] Iter: 554500 Loss: 0.004647344350814819  PSNR: 27.912260055541992
[TRAIN] Iter: 554600 Loss: 0.006114914081990719  PSNR: 26.41777801513672
[TRAIN] Iter: 554700 Loss: 0.005195241887122393  PSNR: 27.448863983154297
[TRAIN] Iter: 554800 Loss: 0.004071529023349285  PSNR: 28.51885223388672
[TRAIN] Iter: 554900 Loss: 0.004447019658982754  PSNR: 28.7139949798584
[TRAIN] Iter: 555000 Loss: 0.00476119015365839  PSNR: 28.134933471679688
[TRAIN] Iter: 555100 Loss: 0.005374740809202194  PSNR: 27.184484481811523
[TRAIN] Iter: 555200 Loss: 0.00458099227398634  PSNR: 28.07904052734375
[TRAIN] Iter: 555300 Loss: 0.004996815230697393  PSNR: 28.132160186767578
[TRAIN] Iter: 555400 Loss: 0.004613139666616917  PSNR: 28.005290985107422
[TRAIN] Iter: 555500 Loss: 0.004709345288574696  PSNR: 28.1259708404541
[TRAIN] Iter: 555600 Loss: 0.004303476773202419  PSNR: 28.192710876464844
[TRAIN] Iter: 555700 Loss: 0.003518032608553767  PSNR: 30.149799346923828
[TRAIN] Iter: 555800 Loss: 0.004239402245730162  PSNR: 29.311229705810547
[TRAIN] Iter: 555900 Loss: 0.005502013023942709  PSNR: 26.63478660583496
[TRAIN] Iter: 556000 Loss: 0.005116933025419712  PSNR: 27.86154556274414
[TRAIN] Iter: 556100 Loss: 0.0039007924497127533  PSNR: 30.61863136291504
[TRAIN] Iter: 556200 Loss: 0.004056061152368784  PSNR: 29.058698654174805
[TRAIN] Iter: 556300 Loss: 0.004977061413228512  PSNR: 27.485729217529297
[TRAIN] Iter: 556400 Loss: 0.005341545678675175  PSNR: 27.886890411376953
[TRAIN] Iter: 556500 Loss: 0.0058713252656161785  PSNR: 26.71466827392578
[TRAIN] Iter: 556600 Loss: 0.0042101494036614895  PSNR: 28.549901962280273
[TRAIN] Iter: 556700 Loss: 0.004275087267160416  PSNR: 28.248014450073242
[TRAIN] Iter: 556800 Loss: 0.003817611839622259  PSNR: 29.684656143188477
[TRAIN] Iter: 556900 Loss: 0.003941820003092289  PSNR: 30.2497501373291
[TRAIN] Iter: 557000 Loss: 0.006305485498160124  PSNR: 26.71889877319336
[TRAIN] Iter: 557100 Loss: 0.005577598698437214  PSNR: 26.73035430908203
[TRAIN] Iter: 557200 Loss: 0.0035381962079554796  PSNR: 29.265254974365234
[TRAIN] Iter: 557300 Loss: 0.005037024617195129  PSNR: 28.076101303100586
[TRAIN] Iter: 557400 Loss: 0.0053514475002884865  PSNR: 27.724916458129883
[TRAIN] Iter: 557500 Loss: 0.004002549685537815  PSNR: 28.700220108032227
[TRAIN] Iter: 557600 Loss: 0.005516082514077425  PSNR: 26.8931827545166
[TRAIN] Iter: 557700 Loss: 0.005754987709224224  PSNR: 26.361522674560547
[TRAIN] Iter: 557800 Loss: 0.0043034967966377735  PSNR: 28.140443801879883
[TRAIN] Iter: 557900 Loss: 0.0034170513972640038  PSNR: 30.247882843017578
[TRAIN] Iter: 558000 Loss: 0.005298494361341  PSNR: 27.41355323791504
[TRAIN] Iter: 558100 Loss: 0.0039001568220555782  PSNR: 30.47080421447754
[TRAIN] Iter: 558200 Loss: 0.004465490113943815  PSNR: 28.336965560913086
[TRAIN] Iter: 558300 Loss: 0.004457931034266949  PSNR: 28.401445388793945
[TRAIN] Iter: 558400 Loss: 0.004925563931465149  PSNR: 28.339576721191406
[TRAIN] Iter: 558500 Loss: 0.004466581158339977  PSNR: 28.33871841430664
[TRAIN] Iter: 558600 Loss: 0.0038752055261284113  PSNR: 29.826175689697266
[TRAIN] Iter: 558700 Loss: 0.0037352382205426693  PSNR: 29.732830047607422
[TRAIN] Iter: 558800 Loss: 0.004537141416221857  PSNR: 28.522462844848633
[TRAIN] Iter: 558900 Loss: 0.004703514743596315  PSNR: 28.019023895263672
[TRAIN] Iter: 559000 Loss: 0.003953386098146439  PSNR: 29.83881187438965
[TRAIN] Iter: 559100 Loss: 0.004584767390042543  PSNR: 28.218017578125
[TRAIN] Iter: 559200 Loss: 0.005175188183784485  PSNR: 27.16956329345703
[TRAIN] Iter: 559300 Loss: 0.005091067403554916  PSNR: 27.62543487548828
[TRAIN] Iter: 559400 Loss: 0.005812769755721092  PSNR: 27.150545120239258
[TRAIN] Iter: 559500 Loss: 0.005090796854346991  PSNR: 27.58761978149414
[TRAIN] Iter: 559600 Loss: 0.004849506542086601  PSNR: 27.612884521484375
[TRAIN] Iter: 559700 Loss: 0.004164581652730703  PSNR: 29.0832576751709
[TRAIN] Iter: 559800 Loss: 0.0037159172352403402  PSNR: 28.616743087768555
[TRAIN] Iter: 559900 Loss: 0.004449754022061825  PSNR: 27.98438262939453
Saved checkpoints at ./logs/TUT-KE101-nerf/560000.tar
[TRAIN] Iter: 560000 Loss: 0.003952755592763424  PSNR: 29.70489501953125
[TRAIN] Iter: 560100 Loss: 0.004287377931177616  PSNR: 28.478269577026367
[TRAIN] Iter: 560200 Loss: 0.0044126929715275764  PSNR: 28.07400131225586
[TRAIN] Iter: 560300 Loss: 0.00465690903365612  PSNR: 27.218891143798828
[TRAIN] Iter: 560400 Loss: 0.0047201779671013355  PSNR: 28.16341209411621
[TRAIN] Iter: 560500 Loss: 0.003878314048051834  PSNR: 28.81503677368164
[TRAIN] Iter: 560600 Loss: 0.003728646319359541  PSNR: 29.823667526245117
[TRAIN] Iter: 560700 Loss: 0.004122306127101183  PSNR: 29.786470413208008
[TRAIN] Iter: 560800 Loss: 0.004761195741593838  PSNR: 28.09950065612793
[TRAIN] Iter: 560900 Loss: 0.004795497749000788  PSNR: 27.866153717041016
[TRAIN] Iter: 561000 Loss: 0.0038215958047658205  PSNR: 29.250062942504883
[TRAIN] Iter: 561100 Loss: 0.0059698279947042465  PSNR: 27.664413452148438
[TRAIN] Iter: 561200 Loss: 0.0035442369990050793  PSNR: 30.04743003845215
[TRAIN] Iter: 561300 Loss: 0.004826704505831003  PSNR: 27.971576690673828
[TRAIN] Iter: 561400 Loss: 0.0062452950514853  PSNR: 27.049880981445312
[TRAIN] Iter: 561500 Loss: 0.004396870732307434  PSNR: 28.877153396606445
[TRAIN] Iter: 561600 Loss: 0.003955111373215914  PSNR: 28.152925491333008
[TRAIN] Iter: 561700 Loss: 0.004334640689194202  PSNR: 28.763261795043945
[TRAIN] Iter: 561800 Loss: 0.004732190631330013  PSNR: 28.12337303161621
[TRAIN] Iter: 561900 Loss: 0.004697522148489952  PSNR: 28.134037017822266
[TRAIN] Iter: 562000 Loss: 0.004188430029898882  PSNR: 29.326169967651367
[TRAIN] Iter: 562100 Loss: 0.0036536692641675472  PSNR: 30.01247787475586
[TRAIN] Iter: 562200 Loss: 0.004533923696726561  PSNR: 28.490793228149414
[TRAIN] Iter: 562300 Loss: 0.0035823192447423935  PSNR: 29.900070190429688
[TRAIN] Iter: 562400 Loss: 0.005088418256491423  PSNR: 27.790510177612305
[TRAIN] Iter: 562500 Loss: 0.00457138754427433  PSNR: 28.026798248291016
[TRAIN] Iter: 562600 Loss: 0.003552541369572282  PSNR: 30.377620697021484
[TRAIN] Iter: 562700 Loss: 0.004769633524119854  PSNR: 28.481380462646484
[TRAIN] Iter: 562800 Loss: 0.0039966655895113945  PSNR: 28.721012115478516
[TRAIN] Iter: 562900 Loss: 0.0037866709753870964  PSNR: 28.80723762512207
[TRAIN] Iter: 563000 Loss: 0.004991834983229637  PSNR: 27.508562088012695
[TRAIN] Iter: 563100 Loss: 0.00512356124818325  PSNR: 27.789783477783203
[TRAIN] Iter: 563200 Loss: 0.004498482681810856  PSNR: 28.284385681152344
[TRAIN] Iter: 563300 Loss: 0.004798744339495897  PSNR: 27.950149536132812
[TRAIN] Iter: 563400 Loss: 0.004535148851573467  PSNR: 27.872671127319336
[TRAIN] Iter: 563500 Loss: 0.003881135955452919  PSNR: 29.511619567871094
[TRAIN] Iter: 563600 Loss: 0.004015713930130005  PSNR: 29.96867561340332
[TRAIN] Iter: 563700 Loss: 0.00592642929404974  PSNR: 26.816850662231445
[TRAIN] Iter: 563800 Loss: 0.004579360131174326  PSNR: 28.433012008666992
[TRAIN] Iter: 563900 Loss: 0.003827121574431658  PSNR: 28.773128509521484
[TRAIN] Iter: 564000 Loss: 0.005137500818818808  PSNR: 27.799942016601562
[TRAIN] Iter: 564100 Loss: 0.004875230602920055  PSNR: 27.50812530517578
[TRAIN] Iter: 564200 Loss: 0.004614108242094517  PSNR: 27.55662727355957
[TRAIN] Iter: 564300 Loss: 0.004188802558928728  PSNR: 29.172393798828125
[TRAIN] Iter: 564400 Loss: 0.00451529398560524  PSNR: 28.383480072021484
[TRAIN] Iter: 564500 Loss: 0.004917613696306944  PSNR: 27.052532196044922
[TRAIN] Iter: 564600 Loss: 0.0035752030089497566  PSNR: 29.953319549560547
[TRAIN] Iter: 564700 Loss: 0.004918280988931656  PSNR: 28.124235153198242
[TRAIN] Iter: 564800 Loss: 0.004633883945643902  PSNR: 28.129701614379883
[TRAIN] Iter: 564900 Loss: 0.005041265860199928  PSNR: 27.50494956970215
[TRAIN] Iter: 565000 Loss: 0.004303727764636278  PSNR: 29.341211318969727
[TRAIN] Iter: 565100 Loss: 0.003956099506467581  PSNR: 29.425697326660156
[TRAIN] Iter: 565200 Loss: 0.003810785710811615  PSNR: 29.470977783203125
[TRAIN] Iter: 565300 Loss: 0.0050859879702329636  PSNR: 28.06264305114746
[TRAIN] Iter: 565400 Loss: 0.004496101289987564  PSNR: 28.532718658447266
[TRAIN] Iter: 565500 Loss: 0.003104406176134944  PSNR: 29.929271697998047
[TRAIN] Iter: 565600 Loss: 0.0056820823810994625  PSNR: 26.800077438354492
[TRAIN] Iter: 565700 Loss: 0.004835603293031454  PSNR: 27.610260009765625
[TRAIN] Iter: 565800 Loss: 0.004129582084715366  PSNR: 29.566152572631836
[TRAIN] Iter: 565900 Loss: 0.004350872710347176  PSNR: 28.293060302734375
[TRAIN] Iter: 566000 Loss: 0.0048751612193882465  PSNR: 28.712629318237305
[TRAIN] Iter: 566100 Loss: 0.003901881165802479  PSNR: 28.497447967529297
[TRAIN] Iter: 566200 Loss: 0.003909525461494923  PSNR: 29.381649017333984
[TRAIN] Iter: 566300 Loss: 0.0037226928398013115  PSNR: 29.817092895507812
[TRAIN] Iter: 566400 Loss: 0.00569632463157177  PSNR: 28.11693000793457
[TRAIN] Iter: 566500 Loss: 0.004865370690822601  PSNR: 28.09848403930664
[TRAIN] Iter: 566600 Loss: 0.004313956014811993  PSNR: 28.499650955200195
[TRAIN] Iter: 566700 Loss: 0.0055436016991734505  PSNR: 27.587982177734375
[TRAIN] Iter: 566800 Loss: 0.0045313467271625996  PSNR: 28.68403434753418
[TRAIN] Iter: 566900 Loss: 0.005930696614086628  PSNR: 27.674942016601562
[TRAIN] Iter: 567000 Loss: 0.004320061765611172  PSNR: 27.710046768188477
[TRAIN] Iter: 567100 Loss: 0.005250397138297558  PSNR: 27.186704635620117
[TRAIN] Iter: 567200 Loss: 0.004246641881763935  PSNR: 27.99212646484375
[TRAIN] Iter: 567300 Loss: 0.003923299256712198  PSNR: 30.73857307434082
[TRAIN] Iter: 567400 Loss: 0.005306220147758722  PSNR: 27.552440643310547
[TRAIN] Iter: 567500 Loss: 0.004202155862003565  PSNR: 28.529117584228516
[TRAIN] Iter: 567600 Loss: 0.003864782163873315  PSNR: 30.808631896972656
[TRAIN] Iter: 567700 Loss: 0.0034549408592283726  PSNR: 29.23768424987793
[TRAIN] Iter: 567800 Loss: 0.0038804099895060062  PSNR: 30.070323944091797
[TRAIN] Iter: 567900 Loss: 0.003476864192634821  PSNR: 30.3046875
[TRAIN] Iter: 568000 Loss: 0.006119053345173597  PSNR: 26.807952880859375
[TRAIN] Iter: 568100 Loss: 0.004888150840997696  PSNR: 27.672565460205078
[TRAIN] Iter: 568200 Loss: 0.003485921071842313  PSNR: 29.942001342773438
[TRAIN] Iter: 568300 Loss: 0.005059209186583757  PSNR: 27.235965728759766
[TRAIN] Iter: 568400 Loss: 0.0038456812035292387  PSNR: 28.520645141601562
[TRAIN] Iter: 568500 Loss: 0.005017307121306658  PSNR: 27.812131881713867
[TRAIN] Iter: 568600 Loss: 0.0040216571651399136  PSNR: 29.617652893066406
[TRAIN] Iter: 568700 Loss: 0.004709393251687288  PSNR: 28.647119522094727
[TRAIN] Iter: 568800 Loss: 0.004100481979548931  PSNR: 29.358510971069336
[TRAIN] Iter: 568900 Loss: 0.004062670283019543  PSNR: 28.68423080444336
[TRAIN] Iter: 569000 Loss: 0.004044634290039539  PSNR: 28.62625503540039
[TRAIN] Iter: 569100 Loss: 0.004925436340272427  PSNR: 28.09295654296875
[TRAIN] Iter: 569200 Loss: 0.004383582156151533  PSNR: 28.16939353942871
[TRAIN] Iter: 569300 Loss: 0.004232565872371197  PSNR: 28.6614933013916
[TRAIN] Iter: 569400 Loss: 0.00515333516523242  PSNR: 27.573471069335938
[TRAIN] Iter: 569500 Loss: 0.004825468175113201  PSNR: 27.76983642578125
[TRAIN] Iter: 569600 Loss: 0.005952693056315184  PSNR: 27.443450927734375
[TRAIN] Iter: 569700 Loss: 0.003983353730291128  PSNR: 29.377779006958008
[TRAIN] Iter: 569800 Loss: 0.004890384152531624  PSNR: 28.151796340942383
[TRAIN] Iter: 569900 Loss: 0.0033387565053999424  PSNR: 30.684711456298828
Saved checkpoints at ./logs/TUT-KE101-nerf/570000.tar
[TRAIN] Iter: 570000 Loss: 0.003249329747632146  PSNR: 30.123476028442383
[TRAIN] Iter: 570100 Loss: 0.004689999856054783  PSNR: 28.499740600585938
[TRAIN] Iter: 570200 Loss: 0.0033004663418978453  PSNR: 30.85352897644043
[TRAIN] Iter: 570300 Loss: 0.0042006089352071285  PSNR: 30.105581283569336
[TRAIN] Iter: 570400 Loss: 0.004148786421865225  PSNR: 28.262357711791992
[TRAIN] Iter: 570500 Loss: 0.004293340723961592  PSNR: 28.601627349853516
[TRAIN] Iter: 570600 Loss: 0.004870029166340828  PSNR: 28.145606994628906
[TRAIN] Iter: 570700 Loss: 0.006077519152313471  PSNR: 26.4859561920166
[TRAIN] Iter: 570800 Loss: 0.005269936751574278  PSNR: 27.88737678527832
[TRAIN] Iter: 570900 Loss: 0.00564738642424345  PSNR: 26.909608840942383
[TRAIN] Iter: 571000 Loss: 0.005514924414455891  PSNR: 27.04155921936035
[TRAIN] Iter: 571100 Loss: 0.0049484651535749435  PSNR: 28.07752227783203
[TRAIN] Iter: 571200 Loss: 0.003967374097555876  PSNR: 28.84743881225586
[TRAIN] Iter: 571300 Loss: 0.0035030138678848743  PSNR: 29.6686954498291
[TRAIN] Iter: 571400 Loss: 0.004707376938313246  PSNR: 28.033212661743164
[TRAIN] Iter: 571500 Loss: 0.004919636063277721  PSNR: 27.72035026550293
[TRAIN] Iter: 571600 Loss: 0.005440869368612766  PSNR: 26.896093368530273
[TRAIN] Iter: 571700 Loss: 0.004131227731704712  PSNR: 28.225936889648438
[TRAIN] Iter: 571800 Loss: 0.004917589016258717  PSNR: 27.511409759521484
[TRAIN] Iter: 571900 Loss: 0.00509229488670826  PSNR: 27.277353286743164
[TRAIN] Iter: 572000 Loss: 0.00407777214422822  PSNR: 30.0964298248291
[TRAIN] Iter: 572100 Loss: 0.0045986296609044075  PSNR: 28.202024459838867
[TRAIN] Iter: 572200 Loss: 0.0036094943061470985  PSNR: 29.710908889770508
[TRAIN] Iter: 572300 Loss: 0.0036011342890560627  PSNR: 29.952720642089844
[TRAIN] Iter: 572400 Loss: 0.0038228309713304043  PSNR: 29.892807006835938
[TRAIN] Iter: 572500 Loss: 0.0049191792495548725  PSNR: 27.745073318481445
[TRAIN] Iter: 572600 Loss: 0.004990534856915474  PSNR: 28.13634490966797
[TRAIN] Iter: 572700 Loss: 0.005567541345953941  PSNR: 26.559961318969727
[TRAIN] Iter: 572800 Loss: 0.0047259205020964146  PSNR: 28.978355407714844
[TRAIN] Iter: 572900 Loss: 0.004693768452852964  PSNR: 28.386606216430664
[TRAIN] Iter: 573000 Loss: 0.003719438798725605  PSNR: 30.03615951538086
[TRAIN] Iter: 573100 Loss: 0.00397574482485652  PSNR: 30.20682144165039
[TRAIN] Iter: 573200 Loss: 0.004117815755307674  PSNR: 27.959482192993164
[TRAIN] Iter: 573300 Loss: 0.0056012459099292755  PSNR: 27.401283264160156
[TRAIN] Iter: 573400 Loss: 0.0041717891581356525  PSNR: 28.64603042602539
[TRAIN] Iter: 573500 Loss: 0.0044773416593670845  PSNR: 28.2575626373291
[TRAIN] Iter: 573600 Loss: 0.005190175957977772  PSNR: 27.842857360839844
[TRAIN] Iter: 573700 Loss: 0.00463503273203969  PSNR: 28.235979080200195
[TRAIN] Iter: 573800 Loss: 0.004583863541483879  PSNR: 27.736160278320312
[TRAIN] Iter: 573900 Loss: 0.0041447291150689125  PSNR: 29.07010269165039
[TRAIN] Iter: 574000 Loss: 0.005011601373553276  PSNR: 27.89884376525879
[TRAIN] Iter: 574100 Loss: 0.004408528562635183  PSNR: 28.650094985961914
[TRAIN] Iter: 574200 Loss: 0.005311972461640835  PSNR: 27.568693161010742
[TRAIN] Iter: 574300 Loss: 0.004899276420474052  PSNR: 27.73555564880371
[TRAIN] Iter: 574400 Loss: 0.005881421267986298  PSNR: 27.058330535888672
[TRAIN] Iter: 574500 Loss: 0.0034638596698641777  PSNR: 30.722471237182617
[TRAIN] Iter: 574600 Loss: 0.005737274885177612  PSNR: 27.186397552490234
[TRAIN] Iter: 574700 Loss: 0.003499172395095229  PSNR: 30.025705337524414
[TRAIN] Iter: 574800 Loss: 0.0038703749887645245  PSNR: 29.641740798950195
[TRAIN] Iter: 574900 Loss: 0.004127906169742346  PSNR: 27.94745445251465
[TRAIN] Iter: 575000 Loss: 0.004296537488698959  PSNR: 28.766939163208008
[TRAIN] Iter: 575100 Loss: 0.003003692254424095  PSNR: 31.123994827270508
[TRAIN] Iter: 575200 Loss: 0.004652620758861303  PSNR: 28.645612716674805
[TRAIN] Iter: 575300 Loss: 0.0050649987533688545  PSNR: 28.478755950927734
[TRAIN] Iter: 575400 Loss: 0.00571519834920764  PSNR: 27.074405670166016
[TRAIN] Iter: 575500 Loss: 0.0036771984305232763  PSNR: 29.513160705566406
[TRAIN] Iter: 575600 Loss: 0.004090454429388046  PSNR: 29.818086624145508
[TRAIN] Iter: 575700 Loss: 0.005630070809274912  PSNR: 27.16847801208496
[TRAIN] Iter: 575800 Loss: 0.003436364233493805  PSNR: 29.907814025878906
[TRAIN] Iter: 575900 Loss: 0.004455692134797573  PSNR: 29.613683700561523
[TRAIN] Iter: 576000 Loss: 0.0040187654085457325  PSNR: 28.38426399230957
[TRAIN] Iter: 576100 Loss: 0.0038089663721621037  PSNR: 29.73257827758789
[TRAIN] Iter: 576200 Loss: 0.0038305814377963543  PSNR: 30.277450561523438
[TRAIN] Iter: 576300 Loss: 0.004811420105397701  PSNR: 28.051082611083984
[TRAIN] Iter: 576400 Loss: 0.006013481877744198  PSNR: 27.261646270751953
[TRAIN] Iter: 576500 Loss: 0.004589289426803589  PSNR: 28.680356979370117
[TRAIN] Iter: 576600 Loss: 0.005038008093833923  PSNR: 28.119237899780273
[TRAIN] Iter: 576700 Loss: 0.0035648720804601908  PSNR: 30.328643798828125
[TRAIN] Iter: 576800 Loss: 0.005093864630907774  PSNR: 27.74803352355957
[TRAIN] Iter: 576900 Loss: 0.0055338796228170395  PSNR: 27.245519638061523
[TRAIN] Iter: 577000 Loss: 0.006115634460002184  PSNR: 26.511371612548828
[TRAIN] Iter: 577100 Loss: 0.005118873436003923  PSNR: 27.311071395874023
[TRAIN] Iter: 577200 Loss: 0.00438656285405159  PSNR: 29.034536361694336
[TRAIN] Iter: 577300 Loss: 0.0047049010172486305  PSNR: 28.143102645874023
[TRAIN] Iter: 577400 Loss: 0.005181248299777508  PSNR: 27.50904655456543
[TRAIN] Iter: 577500 Loss: 0.004204310942441225  PSNR: 29.97593116760254
[TRAIN] Iter: 577600 Loss: 0.004783075302839279  PSNR: 28.665433883666992
[TRAIN] Iter: 577700 Loss: 0.0037864064797759056  PSNR: 29.57244110107422
[TRAIN] Iter: 577800 Loss: 0.004100375343114138  PSNR: 29.77933692932129
[TRAIN] Iter: 577900 Loss: 0.005742417182773352  PSNR: 27.325254440307617
[TRAIN] Iter: 578000 Loss: 0.0030584116466343403  PSNR: 30.89645004272461
[TRAIN] Iter: 578100 Loss: 0.004147014115005732  PSNR: 28.845218658447266
[TRAIN] Iter: 578200 Loss: 0.0048158252611756325  PSNR: 28.346952438354492
[TRAIN] Iter: 578300 Loss: 0.004277836065739393  PSNR: 28.714418411254883
[TRAIN] Iter: 578400 Loss: 0.004657721612602472  PSNR: 27.918880462646484
[TRAIN] Iter: 578500 Loss: 0.003521895967423916  PSNR: 29.287630081176758
[TRAIN] Iter: 578600 Loss: 0.0034356508404016495  PSNR: 30.162857055664062
[TRAIN] Iter: 578700 Loss: 0.0031732264906167984  PSNR: 30.700334548950195
[TRAIN] Iter: 578800 Loss: 0.00442272936925292  PSNR: 29.14824676513672
[TRAIN] Iter: 578900 Loss: 0.003901679767295718  PSNR: 29.637502670288086
[TRAIN] Iter: 579000 Loss: 0.004297051578760147  PSNR: 30.255552291870117
[TRAIN] Iter: 579100 Loss: 0.0045124744065105915  PSNR: 28.456727981567383
[TRAIN] Iter: 579200 Loss: 0.004835831001400948  PSNR: 27.985353469848633
[TRAIN] Iter: 579300 Loss: 0.0050236862152814865  PSNR: 28.27730369567871
[TRAIN] Iter: 579400 Loss: 0.004722274839878082  PSNR: 28.255966186523438
[TRAIN] Iter: 579500 Loss: 0.004300729371607304  PSNR: 28.411542892456055
[TRAIN] Iter: 579600 Loss: 0.004028854425996542  PSNR: 29.45516014099121
[TRAIN] Iter: 579700 Loss: 0.005395850166678429  PSNR: 27.65814781188965
[TRAIN] Iter: 579800 Loss: 0.004826278891414404  PSNR: 28.327253341674805
[TRAIN] Iter: 579900 Loss: 0.005098656751215458  PSNR: 27.65949821472168
Saved checkpoints at ./logs/TUT-KE101-nerf/580000.tar
[TRAIN] Iter: 580000 Loss: 0.003864703234285116  PSNR: 29.176551818847656
[TRAIN] Iter: 580100 Loss: 0.004371558781713247  PSNR: 28.078739166259766
[TRAIN] Iter: 580200 Loss: 0.0037725798320025206  PSNR: 29.67852020263672
[TRAIN] Iter: 580300 Loss: 0.005019328556954861  PSNR: 27.77460479736328
[TRAIN] Iter: 580400 Loss: 0.0053902436047792435  PSNR: 27.994754791259766
[TRAIN] Iter: 580500 Loss: 0.003289099782705307  PSNR: 30.183191299438477
[TRAIN] Iter: 580600 Loss: 0.004168905317783356  PSNR: 29.13711929321289
[TRAIN] Iter: 580700 Loss: 0.005017229355871677  PSNR: 27.534975051879883
[TRAIN] Iter: 580800 Loss: 0.005110963247716427  PSNR: 28.097347259521484
[TRAIN] Iter: 580900 Loss: 0.00552460178732872  PSNR: 27.601789474487305
[TRAIN] Iter: 581000 Loss: 0.005121520254760981  PSNR: 28.123506546020508
[TRAIN] Iter: 581100 Loss: 0.0033847198355942965  PSNR: 31.374860763549805
[TRAIN] Iter: 581200 Loss: 0.005692067556083202  PSNR: 26.82737922668457
[TRAIN] Iter: 581300 Loss: 0.006496524903923273  PSNR: 26.755788803100586
[TRAIN] Iter: 581400 Loss: 0.004855872597545385  PSNR: 27.976755142211914
[TRAIN] Iter: 581500 Loss: 0.004957951605319977  PSNR: 27.82220458984375
[TRAIN] Iter: 581600 Loss: 0.00492461072281003  PSNR: 28.087167739868164
[TRAIN] Iter: 581700 Loss: 0.003390899859368801  PSNR: 29.40216636657715
[TRAIN] Iter: 581800 Loss: 0.004038019105792046  PSNR: 29.280405044555664
[TRAIN] Iter: 581900 Loss: 0.00426737405359745  PSNR: 27.991363525390625
[TRAIN] Iter: 582000 Loss: 0.005277696065604687  PSNR: 27.954729080200195
[TRAIN] Iter: 582100 Loss: 0.0032681385055184364  PSNR: 30.541154861450195
[TRAIN] Iter: 582200 Loss: 0.004484168719500303  PSNR: 28.222557067871094
[TRAIN] Iter: 582300 Loss: 0.003413184778764844  PSNR: 30.394182205200195
[TRAIN] Iter: 582400 Loss: 0.004747559316456318  PSNR: 27.398115158081055
[TRAIN] Iter: 582500 Loss: 0.004051936324685812  PSNR: 29.55487632751465
[TRAIN] Iter: 582600 Loss: 0.004492061212658882  PSNR: 28.91181182861328
[TRAIN] Iter: 582700 Loss: 0.004843931645154953  PSNR: 27.658336639404297
[TRAIN] Iter: 582800 Loss: 0.004983525723218918  PSNR: 27.751277923583984
[TRAIN] Iter: 582900 Loss: 0.004634922370314598  PSNR: 28.674203872680664
[TRAIN] Iter: 583000 Loss: 0.00500504719093442  PSNR: 28.278438568115234
[TRAIN] Iter: 583100 Loss: 0.005220793653279543  PSNR: 27.80015754699707
[TRAIN] Iter: 583200 Loss: 0.003236810676753521  PSNR: 30.176464080810547
[TRAIN] Iter: 583300 Loss: 0.004348136950284243  PSNR: 28.57851219177246
[TRAIN] Iter: 583400 Loss: 0.005836037918925285  PSNR: 26.790557861328125
[TRAIN] Iter: 583500 Loss: 0.004235615022480488  PSNR: 29.431163787841797
[TRAIN] Iter: 583600 Loss: 0.005791238509118557  PSNR: 27.46262550354004
[TRAIN] Iter: 583700 Loss: 0.005626896396279335  PSNR: 27.35969352722168
[TRAIN] Iter: 583800 Loss: 0.004691652953624725  PSNR: 28.02665901184082
[TRAIN] Iter: 583900 Loss: 0.0037861932069063187  PSNR: 28.95171546936035
[TRAIN] Iter: 584000 Loss: 0.004484641365706921  PSNR: 28.268159866333008
[TRAIN] Iter: 584100 Loss: 0.005072529427707195  PSNR: 27.984207153320312
[TRAIN] Iter: 584200 Loss: 0.004197865724563599  PSNR: 27.96009635925293
[TRAIN] Iter: 584300 Loss: 0.0035691489465534687  PSNR: 29.630111694335938
[TRAIN] Iter: 584400 Loss: 0.004174114670604467  PSNR: 28.552019119262695
[TRAIN] Iter: 584500 Loss: 0.004239614121615887  PSNR: 28.738895416259766
[TRAIN] Iter: 584600 Loss: 0.004695190116763115  PSNR: 28.389068603515625
[TRAIN] Iter: 584700 Loss: 0.00477752136066556  PSNR: 28.164499282836914
[TRAIN] Iter: 584800 Loss: 0.004696205258369446  PSNR: 27.544279098510742
[TRAIN] Iter: 584900 Loss: 0.003895139554515481  PSNR: 28.759334564208984
[TRAIN] Iter: 585000 Loss: 0.005109650082886219  PSNR: 27.844533920288086
[TRAIN] Iter: 585100 Loss: 0.003559014294296503  PSNR: 30.28200340270996
[TRAIN] Iter: 585200 Loss: 0.003964732866734266  PSNR: 28.354814529418945
[TRAIN] Iter: 585300 Loss: 0.003580803982913494  PSNR: 29.305702209472656
[TRAIN] Iter: 585400 Loss: 0.004779187962412834  PSNR: 27.591915130615234
[TRAIN] Iter: 585500 Loss: 0.004063861444592476  PSNR: 29.790489196777344
[TRAIN] Iter: 585600 Loss: 0.003980651963502169  PSNR: 28.635278701782227
[TRAIN] Iter: 585700 Loss: 0.003920556977391243  PSNR: 29.131847381591797
[TRAIN] Iter: 585800 Loss: 0.003865673439577222  PSNR: 28.754281997680664
[TRAIN] Iter: 585900 Loss: 0.005774718709290028  PSNR: 27.977935791015625
[TRAIN] Iter: 586000 Loss: 0.004973913077265024  PSNR: 27.45613670349121
[TRAIN] Iter: 586100 Loss: 0.005301502533257008  PSNR: 27.11458396911621
[TRAIN] Iter: 586200 Loss: 0.006837358698248863  PSNR: 25.97707748413086
[TRAIN] Iter: 586300 Loss: 0.005223248619586229  PSNR: 27.465112686157227
[TRAIN] Iter: 586400 Loss: 0.005454283207654953  PSNR: 27.287822723388672
[TRAIN] Iter: 586500 Loss: 0.005457399412989616  PSNR: 27.163022994995117
[TRAIN] Iter: 586600 Loss: 0.005792248994112015  PSNR: 27.46944236755371
[TRAIN] Iter: 586700 Loss: 0.005395279265940189  PSNR: 27.609195709228516
[TRAIN] Iter: 586800 Loss: 0.004465394653379917  PSNR: 28.288612365722656
[TRAIN] Iter: 586900 Loss: 0.00531737320125103  PSNR: 27.37603187561035
[TRAIN] Iter: 587000 Loss: 0.004377974197268486  PSNR: 28.3713321685791
[TRAIN] Iter: 587100 Loss: 0.00476125767454505  PSNR: 28.033464431762695
[TRAIN] Iter: 587200 Loss: 0.004191279876977205  PSNR: 28.678518295288086
[TRAIN] Iter: 587300 Loss: 0.005749494768679142  PSNR: 27.07754898071289
[TRAIN] Iter: 587400 Loss: 0.004064913373440504  PSNR: 29.48872184753418
[TRAIN] Iter: 587500 Loss: 0.006192255765199661  PSNR: 26.65799331665039
[TRAIN] Iter: 587600 Loss: 0.005131145007908344  PSNR: 27.93799591064453
[TRAIN] Iter: 587700 Loss: 0.003846749197691679  PSNR: 29.95175552368164
[TRAIN] Iter: 587800 Loss: 0.003524548839777708  PSNR: 29.988269805908203
[TRAIN] Iter: 587900 Loss: 0.0035554326605051756  PSNR: 29.796241760253906
[TRAIN] Iter: 588000 Loss: 0.0035602375864982605  PSNR: 30.08837890625
[TRAIN] Iter: 588100 Loss: 0.003794353920966387  PSNR: 29.076629638671875
[TRAIN] Iter: 588200 Loss: 0.0044551086612045765  PSNR: 28.245155334472656
[TRAIN] Iter: 588300 Loss: 0.0035766498185694218  PSNR: 29.412595748901367
[TRAIN] Iter: 588400 Loss: 0.004576106555759907  PSNR: 28.136137008666992
[TRAIN] Iter: 588500 Loss: 0.0063364943489432335  PSNR: 26.3629093170166
[TRAIN] Iter: 588600 Loss: 0.004919847473502159  PSNR: 27.9671630859375
[TRAIN] Iter: 588700 Loss: 0.005112434737384319  PSNR: 27.887956619262695
[TRAIN] Iter: 588800 Loss: 0.004062970168888569  PSNR: 29.65785026550293
[TRAIN] Iter: 588900 Loss: 0.004246031399816275  PSNR: 29.710847854614258
[TRAIN] Iter: 589000 Loss: 0.0036953845992684364  PSNR: 28.715435028076172
[TRAIN] Iter: 589100 Loss: 0.005066234618425369  PSNR: 27.68051528930664
[TRAIN] Iter: 589200 Loss: 0.005691934376955032  PSNR: 27.194915771484375
[TRAIN] Iter: 589300 Loss: 0.003588070161640644  PSNR: 29.90033531188965
[TRAIN] Iter: 589400 Loss: 0.005118772387504578  PSNR: 27.749977111816406
[TRAIN] Iter: 589500 Loss: 0.004072058945894241  PSNR: 29.588531494140625
[TRAIN] Iter: 589600 Loss: 0.004452705383300781  PSNR: 28.27522087097168
[TRAIN] Iter: 589700 Loss: 0.003867496969178319  PSNR: 29.684398651123047
[TRAIN] Iter: 589800 Loss: 0.003956557717174292  PSNR: 29.27752113342285
[TRAIN] Iter: 589900 Loss: 0.004608546383678913  PSNR: 28.308176040649414
Saved checkpoints at ./logs/TUT-KE101-nerf/590000.tar
[TRAIN] Iter: 590000 Loss: 0.00370804057456553  PSNR: 30.07405662536621
[TRAIN] Iter: 590100 Loss: 0.005517682526260614  PSNR: 27.919706344604492
[TRAIN] Iter: 590200 Loss: 0.004175771959125996  PSNR: 28.520736694335938
[TRAIN] Iter: 590300 Loss: 0.0057817306369543076  PSNR: 27.05372428894043
[TRAIN] Iter: 590400 Loss: 0.0038939579389989376  PSNR: 28.80026626586914
[TRAIN] Iter: 590500 Loss: 0.0031242454424500465  PSNR: 30.257112503051758
[TRAIN] Iter: 590600 Loss: 0.005245583597570658  PSNR: 27.42949676513672
[TRAIN] Iter: 590700 Loss: 0.003814506810158491  PSNR: 29.564916610717773
[TRAIN] Iter: 590800 Loss: 0.00541426707059145  PSNR: 27.667503356933594
[TRAIN] Iter: 590900 Loss: 0.0041183787398040295  PSNR: 28.89431381225586
[TRAIN] Iter: 591000 Loss: 0.004358640871942043  PSNR: 28.764692306518555
[TRAIN] Iter: 591100 Loss: 0.005325605161488056  PSNR: 27.032487869262695
[TRAIN] Iter: 591200 Loss: 0.0035359489265829325  PSNR: 30.431806564331055
[TRAIN] Iter: 591300 Loss: 0.005292566027492285  PSNR: 27.315677642822266
[TRAIN] Iter: 591400 Loss: 0.0035961358807981014  PSNR: 30.128833770751953
[TRAIN] Iter: 591500 Loss: 0.0052276416681706905  PSNR: 27.789260864257812
[TRAIN] Iter: 591600 Loss: 0.003369702026247978  PSNR: 30.225000381469727
[TRAIN] Iter: 591700 Loss: 0.0055699944496154785  PSNR: 26.48888397216797
[TRAIN] Iter: 591800 Loss: 0.00460974033921957  PSNR: 28.583049774169922
[TRAIN] Iter: 591900 Loss: 0.003943611867725849  PSNR: 29.346406936645508
[TRAIN] Iter: 592000 Loss: 0.00622662203386426  PSNR: 27.333843231201172
[TRAIN] Iter: 592100 Loss: 0.003814051393419504  PSNR: 29.774442672729492
[TRAIN] Iter: 592200 Loss: 0.004090623930096626  PSNR: 28.728073120117188
[TRAIN] Iter: 592300 Loss: 0.003678577020764351  PSNR: 29.7025146484375
[TRAIN] Iter: 592400 Loss: 0.004041554406285286  PSNR: 28.71588897705078
[TRAIN] Iter: 592500 Loss: 0.005179163068532944  PSNR: 27.763427734375
[TRAIN] Iter: 592600 Loss: 0.00353552820160985  PSNR: 29.475629806518555
[TRAIN] Iter: 592700 Loss: 0.005055923014879227  PSNR: 27.829944610595703
[TRAIN] Iter: 592800 Loss: 0.004991963971406221  PSNR: 27.954936981201172
[TRAIN] Iter: 592900 Loss: 0.005379392299801111  PSNR: 27.575969696044922
[TRAIN] Iter: 593000 Loss: 0.005323268007487059  PSNR: 27.718446731567383
[TRAIN] Iter: 593100 Loss: 0.00424516387283802  PSNR: 28.762901306152344
[TRAIN] Iter: 593200 Loss: 0.0036882043350487947  PSNR: 29.893156051635742
[TRAIN] Iter: 593300 Loss: 0.004636947065591812  PSNR: 28.623870849609375
[TRAIN] Iter: 593400 Loss: 0.0057512433268129826  PSNR: 26.565095901489258
[TRAIN] Iter: 593500 Loss: 0.0055700624361634254  PSNR: 27.410003662109375
[TRAIN] Iter: 593600 Loss: 0.004554299637675285  PSNR: 28.48335838317871
[TRAIN] Iter: 593700 Loss: 0.004780369810760021  PSNR: 28.424814224243164
[TRAIN] Iter: 593800 Loss: 0.0053519681096076965  PSNR: 27.795806884765625
[TRAIN] Iter: 593900 Loss: 0.004060639534145594  PSNR: 29.253324508666992
[TRAIN] Iter: 594000 Loss: 0.004324615933001041  PSNR: 28.387575149536133
[TRAIN] Iter: 594100 Loss: 0.004953391849994659  PSNR: 28.085391998291016
[TRAIN] Iter: 594200 Loss: 0.004523044917732477  PSNR: 28.377227783203125
[TRAIN] Iter: 594300 Loss: 0.005326049868017435  PSNR: 27.84185028076172
[TRAIN] Iter: 594400 Loss: 0.005083939991891384  PSNR: 27.62006187438965
[TRAIN] Iter: 594500 Loss: 0.004408853128552437  PSNR: 28.265979766845703
[TRAIN] Iter: 594600 Loss: 0.004918995313346386  PSNR: 27.458078384399414
[TRAIN] Iter: 594700 Loss: 0.0031284743454307318  PSNR: 30.382518768310547
[TRAIN] Iter: 594800 Loss: 0.006349647883325815  PSNR: 26.39573860168457
[TRAIN] Iter: 594900 Loss: 0.004703527316451073  PSNR: 27.909732818603516
[TRAIN] Iter: 595000 Loss: 0.005892906803637743  PSNR: 27.12131118774414
[TRAIN] Iter: 595100 Loss: 0.0032943496480584145  PSNR: 30.532367706298828
[TRAIN] Iter: 595200 Loss: 0.00514864781871438  PSNR: 27.44440460205078
[TRAIN] Iter: 595300 Loss: 0.004731465131044388  PSNR: 27.815183639526367
[TRAIN] Iter: 595400 Loss: 0.005335688591003418  PSNR: 27.185701370239258
[TRAIN] Iter: 595500 Loss: 0.005016489885747433  PSNR: 28.090381622314453
[TRAIN] Iter: 595600 Loss: 0.0045026494190096855  PSNR: 27.532745361328125
[TRAIN] Iter: 595700 Loss: 0.004992923699319363  PSNR: 28.004554748535156
[TRAIN] Iter: 595800 Loss: 0.003609419334679842  PSNR: 29.53058624267578
[TRAIN] Iter: 595900 Loss: 0.003507917746901512  PSNR: 29.073564529418945
[TRAIN] Iter: 596000 Loss: 0.0052389297634363174  PSNR: 27.30028533935547
[TRAIN] Iter: 596100 Loss: 0.0039006900042295456  PSNR: 29.48957633972168
[TRAIN] Iter: 596200 Loss: 0.00541429128497839  PSNR: 27.7974910736084
[TRAIN] Iter: 596300 Loss: 0.0037451942916959524  PSNR: 29.397510528564453
[TRAIN] Iter: 596400 Loss: 0.0038707831408828497  PSNR: 28.77815818786621
[TRAIN] Iter: 596500 Loss: 0.005804938264191151  PSNR: 27.303442001342773
[TRAIN] Iter: 596600 Loss: 0.006040127016603947  PSNR: 27.416818618774414
[TRAIN] Iter: 596700 Loss: 0.00476014893501997  PSNR: 27.579092025756836
[TRAIN] Iter: 596800 Loss: 0.003802189603447914  PSNR: 29.98910903930664
[TRAIN] Iter: 596900 Loss: 0.005748949479311705  PSNR: 26.71245765686035
[TRAIN] Iter: 597000 Loss: 0.0053587304428219795  PSNR: 27.11114501953125
[TRAIN] Iter: 597100 Loss: 0.0052421302534639835  PSNR: 27.942441940307617
[TRAIN] Iter: 597200 Loss: 0.004092392511665821  PSNR: 28.519329071044922
[TRAIN] Iter: 597300 Loss: 0.0036043585278093815  PSNR: 29.685361862182617
[TRAIN] Iter: 597400 Loss: 0.005581690929830074  PSNR: 28.461275100708008
[TRAIN] Iter: 597500 Loss: 0.004701307509094477  PSNR: 28.321481704711914
[TRAIN] Iter: 597600 Loss: 0.0053842440247535706  PSNR: 27.520832061767578
[TRAIN] Iter: 597700 Loss: 0.003983187954872847  PSNR: 28.36787223815918
[TRAIN] Iter: 597800 Loss: 0.00324296485632658  PSNR: 30.43634796142578
[TRAIN] Iter: 597900 Loss: 0.0047525037080049515  PSNR: 28.136537551879883
[TRAIN] Iter: 598000 Loss: 0.003469911403954029  PSNR: 30.027557373046875
[TRAIN] Iter: 598100 Loss: 0.005849565379321575  PSNR: 27.421045303344727
[TRAIN] Iter: 598200 Loss: 0.004552966449409723  PSNR: 27.907787322998047
[TRAIN] Iter: 598300 Loss: 0.004696664400398731  PSNR: 28.271207809448242
[TRAIN] Iter: 598400 Loss: 0.005089134443551302  PSNR: 28.22278594970703
[TRAIN] Iter: 598500 Loss: 0.0032820317428559065  PSNR: 30.682945251464844
[TRAIN] Iter: 598600 Loss: 0.005563023034483194  PSNR: 27.26224708557129
[TRAIN] Iter: 598700 Loss: 0.004759236238896847  PSNR: 27.885372161865234
[TRAIN] Iter: 598800 Loss: 0.0035190246999263763  PSNR: 30.29684829711914
[TRAIN] Iter: 598900 Loss: 0.004201245028525591  PSNR: 29.11235237121582
[TRAIN] Iter: 599000 Loss: 0.0036045610904693604  PSNR: 30.47599983215332
[TRAIN] Iter: 599100 Loss: 0.0037962133064866066  PSNR: 29.919679641723633
[TRAIN] Iter: 599200 Loss: 0.0037559526972472668  PSNR: 29.387325286865234
[TRAIN] Iter: 599300 Loss: 0.004389997571706772  PSNR: 28.92623519897461
[TRAIN] Iter: 599400 Loss: 0.0037432610988616943  PSNR: 29.67168426513672
[TRAIN] Iter: 599500 Loss: 0.004094837233424187  PSNR: 28.777549743652344
[TRAIN] Iter: 599600 Loss: 0.0040748524479568005  PSNR: 28.7520751953125
[TRAIN] Iter: 599700 Loss: 0.004383573308587074  PSNR: 27.48296356201172
[TRAIN] Iter: 599800 Loss: 0.003580424701794982  PSNR: 29.578445434570312
[TRAIN] Iter: 599900 Loss: 0.005297199822962284  PSNR: 27.82513999938965
Saved checkpoints at ./logs/TUT-KE101-nerf/600000.tar
0 0.00036978721618652344
torch.Size([320, 640, 3]) torch.Size([320, 640])
1 15.070045709609985
2 13.463874578475952
3 15.309444904327393
4 13.580563306808472
5 13.505159616470337
6 15.101977109909058
7 13.492467164993286
8 13.55401611328125
9 15.915424585342407
10 13.314854621887207
11 15.31314754486084
12 13.572726011276245
13 13.526059627532959
14 15.136450290679932
15 13.48313570022583
16 15.037336349487305
17 13.53248405456543
18 13.493053197860718
19 15.193076372146606
20 13.527859926223755
21 15.169586658477783
22 13.504555463790894
23 15.136245727539062
24 13.497083902359009
25 13.49533224105835
26 15.164530992507935
27 13.526183605194092
28 15.204965353012085
29 13.512767791748047
30 13.503607511520386
31 15.19880986213684
32 13.476766347885132
33 15.411137580871582
34 13.415846347808838
35 15.152342796325684
36 13.562585353851318
37 13.371671438217163
38 15.323566198348999
39 13.305477142333984
40 15.412423372268677
41 13.376160144805908
42 15.083081483840942
43 13.397041082382202
44 13.450704574584961
45 15.69997525215149
46 13.075785875320435
47 15.494516611099243
48 13.377408027648926
49 13.259284734725952
50 15.469238758087158
51 13.253134965896606
52 15.562943935394287
53 13.17630648612976
54 15.357956647872925
55 13.348138093948364
56 13.518182754516602
57 15.274914503097534
58 13.36427640914917
59 15.48352313041687
60 13.26551103591919
61 13.428343057632446
62 15.256063461303711
63 13.349441289901733
64 15.542354583740234
65 13.123807430267334
66 15.543893098831177
67 13.149478912353516
68 13.508004188537598
69 15.429772138595581
70 13.474364280700684
71 15.195029497146606
72 13.50397515296936
73 13.488404273986816
74 15.167725086212158
75 13.43129825592041
76 15.253577709197998
77 13.483922004699707
78 15.158936500549316
79 13.480676412582397
80 13.48872447013855
81 15.173396825790405
82 13.489606380462646
83 15.238171339035034
84 13.495154619216919
85 13.49980616569519
86 15.189620018005371
87 13.517215251922607
88 15.200769662857056
89 13.484789371490479
90 15.202878475189209
91 13.483058452606201
92 13.478789567947388
93 15.205585718154907
94 13.44730544090271
95 15.213579416275024
96 13.524122476577759
97 13.514639616012573
98 15.201518535614014
99 13.444467782974243
100 15.266684532165527
101 13.484073638916016
102 15.204571723937988
103 13.451835870742798
104 13.462537050247192
105 15.210559129714966
106 13.45354962348938
107 15.178870916366577
108 13.504334449768066
109 15.196754217147827
110 13.452739238739014
111 13.464057445526123
112 15.204450607299805
113 13.455775022506714
114 15.200758457183838
115 13.502899885177612
116 13.46675992012024
117 15.24481987953186
118 13.369660139083862
119 15.436957359313965
Done, saving (120, 320, 640, 3) (120, 320, 640)
extras:{'raw': tensor([[[-2.2363e+00, -2.6481e+00, -4.0810e+00, -8.7653e+01],
         [-5.8920e-01, -5.2831e-01, -2.7356e-01, -3.1261e+01],
         [ 2.0698e-01,  1.4682e-01,  1.0244e-01,  7.5097e-01],
         ...,
         [-1.0208e+00, -1.9066e+00, -3.2154e+00,  5.5655e+01],
         [-1.3412e+00, -2.3446e+00, -3.9757e+00,  4.5742e+01],
         [-8.3777e-01, -1.8198e+00, -3.2627e+00,  5.8547e+01]],

        [[ 2.4326e-01,  2.5305e-02, -1.0125e-01, -5.0785e+01],
         [-6.1910e-02, -2.3928e-01, -5.4835e-01, -1.2612e+01],
         [-1.2015e-01, -2.8295e-01, -5.4789e-01,  7.6920e+00],
         ...,
         [ 9.7804e+00,  9.9600e+00,  1.0348e+01,  1.2481e+03],
         [ 1.0382e+01,  1.0832e+01,  1.1797e+01,  1.2059e+03],
         [ 9.5799e+00,  1.0160e+01,  1.1410e+01,  1.2599e+03]],

        [[-3.2447e-01, -1.0101e+00, -3.0377e+00, -6.7712e+01],
         [-4.0282e+00, -4.0366e+00, -3.8984e+00, -4.0683e+01],
         [-7.3148e+00, -7.4076e+00, -7.2611e+00, -4.1792e+01],
         ...,
         [-1.5591e+00, -2.1519e+00, -3.1753e+00,  3.5756e+01],
         [-2.8177e+00, -3.1230e+00, -3.8338e+00,  2.0856e+01],
         [-1.0787e+00, -1.2313e+00, -1.7814e+00,  4.0233e+01]],

        ...,

        [[-1.1977e+00, -6.2904e-01,  3.3948e-01, -6.6364e+01],
         [-3.1549e+00, -2.9003e+00, -1.6894e+00, -5.2045e+01],
         [-6.8714e-01, -7.2962e-01, -7.9826e-01, -9.1728e+00],
         ...,
         [-2.2083e+01, -2.2263e+01, -2.4600e+01, -4.5560e+02],
         [-2.1960e+01, -2.1686e+01, -2.2836e+01, -2.6688e+02],
         [-2.1668e+01, -2.1432e+01, -2.2984e+01, -4.1075e+02]],

        [[-4.6356e-01,  4.1616e-01,  2.4801e+00, -6.2613e+01],
         [ 3.3183e-02, -2.2675e-02,  3.2962e-02, -2.5605e+01],
         [ 8.2511e-03, -5.5797e-02, -2.4211e-02, -2.5355e+01],
         ...,
         [ 5.2182e+01,  4.7382e+01,  4.2748e+01,  1.0998e+02],
         [ 5.8068e+01,  5.3643e+01,  5.1916e+01,  8.4948e+01],
         [ 4.2214e+01,  3.5727e+01,  2.6487e+01,  1.3884e+02]],

        [[ 5.2427e+00,  5.3684e+00,  6.7069e+00, -6.2634e+01],
         [-4.4452e+00, -5.4217e+00, -7.3376e+00, -1.2766e+01],
         [-3.4720e+00, -2.8545e+00, -1.8266e+00, -4.0620e+01],
         ...,
         [-1.9744e+01, -1.9320e+01, -2.2375e+01, -2.4737e+02],
         [-1.3771e+01, -1.4139e+01, -1.9562e+01, -2.3568e+02],
         [-2.8127e+01, -2.8356e+01, -3.1555e+01, -2.0964e+02]]],
       grad_fn=<ReshapeAliasBackward0>), 'rgb0': tensor([[0.5607, 0.5435, 0.5016],
        [0.4735, 0.4289, 0.3577],
        [0.5025, 0.4922, 0.4800],
        ...,
        [0.2131, 0.2296, 0.2679],
        [0.4789, 0.4301, 0.3681],
        [0.3776, 0.3590, 0.3092]], grad_fn=<ReshapeAliasBackward0>), 'disp0': tensor([ 60.0445,  55.6738,  62.1721,  ...,  28.6531, 298.1572,  68.9596],
       grad_fn=<ReshapeAliasBackward0>), 'acc0': tensor([1., 1., 1.,  ..., 1., 1., 1.], grad_fn=<ReshapeAliasBackward0>), 'z_std': tensor([0.0604, 0.0030, 0.0046,  ..., 0.0021, 0.0018, 0.0024])}
0 0.0005497932434082031
torch.Size([320, 640, 3]) torch.Size([320, 640])
1 15.257038116455078
2 13.58021855354309
3 13.37770390510559
4 15.368392944335938
5 13.267686367034912
6 15.410995483398438
7 13.29177975654602
8 15.183950424194336
9 13.376575469970703
10 13.342366456985474
11 15.621800661087036
12 13.105309009552002
13 15.523671388626099
14 13.221712350845337
15 13.440468788146973
16 15.356343030929565
17 13.331396341323853
18 15.547116756439209
19 13.169984579086304
20 13.34871768951416
21 15.326395034790039
22 13.338062763214111
23 15.625977993011475
24 13.13044548034668
25 15.514822483062744
26 13.318628311157227
27 13.256536722183228
28 15.410646200180054
29 13.319656372070312
30 15.532355070114136
31 13.21732211112976
32 13.648066759109497
33 14.968428373336792
34 13.542755126953125
35 15.44676947593689
36 13.121153354644775
37 15.54932689666748
38 13.132317066192627
39 13.460883617401123
40 15.556785106658936
41 13.480704307556152
42 15.150916337966919
43 13.461181163787842
44 16.397292852401733
45 15.361136198043823
46 15.273331880569458
47 17.076486110687256
48 15.342122077941895
49 17.05698013305664
50 15.363334894180298
51 17.05277991294861
52 15.352073192596436
53 17.07193636894226
54 15.414486408233643
55 16.9811007976532
56 15.32678747177124
57 17.048076152801514
58 15.335657119750977
59 17.066301107406616
60 15.401243925094604
61 17.085388898849487
62 15.286805629730225
63 17.060885190963745
64 15.31899905204773
65 17.151352167129517
66 15.202996253967285
67 15.370585203170776
68 17.439632415771484
69 15.287650346755981
70 16.86648154258728
71 15.315671443939209
72 17.13607382774353
73 15.332107543945312
74 17.107574462890625
75 15.21849274635315
76 17.1105854511261
77 15.218852519989014
78 17.198811292648315
79 15.193300247192383
80 17.246171712875366
81 15.294940710067749
82 17.08334517478943
83 15.343119621276855
84 17.07675075531006
85 15.37180495262146
86 17.061156272888184
87 15.347755432128906
88 15.318750143051147
89 17.067818880081177
90 15.31633448600769
91 17.04967451095581
92 15.1926109790802
93 17.33030915260315
94 15.011301517486572
95 17.57716965675354
96 14.81464147567749
97 17.57775640487671
98 14.917172193527222
99 17.467084169387817
100 15.022095918655396
101 17.33674383163452
102 14.97558331489563
103 17.39785599708557
104 15.043874979019165
105 17.32428812980652
106 15.18445897102356
107 15.269733905792236
108 16.980475425720215
109 15.63996696472168
110 17.244807958602905
111 15.699695110321045
112 17.836003303527832
113 15.801355123519897
114 17.37433385848999
115 15.536987781524658
116 17.142486095428467
117 15.32968544960022
118 17.134475708007812
119 15.374254703521729
test poses shape torch.Size([4, 3, 4])
0 0.0005826950073242188
torch.Size([320, 640, 3]) torch.Size([320, 640])
1 15.369590997695923
2 17.134586811065674
3 15.381149053573608
Saved test set
[TRAIN] Iter: 600000 Loss: 0.004338906612247229  PSNR: 28.27682113647461
[TRAIN] Iter: 600100 Loss: 0.003522391663864255  PSNR: 29.961679458618164
[TRAIN] Iter: 600200 Loss: 0.003580888733267784  PSNR: 30.12619972229004
[TRAIN] Iter: 600300 Loss: 0.004071634262800217  PSNR: 29.81929588317871
[TRAIN] Iter: 600400 Loss: 0.0034868218936026096  PSNR: 30.551937103271484
[TRAIN] Iter: 600500 Loss: 0.005538539029657841  PSNR: 26.869565963745117
[TRAIN] Iter: 600600 Loss: 0.0036094512324780226  PSNR: 29.712982177734375
[TRAIN] Iter: 600700 Loss: 0.0037091351114213467  PSNR: 29.529556274414062
[TRAIN] Iter: 600800 Loss: 0.005401821807026863  PSNR: 27.275781631469727
[TRAIN] Iter: 600900 Loss: 0.005761083215475082  PSNR: 27.507076263427734
[TRAIN] Iter: 601000 Loss: 0.0048764729872345924  PSNR: 28.589218139648438
[TRAIN] Iter: 601100 Loss: 0.0033904146403074265  PSNR: 29.74816131591797
[TRAIN] Iter: 601200 Loss: 0.004147929139435291  PSNR: 29.78162956237793
[TRAIN] Iter: 601300 Loss: 0.004652865696698427  PSNR: 28.462112426757812
[TRAIN] Iter: 601400 Loss: 0.004595982376486063  PSNR: 28.282377243041992
[TRAIN] Iter: 601500 Loss: 0.005202661268413067  PSNR: 27.135040283203125
[TRAIN] Iter: 601600 Loss: 0.005932031199336052  PSNR: 26.7113037109375
[TRAIN] Iter: 601700 Loss: 0.00464105699211359  PSNR: 28.46741485595703
[TRAIN] Iter: 601800 Loss: 0.004994875285774469  PSNR: 27.51666259765625
[TRAIN] Iter: 601900 Loss: 0.004587175790220499  PSNR: 28.103748321533203
[TRAIN] Iter: 602000 Loss: 0.00393378920853138  PSNR: 29.06489372253418
[TRAIN] Iter: 602100 Loss: 0.003790077520534396  PSNR: 30.552274703979492
[TRAIN] Iter: 602200 Loss: 0.005998097825795412  PSNR: 27.187448501586914
[TRAIN] Iter: 602300 Loss: 0.003825374413281679  PSNR: 30.41930389404297
[TRAIN] Iter: 602400 Loss: 0.005422024056315422  PSNR: 27.425901412963867
[TRAIN] Iter: 602500 Loss: 0.0036214529536664486  PSNR: 29.60425567626953
[TRAIN] Iter: 602600 Loss: 0.006209095008671284  PSNR: 27.577573776245117
[TRAIN] Iter: 602700 Loss: 0.003306967206299305  PSNR: 29.606271743774414
[TRAIN] Iter: 602800 Loss: 0.004767176229506731  PSNR: 27.69013023376465
[TRAIN] Iter: 602900 Loss: 0.004409324377775192  PSNR: 28.191299438476562
[TRAIN] Iter: 603000 Loss: 0.004859326407313347  PSNR: 27.52728271484375
[TRAIN] Iter: 603100 Loss: 0.0038842959329485893  PSNR: 30.212848663330078
[TRAIN] Iter: 603200 Loss: 0.005934730637818575  PSNR: 26.425811767578125
[TRAIN] Iter: 603300 Loss: 0.0048873103223741055  PSNR: 27.295255661010742
[TRAIN] Iter: 603400 Loss: 0.003718467429280281  PSNR: 30.34531021118164
[TRAIN] Iter: 603500 Loss: 0.0047498359344899654  PSNR: 27.54682159423828
[TRAIN] Iter: 603600 Loss: 0.004013050347566605  PSNR: 28.716466903686523
[TRAIN] Iter: 603700 Loss: 0.004576410166919231  PSNR: 27.713642120361328
[TRAIN] Iter: 603800 Loss: 0.005220432765781879  PSNR: 28.401891708374023
[TRAIN] Iter: 603900 Loss: 0.004158697556704283  PSNR: 28.1140193939209
[TRAIN] Iter: 604000 Loss: 0.004814474377781153  PSNR: 28.40652084350586
[TRAIN] Iter: 604100 Loss: 0.005313841160386801  PSNR: 27.22297477722168
[TRAIN] Iter: 604200 Loss: 0.004206975921988487  PSNR: 28.845834732055664
[TRAIN] Iter: 604300 Loss: 0.004445504397153854  PSNR: 28.3116512298584
[TRAIN] Iter: 604400 Loss: 0.005000153556466103  PSNR: 27.967397689819336
[TRAIN] Iter: 604500 Loss: 0.004275747574865818  PSNR: 28.127470016479492
[TRAIN] Iter: 604600 Loss: 0.005136760417371988  PSNR: 26.778486251831055
[TRAIN] Iter: 604700 Loss: 0.004140549339354038  PSNR: 27.80251693725586
[TRAIN] Iter: 604800 Loss: 0.0053453268483281136  PSNR: 26.52825355529785
[TRAIN] Iter: 604900 Loss: 0.005698614753782749  PSNR: 27.74918556213379
[TRAIN] Iter: 605000 Loss: 0.006145843304693699  PSNR: 26.675268173217773
[TRAIN] Iter: 605100 Loss: 0.004755089059472084  PSNR: 28.008249282836914
[TRAIN] Iter: 605200 Loss: 0.0063362084329128265  PSNR: 26.696029663085938
[TRAIN] Iter: 605300 Loss: 0.004720293451100588  PSNR: 27.840471267700195
[TRAIN] Iter: 605400 Loss: 0.003401179565116763  PSNR: 30.22052574157715
[TRAIN] Iter: 605500 Loss: 0.004930200055241585  PSNR: 28.88760757446289
[TRAIN] Iter: 605600 Loss: 0.006245431490242481  PSNR: 26.94411277770996
[TRAIN] Iter: 605700 Loss: 0.004385286942124367  PSNR: 28.05022621154785
[TRAIN] Iter: 605800 Loss: 0.005313990171998739  PSNR: 27.700876235961914
[TRAIN] Iter: 605900 Loss: 0.004809739533811808  PSNR: 27.707000732421875
[TRAIN] Iter: 606000 Loss: 0.00321891438215971  PSNR: 29.927499771118164
[TRAIN] Iter: 606100 Loss: 0.00387516594491899  PSNR: 28.923282623291016
[TRAIN] Iter: 606200 Loss: 0.0029703867621719837  PSNR: 30.756622314453125
[TRAIN] Iter: 606300 Loss: 0.005037818104028702  PSNR: 27.61743927001953
[TRAIN] Iter: 606400 Loss: 0.004767420701682568  PSNR: 28.23880958557129
[TRAIN] Iter: 606500 Loss: 0.0038855066522955894  PSNR: 29.72237205505371
[TRAIN] Iter: 606600 Loss: 0.005225952714681625  PSNR: 27.15974235534668
[TRAIN] Iter: 606700 Loss: 0.004275517538189888  PSNR: 28.563974380493164
[TRAIN] Iter: 606800 Loss: 0.004052344243973494  PSNR: 28.44449234008789
[TRAIN] Iter: 606900 Loss: 0.00404767319560051  PSNR: 29.371814727783203
[TRAIN] Iter: 607000 Loss: 0.005388327874243259  PSNR: 26.881736755371094
[TRAIN] Iter: 607100 Loss: 0.0037811873480677605  PSNR: 29.465057373046875
[TRAIN] Iter: 607200 Loss: 0.003899150062352419  PSNR: 29.504714965820312
[TRAIN] Iter: 607300 Loss: 0.005205802619457245  PSNR: 27.512483596801758
[TRAIN] Iter: 607400 Loss: 0.0037738375831395388  PSNR: 29.876270294189453
[TRAIN] Iter: 607500 Loss: 0.00408881064504385  PSNR: 29.9704532623291
[TRAIN] Iter: 607600 Loss: 0.005325456149876118  PSNR: 27.380752563476562
[TRAIN] Iter: 607700 Loss: 0.004385925829410553  PSNR: 29.989416122436523
[TRAIN] Iter: 607800 Loss: 0.005266378168016672  PSNR: 27.510456085205078
[TRAIN] Iter: 607900 Loss: 0.005397625733166933  PSNR: 27.45688247680664
[TRAIN] Iter: 608000 Loss: 0.0043283551931381226  PSNR: 27.96550750732422
[TRAIN] Iter: 608100 Loss: 0.0065182652324438095  PSNR: 26.49958610534668
[TRAIN] Iter: 608200 Loss: 0.0058212243020534515  PSNR: 26.77496910095215
[TRAIN] Iter: 608300 Loss: 0.004365541506558657  PSNR: 28.0887451171875
[TRAIN] Iter: 608400 Loss: 0.004913343582302332  PSNR: 28.081417083740234
[TRAIN] Iter: 608500 Loss: 0.004541041795164347  PSNR: 28.10106086730957
[TRAIN] Iter: 608600 Loss: 0.00385088543407619  PSNR: 30.045833587646484
[TRAIN] Iter: 608700 Loss: 0.005309673026204109  PSNR: 27.518159866333008
[TRAIN] Iter: 608800 Loss: 0.0037404894828796387  PSNR: 29.383262634277344
[TRAIN] Iter: 608900 Loss: 0.0036343259271234274  PSNR: 29.856182098388672
[TRAIN] Iter: 609000 Loss: 0.0043802144937217236  PSNR: 28.47821617126465
[TRAIN] Iter: 609100 Loss: 0.00388304702937603  PSNR: 29.014741897583008
[TRAIN] Iter: 609200 Loss: 0.005015055648982525  PSNR: 27.846357345581055
[TRAIN] Iter: 609300 Loss: 0.003714078338816762  PSNR: 30.568145751953125
[TRAIN] Iter: 609400 Loss: 0.0037261568941175938  PSNR: 30.11531639099121
[TRAIN] Iter: 609500 Loss: 0.004373561590909958  PSNR: 28.145130157470703
[TRAIN] Iter: 609600 Loss: 0.004779066890478134  PSNR: 27.64216423034668
[TRAIN] Iter: 609700 Loss: 0.00545470742508769  PSNR: 28.51369857788086
[TRAIN] Iter: 609800 Loss: 0.003001315053552389  PSNR: 30.408035278320312
[TRAIN] Iter: 609900 Loss: 0.005170035175979137  PSNR: 27.167423248291016
Saved checkpoints at ./logs/TUT-KE101-nerf/610000.tar
[TRAIN] Iter: 610000 Loss: 0.004248380661010742  PSNR: 29.637109756469727
[TRAIN] Iter: 610100 Loss: 0.0034562714863568544  PSNR: 29.706707000732422
[TRAIN] Iter: 610200 Loss: 0.004284483380615711  PSNR: 29.478214263916016
[TRAIN] Iter: 610300 Loss: 0.003710977267473936  PSNR: 29.4874324798584
[TRAIN] Iter: 610400 Loss: 0.004067833069711924  PSNR: 29.879070281982422
[TRAIN] Iter: 610500 Loss: 0.0048232609406113625  PSNR: 27.996742248535156
[TRAIN] Iter: 610600 Loss: 0.004052340053021908  PSNR: 29.883892059326172
[TRAIN] Iter: 610700 Loss: 0.004759663715958595  PSNR: 27.780969619750977
[TRAIN] Iter: 610800 Loss: 0.004147949628531933  PSNR: 29.5185489654541
[TRAIN] Iter: 610900 Loss: 0.0036077327094972134  PSNR: 29.882266998291016
[TRAIN] Iter: 611000 Loss: 0.004146496765315533  PSNR: 28.581501007080078
[TRAIN] Iter: 611100 Loss: 0.004941239953041077  PSNR: 27.391847610473633
[TRAIN] Iter: 611200 Loss: 0.0040084198117256165  PSNR: 29.366321563720703
[TRAIN] Iter: 611300 Loss: 0.0053249867632985115  PSNR: 27.379072189331055
[TRAIN] Iter: 611400 Loss: 0.0033979052677750587  PSNR: 30.34710121154785
[TRAIN] Iter: 611500 Loss: 0.003291504457592964  PSNR: 29.952091217041016
[TRAIN] Iter: 611600 Loss: 0.004539660643786192  PSNR: 28.56143569946289
[TRAIN] Iter: 611700 Loss: 0.004076687153428793  PSNR: 29.480775833129883
[TRAIN] Iter: 611800 Loss: 0.003991290926933289  PSNR: 30.29627799987793
[TRAIN] Iter: 611900 Loss: 0.005067664198577404  PSNR: 28.013912200927734
[TRAIN] Iter: 612000 Loss: 0.0036587193608283997  PSNR: 29.675498962402344
[TRAIN] Iter: 612100 Loss: 0.005636874120682478  PSNR: 27.320871353149414
[TRAIN] Iter: 612200 Loss: 0.0037283841520547867  PSNR: 29.199169158935547
[TRAIN] Iter: 612300 Loss: 0.004017326980829239  PSNR: 29.541574478149414
[TRAIN] Iter: 612400 Loss: 0.003519374644383788  PSNR: 29.991443634033203
[TRAIN] Iter: 612500 Loss: 0.004164822865277529  PSNR: 29.854408264160156
[TRAIN] Iter: 612600 Loss: 0.004305989481508732  PSNR: 28.268007278442383
[TRAIN] Iter: 612700 Loss: 0.004197105299681425  PSNR: 28.5384464263916
[TRAIN] Iter: 612800 Loss: 0.0036649948451668024  PSNR: 29.86139678955078
[TRAIN] Iter: 612900 Loss: 0.003984883427619934  PSNR: 29.38698959350586
[TRAIN] Iter: 613000 Loss: 0.003786652348935604  PSNR: 29.49753189086914
[TRAIN] Iter: 613100 Loss: 0.004968137014657259  PSNR: 27.640949249267578
[TRAIN] Iter: 613200 Loss: 0.004814008250832558  PSNR: 28.078083038330078
[TRAIN] Iter: 613300 Loss: 0.004030249081552029  PSNR: 28.94529914855957
[TRAIN] Iter: 613400 Loss: 0.005288926884531975  PSNR: 27.220426559448242
[TRAIN] Iter: 613500 Loss: 0.0049044908955693245  PSNR: 27.851316452026367
[TRAIN] Iter: 613600 Loss: 0.0036008087918162346  PSNR: 31.19785499572754
[TRAIN] Iter: 613700 Loss: 0.004160884767770767  PSNR: 29.391996383666992
[TRAIN] Iter: 613800 Loss: 0.0046028858050704  PSNR: 28.568450927734375
[TRAIN] Iter: 613900 Loss: 0.004666829947382212  PSNR: 28.403833389282227
[TRAIN] Iter: 614000 Loss: 0.003496594727039337  PSNR: 29.880640029907227
[TRAIN] Iter: 614100 Loss: 0.004009111784398556  PSNR: 28.665145874023438
[TRAIN] Iter: 614200 Loss: 0.00391228124499321  PSNR: 29.967187881469727
[TRAIN] Iter: 614300 Loss: 0.005130670964717865  PSNR: 27.60360336303711
[TRAIN] Iter: 614400 Loss: 0.0048066615127027035  PSNR: 28.244123458862305
[TRAIN] Iter: 614500 Loss: 0.0044141183607280254  PSNR: 28.692066192626953
[TRAIN] Iter: 614600 Loss: 0.004102781880646944  PSNR: 28.216970443725586
[TRAIN] Iter: 614700 Loss: 0.005041024647653103  PSNR: 28.75163459777832
[TRAIN] Iter: 614800 Loss: 0.004922003950923681  PSNR: 27.935199737548828
[TRAIN] Iter: 614900 Loss: 0.005211134906858206  PSNR: 27.41145896911621
[TRAIN] Iter: 615000 Loss: 0.004629121627658606  PSNR: 28.890668869018555
[TRAIN] Iter: 615100 Loss: 0.0046622902154922485  PSNR: 27.791425704956055
[TRAIN] Iter: 615200 Loss: 0.00445844791829586  PSNR: 27.88889503479004
[TRAIN] Iter: 615300 Loss: 0.0037684114649891853  PSNR: 29.354806900024414
[TRAIN] Iter: 615400 Loss: 0.0038561858236789703  PSNR: 30.566240310668945
[TRAIN] Iter: 615500 Loss: 0.003643073607236147  PSNR: 29.926219940185547
[TRAIN] Iter: 615600 Loss: 0.004917774349451065  PSNR: 28.16657066345215
[TRAIN] Iter: 615700 Loss: 0.005205783527344465  PSNR: 27.721057891845703
[TRAIN] Iter: 615800 Loss: 0.003936003893613815  PSNR: 29.021648406982422
[TRAIN] Iter: 615900 Loss: 0.005088267847895622  PSNR: 27.99919319152832
[TRAIN] Iter: 616000 Loss: 0.004456283058971167  PSNR: 27.809736251831055
[TRAIN] Iter: 616100 Loss: 0.004182522185146809  PSNR: 28.41317367553711
[TRAIN] Iter: 616200 Loss: 0.0035552859771996737  PSNR: 30.149749755859375
[TRAIN] Iter: 616300 Loss: 0.0041947802528738976  PSNR: 28.662891387939453
[TRAIN] Iter: 616400 Loss: 0.004949662834405899  PSNR: 28.041582107543945
[TRAIN] Iter: 616500 Loss: 0.00551875215023756  PSNR: 27.97633171081543
[TRAIN] Iter: 616600 Loss: 0.004779561422765255  PSNR: 29.751245498657227
[TRAIN] Iter: 616700 Loss: 0.004015323705971241  PSNR: 30.17916488647461
[TRAIN] Iter: 616800 Loss: 0.004941472318023443  PSNR: 27.599193572998047
[TRAIN] Iter: 616900 Loss: 0.004486099351197481  PSNR: 29.8106746673584
[TRAIN] Iter: 617000 Loss: 0.005373752675950527  PSNR: 27.655574798583984
[TRAIN] Iter: 617100 Loss: 0.004809377249330282  PSNR: 28.636871337890625
[TRAIN] Iter: 617200 Loss: 0.0038131061010062695  PSNR: 28.856828689575195
[TRAIN] Iter: 617300 Loss: 0.003487862879410386  PSNR: 29.90397071838379
[TRAIN] Iter: 617400 Loss: 0.003247881308197975  PSNR: 30.32817840576172
[TRAIN] Iter: 617500 Loss: 0.004313797224313021  PSNR: 29.785783767700195
[TRAIN] Iter: 617600 Loss: 0.005826553795486689  PSNR: 27.43737030029297
[TRAIN] Iter: 617700 Loss: 0.004604589194059372  PSNR: 28.343029022216797
[TRAIN] Iter: 617800 Loss: 0.004759949631989002  PSNR: 28.223726272583008
[TRAIN] Iter: 617900 Loss: 0.003920926712453365  PSNR: 28.774799346923828
[TRAIN] Iter: 618000 Loss: 0.004201425239443779  PSNR: 28.672828674316406
[TRAIN] Iter: 618100 Loss: 0.0037985111121088266  PSNR: 29.745798110961914
[TRAIN] Iter: 618200 Loss: 0.0033501507714390755  PSNR: 29.8810977935791
[TRAIN] Iter: 618300 Loss: 0.005510597489774227  PSNR: 26.731109619140625
[TRAIN] Iter: 618400 Loss: 0.0040905289351940155  PSNR: 29.30402183532715
[TRAIN] Iter: 618500 Loss: 0.0050559560768306255  PSNR: 28.030372619628906
[TRAIN] Iter: 618600 Loss: 0.004448838997632265  PSNR: 28.686477661132812
[TRAIN] Iter: 618700 Loss: 0.005104479379951954  PSNR: 27.601198196411133
[TRAIN] Iter: 618800 Loss: 0.0049154567532241344  PSNR: 27.847440719604492
[TRAIN] Iter: 618900 Loss: 0.004728199914097786  PSNR: 27.83571434020996
[TRAIN] Iter: 619000 Loss: 0.005013609305024147  PSNR: 27.038930892944336
[TRAIN] Iter: 619100 Loss: 0.00423862598836422  PSNR: 28.745481491088867
[TRAIN] Iter: 619200 Loss: 0.0048567261546850204  PSNR: 28.073331832885742
[TRAIN] Iter: 619300 Loss: 0.0048881289549171925  PSNR: 28.215478897094727
[TRAIN] Iter: 619400 Loss: 0.004795596934854984  PSNR: 27.54238510131836
[TRAIN] Iter: 619500 Loss: 0.005113898776471615  PSNR: 28.26069450378418
[TRAIN] Iter: 619600 Loss: 0.0052291699685156345  PSNR: 27.546152114868164
[TRAIN] Iter: 619700 Loss: 0.004348140209913254  PSNR: 28.476579666137695
[TRAIN] Iter: 619800 Loss: 0.005198391154408455  PSNR: 27.80207061767578
[TRAIN] Iter: 619900 Loss: 0.005143848247826099  PSNR: 27.407995223999023
Saved checkpoints at ./logs/TUT-KE101-nerf/620000.tar
[TRAIN] Iter: 620000 Loss: 0.004014797508716583  PSNR: 28.13495445251465
[TRAIN] Iter: 620100 Loss: 0.0054035913199186325  PSNR: 27.496248245239258
[TRAIN] Iter: 620200 Loss: 0.0046564871445298195  PSNR: 28.141664505004883
[TRAIN] Iter: 620300 Loss: 0.0039243390783667564  PSNR: 29.087533950805664
[TRAIN] Iter: 620400 Loss: 0.005840147379785776  PSNR: 27.54376220703125
[TRAIN] Iter: 620500 Loss: 0.004767238162457943  PSNR: 28.220458984375
[TRAIN] Iter: 620600 Loss: 0.005446108523756266  PSNR: 27.31458282470703
[TRAIN] Iter: 620700 Loss: 0.0036522438749670982  PSNR: 29.734790802001953
[TRAIN] Iter: 620800 Loss: 0.0036657173186540604  PSNR: 30.179285049438477
[TRAIN] Iter: 620900 Loss: 0.004440956749022007  PSNR: 29.685104370117188
[TRAIN] Iter: 621000 Loss: 0.005780484061688185  PSNR: 27.173250198364258
[TRAIN] Iter: 621100 Loss: 0.005327518563717604  PSNR: 27.273746490478516
[TRAIN] Iter: 621200 Loss: 0.004633625503629446  PSNR: 28.378360748291016
[TRAIN] Iter: 621300 Loss: 0.004341429099440575  PSNR: 28.321216583251953
[TRAIN] Iter: 621400 Loss: 0.005152122117578983  PSNR: 27.941364288330078
[TRAIN] Iter: 621500 Loss: 0.004180002026259899  PSNR: 29.620555877685547
[TRAIN] Iter: 621600 Loss: 0.003631420899182558  PSNR: 29.990386962890625
[TRAIN] Iter: 621700 Loss: 0.004423929378390312  PSNR: 28.588966369628906
[TRAIN] Iter: 621800 Loss: 0.004477544687688351  PSNR: 28.270771026611328
[TRAIN] Iter: 621900 Loss: 0.004490653518587351  PSNR: 28.308984756469727
[TRAIN] Iter: 622000 Loss: 0.004398622550070286  PSNR: 29.110767364501953
[TRAIN] Iter: 622100 Loss: 0.004811045713722706  PSNR: 28.281131744384766
[TRAIN] Iter: 622200 Loss: 0.004850630648434162  PSNR: 27.657922744750977
[TRAIN] Iter: 622300 Loss: 0.003410948906093836  PSNR: 30.880861282348633
[TRAIN] Iter: 622400 Loss: 0.004621502477675676  PSNR: 27.84239959716797
[TRAIN] Iter: 622500 Loss: 0.0047887228429317474  PSNR: 28.471641540527344
[TRAIN] Iter: 622600 Loss: 0.004250412341207266  PSNR: 27.766639709472656
[TRAIN] Iter: 622700 Loss: 0.005111972335726023  PSNR: 27.391254425048828
[TRAIN] Iter: 622800 Loss: 0.004080872517079115  PSNR: 28.833181381225586
[TRAIN] Iter: 622900 Loss: 0.004094197880476713  PSNR: 29.245328903198242
[TRAIN] Iter: 623000 Loss: 0.003065875731408596  PSNR: 30.056039810180664
[TRAIN] Iter: 623100 Loss: 0.004881879314780235  PSNR: 28.076828002929688
[TRAIN] Iter: 623200 Loss: 0.005389781203120947  PSNR: 27.2641544342041
[TRAIN] Iter: 623300 Loss: 0.004613596014678478  PSNR: 28.36758804321289
[TRAIN] Iter: 623400 Loss: 0.0037006728816777468  PSNR: 29.794052124023438
[TRAIN] Iter: 623500 Loss: 0.0046682595275342464  PSNR: 28.444658279418945
[TRAIN] Iter: 623600 Loss: 0.005020784679800272  PSNR: 28.23567771911621
[TRAIN] Iter: 623700 Loss: 0.004259071312844753  PSNR: 29.5353946685791
[TRAIN] Iter: 623800 Loss: 0.004424732178449631  PSNR: 28.484878540039062
[TRAIN] Iter: 623900 Loss: 0.00474278861656785  PSNR: 27.77851104736328
[TRAIN] Iter: 624000 Loss: 0.005455102305859327  PSNR: 27.496522903442383
[TRAIN] Iter: 624100 Loss: 0.0032792938873171806  PSNR: 30.151926040649414
[TRAIN] Iter: 624200 Loss: 0.0037467104848474264  PSNR: 29.99883270263672
[TRAIN] Iter: 624300 Loss: 0.004682596772909164  PSNR: 28.437854766845703
[TRAIN] Iter: 624400 Loss: 0.00405106320977211  PSNR: 29.81236457824707
[TRAIN] Iter: 624500 Loss: 0.00404235627502203  PSNR: 29.452777862548828
[TRAIN] Iter: 624600 Loss: 0.00365738314576447  PSNR: 29.874828338623047
[TRAIN] Iter: 624700 Loss: 0.0034744550939649343  PSNR: 29.798768997192383
[TRAIN] Iter: 624800 Loss: 0.00520735327154398  PSNR: 27.83469009399414
[TRAIN] Iter: 624900 Loss: 0.0038957546930760145  PSNR: 30.189613342285156
[TRAIN] Iter: 625000 Loss: 0.004420325625687838  PSNR: 28.77317237854004
[TRAIN] Iter: 625100 Loss: 0.004534333012998104  PSNR: 28.320892333984375
[TRAIN] Iter: 625200 Loss: 0.005322137847542763  PSNR: 27.064603805541992
[TRAIN] Iter: 625300 Loss: 0.004936934914439917  PSNR: 28.29917335510254
[TRAIN] Iter: 625400 Loss: 0.004927495028823614  PSNR: 28.57438087463379
[TRAIN] Iter: 625500 Loss: 0.004708447493612766  PSNR: 28.290502548217773
[TRAIN] Iter: 625600 Loss: 0.005824815947562456  PSNR: 26.737520217895508
[TRAIN] Iter: 625700 Loss: 0.003845139406621456  PSNR: 28.31255340576172
[TRAIN] Iter: 625800 Loss: 0.005019339732825756  PSNR: 27.41511344909668
[TRAIN] Iter: 625900 Loss: 0.00499272346496582  PSNR: 28.04755401611328
[TRAIN] Iter: 626000 Loss: 0.005839112214744091  PSNR: 26.810365676879883
[TRAIN] Iter: 626100 Loss: 0.0049184998497366905  PSNR: 27.849720001220703
[TRAIN] Iter: 626200 Loss: 0.004173812456429005  PSNR: 28.660022735595703
[TRAIN] Iter: 626300 Loss: 0.004457294009625912  PSNR: 29.317293167114258
[TRAIN] Iter: 626400 Loss: 0.004725071135908365  PSNR: 27.667560577392578
[TRAIN] Iter: 626500 Loss: 0.004217381589114666  PSNR: 28.658849716186523
[TRAIN] Iter: 626600 Loss: 0.00547893438488245  PSNR: 27.718547821044922
[TRAIN] Iter: 626700 Loss: 0.004474970046430826  PSNR: 28.543371200561523
[TRAIN] Iter: 626800 Loss: 0.005356580950319767  PSNR: 27.527822494506836
[TRAIN] Iter: 626900 Loss: 0.004288387950509787  PSNR: 29.147573471069336
[TRAIN] Iter: 627000 Loss: 0.003821840975433588  PSNR: 29.13612174987793
[TRAIN] Iter: 627100 Loss: 0.005171079654246569  PSNR: 27.712854385375977
[TRAIN] Iter: 627200 Loss: 0.0038319118320941925  PSNR: 29.495656967163086
[TRAIN] Iter: 627300 Loss: 0.004170991480350494  PSNR: 28.695972442626953
[TRAIN] Iter: 627400 Loss: 0.005011206958442926  PSNR: 27.45449447631836
[TRAIN] Iter: 627500 Loss: 0.003751251846551895  PSNR: 29.256620407104492
[TRAIN] Iter: 627600 Loss: 0.0035192561335861683  PSNR: 29.612146377563477
[TRAIN] Iter: 627700 Loss: 0.005946706049144268  PSNR: 27.25163459777832
[TRAIN] Iter: 627800 Loss: 0.004406251944601536  PSNR: 28.947616577148438
[TRAIN] Iter: 627900 Loss: 0.005239930935204029  PSNR: 27.130342483520508
[TRAIN] Iter: 628000 Loss: 0.005505834240466356  PSNR: 27.272207260131836
[TRAIN] Iter: 628100 Loss: 0.003765527391806245  PSNR: 29.771223068237305
[TRAIN] Iter: 628200 Loss: 0.003946235869079828  PSNR: 29.447675704956055
[TRAIN] Iter: 628300 Loss: 0.004508160054683685  PSNR: 28.2718505859375
[TRAIN] Iter: 628400 Loss: 0.0031229734886437654  PSNR: 30.638214111328125
[TRAIN] Iter: 628500 Loss: 0.004062054678797722  PSNR: 28.71541976928711
[TRAIN] Iter: 628600 Loss: 0.004414414055645466  PSNR: 28.664552688598633
[TRAIN] Iter: 628700 Loss: 0.0034263068810105324  PSNR: 30.385759353637695
[TRAIN] Iter: 628800 Loss: 0.005406093783676624  PSNR: 27.404796600341797
[TRAIN] Iter: 628900 Loss: 0.00374556053429842  PSNR: 30.020479202270508
[TRAIN] Iter: 629000 Loss: 0.0048800380900502205  PSNR: 27.607275009155273
[TRAIN] Iter: 629100 Loss: 0.00368848186917603  PSNR: 29.67685317993164
[TRAIN] Iter: 629200 Loss: 0.003642495721578598  PSNR: 30.230472564697266
[TRAIN] Iter: 629300 Loss: 0.005771089345216751  PSNR: 27.354888916015625
[TRAIN] Iter: 629400 Loss: 0.003809187561273575  PSNR: 29.881336212158203
[TRAIN] Iter: 629500 Loss: 0.005265542306005955  PSNR: 27.405372619628906
[TRAIN] Iter: 629600 Loss: 0.004462120588868856  PSNR: 28.06336212158203
[TRAIN] Iter: 629700 Loss: 0.0036178603768348694  PSNR: 29.978240966796875
[TRAIN] Iter: 629800 Loss: 0.004548173397779465  PSNR: 28.360912322998047
[TRAIN] Iter: 629900 Loss: 0.0058466470800340176  PSNR: 27.58264923095703
Saved checkpoints at ./logs/TUT-KE101-nerf/630000.tar
[TRAIN] Iter: 630000 Loss: 0.004951091483235359  PSNR: 28.176013946533203
[TRAIN] Iter: 630100 Loss: 0.003440428990870714  PSNR: 29.62017822265625
[TRAIN] Iter: 630200 Loss: 0.00502974446862936  PSNR: 27.974346160888672
[TRAIN] Iter: 630300 Loss: 0.004307485651224852  PSNR: 28.388259887695312
[TRAIN] Iter: 630400 Loss: 0.0048501635901629925  PSNR: 28.00225257873535
[TRAIN] Iter: 630500 Loss: 0.004285103641450405  PSNR: 28.69542694091797
[TRAIN] Iter: 630600 Loss: 0.00479881651699543  PSNR: 27.750455856323242
[TRAIN] Iter: 630700 Loss: 0.00444029550999403  PSNR: 29.17052459716797
[TRAIN] Iter: 630800 Loss: 0.004919537343084812  PSNR: 27.05449104309082
[TRAIN] Iter: 630900 Loss: 0.004492904990911484  PSNR: 28.5012149810791
[TRAIN] Iter: 631000 Loss: 0.004565319046378136  PSNR: 28.236005783081055
[TRAIN] Iter: 631100 Loss: 0.004568432457745075  PSNR: 28.659582138061523
[TRAIN] Iter: 631200 Loss: 0.004144197329878807  PSNR: 29.32709312438965
[TRAIN] Iter: 631300 Loss: 0.00603245897218585  PSNR: 26.9090633392334
[TRAIN] Iter: 631400 Loss: 0.005106171127408743  PSNR: 27.647920608520508
[TRAIN] Iter: 631500 Loss: 0.005170376040041447  PSNR: 27.185035705566406
[TRAIN] Iter: 631600 Loss: 0.004718571435660124  PSNR: 28.197498321533203
[TRAIN] Iter: 631700 Loss: 0.004751209169626236  PSNR: 28.17236328125
[TRAIN] Iter: 631800 Loss: 0.004213154781609774  PSNR: 28.123781204223633
[TRAIN] Iter: 631900 Loss: 0.004945970140397549  PSNR: 27.791439056396484
[TRAIN] Iter: 632000 Loss: 0.004415201023221016  PSNR: 28.76224136352539
[TRAIN] Iter: 632100 Loss: 0.005581745877861977  PSNR: 27.847518920898438
[TRAIN] Iter: 632200 Loss: 0.005512861534953117  PSNR: 26.603532791137695
[TRAIN] Iter: 632300 Loss: 0.004060373175889254  PSNR: 28.340200424194336
[TRAIN] Iter: 632400 Loss: 0.005276641808450222  PSNR: 27.922313690185547
[TRAIN] Iter: 632500 Loss: 0.004079103469848633  PSNR: 30.119304656982422
[TRAIN] Iter: 632600 Loss: 0.0031105424277484417  PSNR: 29.791872024536133
[TRAIN] Iter: 632700 Loss: 0.005866267718374729  PSNR: 26.89083480834961
[TRAIN] Iter: 632800 Loss: 0.004523702897131443  PSNR: 28.602067947387695
[TRAIN] Iter: 632900 Loss: 0.0031099270563572645  PSNR: 31.066709518432617
[TRAIN] Iter: 633000 Loss: 0.004776161629706621  PSNR: 28.112781524658203
[TRAIN] Iter: 633100 Loss: 0.004520983900874853  PSNR: 27.55573272705078
[TRAIN] Iter: 633200 Loss: 0.00594034418463707  PSNR: 27.040754318237305
[TRAIN] Iter: 633300 Loss: 0.004260374698787928  PSNR: 28.53148651123047
[TRAIN] Iter: 633400 Loss: 0.003783641615882516  PSNR: 29.488121032714844
[TRAIN] Iter: 633500 Loss: 0.005283081904053688  PSNR: 27.573110580444336
[TRAIN] Iter: 633600 Loss: 0.003194774966686964  PSNR: 30.198366165161133
[TRAIN] Iter: 633700 Loss: 0.0037623532116413116  PSNR: 29.36988639831543
[TRAIN] Iter: 633800 Loss: 0.005067893303930759  PSNR: 27.358972549438477
[TRAIN] Iter: 633900 Loss: 0.004267062991857529  PSNR: 29.51189613342285
[TRAIN] Iter: 634000 Loss: 0.004630871582776308  PSNR: 28.697298049926758
[TRAIN] Iter: 634100 Loss: 0.0035592694766819477  PSNR: 29.63003158569336
[TRAIN] Iter: 634200 Loss: 0.00444763945415616  PSNR: 28.404857635498047
[TRAIN] Iter: 634300 Loss: 0.0050715175457298756  PSNR: 27.66127586364746
[TRAIN] Iter: 634400 Loss: 0.004490611143410206  PSNR: 29.753253936767578
[TRAIN] Iter: 634500 Loss: 0.004168028477579355  PSNR: 29.52217674255371
[TRAIN] Iter: 634600 Loss: 0.003553649177774787  PSNR: 30.20613670349121
[TRAIN] Iter: 634700 Loss: 0.004024221561849117  PSNR: 28.747772216796875
[TRAIN] Iter: 634800 Loss: 0.005034937523305416  PSNR: 27.966354370117188
[TRAIN] Iter: 634900 Loss: 0.004786757752299309  PSNR: 28.114765167236328
[TRAIN] Iter: 635000 Loss: 0.0037678894586861134  PSNR: 30.685108184814453
[TRAIN] Iter: 635100 Loss: 0.004012414254248142  PSNR: 28.978540420532227
[TRAIN] Iter: 635200 Loss: 0.004970455542206764  PSNR: 28.10807991027832
[TRAIN] Iter: 635300 Loss: 0.004782858770340681  PSNR: 28.544490814208984
[TRAIN] Iter: 635400 Loss: 0.006055166479200125  PSNR: 26.958110809326172
[TRAIN] Iter: 635500 Loss: 0.004774987697601318  PSNR: 27.951337814331055
[TRAIN] Iter: 635600 Loss: 0.0036025503650307655  PSNR: 30.370697021484375
[TRAIN] Iter: 635700 Loss: 0.004965001717209816  PSNR: 27.49625015258789
[TRAIN] Iter: 635800 Loss: 0.004592403303831816  PSNR: 28.23528480529785
[TRAIN] Iter: 635900 Loss: 0.0035087682772427797  PSNR: 29.68113136291504
[TRAIN] Iter: 636000 Loss: 0.0036992875393480062  PSNR: 29.9567928314209
[TRAIN] Iter: 636100 Loss: 0.003970992751419544  PSNR: 29.25466537475586
[TRAIN] Iter: 636200 Loss: 0.0035566044971346855  PSNR: 29.933258056640625
[TRAIN] Iter: 636300 Loss: 0.003365294309332967  PSNR: 30.1066951751709
[TRAIN] Iter: 636400 Loss: 0.0037308072205632925  PSNR: 30.12240982055664
[TRAIN] Iter: 636500 Loss: 0.0035185590386390686  PSNR: 29.728708267211914
[TRAIN] Iter: 636600 Loss: 0.004424192011356354  PSNR: 28.756017684936523
[TRAIN] Iter: 636700 Loss: 0.004286780953407288  PSNR: 28.392488479614258
[TRAIN] Iter: 636800 Loss: 0.004803390707820654  PSNR: 28.108980178833008
[TRAIN] Iter: 636900 Loss: 0.0045518456026911736  PSNR: 27.581674575805664
[TRAIN] Iter: 637000 Loss: 0.005498376674950123  PSNR: 27.755903244018555
[TRAIN] Iter: 637100 Loss: 0.004699121695011854  PSNR: 28.103361129760742
[TRAIN] Iter: 637200 Loss: 0.005534340627491474  PSNR: 27.17442512512207
[TRAIN] Iter: 637300 Loss: 0.0037777158431708813  PSNR: 29.703022003173828
[TRAIN] Iter: 637400 Loss: 0.005159185733646154  PSNR: 27.680631637573242
[TRAIN] Iter: 637500 Loss: 0.004074426367878914  PSNR: 28.287752151489258
[TRAIN] Iter: 637600 Loss: 0.005566711537539959  PSNR: 27.163705825805664
[TRAIN] Iter: 637700 Loss: 0.004754100926220417  PSNR: 29.10533332824707
[TRAIN] Iter: 637800 Loss: 0.004939328879117966  PSNR: 27.966716766357422
[TRAIN] Iter: 637900 Loss: 0.005020563490688801  PSNR: 27.86418342590332
[TRAIN] Iter: 638000 Loss: 0.004500735551118851  PSNR: 28.50465965270996
[TRAIN] Iter: 638100 Loss: 0.005353889428079128  PSNR: 27.99765968322754
[TRAIN] Iter: 638200 Loss: 0.005617004819214344  PSNR: 27.38959312438965
[TRAIN] Iter: 638300 Loss: 0.0037852716632187366  PSNR: 29.7983455657959
[TRAIN] Iter: 638400 Loss: 0.004453170113265514  PSNR: 28.718814849853516
[TRAIN] Iter: 638500 Loss: 0.0053448365069925785  PSNR: 26.689603805541992
[TRAIN] Iter: 638600 Loss: 0.003936683759093285  PSNR: 30.28424644470215
[TRAIN] Iter: 638700 Loss: 0.005021061282604933  PSNR: 28.008272171020508
[TRAIN] Iter: 638800 Loss: 0.0036456440575420856  PSNR: 30.489595413208008
[TRAIN] Iter: 638900 Loss: 0.004022257402539253  PSNR: 29.03322410583496
[TRAIN] Iter: 639000 Loss: 0.0037882907781749964  PSNR: 29.8305721282959
[TRAIN] Iter: 639100 Loss: 0.003949938341975212  PSNR: 29.82466697692871
[TRAIN] Iter: 639200 Loss: 0.004727596417069435  PSNR: 27.694555282592773
[TRAIN] Iter: 639300 Loss: 0.0048753442242741585  PSNR: 28.155059814453125
[TRAIN] Iter: 639400 Loss: 0.005623647011816502  PSNR: 27.14495849609375
[TRAIN] Iter: 639500 Loss: 0.003888869658112526  PSNR: 29.101228713989258
[TRAIN] Iter: 639600 Loss: 0.004045196808874607  PSNR: 28.886072158813477
[TRAIN] Iter: 639700 Loss: 0.0035058571957051754  PSNR: 29.67211151123047
[TRAIN] Iter: 639800 Loss: 0.00581755954772234  PSNR: 27.555360794067383
[TRAIN] Iter: 639900 Loss: 0.005445824936032295  PSNR: 26.608713150024414
Saved checkpoints at ./logs/TUT-KE101-nerf/640000.tar
[TRAIN] Iter: 640000 Loss: 0.003332090564072132  PSNR: 30.62422752380371
[TRAIN] Iter: 640100 Loss: 0.004417929332703352  PSNR: 27.41892433166504
[TRAIN] Iter: 640200 Loss: 0.003259303281083703  PSNR: 30.13068962097168
[TRAIN] Iter: 640300 Loss: 0.005541876889765263  PSNR: 26.04965591430664
[TRAIN] Iter: 640400 Loss: 0.005188199691474438  PSNR: 27.59309959411621
[TRAIN] Iter: 640500 Loss: 0.0035020322538912296  PSNR: 30.301429748535156
[TRAIN] Iter: 640600 Loss: 0.005093937274068594  PSNR: 27.36623191833496
[TRAIN] Iter: 640700 Loss: 0.0052443379536271095  PSNR: 27.314638137817383
[TRAIN] Iter: 640800 Loss: 0.004644949920475483  PSNR: 27.76392364501953
[TRAIN] Iter: 640900 Loss: 0.005763217806816101  PSNR: 27.00368881225586
[TRAIN] Iter: 641000 Loss: 0.0037718922831118107  PSNR: 29.94444465637207
[TRAIN] Iter: 641100 Loss: 0.005826530046761036  PSNR: 27.04149055480957
[TRAIN] Iter: 641200 Loss: 0.00418403884395957  PSNR: 27.7218017578125
[TRAIN] Iter: 641300 Loss: 0.004144774284213781  PSNR: 27.76508140563965
[TRAIN] Iter: 641400 Loss: 0.0060997107066214085  PSNR: 26.640697479248047
[TRAIN] Iter: 641500 Loss: 0.0045154402032494545  PSNR: 28.197580337524414
[TRAIN] Iter: 641600 Loss: 0.004278360400348902  PSNR: 29.689680099487305
[TRAIN] Iter: 641700 Loss: 0.003297235816717148  PSNR: 29.883848190307617
[TRAIN] Iter: 641800 Loss: 0.0042058369144797325  PSNR: 28.644750595092773
[TRAIN] Iter: 641900 Loss: 0.006116412114351988  PSNR: 27.242952346801758
[TRAIN] Iter: 642000 Loss: 0.00506626395508647  PSNR: 27.745641708374023
[TRAIN] Iter: 642100 Loss: 0.005229178816080093  PSNR: 27.491525650024414
[TRAIN] Iter: 642200 Loss: 0.004640369676053524  PSNR: 28.3935489654541
[TRAIN] Iter: 642300 Loss: 0.0043322984129190445  PSNR: 28.099693298339844
[TRAIN] Iter: 642400 Loss: 0.0041787647642195225  PSNR: 29.332359313964844
[TRAIN] Iter: 642500 Loss: 0.004689362831413746  PSNR: 28.056989669799805
[TRAIN] Iter: 642600 Loss: 0.003388167591765523  PSNR: 30.1942081451416
[TRAIN] Iter: 642700 Loss: 0.004839075263589621  PSNR: 27.34490203857422
[TRAIN] Iter: 642800 Loss: 0.0039433869533240795  PSNR: 29.836088180541992
[TRAIN] Iter: 642900 Loss: 0.004557236097753048  PSNR: 28.096027374267578
[TRAIN] Iter: 643000 Loss: 0.0035545569844543934  PSNR: 30.003164291381836
[TRAIN] Iter: 643100 Loss: 0.002857575425878167  PSNR: 31.398822784423828
[TRAIN] Iter: 643200 Loss: 0.005005798302590847  PSNR: 27.8903751373291
[TRAIN] Iter: 643300 Loss: 0.004418168682605028  PSNR: 28.23259925842285
[TRAIN] Iter: 643400 Loss: 0.004616658668965101  PSNR: 28.246231079101562
[TRAIN] Iter: 643500 Loss: 0.004997912794351578  PSNR: 27.27743148803711
[TRAIN] Iter: 643600 Loss: 0.005251887254416943  PSNR: 27.478357315063477
[TRAIN] Iter: 643700 Loss: 0.005308779887855053  PSNR: 27.30223274230957
[TRAIN] Iter: 643800 Loss: 0.005293807480484247  PSNR: 27.614879608154297
[TRAIN] Iter: 643900 Loss: 0.0039202021434903145  PSNR: 29.038082122802734
[TRAIN] Iter: 644000 Loss: 0.004480961710214615  PSNR: 27.57203483581543
[TRAIN] Iter: 644100 Loss: 0.005664506461471319  PSNR: 26.949743270874023
[TRAIN] Iter: 644200 Loss: 0.004729202948510647  PSNR: 28.171581268310547
[TRAIN] Iter: 644300 Loss: 0.003937619272619486  PSNR: 28.885290145874023
[TRAIN] Iter: 644400 Loss: 0.005755739286541939  PSNR: 26.547191619873047
[TRAIN] Iter: 644500 Loss: 0.004203147254884243  PSNR: 29.404401779174805
[TRAIN] Iter: 644600 Loss: 0.0034091402776539326  PSNR: 30.128496170043945
[TRAIN] Iter: 644700 Loss: 0.0034608482383191586  PSNR: 30.03409767150879
[TRAIN] Iter: 644800 Loss: 0.004640297032892704  PSNR: 29.805797576904297
[TRAIN] Iter: 644900 Loss: 0.005219636484980583  PSNR: 28.380950927734375
[TRAIN] Iter: 645000 Loss: 0.004332235082983971  PSNR: 28.186378479003906
[TRAIN] Iter: 645100 Loss: 0.004416226875036955  PSNR: 28.838998794555664
[TRAIN] Iter: 645200 Loss: 0.0045732855796813965  PSNR: 28.195674896240234
[TRAIN] Iter: 645300 Loss: 0.004519688431173563  PSNR: 28.1569881439209
[TRAIN] Iter: 645400 Loss: 0.004438753239810467  PSNR: 28.193031311035156
[TRAIN] Iter: 645500 Loss: 0.005587378516793251  PSNR: 27.59990119934082
[TRAIN] Iter: 645600 Loss: 0.002752500819042325  PSNR: 31.611413955688477
[TRAIN] Iter: 645700 Loss: 0.003444578032940626  PSNR: 29.855199813842773
[TRAIN] Iter: 645800 Loss: 0.003905283287167549  PSNR: 28.645883560180664
[TRAIN] Iter: 645900 Loss: 0.005398354958742857  PSNR: 27.61112403869629
[TRAIN] Iter: 646000 Loss: 0.0033246378879994154  PSNR: 31.354936599731445
[TRAIN] Iter: 646100 Loss: 0.004203470423817635  PSNR: 28.905363082885742
[TRAIN] Iter: 646200 Loss: 0.005129990167915821  PSNR: 27.773174285888672
[TRAIN] Iter: 646300 Loss: 0.0051473951898515224  PSNR: 28.119169235229492
[TRAIN] Iter: 646400 Loss: 0.006168874446302652  PSNR: 26.813392639160156
[TRAIN] Iter: 646500 Loss: 0.004245485179126263  PSNR: 29.126718521118164
[TRAIN] Iter: 646600 Loss: 0.005672471597790718  PSNR: 27.77318000793457
[TRAIN] Iter: 646700 Loss: 0.004213311243802309  PSNR: 29.872028350830078
[TRAIN] Iter: 646800 Loss: 0.004751131869852543  PSNR: 28.621295928955078
[TRAIN] Iter: 646900 Loss: 0.004439930897206068  PSNR: 28.28926658630371
[TRAIN] Iter: 647000 Loss: 0.004969630390405655  PSNR: 29.075122833251953
[TRAIN] Iter: 647100 Loss: 0.004128157161176205  PSNR: 28.676908493041992
[TRAIN] Iter: 647200 Loss: 0.004032488446682692  PSNR: 29.365333557128906
[TRAIN] Iter: 647300 Loss: 0.004381858743727207  PSNR: 29.356155395507812
[TRAIN] Iter: 647400 Loss: 0.004264391027390957  PSNR: 29.497604370117188
[TRAIN] Iter: 647500 Loss: 0.005525498650968075  PSNR: 27.51398277282715
[TRAIN] Iter: 647600 Loss: 0.004223693162202835  PSNR: 28.305179595947266
[TRAIN] Iter: 647700 Loss: 0.005489233881235123  PSNR: 27.841651916503906
[TRAIN] Iter: 647800 Loss: 0.0029748519882559776  PSNR: 31.286638259887695
[TRAIN] Iter: 647900 Loss: 0.004952836781740189  PSNR: 27.76653289794922
[TRAIN] Iter: 648000 Loss: 0.004271526820957661  PSNR: 28.806747436523438
[TRAIN] Iter: 648100 Loss: 0.003922071307897568  PSNR: 28.936893463134766
[TRAIN] Iter: 648200 Loss: 0.0035509630106389523  PSNR: 29.338396072387695
[TRAIN] Iter: 648300 Loss: 0.0035123787820339203  PSNR: 30.14371109008789
[TRAIN] Iter: 648400 Loss: 0.00444444827735424  PSNR: 28.622528076171875
[TRAIN] Iter: 648500 Loss: 0.004182866774499416  PSNR: 28.5454044342041
[TRAIN] Iter: 648600 Loss: 0.0030341586098074913  PSNR: 31.14744758605957
[TRAIN] Iter: 648700 Loss: 0.0034553275909274817  PSNR: 30.14183235168457
[TRAIN] Iter: 648800 Loss: 0.00556023046374321  PSNR: 27.537046432495117
[TRAIN] Iter: 648900 Loss: 0.0061111897230148315  PSNR: 26.3394832611084
[TRAIN] Iter: 649000 Loss: 0.004399214405566454  PSNR: 28.721416473388672
[TRAIN] Iter: 649100 Loss: 0.005262911785393953  PSNR: 27.32491683959961
[TRAIN] Iter: 649200 Loss: 0.0044945841655135155  PSNR: 28.122512817382812
[TRAIN] Iter: 649300 Loss: 0.003770897164940834  PSNR: 30.121034622192383
[TRAIN] Iter: 649400 Loss: 0.00377366179600358  PSNR: 30.26331329345703
[TRAIN] Iter: 649500 Loss: 0.004875864367932081  PSNR: 28.11153221130371
[TRAIN] Iter: 649600 Loss: 0.004376265220344067  PSNR: 28.356040954589844
[TRAIN] Iter: 649700 Loss: 0.0036941010039299726  PSNR: 30.827238082885742
[TRAIN] Iter: 649800 Loss: 0.004298639949411154  PSNR: 28.31755828857422
[TRAIN] Iter: 649900 Loss: 0.004600256681442261  PSNR: 28.094907760620117
Saved checkpoints at ./logs/TUT-KE101-nerf/650000.tar
0 0.0004191398620605469
torch.Size([320, 640, 3]) torch.Size([320, 640])
1 15.593565940856934
2 13.312494277954102
3 15.49107837677002
4 13.269235610961914
5 13.28882646560669
6 15.510170459747314
7 13.138494491577148
8 15.70738673210144
9 13.034507274627686
10 13.349015712738037
11 15.646720886230469
12 13.09436821937561
13 15.689671039581299
14 12.999019384384155
15 15.695480823516846
16 13.163955211639404
17 13.389992952346802
18 15.443930387496948
19 13.207009553909302
20 15.561097383499146
21 13.057401657104492
22 13.338265419006348
23 15.667989492416382
24 13.297147035598755
25 15.374638080596924
26 13.338715314865112
27 13.384765625
28 15.32662320137024
29 13.304736614227295
30 15.40697431564331
31 13.29768443107605
32 15.432571411132812
33 13.340678453445435
34 13.363149404525757
35 15.44788408279419
36 13.324795961380005
37 15.428173303604126
38 13.367740869522095
39 13.354653120040894
40 15.375635623931885
41 13.3605477809906
42 15.433934450149536
43 13.358278751373291
44 15.403154373168945
45 13.323770999908447
46 13.31609320640564
47 15.475616693496704
48 13.23103380203247
49 15.576970100402832
50 13.158741474151611
51 13.249978065490723
52 15.697696685791016
53 13.444318294525146
54 15.288282632827759
55 13.376705408096313
56 13.310635089874268
57 15.349318027496338
58 13.321897268295288
59 15.531956911087036
60 13.289018869400024
61 15.460092544555664
62 13.193183898925781
63 13.40073847770691
64 15.535681247711182
65 13.174630403518677
66 15.564067363739014
67 13.196310997009277
68 13.429610013961792
69 15.489397048950195
70 13.2361319065094
71 15.586086988449097
72 13.098262310028076
73 15.508064031600952
74 13.362632274627686
75 13.052513122558594
76 16.060649394989014
77 12.784895420074463
78 15.797300815582275
79 13.35358738899231
80 13.037009000778198
81 15.74736213684082
82 12.901047945022583
83 15.611705780029297
84 13.144314765930176
85 13.30501914024353
86 15.562509775161743
87 13.001493692398071
88 15.886700630187988
89 12.989213466644287
90 15.56324315071106
91 13.365357398986816
92 13.150979042053223
93 15.686267137527466
94 13.104844093322754
95 15.611031770706177
96 13.173526763916016
97 13.400083780288696
98 15.598008871078491
99 12.972445249557495
100 15.718232154846191
101 12.872999429702759
102 13.514142274856567
103 15.369590044021606
104 13.18132758140564
105 15.844822883605957
106 12.772789716720581
107 15.88994574546814
108 13.353372812271118
109 13.29130744934082
110 15.400153875350952
111 13.355279684066772
112 15.447699546813965
113 13.366840600967407
114 13.342962503433228
115 15.4215247631073
116 13.349894523620605
117 15.460084438323975
118 13.32961106300354
119 13.324206113815308
Done, saving (120, 320, 640, 3) (120, 320, 640)
extras:{'raw': tensor([[[-1.4527e+00, -1.4274e+00, -1.6879e+00, -6.4709e+01],
         [-1.1638e+00, -7.2298e-01,  4.4948e-02, -6.7440e+01],
         [-1.0114e+00, -5.8836e-01,  1.7709e-01, -6.6865e+01],
         ...,
         [-1.0171e+01, -8.2985e+00, -7.7897e+00, -1.2299e+02],
         [-1.2740e+01, -1.0166e+01, -8.9243e+00,  4.9771e+00],
         [-1.2987e+01, -1.0216e+01, -8.2100e+00, -1.4008e+01]],

        [[-2.9662e+00, -3.2114e+00, -3.3722e+00, -7.1532e+01],
         [-1.0920e+00, -1.3957e-01,  1.7564e+00, -4.3838e+01],
         [-1.0606e+00, -6.8448e-01, -2.7016e-01, -2.2315e+01],
         ...,
         [-2.0154e+01, -2.1069e+01, -2.5444e+01, -3.4460e+02],
         [-1.8677e+01, -1.8827e+01, -2.2244e+01, -3.2744e+02],
         [-2.1206e+01, -2.0887e+01, -2.2612e+01, -2.8168e+02]],

        [[ 6.2971e+00,  7.4529e+00,  1.0071e+01, -7.8702e+01],
         [ 2.4804e-01,  2.5158e-01,  3.2383e-01, -7.1102e+00],
         [ 2.6503e-01,  2.7225e-01,  3.5153e-01, -6.9214e+00],
         ...,
         [ 9.5235e+01,  9.3396e+01,  1.0229e+02, -6.6128e+01],
         [ 1.1061e+02,  1.1265e+02,  1.3264e+02, -1.2348e+02],
         [ 8.6253e+01,  8.3096e+01,  8.6473e+01, -3.8236e+01]],

        ...,

        [[-6.5396e+00, -6.2092e+00, -5.4692e+00, -6.7627e+01],
         [-1.1048e-01, -2.8450e-01, -6.2512e-01,  2.7128e+01],
         [-7.8782e-02, -2.3621e-01, -4.9722e-01,  3.3020e+01],
         ...,
         [ 2.5617e+00,  6.5748e-01, -4.1334e+00,  1.3149e+03],
         [ 4.4685e+00,  2.9429e+00, -9.5764e-01,  1.2291e+03],
         [ 2.8643e+00,  1.1168e+00, -3.1147e+00,  1.3006e+03]],

        [[ 7.7796e+00,  8.9568e+00,  1.2261e+01, -7.1415e+01],
         [ 3.8175e-01,  2.1055e-01, -1.6037e-01, -1.3646e+01],
         [-4.9313e-02, -2.6809e-01, -7.8430e-01,  1.3861e+00],
         ...,
         [ 1.1350e+00, -4.0117e+00, -1.7502e+01,  5.5701e+02],
         [ 4.8568e-01, -4.6362e+00, -1.7791e+01,  5.0437e+02],
         [ 8.4838e-01, -4.0688e+00, -1.7417e+01,  6.1591e+02]],

        [[ 2.4784e+01,  2.6940e+01,  3.3651e+01, -7.2335e+01],
         [ 6.6862e-01,  7.2914e-01,  1.0517e+00, -4.4272e+01],
         [-1.6236e-01, -2.5892e-01, -3.8894e-01, -8.8700e+00],
         ...,
         [ 3.7521e-01, -3.3559e+00, -1.2988e+01,  3.8859e+02],
         [ 6.9379e-01, -2.9719e+00, -1.2366e+01,  4.3422e+02],
         [ 7.9073e-01, -2.6624e+00, -1.1964e+01,  4.8840e+02]]],
       grad_fn=<ReshapeAliasBackward0>), 'rgb0': tensor([[0.4262, 0.2660, 0.0919],
        [0.3834, 0.3663, 0.3276],
        [0.6032, 0.5809, 0.6107],
        ...,
        [0.4666, 0.4263, 0.3520],
        [0.4680, 0.4326, 0.3447],
        [0.4858, 0.4510, 0.3814]], grad_fn=<ReshapeAliasBackward0>), 'disp0': tensor([167.5233,  61.6167,  89.4470,  ..., 874.3796,  67.8029,  55.3576],
       grad_fn=<ReshapeAliasBackward0>), 'acc0': tensor([1., 1., 1.,  ..., 1., 1., 1.], grad_fn=<ReshapeAliasBackward0>), 'z_std': tensor([0.0015, 0.1201, 0.0020,  ..., 0.2589, 0.0194, 0.0025])}
0 0.0005407333374023438
torch.Size([320, 640, 3]) torch.Size([320, 640])
1 13.367785215377808
2 15.436376094818115
3 13.302394151687622
4 15.406971454620361
5 13.297454118728638
6 13.304334878921509
7 15.449012041091919
8 13.313385725021362
9 15.390870332717896
10 13.362318515777588
11 15.420450687408447
12 13.346143960952759
13 13.323775291442871
14 15.432376623153687
15 13.25429630279541
16 15.389081954956055
17 13.38172459602356
18 13.36518669128418
19 15.051674604415894
20 13.464218139648438
21 15.54297161102295
22 13.249878883361816
23 13.332538843154907
24 15.520749807357788
25 13.323225975036621
26 15.457874536514282
27 13.247493505477905
28 15.525970220565796
29 13.190163850784302
30 13.441898345947266
31 15.562963485717773
32 13.154690265655518
33 15.559096336364746
34 13.185295343399048
35 13.415886402130127
36 15.446161270141602
37 13.194034814834595
38 15.628047466278076
39 13.123492240905762
40 13.43039345741272
41 15.53203821182251
42 13.046962976455688
43 16.04524517059326
44 12.934167385101318
45 15.621531248092651
46 13.214431285858154
47 13.020630359649658
48 15.993941068649292
49 12.921314716339111
50 15.82587194442749
51 13.173353910446167
52 13.198857545852661
53 15.563037633895874
54 13.298165082931519
55 15.315384149551392
56 15.262850046157837
57 17.013057470321655
58 14.887170791625977
59 17.577772617340088
60 15.140483379364014
61 15.048393964767456
62 17.451087713241577
63 15.22935152053833
64 17.344926834106445
65 15.509352922439575
66 17.681984186172485
67 15.679943561553955
68 17.68484854698181
69 15.474082231521606
70 17.407256603240967
71 15.273862838745117
72 17.314647912979126
73 15.117361545562744
74 17.252326726913452
75 15.199068307876587
76 17.215242624282837
77 15.193593502044678
78 17.237358570098877
79 15.194109678268433
80 17.225520372390747
81 15.221454620361328
82 15.220699787139893
83 17.242714166641235
84 15.026810884475708
85 17.37509846687317
86 14.754024982452393
87 18.11363387107849
88 15.260812282562256
89 16.800593852996826
90 15.14612603187561
91 17.35874319076538
92 15.062214374542236
93 17.26955795288086
94 15.151848077774048
95 17.327248573303223
96 15.114395141601562
97 17.31298565864563
98 15.13071322441101
99 15.230602741241455
100 17.24833583831787
101 15.194969654083252
102 17.29308772087097
103 15.101525783538818
104 17.319214582443237
105 15.066312313079834
106 17.4285306930542
107 15.04113507270813
108 17.45051097869873
109 15.014763593673706
110 17.450581550598145
111 14.909043073654175
112 17.37136149406433
113 15.011967658996582
114 17.413265228271484
115 15.075606346130371
116 15.04978609085083
117 17.58909797668457
118 14.797393560409546
119 17.787476539611816
test poses shape torch.Size([4, 3, 4])
0 0.0005660057067871094
torch.Size([320, 640, 3]) torch.Size([320, 640])
1 17.723427295684814
2 14.768913269042969
3 17.664337396621704
Saved test set
[TRAIN] Iter: 650000 Loss: 0.00543572660535574  PSNR: 27.604793548583984
[TRAIN] Iter: 650100 Loss: 0.00436011515557766  PSNR: 28.846908569335938
[TRAIN] Iter: 650200 Loss: 0.0051398323848843575  PSNR: 27.550939559936523
[TRAIN] Iter: 650300 Loss: 0.005479720421135426  PSNR: 26.981542587280273
[TRAIN] Iter: 650400 Loss: 0.004065780434757471  PSNR: 30.033184051513672
[TRAIN] Iter: 650500 Loss: 0.0030255981255322695  PSNR: 31.34609031677246
[TRAIN] Iter: 650600 Loss: 0.004391548223793507  PSNR: 28.376182556152344
[TRAIN] Iter: 650700 Loss: 0.0036771176382899284  PSNR: 30.49985694885254
[TRAIN] Iter: 650800 Loss: 0.0048231156542897224  PSNR: 28.649982452392578
[TRAIN] Iter: 650900 Loss: 0.005074257031083107  PSNR: 27.205257415771484
[TRAIN] Iter: 651000 Loss: 0.004370030947029591  PSNR: 28.28207015991211
[TRAIN] Iter: 651100 Loss: 0.004311867989599705  PSNR: 28.252735137939453
[TRAIN] Iter: 651200 Loss: 0.004380018450319767  PSNR: 28.5800838470459
[TRAIN] Iter: 651300 Loss: 0.004818030633032322  PSNR: 27.72373390197754
[TRAIN] Iter: 651400 Loss: 0.006071252282708883  PSNR: 27.20330810546875
[TRAIN] Iter: 651500 Loss: 0.0037742801941931248  PSNR: 30.038848876953125
[TRAIN] Iter: 651600 Loss: 0.0035614296793937683  PSNR: 30.511005401611328
[TRAIN] Iter: 651700 Loss: 0.00488140806555748  PSNR: 27.933942794799805
[TRAIN] Iter: 651800 Loss: 0.004627841524779797  PSNR: 27.888343811035156
[TRAIN] Iter: 651900 Loss: 0.0038440157659351826  PSNR: 29.982454299926758
[TRAIN] Iter: 652000 Loss: 0.003162760753184557  PSNR: 30.825759887695312
[TRAIN] Iter: 652100 Loss: 0.005004945211112499  PSNR: 27.680118560791016
[TRAIN] Iter: 652200 Loss: 0.0036924013402312994  PSNR: 30.273054122924805
[TRAIN] Iter: 652300 Loss: 0.0053288317285478115  PSNR: 27.21591567993164
[TRAIN] Iter: 652400 Loss: 0.005933727137744427  PSNR: 27.040164947509766
[TRAIN] Iter: 652500 Loss: 0.0043163178488612175  PSNR: 29.00890350341797
[TRAIN] Iter: 652600 Loss: 0.0040677920915186405  PSNR: 28.66144371032715
[TRAIN] Iter: 652700 Loss: 0.003997094929218292  PSNR: 28.85428810119629
[TRAIN] Iter: 652800 Loss: 0.0036843321286141872  PSNR: 29.64651870727539
[TRAIN] Iter: 652900 Loss: 0.004454480484127998  PSNR: 28.619157791137695
[TRAIN] Iter: 653000 Loss: 0.004508859012275934  PSNR: 29.16971778869629
[TRAIN] Iter: 653100 Loss: 0.0033782527316361666  PSNR: 30.349599838256836
[TRAIN] Iter: 653200 Loss: 0.0032230555079877377  PSNR: 30.20526885986328
[TRAIN] Iter: 653300 Loss: 0.00557518657296896  PSNR: 28.142101287841797
[TRAIN] Iter: 653400 Loss: 0.004348743706941605  PSNR: 28.194854736328125
[TRAIN] Iter: 653500 Loss: 0.004886922892183065  PSNR: 27.820438385009766
[TRAIN] Iter: 653600 Loss: 0.004168986808508635  PSNR: 29.319942474365234
[TRAIN] Iter: 653700 Loss: 0.0043556345626711845  PSNR: 28.092124938964844
[TRAIN] Iter: 653800 Loss: 0.0046716067008674145  PSNR: 28.442607879638672
[TRAIN] Iter: 653900 Loss: 0.004294938407838345  PSNR: 28.04497528076172
[TRAIN] Iter: 654000 Loss: 0.003522696206346154  PSNR: 30.420547485351562
[TRAIN] Iter: 654100 Loss: 0.003954706713557243  PSNR: 28.958410263061523
[TRAIN] Iter: 654200 Loss: 0.004112533293664455  PSNR: 29.229076385498047
[TRAIN] Iter: 654300 Loss: 0.005441920831799507  PSNR: 26.830951690673828
[TRAIN] Iter: 654400 Loss: 0.005782783962786198  PSNR: 27.575681686401367
[TRAIN] Iter: 654500 Loss: 0.0037739118561148643  PSNR: 29.863798141479492
[TRAIN] Iter: 654600 Loss: 0.0035850759595632553  PSNR: 29.473520278930664
[TRAIN] Iter: 654700 Loss: 0.003339152317494154  PSNR: 30.314786911010742
[TRAIN] Iter: 654800 Loss: 0.0036756268236786127  PSNR: 29.243671417236328
[TRAIN] Iter: 654900 Loss: 0.004068430978804827  PSNR: 28.99362564086914
[TRAIN] Iter: 655000 Loss: 0.005419991910457611  PSNR: 27.31622886657715
[TRAIN] Iter: 655100 Loss: 0.003952628932893276  PSNR: 28.869813919067383
[TRAIN] Iter: 655200 Loss: 0.004505550023168325  PSNR: 28.242752075195312
[TRAIN] Iter: 655300 Loss: 0.00443907268345356  PSNR: 28.151819229125977
[TRAIN] Iter: 655400 Loss: 0.004652530886232853  PSNR: 28.84562873840332
[TRAIN] Iter: 655500 Loss: 0.0042869942262768745  PSNR: 28.66805076599121
[TRAIN] Iter: 655600 Loss: 0.003617602866142988  PSNR: 29.957176208496094
[TRAIN] Iter: 655700 Loss: 0.004153579939156771  PSNR: 30.074195861816406
[TRAIN] Iter: 655800 Loss: 0.005236727185547352  PSNR: 28.450448989868164
[TRAIN] Iter: 655900 Loss: 0.00366389867849648  PSNR: 29.53145408630371
[TRAIN] Iter: 656000 Loss: 0.004246107302606106  PSNR: 29.21131706237793
[TRAIN] Iter: 656100 Loss: 0.004799016751348972  PSNR: 28.28911018371582
[TRAIN] Iter: 656200 Loss: 0.003924990072846413  PSNR: 28.646169662475586
[TRAIN] Iter: 656300 Loss: 0.005797250661998987  PSNR: 27.369380950927734
[TRAIN] Iter: 656400 Loss: 0.004374190233647823  PSNR: 28.474037170410156
[TRAIN] Iter: 656500 Loss: 0.004014035686850548  PSNR: 28.518457412719727
[TRAIN] Iter: 656600 Loss: 0.0042313383892178535  PSNR: 28.374908447265625
[TRAIN] Iter: 656700 Loss: 0.005930542480200529  PSNR: 27.11418342590332
[TRAIN] Iter: 656800 Loss: 0.00531091308221221  PSNR: 27.771921157836914
[TRAIN] Iter: 656900 Loss: 0.004599465988576412  PSNR: 27.89915657043457
[TRAIN] Iter: 657000 Loss: 0.00385966245085001  PSNR: 29.581361770629883
[TRAIN] Iter: 657100 Loss: 0.0037548444233834743  PSNR: 29.641151428222656
[TRAIN] Iter: 657200 Loss: 0.00455364678055048  PSNR: 28.797569274902344
[TRAIN] Iter: 657300 Loss: 0.0042238542810082436  PSNR: 28.765459060668945
[TRAIN] Iter: 657400 Loss: 0.005724570248275995  PSNR: 26.54853057861328
[TRAIN] Iter: 657500 Loss: 0.00426842924207449  PSNR: 30.068321228027344
[TRAIN] Iter: 657600 Loss: 0.004218315705657005  PSNR: 28.422849655151367
[TRAIN] Iter: 657700 Loss: 0.005805766209959984  PSNR: 27.500871658325195
[TRAIN] Iter: 657800 Loss: 0.004717119969427586  PSNR: 27.998382568359375
[TRAIN] Iter: 657900 Loss: 0.004358750302344561  PSNR: 28.207998275756836
[TRAIN] Iter: 658000 Loss: 0.003471221774816513  PSNR: 30.27429962158203
[TRAIN] Iter: 658100 Loss: 0.004617689643055201  PSNR: 27.95414161682129
[TRAIN] Iter: 658200 Loss: 0.004204791970551014  PSNR: 28.278017044067383
[TRAIN] Iter: 658300 Loss: 0.0037915229331701994  PSNR: 29.930566787719727
[TRAIN] Iter: 658400 Loss: 0.004356956109404564  PSNR: 28.317705154418945
[TRAIN] Iter: 658500 Loss: 0.004004873801022768  PSNR: 29.347970962524414
[TRAIN] Iter: 658600 Loss: 0.003910873085260391  PSNR: 29.436038970947266
[TRAIN] Iter: 658700 Loss: 0.0040550027042627335  PSNR: 28.644479751586914
[TRAIN] Iter: 658800 Loss: 0.003868218045681715  PSNR: 29.64323616027832
[TRAIN] Iter: 658900 Loss: 0.004652127623558044  PSNR: 28.035110473632812
[TRAIN] Iter: 659000 Loss: 0.00447393674403429  PSNR: 30.00057029724121
[TRAIN] Iter: 659100 Loss: 0.0037917690351605415  PSNR: 29.32378387451172
[TRAIN] Iter: 659200 Loss: 0.0034924286883324385  PSNR: 30.269315719604492
[TRAIN] Iter: 659300 Loss: 0.005260840989649296  PSNR: 28.16217613220215
[TRAIN] Iter: 659400 Loss: 0.003223084844648838  PSNR: 30.27092742919922
[TRAIN] Iter: 659500 Loss: 0.004813384264707565  PSNR: 28.058082580566406
[TRAIN] Iter: 659600 Loss: 0.0037294765934348106  PSNR: 30.638561248779297
[TRAIN] Iter: 659700 Loss: 0.00375393801368773  PSNR: 30.36745262145996
[TRAIN] Iter: 659800 Loss: 0.004885341506451368  PSNR: 28.080739974975586
[TRAIN] Iter: 659900 Loss: 0.0044534956105053425  PSNR: 28.599214553833008
Saved checkpoints at ./logs/TUT-KE101-nerf/660000.tar
[TRAIN] Iter: 660000 Loss: 0.005542093422263861  PSNR: 27.021053314208984
[TRAIN] Iter: 660100 Loss: 0.006102366838604212  PSNR: 27.11775779724121
[TRAIN] Iter: 660200 Loss: 0.003796251490712166  PSNR: 28.494647979736328
[TRAIN] Iter: 660300 Loss: 0.0042844354175031185  PSNR: 28.86936378479004
[TRAIN] Iter: 660400 Loss: 0.004893025383353233  PSNR: 27.54556655883789
[TRAIN] Iter: 660500 Loss: 0.004249415826052427  PSNR: 28.7294979095459
[TRAIN] Iter: 660600 Loss: 0.005352184176445007  PSNR: 26.719982147216797
[TRAIN] Iter: 660700 Loss: 0.004861998371779919  PSNR: 27.669586181640625
[TRAIN] Iter: 660800 Loss: 0.004057489335536957  PSNR: 29.170177459716797
[TRAIN] Iter: 660900 Loss: 0.004822173155844212  PSNR: 28.223711013793945
[TRAIN] Iter: 661000 Loss: 0.0047423215582966805  PSNR: 28.15151023864746
[TRAIN] Iter: 661100 Loss: 0.004244381096214056  PSNR: 28.67217254638672
[TRAIN] Iter: 661200 Loss: 0.0036590390373021364  PSNR: 30.48570442199707
[TRAIN] Iter: 661300 Loss: 0.005156446248292923  PSNR: 27.32819938659668
[TRAIN] Iter: 661400 Loss: 0.005101847928017378  PSNR: 27.566307067871094
[TRAIN] Iter: 661500 Loss: 0.005594536662101746  PSNR: 27.583669662475586
[TRAIN] Iter: 661600 Loss: 0.004856109619140625  PSNR: 27.78040313720703
[TRAIN] Iter: 661700 Loss: 0.003901418996974826  PSNR: 30.064958572387695
[TRAIN] Iter: 661800 Loss: 0.004245024174451828  PSNR: 28.429018020629883
[TRAIN] Iter: 661900 Loss: 0.005718331318348646  PSNR: 27.397838592529297
[TRAIN] Iter: 662000 Loss: 0.004039570689201355  PSNR: 29.575151443481445
[TRAIN] Iter: 662100 Loss: 0.005073361564427614  PSNR: 28.223875045776367
[TRAIN] Iter: 662200 Loss: 0.006175575777888298  PSNR: 27.17011833190918
[TRAIN] Iter: 662300 Loss: 0.004912549164146185  PSNR: 27.771099090576172
[TRAIN] Iter: 662400 Loss: 0.004103609826415777  PSNR: 29.52771759033203
[TRAIN] Iter: 662500 Loss: 0.004350499249994755  PSNR: 29.103557586669922
[TRAIN] Iter: 662600 Loss: 0.005938589572906494  PSNR: 27.0645809173584
[TRAIN] Iter: 662700 Loss: 0.004376676864922047  PSNR: 28.168624877929688
[TRAIN] Iter: 662800 Loss: 0.004785842262208462  PSNR: 28.53453826904297
[TRAIN] Iter: 662900 Loss: 0.005040447227656841  PSNR: 28.176183700561523
[TRAIN] Iter: 663000 Loss: 0.005142645910382271  PSNR: 28.04916763305664
[TRAIN] Iter: 663100 Loss: 0.0037948195822536945  PSNR: 29.884414672851562
[TRAIN] Iter: 663200 Loss: 0.004618370905518532  PSNR: 27.70091438293457
[TRAIN] Iter: 663300 Loss: 0.004700414836406708  PSNR: 28.06796646118164
[TRAIN] Iter: 663400 Loss: 0.004376614466309547  PSNR: 28.22609519958496
[TRAIN] Iter: 663500 Loss: 0.004349985159933567  PSNR: 28.661447525024414
[TRAIN] Iter: 663600 Loss: 0.005018860101699829  PSNR: 27.870500564575195
[TRAIN] Iter: 663700 Loss: 0.0046813637018203735  PSNR: 28.01796531677246
[TRAIN] Iter: 663800 Loss: 0.003622600110247731  PSNR: 29.337554931640625
[TRAIN] Iter: 663900 Loss: 0.0033239303156733513  PSNR: 30.835739135742188
[TRAIN] Iter: 664000 Loss: 0.004630471579730511  PSNR: 27.50446319580078
[TRAIN] Iter: 664100 Loss: 0.004943397827446461  PSNR: 27.473154067993164
[TRAIN] Iter: 664200 Loss: 0.00512548815459013  PSNR: 27.400556564331055
[TRAIN] Iter: 664300 Loss: 0.004309643525630236  PSNR: 29.59284210205078
[TRAIN] Iter: 664400 Loss: 0.005544803570955992  PSNR: 27.03609275817871
[TRAIN] Iter: 664500 Loss: 0.004690766334533691  PSNR: 28.065744400024414
[TRAIN] Iter: 664600 Loss: 0.0049441782757639885  PSNR: 27.867191314697266
[TRAIN] Iter: 664700 Loss: 0.004084235988557339  PSNR: 28.624549865722656
[TRAIN] Iter: 664800 Loss: 0.0046877809800207615  PSNR: 28.746034622192383
[TRAIN] Iter: 664900 Loss: 0.005310165695846081  PSNR: 27.825143814086914
[TRAIN] Iter: 665000 Loss: 0.0036829987075179815  PSNR: 30.506675720214844
[TRAIN] Iter: 665100 Loss: 0.004736557137221098  PSNR: 28.96311378479004
[TRAIN] Iter: 665200 Loss: 0.0045389775186777115  PSNR: 29.186519622802734
[TRAIN] Iter: 665300 Loss: 0.00469790818169713  PSNR: 27.610912322998047
[TRAIN] Iter: 665400 Loss: 0.004909721203148365  PSNR: 28.730804443359375
[TRAIN] Iter: 665500 Loss: 0.00416862778365612  PSNR: 29.65510368347168
[TRAIN] Iter: 665600 Loss: 0.005147373303771019  PSNR: 27.69999122619629
[TRAIN] Iter: 665700 Loss: 0.004481003154069185  PSNR: 28.532512664794922
[TRAIN] Iter: 665800 Loss: 0.005089547485113144  PSNR: 27.639982223510742
[TRAIN] Iter: 665900 Loss: 0.004119290504604578  PSNR: 28.754623413085938
[TRAIN] Iter: 666000 Loss: 0.0035279360599815845  PSNR: 29.655160903930664
[TRAIN] Iter: 666100 Loss: 0.004751860164105892  PSNR: 27.838119506835938
[TRAIN] Iter: 666200 Loss: 0.005709381774067879  PSNR: 27.059402465820312
[TRAIN] Iter: 666300 Loss: 0.004710017703473568  PSNR: 27.90813446044922
[TRAIN] Iter: 666400 Loss: 0.003936107270419598  PSNR: 30.0470027923584
[TRAIN] Iter: 666500 Loss: 0.005980560556054115  PSNR: 27.639665603637695
[TRAIN] Iter: 666600 Loss: 0.005393681116402149  PSNR: 27.36151695251465
[TRAIN] Iter: 666700 Loss: 0.0045552742667496204  PSNR: 28.211076736450195
[TRAIN] Iter: 666800 Loss: 0.004392256960272789  PSNR: 28.453760147094727
[TRAIN] Iter: 666900 Loss: 0.004427512641996145  PSNR: 28.15534782409668
[TRAIN] Iter: 667000 Loss: 0.003879759693518281  PSNR: 30.185344696044922
[TRAIN] Iter: 667100 Loss: 0.003972676116973162  PSNR: 28.78877830505371
[TRAIN] Iter: 667200 Loss: 0.00433732196688652  PSNR: 28.254823684692383
[TRAIN] Iter: 667300 Loss: 0.0033183894120156765  PSNR: 30.643936157226562
[TRAIN] Iter: 667400 Loss: 0.005416756495833397  PSNR: 27.106122970581055
[TRAIN] Iter: 667500 Loss: 0.004931991919875145  PSNR: 27.546531677246094
[TRAIN] Iter: 667600 Loss: 0.005282609723508358  PSNR: 27.13702392578125
[TRAIN] Iter: 667700 Loss: 0.004141292534768581  PSNR: 28.794841766357422
[TRAIN] Iter: 667800 Loss: 0.004369816742837429  PSNR: 28.159652709960938
[TRAIN] Iter: 667900 Loss: 0.003983380272984505  PSNR: 29.317428588867188
[TRAIN] Iter: 668000 Loss: 0.00396508676931262  PSNR: 30.18698501586914
[TRAIN] Iter: 668100 Loss: 0.0034865587949752808  PSNR: 30.358861923217773
[TRAIN] Iter: 668200 Loss: 0.0036744256503880024  PSNR: 30.1033935546875
[TRAIN] Iter: 668300 Loss: 0.005140878260135651  PSNR: 27.40953826904297
[TRAIN] Iter: 668400 Loss: 0.0048483810387551785  PSNR: 28.179838180541992
[TRAIN] Iter: 668500 Loss: 0.003719391068443656  PSNR: 30.509103775024414
[TRAIN] Iter: 668600 Loss: 0.003991199657320976  PSNR: 28.248811721801758
[TRAIN] Iter: 668700 Loss: 0.0045465268194675446  PSNR: 28.012968063354492
[TRAIN] Iter: 668800 Loss: 0.0055845994502305984  PSNR: 27.402511596679688
[TRAIN] Iter: 668900 Loss: 0.005127590615302324  PSNR: 27.954042434692383
[TRAIN] Iter: 669000 Loss: 0.004317694343626499  PSNR: 28.85980796813965
[TRAIN] Iter: 669100 Loss: 0.004026663489639759  PSNR: 29.150836944580078
[TRAIN] Iter: 669200 Loss: 0.0046200561337172985  PSNR: 28.23635482788086
[TRAIN] Iter: 669300 Loss: 0.004937976598739624  PSNR: 27.822837829589844
[TRAIN] Iter: 669400 Loss: 0.004760913550853729  PSNR: 28.178791046142578
[TRAIN] Iter: 669500 Loss: 0.003988997079432011  PSNR: 30.355607986450195
[TRAIN] Iter: 669600 Loss: 0.005587485618889332  PSNR: 27.212356567382812
[TRAIN] Iter: 669700 Loss: 0.0034548132680356503  PSNR: 30.327678680419922
[TRAIN] Iter: 669800 Loss: 0.0038423556834459305  PSNR: 29.89739990234375
[TRAIN] Iter: 669900 Loss: 0.00407357607036829  PSNR: 29.8061580657959
Saved checkpoints at ./logs/TUT-KE101-nerf/670000.tar
[TRAIN] Iter: 670000 Loss: 0.005234910175204277  PSNR: 27.430116653442383
[TRAIN] Iter: 670100 Loss: 0.0036666272208094597  PSNR: 30.78728675842285
[TRAIN] Iter: 670200 Loss: 0.005094076041132212  PSNR: 28.73032569885254
[TRAIN] Iter: 670300 Loss: 0.003985034767538309  PSNR: 28.316452026367188
[TRAIN] Iter: 670400 Loss: 0.004263356328010559  PSNR: 28.640687942504883
[TRAIN] Iter: 670500 Loss: 0.0049082571640610695  PSNR: 27.651147842407227
[TRAIN] Iter: 670600 Loss: 0.004532891325652599  PSNR: 28.307632446289062
[TRAIN] Iter: 670700 Loss: 0.005087079480290413  PSNR: 28.29288673400879
[TRAIN] Iter: 670800 Loss: 0.005710773169994354  PSNR: 27.50695037841797
[TRAIN] Iter: 670900 Loss: 0.004699253011494875  PSNR: 27.85533905029297
[TRAIN] Iter: 671000 Loss: 0.003906603902578354  PSNR: 28.050302505493164
[TRAIN] Iter: 671100 Loss: 0.0036989720538258553  PSNR: 29.892131805419922
[TRAIN] Iter: 671200 Loss: 0.003671364625915885  PSNR: 29.904226303100586
[TRAIN] Iter: 671300 Loss: 0.004158892668783665  PSNR: 28.756132125854492
[TRAIN] Iter: 671400 Loss: 0.004698243457823992  PSNR: 28.069650650024414
[TRAIN] Iter: 671500 Loss: 0.0037311799824237823  PSNR: 29.833454132080078
[TRAIN] Iter: 671600 Loss: 0.005193283781409264  PSNR: 27.180978775024414
[TRAIN] Iter: 671700 Loss: 0.006348386872559786  PSNR: 26.525604248046875
[TRAIN] Iter: 671800 Loss: 0.00451253354549408  PSNR: 28.135696411132812
[TRAIN] Iter: 671900 Loss: 0.004298034124076366  PSNR: 28.680419921875
[TRAIN] Iter: 672000 Loss: 0.0040322295390069485  PSNR: 28.965829849243164
[TRAIN] Iter: 672100 Loss: 0.003547830507159233  PSNR: 29.864110946655273
[TRAIN] Iter: 672200 Loss: 0.004427770152688026  PSNR: 28.484962463378906
[TRAIN] Iter: 672300 Loss: 0.003776603378355503  PSNR: 29.483251571655273
[TRAIN] Iter: 672400 Loss: 0.00627525057643652  PSNR: 27.02974510192871
[TRAIN] Iter: 672500 Loss: 0.006162471137940884  PSNR: 26.272119522094727
[TRAIN] Iter: 672600 Loss: 0.0066946567967534065  PSNR: 25.945253372192383
[TRAIN] Iter: 672700 Loss: 0.003943950869143009  PSNR: 28.205188751220703
[TRAIN] Iter: 672800 Loss: 0.003558593802154064  PSNR: 29.64697265625
[TRAIN] Iter: 672900 Loss: 0.00494647491723299  PSNR: 28.16843032836914
[TRAIN] Iter: 673000 Loss: 0.0032508475705981255  PSNR: 30.624088287353516
[TRAIN] Iter: 673100 Loss: 0.004547493997961283  PSNR: 28.32200813293457
[TRAIN] Iter: 673200 Loss: 0.0042122239246964455  PSNR: 30.41508674621582
[TRAIN] Iter: 673300 Loss: 0.0049627842381596565  PSNR: 28.017871856689453
[TRAIN] Iter: 673400 Loss: 0.0040679085068404675  PSNR: 30.07401466369629
[TRAIN] Iter: 673500 Loss: 0.004172518849372864  PSNR: 29.70894432067871
[TRAIN] Iter: 673600 Loss: 0.004641828127205372  PSNR: 28.456951141357422
[TRAIN] Iter: 673700 Loss: 0.003520742990076542  PSNR: 30.03904151916504
[TRAIN] Iter: 673800 Loss: 0.005165877752006054  PSNR: 27.956783294677734
[TRAIN] Iter: 673900 Loss: 0.0042718625627458096  PSNR: 28.351402282714844
[TRAIN] Iter: 674000 Loss: 0.004286276642233133  PSNR: 28.609987258911133
[TRAIN] Iter: 674100 Loss: 0.0036932379007339478  PSNR: 29.191490173339844
[TRAIN] Iter: 674200 Loss: 0.00332253728993237  PSNR: 30.4078426361084
[TRAIN] Iter: 674300 Loss: 0.003581615164875984  PSNR: 30.35225486755371
[TRAIN] Iter: 674400 Loss: 0.0036565596237778664  PSNR: 29.712472915649414
[TRAIN] Iter: 674500 Loss: 0.003767632879316807  PSNR: 29.137603759765625
[TRAIN] Iter: 674600 Loss: 0.005764266010373831  PSNR: 27.25835418701172
[TRAIN] Iter: 674700 Loss: 0.0039472756907343864  PSNR: 29.64396095275879
[TRAIN] Iter: 674800 Loss: 0.004391269292682409  PSNR: 28.663740158081055
[TRAIN] Iter: 674900 Loss: 0.005092351231724024  PSNR: 27.581615447998047
[TRAIN] Iter: 675000 Loss: 0.00416713859885931  PSNR: 28.147903442382812
[TRAIN] Iter: 675100 Loss: 0.003526118816807866  PSNR: 30.129901885986328
[TRAIN] Iter: 675200 Loss: 0.0048957583494484425  PSNR: 27.791725158691406
[TRAIN] Iter: 675300 Loss: 0.004091406241059303  PSNR: 28.65590476989746
[TRAIN] Iter: 675400 Loss: 0.004624977707862854  PSNR: 27.775129318237305
[TRAIN] Iter: 675500 Loss: 0.004459291696548462  PSNR: 27.474740982055664
[TRAIN] Iter: 675600 Loss: 0.004865865223109722  PSNR: 27.75543975830078
[TRAIN] Iter: 675700 Loss: 0.004872631747275591  PSNR: 28.10501480102539
[TRAIN] Iter: 675800 Loss: 0.005814943462610245  PSNR: 26.730424880981445
[TRAIN] Iter: 675900 Loss: 0.004521161317825317  PSNR: 28.172237396240234
[TRAIN] Iter: 676000 Loss: 0.00368484016507864  PSNR: 29.757190704345703
[TRAIN] Iter: 676100 Loss: 0.0038103742990642786  PSNR: 30.0229549407959
[TRAIN] Iter: 676200 Loss: 0.005024517886340618  PSNR: 27.68721580505371
[TRAIN] Iter: 676300 Loss: 0.004168392159044743  PSNR: 29.29805564880371
[TRAIN] Iter: 676400 Loss: 0.004930452443659306  PSNR: 28.476350784301758
[TRAIN] Iter: 676500 Loss: 0.0044644782319664955  PSNR: 28.780134201049805
[TRAIN] Iter: 676600 Loss: 0.0036025543231517076  PSNR: 30.26322364807129
[TRAIN] Iter: 676700 Loss: 0.005140191409736872  PSNR: 27.19181251525879
[TRAIN] Iter: 676800 Loss: 0.00547987362369895  PSNR: 27.354700088500977
[TRAIN] Iter: 676900 Loss: 0.004627445247024298  PSNR: 29.242443084716797
[TRAIN] Iter: 677000 Loss: 0.005401751957833767  PSNR: 26.926183700561523
[TRAIN] Iter: 677100 Loss: 0.0033817391376942396  PSNR: 29.016305923461914
[TRAIN] Iter: 677200 Loss: 0.0037600595969706774  PSNR: 29.419279098510742
[TRAIN] Iter: 677300 Loss: 0.004239494446665049  PSNR: 28.497817993164062
[TRAIN] Iter: 677400 Loss: 0.006312012672424316  PSNR: 26.622671127319336
[TRAIN] Iter: 677500 Loss: 0.0036190084647387266  PSNR: 30.021076202392578
[TRAIN] Iter: 677600 Loss: 0.005572556518018246  PSNR: 26.804622650146484
[TRAIN] Iter: 677700 Loss: 0.005502714775502682  PSNR: 27.13496971130371
[TRAIN] Iter: 677800 Loss: 0.005641726776957512  PSNR: 27.291301727294922
[TRAIN] Iter: 677900 Loss: 0.004838105291128159  PSNR: 27.924121856689453
[TRAIN] Iter: 678000 Loss: 0.005193362943828106  PSNR: 27.240854263305664
[TRAIN] Iter: 678100 Loss: 0.004791304003447294  PSNR: 27.906190872192383
[TRAIN] Iter: 678200 Loss: 0.004114418290555477  PSNR: 30.455018997192383
[TRAIN] Iter: 678300 Loss: 0.005397553090006113  PSNR: 27.334253311157227
[TRAIN] Iter: 678400 Loss: 0.004690774716436863  PSNR: 28.493099212646484
[TRAIN] Iter: 678500 Loss: 0.005328079219907522  PSNR: 27.35950469970703
[TRAIN] Iter: 678600 Loss: 0.005088320467621088  PSNR: 27.812108993530273
[TRAIN] Iter: 678700 Loss: 0.004786610137671232  PSNR: 27.898853302001953
[TRAIN] Iter: 678800 Loss: 0.0038259034045040607  PSNR: 29.68777084350586
[TRAIN] Iter: 678900 Loss: 0.004392413422465324  PSNR: 28.431638717651367
[TRAIN] Iter: 679000 Loss: 0.004739908967167139  PSNR: 28.216951370239258
[TRAIN] Iter: 679100 Loss: 0.0039043158758431673  PSNR: 29.49012565612793
[TRAIN] Iter: 679200 Loss: 0.005266771651804447  PSNR: 27.47552490234375
[TRAIN] Iter: 679300 Loss: 0.005059914663434029  PSNR: 27.849576950073242
[TRAIN] Iter: 679400 Loss: 0.004179961048066616  PSNR: 28.178377151489258
[TRAIN] Iter: 679500 Loss: 0.0045840563252568245  PSNR: 28.078542709350586
[TRAIN] Iter: 679600 Loss: 0.004445966333150864  PSNR: 28.372146606445312
[TRAIN] Iter: 679700 Loss: 0.006016741972416639  PSNR: 27.28015899658203
[TRAIN] Iter: 679800 Loss: 0.003727699164301157  PSNR: 30.118125915527344
[TRAIN] Iter: 679900 Loss: 0.004646120592951775  PSNR: 28.243227005004883
Saved checkpoints at ./logs/TUT-KE101-nerf/680000.tar
[TRAIN] Iter: 680000 Loss: 0.0040940213948488235  PSNR: 29.289012908935547
[TRAIN] Iter: 680100 Loss: 0.0051423776894807816  PSNR: 28.152910232543945
[TRAIN] Iter: 680200 Loss: 0.004648304544389248  PSNR: 28.48392105102539
[TRAIN] Iter: 680300 Loss: 0.004370161332190037  PSNR: 28.153575897216797
[TRAIN] Iter: 680400 Loss: 0.005212350748479366  PSNR: 27.391347885131836
[TRAIN] Iter: 680500 Loss: 0.0037701132241636515  PSNR: 29.931623458862305
[TRAIN] Iter: 680600 Loss: 0.004975469782948494  PSNR: 27.7908935546875
[TRAIN] Iter: 680700 Loss: 0.004244999960064888  PSNR: 28.511634826660156
[TRAIN] Iter: 680800 Loss: 0.004262326285243034  PSNR: 27.90279769897461
[TRAIN] Iter: 680900 Loss: 0.005690325051546097  PSNR: 27.800809860229492
[TRAIN] Iter: 681000 Loss: 0.0038047810085117817  PSNR: 29.478370666503906
[TRAIN] Iter: 681100 Loss: 0.004589110147207975  PSNR: 27.721271514892578
[TRAIN] Iter: 681200 Loss: 0.0046039302833378315  PSNR: 28.222610473632812
[TRAIN] Iter: 681300 Loss: 0.005613537505269051  PSNR: 27.53681755065918
[TRAIN] Iter: 681400 Loss: 0.005246824584901333  PSNR: 27.836496353149414
[TRAIN] Iter: 681500 Loss: 0.004406158812344074  PSNR: 28.60354232788086
[TRAIN] Iter: 681600 Loss: 0.0037552143912762403  PSNR: 29.768882751464844
[TRAIN] Iter: 681700 Loss: 0.0033683148212730885  PSNR: 30.49262237548828
[TRAIN] Iter: 681800 Loss: 0.0034569157287478447  PSNR: 31.12129783630371
[TRAIN] Iter: 681900 Loss: 0.005562350153923035  PSNR: 27.055519104003906
[TRAIN] Iter: 682000 Loss: 0.004195415414869785  PSNR: 29.801450729370117
[TRAIN] Iter: 682100 Loss: 0.00533809419721365  PSNR: 27.8228816986084
[TRAIN] Iter: 682200 Loss: 0.004020024091005325  PSNR: 28.96754264831543
[TRAIN] Iter: 682300 Loss: 0.005348286125808954  PSNR: 27.8184814453125
[TRAIN] Iter: 682400 Loss: 0.003326517529785633  PSNR: 30.064369201660156
[TRAIN] Iter: 682500 Loss: 0.005005734972655773  PSNR: 27.899585723876953
[TRAIN] Iter: 682600 Loss: 0.003375261789187789  PSNR: 29.883129119873047
[TRAIN] Iter: 682700 Loss: 0.004334736615419388  PSNR: 28.491121292114258
[TRAIN] Iter: 682800 Loss: 0.005400490947067738  PSNR: 27.757503509521484
[TRAIN] Iter: 682900 Loss: 0.004662902094423771  PSNR: 28.357196807861328
[TRAIN] Iter: 683000 Loss: 0.00626531895250082  PSNR: 27.663423538208008
[TRAIN] Iter: 683100 Loss: 0.004547751508653164  PSNR: 28.688518524169922
[TRAIN] Iter: 683200 Loss: 0.005000314675271511  PSNR: 27.70810890197754
[TRAIN] Iter: 683300 Loss: 0.0039844028651714325  PSNR: 30.022686004638672
[TRAIN] Iter: 683400 Loss: 0.004156156443059444  PSNR: 28.560951232910156
[TRAIN] Iter: 683500 Loss: 0.004970467183738947  PSNR: 27.593931198120117
[TRAIN] Iter: 683600 Loss: 0.005125856027007103  PSNR: 27.69666290283203
[TRAIN] Iter: 683700 Loss: 0.005677064415067434  PSNR: 26.915754318237305
[TRAIN] Iter: 683800 Loss: 0.004507711157202721  PSNR: 28.67279815673828
[TRAIN] Iter: 683900 Loss: 0.004615042358636856  PSNR: 27.923059463500977
[TRAIN] Iter: 684000 Loss: 0.0034628352150321007  PSNR: 30.110557556152344
[TRAIN] Iter: 684100 Loss: 0.0052590398117899895  PSNR: 27.949426651000977
[TRAIN] Iter: 684200 Loss: 0.0048402659595012665  PSNR: 28.542781829833984
[TRAIN] Iter: 684300 Loss: 0.004834070801734924  PSNR: 28.468664169311523
[TRAIN] Iter: 684400 Loss: 0.005731204524636269  PSNR: 27.279579162597656
[TRAIN] Iter: 684500 Loss: 0.0035939014051109552  PSNR: 30.246728897094727
[TRAIN] Iter: 684600 Loss: 0.004682755097746849  PSNR: 28.184707641601562
[TRAIN] Iter: 684700 Loss: 0.004570254124701023  PSNR: 28.017309188842773
[TRAIN] Iter: 684800 Loss: 0.005396777763962746  PSNR: 28.037208557128906
[TRAIN] Iter: 684900 Loss: 0.00440484331920743  PSNR: 28.181861877441406
[TRAIN] Iter: 685000 Loss: 0.005063842982053757  PSNR: 27.566139221191406
[TRAIN] Iter: 685100 Loss: 0.003634479595348239  PSNR: 29.65477180480957
[TRAIN] Iter: 685200 Loss: 0.0036287985276430845  PSNR: 29.934005737304688
[TRAIN] Iter: 685300 Loss: 0.003438830841332674  PSNR: 30.896413803100586
[TRAIN] Iter: 685400 Loss: 0.005243418738245964  PSNR: 27.3411808013916
[TRAIN] Iter: 685500 Loss: 0.003691646736115217  PSNR: 30.721019744873047
[TRAIN] Iter: 685600 Loss: 0.00536989513784647  PSNR: 26.911828994750977
[TRAIN] Iter: 685700 Loss: 0.005136159248650074  PSNR: 27.3385009765625
[TRAIN] Iter: 685800 Loss: 0.006006493233144283  PSNR: 26.735036849975586
[TRAIN] Iter: 685900 Loss: 0.003848177148029208  PSNR: 29.585363388061523
[TRAIN] Iter: 686000 Loss: 0.004152950365096331  PSNR: 29.752031326293945
[TRAIN] Iter: 686100 Loss: 0.00503837876021862  PSNR: 28.004369735717773
[TRAIN] Iter: 686200 Loss: 0.004083152860403061  PSNR: 29.825254440307617
[TRAIN] Iter: 686300 Loss: 0.004924168810248375  PSNR: 27.784099578857422
[TRAIN] Iter: 686400 Loss: 0.0037364705931395292  PSNR: 29.440502166748047
[TRAIN] Iter: 686500 Loss: 0.003807583823800087  PSNR: 29.333200454711914
[TRAIN] Iter: 686600 Loss: 0.003733313176780939  PSNR: 30.455873489379883
[TRAIN] Iter: 686700 Loss: 0.0042744772508740425  PSNR: 28.478511810302734
[TRAIN] Iter: 686800 Loss: 0.004747603554278612  PSNR: 27.936988830566406
[TRAIN] Iter: 686900 Loss: 0.0042149643413722515  PSNR: 28.398508071899414
[TRAIN] Iter: 687000 Loss: 0.0064591155387461185  PSNR: 26.29641342163086
[TRAIN] Iter: 687100 Loss: 0.0038463962264358997  PSNR: 29.483858108520508
[TRAIN] Iter: 687200 Loss: 0.004818830639123917  PSNR: 27.881250381469727
[TRAIN] Iter: 687300 Loss: 0.0042608254589140415  PSNR: 28.53546905517578
[TRAIN] Iter: 687400 Loss: 0.005184284411370754  PSNR: 27.591651916503906
[TRAIN] Iter: 687500 Loss: 0.0042384592816233635  PSNR: 29.54981803894043
[TRAIN] Iter: 687600 Loss: 0.004530284553766251  PSNR: 28.082313537597656
[TRAIN] Iter: 687700 Loss: 0.0035567251034080982  PSNR: 30.142335891723633
[TRAIN] Iter: 687800 Loss: 0.004228197503834963  PSNR: 28.76139259338379
[TRAIN] Iter: 687900 Loss: 0.005303668789565563  PSNR: 27.3988094329834
[TRAIN] Iter: 688000 Loss: 0.0043156021274626255  PSNR: 28.716480255126953
[TRAIN] Iter: 688100 Loss: 0.003909414634108543  PSNR: 29.07608413696289
[TRAIN] Iter: 688200 Loss: 0.005508565343916416  PSNR: 27.290063858032227
[TRAIN] Iter: 688300 Loss: 0.0046557169407606125  PSNR: 28.78748893737793
[TRAIN] Iter: 688400 Loss: 0.005175075959414244  PSNR: 27.88308334350586
[TRAIN] Iter: 688500 Loss: 0.004165214020758867  PSNR: 28.653244018554688
[TRAIN] Iter: 688600 Loss: 0.00469328835606575  PSNR: 27.38889503479004
[TRAIN] Iter: 688700 Loss: 0.005769362207502127  PSNR: 27.30162811279297
[TRAIN] Iter: 688800 Loss: 0.003059860784560442  PSNR: 31.038990020751953
[TRAIN] Iter: 688900 Loss: 0.004255697131156921  PSNR: 28.70186996459961
[TRAIN] Iter: 689000 Loss: 0.003879105905070901  PSNR: 29.25607681274414
[TRAIN] Iter: 689100 Loss: 0.0045399777591228485  PSNR: 28.04746437072754
[TRAIN] Iter: 689200 Loss: 0.004857547581195831  PSNR: 28.939414978027344
[TRAIN] Iter: 689300 Loss: 0.005427277646958828  PSNR: 27.10243034362793
[TRAIN] Iter: 689400 Loss: 0.0028445590287446976  PSNR: 30.650182723999023
[TRAIN] Iter: 689500 Loss: 0.003617989830672741  PSNR: 29.695756912231445
[TRAIN] Iter: 689600 Loss: 0.0035420407075434923  PSNR: 29.705503463745117
[TRAIN] Iter: 689700 Loss: 0.00537099177017808  PSNR: 27.498720169067383
[TRAIN] Iter: 689800 Loss: 0.004346592351794243  PSNR: 28.334781646728516
[TRAIN] Iter: 689900 Loss: 0.005491479765623808  PSNR: 27.566204071044922
Saved checkpoints at ./logs/TUT-KE101-nerf/690000.tar
[TRAIN] Iter: 690000 Loss: 0.004518269095569849  PSNR: 28.990432739257812
[TRAIN] Iter: 690100 Loss: 0.0038800195325165987  PSNR: 28.170372009277344
[TRAIN] Iter: 690200 Loss: 0.00558121781796217  PSNR: 27.542434692382812
[TRAIN] Iter: 690300 Loss: 0.003897059243172407  PSNR: 29.923297882080078
[TRAIN] Iter: 690400 Loss: 0.004280957393348217  PSNR: 28.849435806274414
[TRAIN] Iter: 690500 Loss: 0.004675440955907106  PSNR: 28.189476013183594
[TRAIN] Iter: 690600 Loss: 0.005221162457019091  PSNR: 27.913930892944336
[TRAIN] Iter: 690700 Loss: 0.005118266679346561  PSNR: 28.633302688598633
[TRAIN] Iter: 690800 Loss: 0.0041710203513503075  PSNR: 28.494306564331055
[TRAIN] Iter: 690900 Loss: 0.00486068706959486  PSNR: 27.8555850982666
[TRAIN] Iter: 691000 Loss: 0.0032921878155320883  PSNR: 30.85304832458496
[TRAIN] Iter: 691100 Loss: 0.003337832633405924  PSNR: 29.707584381103516
[TRAIN] Iter: 691200 Loss: 0.0039528049528598785  PSNR: 28.27914047241211
[TRAIN] Iter: 691300 Loss: 0.003930358216166496  PSNR: 29.52583122253418
[TRAIN] Iter: 691400 Loss: 0.005279328674077988  PSNR: 28.00838851928711
[TRAIN] Iter: 691500 Loss: 0.004762913100421429  PSNR: 28.514341354370117
[TRAIN] Iter: 691600 Loss: 0.005153084173798561  PSNR: 27.5876407623291
[TRAIN] Iter: 691700 Loss: 0.0042243883945047855  PSNR: 28.764448165893555
[TRAIN] Iter: 691800 Loss: 0.004880005493760109  PSNR: 28.416799545288086
[TRAIN] Iter: 691900 Loss: 0.004749950021505356  PSNR: 28.152902603149414
[TRAIN] Iter: 692000 Loss: 0.005876482930034399  PSNR: 26.491775512695312
[TRAIN] Iter: 692100 Loss: 0.004653433337807655  PSNR: 28.04581642150879
[TRAIN] Iter: 692200 Loss: 0.0038418639451265335  PSNR: 29.395408630371094
[TRAIN] Iter: 692300 Loss: 0.0051409294828772545  PSNR: 28.16570281982422
[TRAIN] Iter: 692400 Loss: 0.003212294541299343  PSNR: 29.81641960144043
[TRAIN] Iter: 692500 Loss: 0.0035578198730945587  PSNR: 30.091867446899414
[TRAIN] Iter: 692600 Loss: 0.004951385781168938  PSNR: 28.182723999023438
[TRAIN] Iter: 692700 Loss: 0.0049078697338700294  PSNR: 28.146699905395508
[TRAIN] Iter: 692800 Loss: 0.004661791957914829  PSNR: 27.86992073059082
[TRAIN] Iter: 692900 Loss: 0.003740491811186075  PSNR: 29.47156524658203
[TRAIN] Iter: 693000 Loss: 0.0061014266684651375  PSNR: 27.261112213134766
[TRAIN] Iter: 693100 Loss: 0.003242315026000142  PSNR: 29.554899215698242
[TRAIN] Iter: 693200 Loss: 0.0038325698114931583  PSNR: 29.74114990234375
[TRAIN] Iter: 693300 Loss: 0.004362258594483137  PSNR: 28.171192169189453
[TRAIN] Iter: 693400 Loss: 0.0056077586486935616  PSNR: 27.4770450592041
[TRAIN] Iter: 693500 Loss: 0.0029567740857601166  PSNR: 30.805570602416992
[TRAIN] Iter: 693600 Loss: 0.005179163068532944  PSNR: 27.655765533447266
[TRAIN] Iter: 693700 Loss: 0.0033939131535589695  PSNR: 30.2508544921875
[TRAIN] Iter: 693800 Loss: 0.003805758897215128  PSNR: 29.48526954650879
[TRAIN] Iter: 693900 Loss: 0.00407794676721096  PSNR: 29.36298179626465
[TRAIN] Iter: 694000 Loss: 0.0044943466782569885  PSNR: 28.050539016723633
[TRAIN] Iter: 694100 Loss: 0.004898061975836754  PSNR: 27.788759231567383
[TRAIN] Iter: 694200 Loss: 0.005242735147476196  PSNR: 28.31378173828125
[TRAIN] Iter: 694300 Loss: 0.003699119668453932  PSNR: 29.787904739379883
[TRAIN] Iter: 694400 Loss: 0.0034237357322126627  PSNR: 30.197826385498047
[TRAIN] Iter: 694500 Loss: 0.0030367146246135235  PSNR: 30.864294052124023
[TRAIN] Iter: 694600 Loss: 0.005855146329849958  PSNR: 26.000591278076172
[TRAIN] Iter: 694700 Loss: 0.004014304373413324  PSNR: 29.78962516784668
[TRAIN] Iter: 694800 Loss: 0.006305604707449675  PSNR: 26.52170181274414
[TRAIN] Iter: 694900 Loss: 0.005700288340449333  PSNR: 27.421110153198242
[TRAIN] Iter: 695000 Loss: 0.0058243172243237495  PSNR: 27.05344009399414
[TRAIN] Iter: 695100 Loss: 0.005023561418056488  PSNR: 27.997425079345703
[TRAIN] Iter: 695200 Loss: 0.004686710424721241  PSNR: 28.24586296081543
[TRAIN] Iter: 695300 Loss: 0.004846553783863783  PSNR: 27.975568771362305
[TRAIN] Iter: 695400 Loss: 0.003856370225548744  PSNR: 28.74243927001953
[TRAIN] Iter: 695500 Loss: 0.005418743472546339  PSNR: 28.07205581665039
[TRAIN] Iter: 695600 Loss: 0.004226556979119778  PSNR: 29.04067039489746
[TRAIN] Iter: 695700 Loss: 0.005543783772736788  PSNR: 27.14457893371582
[TRAIN] Iter: 695800 Loss: 0.0034241420216858387  PSNR: 30.200387954711914
[TRAIN] Iter: 695900 Loss: 0.004908260889351368  PSNR: 28.0748233795166
[TRAIN] Iter: 696000 Loss: 0.004932763054966927  PSNR: 27.500652313232422
[TRAIN] Iter: 696100 Loss: 0.0042405626736581326  PSNR: 29.512786865234375
[TRAIN] Iter: 696200 Loss: 0.004713305737823248  PSNR: 28.037126541137695
[TRAIN] Iter: 696300 Loss: 0.005632741376757622  PSNR: 27.527027130126953
[TRAIN] Iter: 696400 Loss: 0.004359858110547066  PSNR: 28.51720428466797
[TRAIN] Iter: 696500 Loss: 0.00392948416993022  PSNR: 30.075090408325195
[TRAIN] Iter: 696600 Loss: 0.0036052230279892683  PSNR: 29.64862632751465
[TRAIN] Iter: 696700 Loss: 0.005072657484561205  PSNR: 27.673965454101562
[TRAIN] Iter: 696800 Loss: 0.003896337002515793  PSNR: 29.758014678955078
[TRAIN] Iter: 696900 Loss: 0.003532695583999157  PSNR: 30.286373138427734
[TRAIN] Iter: 697000 Loss: 0.004822532646358013  PSNR: 28.24474334716797
[TRAIN] Iter: 697100 Loss: 0.0036140300799161196  PSNR: 29.345947265625
[TRAIN] Iter: 697200 Loss: 0.004972366150468588  PSNR: 28.204748153686523
[TRAIN] Iter: 697300 Loss: 0.004508637823164463  PSNR: 27.653459548950195
[TRAIN] Iter: 697400 Loss: 0.004003767389804125  PSNR: 29.478330612182617
[TRAIN] Iter: 697500 Loss: 0.0040631904266774654  PSNR: 28.699892044067383
[TRAIN] Iter: 697600 Loss: 0.004591853357851505  PSNR: 28.2668514251709
[TRAIN] Iter: 697700 Loss: 0.004565010312944651  PSNR: 28.026016235351562
[TRAIN] Iter: 697800 Loss: 0.003951139748096466  PSNR: 28.40985107421875
[TRAIN] Iter: 697900 Loss: 0.004604501184076071  PSNR: 29.771915435791016
[TRAIN] Iter: 698000 Loss: 0.005459907464683056  PSNR: 27.82343101501465
[TRAIN] Iter: 698100 Loss: 0.005507258698344231  PSNR: 26.88089942932129
[TRAIN] Iter: 698200 Loss: 0.003908393904566765  PSNR: 29.40874671936035
[TRAIN] Iter: 698300 Loss: 0.005073044914752245  PSNR: 27.531557083129883
[TRAIN] Iter: 698400 Loss: 0.004626041278243065  PSNR: 28.444042205810547
[TRAIN] Iter: 698500 Loss: 0.004634643439203501  PSNR: 27.96922492980957
[TRAIN] Iter: 698600 Loss: 0.004918593913316727  PSNR: 27.31730842590332
[TRAIN] Iter: 698700 Loss: 0.0044413283467292786  PSNR: 28.222898483276367
[TRAIN] Iter: 698800 Loss: 0.004580444656312466  PSNR: 28.561494827270508
[TRAIN] Iter: 698900 Loss: 0.005953973159193993  PSNR: 26.519166946411133
[TRAIN] Iter: 699000 Loss: 0.003346930956467986  PSNR: 29.956626892089844
[TRAIN] Iter: 699100 Loss: 0.005664902739226818  PSNR: 27.032215118408203
[TRAIN] Iter: 699200 Loss: 0.004280470311641693  PSNR: 29.181806564331055
[TRAIN] Iter: 699300 Loss: 0.004516636487096548  PSNR: 27.452123641967773
[TRAIN] Iter: 699400 Loss: 0.004624849185347557  PSNR: 28.201332092285156
[TRAIN] Iter: 699500 Loss: 0.005435195751488209  PSNR: 27.949459075927734
[TRAIN] Iter: 699600 Loss: 0.005160545464605093  PSNR: 27.75165557861328
[TRAIN] Iter: 699700 Loss: 0.0033668214455246925  PSNR: 30.344812393188477
[TRAIN] Iter: 699800 Loss: 0.0038891667500138283  PSNR: 30.116004943847656
[TRAIN] Iter: 699900 Loss: 0.003957780078053474  PSNR: 28.733057022094727
Saved checkpoints at ./logs/TUT-KE101-nerf/700000.tar
0 0.00043511390686035156
torch.Size([320, 640, 3]) torch.Size([320, 640])
1 15.593811750411987
2 13.063583612442017
3 15.51492190361023
4 13.214583158493042
5 13.105098724365234
6 15.819934368133545
7 12.738298416137695
8 16.05704951286316
9 12.991641283035278
10 13.052020788192749
11 16.080644369125366
12 12.85596776008606
13 15.74970269203186
14 12.87551236152649
15 13.103363990783691
16 16.007713317871094
17 12.898319005966187
18 15.836753606796265
19 13.358108282089233
20 13.337323188781738
21 15.160688877105713
22 13.317953109741211
23 15.370770692825317
24 13.326062440872192
25 15.363615036010742
26 13.329934358596802
27 13.302513122558594
28 15.353204011917114
29 13.372375011444092
30 15.379642724990845
31 13.344999313354492
32 13.321855783462524
33 15.411983966827393
34 13.325608968734741
35 15.358607769012451
36 13.344346761703491
37 15.377214193344116
38 13.36877703666687
39 13.220957279205322
40 15.381739139556885
41 13.295340299606323
42 15.395155906677246
43 13.344601392745972
44 13.319820404052734
45 15.395573616027832
46 13.335416793823242
47 15.448225498199463
48 13.302446365356445
49 13.228003025054932
50 15.461876153945923
51 13.187078952789307
52 15.66207504272461
53 13.318561315536499
54 15.387628555297852
55 13.299060106277466
56 13.240456342697144
57 15.438987970352173
58 13.270991325378418
59 15.431273698806763
60 13.264224529266357
61 13.28718614578247
62 15.524176597595215
63 13.255173683166504
64 15.487800359725952
65 13.254170894622803
66 13.214075803756714
67 15.409745454788208
68 13.08515977859497
69 15.907875776290894
70 13.003533124923706
71 15.613473176956177
72 13.276692390441895
73 12.841368675231934
74 16.35838794708252
75 12.888124465942383
76 15.48833966255188
77 13.206713438034058
78 12.885308742523193
79 16.144579648971558
80 12.709399700164795
81 15.862847566604614
82 13.49186635017395
83 12.878829002380371
84 15.565804958343506
85 12.769300937652588
86 16.1887264251709
87 12.875439882278442
88 13.028194427490234
89 15.913824558258057
90 12.76502513885498
91 16.13094401359558
92 12.838326454162598
93 13.167710304260254
94 15.828555345535278
95 12.850484848022461
96 16.117185354232788
97 12.776123762130737
98 13.269726753234863
99 15.652525186538696
100 12.938563346862793
101 16.12844944000244
102 12.762020349502563
103 15.932390213012695
104 12.96332335472107
105 12.985161781311035
106 16.24772548675537
107 12.851197242736816
108 15.75327730178833
109 12.867435693740845
110 13.094117641448975
111 16.502267837524414
112 14.958721399307251
113 13.428960084915161
114 12.984899997711182
115 15.632610082626343
116 13.712947845458984
117 12.995981693267822
118 15.26119327545166
119 12.98122763633728
Done, saving (120, 320, 640, 3) (120, 320, 640)
extras:{'raw': tensor([[[-5.5193e-01, -3.5990e-01,  4.4748e-01, -3.7897e+01],
         [ 6.9350e-01,  8.1325e-01,  1.2655e+00, -5.4826e+01],
         [ 9.4567e-01,  1.0268e+00,  1.4978e+00, -8.5300e+00],
         ...,
         [ 3.1435e-01, -1.7894e+00, -7.6527e+00,  2.4087e+02],
         [-4.1113e-01, -2.7615e+00, -8.9600e+00,  2.0097e+02],
         [ 2.4655e-01, -1.9565e+00, -7.7760e+00,  2.3727e+02]],

        [[-5.8489e+00, -6.7668e+00, -8.7214e+00, -7.8114e+01],
         [-2.5070e+00, -4.0137e+00, -7.6961e+00, -7.9052e+01],
         [-4.7351e-01, -7.4759e-01, -1.4334e+00, -2.2721e+01],
         ...,
         [-6.3088e+00, -7.1164e+00, -8.7062e+00,  1.6862e+00],
         [-5.9340e+00, -6.8664e+00, -8.6269e+00, -4.8940e+00],
         [-5.9652e+00, -6.8074e+00, -8.4634e+00,  1.7687e+00]],

        [[-3.0044e+00, -3.1523e+00, -2.8540e+00, -4.9052e+01],
         [-3.1718e+00, -2.9355e+00, -2.3397e+00, -2.9477e+01],
         [-2.3802e+00, -2.1781e+00, -1.5313e+00, -1.7157e+01],
         ...,
         [-3.7201e+00, -4.0284e+00, -4.8020e+00, -2.3011e+01],
         [-4.6801e+00, -4.9802e+00, -5.8083e+00, -2.5544e+01],
         [-3.0960e+00, -3.2803e+00, -3.9305e+00, -6.5173e+00]],

        ...,

        [[ 4.1999e-01,  5.5335e-01,  1.1035e+00, -2.4111e+01],
         [ 6.2027e+00,  6.9236e+00,  8.9242e+00, -2.3715e+01],
         [ 6.7569e-01,  5.2530e-01,  6.2023e-01,  5.4996e+01],
         ...,
         [ 1.2304e+01,  4.1602e+00, -1.9505e+01,  3.0436e+02],
         [ 1.0666e+01,  2.8270e+00, -2.1181e+01,  2.5200e+02],
         [ 1.2112e+01,  3.3244e+00, -1.8592e+01,  2.2014e+02]],

        [[-9.2327e-01, -5.6616e-01,  2.3072e-01, -3.4821e+01],
         [ 5.9174e-01,  5.4170e-01,  6.8655e-01, -4.0764e+01],
         [ 9.7277e-01,  9.6638e-01,  8.9516e-01, -1.5929e+01],
         ...,
         [ 3.9625e-01, -7.7277e-01, -3.1316e+00,  1.4855e+02],
         [ 3.3609e-01, -7.7451e-01, -3.0820e+00,  1.4481e+02],
         [ 1.2879e+00, -1.1049e-02, -2.1997e+00,  1.2763e+02]],

        [[ 5.3595e-02, -1.8045e-01, -4.5218e-01, -4.4393e+01],
         [ 1.2258e+00,  8.8556e-01,  4.7371e-01, -5.0839e+01],
         [ 1.2180e+00,  8.7455e-01,  4.5850e-01, -5.0773e+01],
         ...,
         [-6.5747e-01, -1.5268e+00, -5.7335e+00,  3.9836e+02],
         [-1.0280e+00, -1.6005e+00, -5.3351e+00,  3.7039e+02],
         [-8.1825e-01, -1.6653e+00, -6.0551e+00,  4.2417e+02]]],
       grad_fn=<ReshapeAliasBackward0>), 'rgb0': tensor([[0.4756, 0.4343, 0.4003],
        [0.3583, 0.3629, 0.3577],
        [0.5953, 0.5674, 0.5274],
        ...,
        [0.6816, 0.6731, 0.7359],
        [0.6353, 0.6263, 0.6156],
        [0.4631, 0.3966, 0.3037]], grad_fn=<ReshapeAliasBackward0>), 'disp0': tensor([ 54.5759,  65.0076,  69.1140,  ...,  51.3979,  59.8933, 258.5452],
       grad_fn=<ReshapeAliasBackward0>), 'acc0': tensor([1., 1., 1.,  ..., 1., 1., 1.], grad_fn=<ReshapeAliasBackward0>), 'z_std': tensor([0.0016, 0.0468, 0.0025,  ..., 0.0013, 0.0026, 0.1379])}
0 0.00048041343688964844
torch.Size([320, 640, 3]) torch.Size([320, 640])
1 13.468681812286377
2 14.995748281478882
3 13.589359521865845
4 13.504467010498047
5 15.117748022079468
6 13.386791944503784
7 15.243321180343628
8 13.365187168121338
9 13.586119174957275
10 15.125415802001953
11 13.334784030914307
12 15.320541143417358
13 13.426888465881348
14 15.234687089920044
15 13.325016498565674
16 13.567793130874634
17 15.145570278167725
18 13.19432282447815
19 15.700943231582642
20 13.150913000106812
21 13.501468420028687
22 15.235253810882568
23 13.39994502067566
24 15.2600576877594
25 13.055731534957886
26 15.679288864135742
27 13.311193704605103
28 13.55799388885498
29 15.129513502120972
30 13.340043067932129
31 15.29111933708191
32 13.44265103340149
33 13.382760763168335
34 15.285629510879517
35 13.491082429885864
36 15.132914781570435
37 13.321364402770996
38 15.493625164031982
39 13.368731021881104
40 13.418798446655273
41 15.206396341323853
42 13.360197067260742
43 15.315752744674683
44 13.306929588317871
45 13.516213655471802
46 15.20592975616455
47 13.40725827217102
48 15.476086139678955
49 13.16478681564331
50 15.300651550292969
51 13.259336948394775
52 13.443536758422852
53 15.377755880355835
54 13.232731580734253
55 15.269637107849121
56 13.455784559249878
57 13.400187492370605
58 15.584710121154785
59 13.028313636779785
60 15.580178022384644
61 13.136779308319092
62 15.36736249923706
63 13.431044101715088
64 13.395038843154907
65 15.433692693710327
66 12.69428277015686
67 17.41169571876526
68 15.049311876296997
69 17.26214027404785
70 15.238502740859985
71 15.526304721832275
72 18.780842065811157
73 15.984413623809814
74 16.997609615325928
75 15.470419645309448
76 17.6942400932312
77 15.569878339767456
78 15.41929817199707
79 17.191081762313843
80 15.085083484649658
81 17.292685747146606
82 14.932590961456299
83 17.576751947402954
84 15.39565634727478
85 17.03856611251831
86 15.319789409637451
87 16.982253551483154
88 15.24331521987915
89 17.028937816619873
90 15.275086402893066
91 17.012855768203735
92 15.330590009689331
93 17.030831813812256
94 15.324198484420776
95 17.044302940368652
96 15.392055034637451
97 17.027992725372314
98 15.304880380630493
99 15.2837393283844
100 17.12363624572754
101 15.29415249824524
102 17.112958192825317
103 15.273572444915771
104 17.135754823684692
105 15.284029483795166
106 17.032296895980835
107 15.249908208847046
108 16.966859817504883
109 15.239113807678223
110 17.361236095428467
111 15.336201906204224
112 16.68739104270935
113 15.22656774520874
114 17.34157705307007
115 15.223413705825806
116 17.128366470336914
117 15.103726863861084
118 15.300213813781738
119 17.05119252204895
test poses shape torch.Size([4, 3, 4])
0 0.0006787776947021484
torch.Size([320, 640, 3]) torch.Size([320, 640])
1 17.14141345024109
2 15.324192523956299
3 17.26438593864441
Saved test set
[TRAIN] Iter: 700000 Loss: 0.0040603093802928925  PSNR: 29.189577102661133
[TRAIN] Iter: 700100 Loss: 0.0038214214146137238  PSNR: 28.320987701416016
[TRAIN] Iter: 700200 Loss: 0.003908282145857811  PSNR: 28.802396774291992
[TRAIN] Iter: 700300 Loss: 0.005189170595258474  PSNR: 27.76889419555664
[TRAIN] Iter: 700400 Loss: 0.004285415634512901  PSNR: 28.286224365234375
[TRAIN] Iter: 700500 Loss: 0.004789205268025398  PSNR: 28.285869598388672
[TRAIN] Iter: 700600 Loss: 0.0045000892132520676  PSNR: 27.89883041381836
[TRAIN] Iter: 700700 Loss: 0.003898666240274906  PSNR: 30.29694938659668
[TRAIN] Iter: 700800 Loss: 0.005721286404877901  PSNR: 26.94938087463379
[TRAIN] Iter: 700900 Loss: 0.004486922174692154  PSNR: 28.52519989013672
[TRAIN] Iter: 701000 Loss: 0.004065398126840591  PSNR: 29.52193832397461
[TRAIN] Iter: 701100 Loss: 0.0034797051921486855  PSNR: 29.676773071289062
[TRAIN] Iter: 701200 Loss: 0.0034899485763162374  PSNR: 30.78763198852539
[TRAIN] Iter: 701300 Loss: 0.004243327304720879  PSNR: 28.42716407775879
[TRAIN] Iter: 701400 Loss: 0.004152983892709017  PSNR: 28.47514533996582
[TRAIN] Iter: 701500 Loss: 0.003036770038306713  PSNR: 29.946292877197266
[TRAIN] Iter: 701600 Loss: 0.003123221918940544  PSNR: 30.387643814086914
[TRAIN] Iter: 701700 Loss: 0.0031381864100694656  PSNR: 30.137283325195312
[TRAIN] Iter: 701800 Loss: 0.003242738079279661  PSNR: 30.75120735168457
[TRAIN] Iter: 701900 Loss: 0.004546691197901964  PSNR: 28.13198471069336
[TRAIN] Iter: 702000 Loss: 0.003681736532598734  PSNR: 30.24791145324707
[TRAIN] Iter: 702100 Loss: 0.004196785390377045  PSNR: 29.232402801513672
[TRAIN] Iter: 702200 Loss: 0.004189024679362774  PSNR: 29.00326156616211
[TRAIN] Iter: 702300 Loss: 0.004619636572897434  PSNR: 28.063682556152344
[TRAIN] Iter: 702400 Loss: 0.004685770254582167  PSNR: 28.125411987304688
[TRAIN] Iter: 702500 Loss: 0.0033278344199061394  PSNR: 29.839866638183594
[TRAIN] Iter: 702600 Loss: 0.0053233918733894825  PSNR: 28.112018585205078
[TRAIN] Iter: 702700 Loss: 0.004334143362939358  PSNR: 27.845611572265625
[TRAIN] Iter: 702800 Loss: 0.005495211575180292  PSNR: 27.405452728271484
[TRAIN] Iter: 702900 Loss: 0.004809245932847261  PSNR: 27.88448715209961
[TRAIN] Iter: 703000 Loss: 0.00532297370955348  PSNR: 27.626066207885742
[TRAIN] Iter: 703100 Loss: 0.004316048696637154  PSNR: 28.233919143676758
[TRAIN] Iter: 703200 Loss: 0.003594841342419386  PSNR: 29.449934005737305
[TRAIN] Iter: 703300 Loss: 0.003708206582814455  PSNR: 30.024913787841797
[TRAIN] Iter: 703400 Loss: 0.0047293915413320065  PSNR: 27.89337730407715
[TRAIN] Iter: 703500 Loss: 0.005154296290129423  PSNR: 28.445877075195312
[TRAIN] Iter: 703600 Loss: 0.004110485315322876  PSNR: 28.470674514770508
[TRAIN] Iter: 703700 Loss: 0.0059762513265013695  PSNR: 27.182104110717773
[TRAIN] Iter: 703800 Loss: 0.005046847742050886  PSNR: 28.116727828979492
[TRAIN] Iter: 703900 Loss: 0.004753621760755777  PSNR: 27.75628089904785
[TRAIN] Iter: 704000 Loss: 0.004059188067913055  PSNR: 29.635637283325195
[TRAIN] Iter: 704100 Loss: 0.0055502308532595634  PSNR: 26.518877029418945
[TRAIN] Iter: 704200 Loss: 0.0047554378397762775  PSNR: 28.010900497436523
[TRAIN] Iter: 704300 Loss: 0.0032733390107750893  PSNR: 30.426063537597656
[TRAIN] Iter: 704400 Loss: 0.005117342807352543  PSNR: 27.513858795166016
[TRAIN] Iter: 704500 Loss: 0.0062308358028531075  PSNR: 26.53923225402832
[TRAIN] Iter: 704600 Loss: 0.005405914504081011  PSNR: 27.74336051940918
[TRAIN] Iter: 704700 Loss: 0.004334163852035999  PSNR: 29.02069664001465
[TRAIN] Iter: 704800 Loss: 0.003977622836828232  PSNR: 29.646018981933594
[TRAIN] Iter: 704900 Loss: 0.003946132957935333  PSNR: 29.6042537689209
[TRAIN] Iter: 705000 Loss: 0.004605457186698914  PSNR: 28.59581184387207
[TRAIN] Iter: 705100 Loss: 0.00406236108392477  PSNR: 28.86095428466797
[TRAIN] Iter: 705200 Loss: 0.004795413464307785  PSNR: 27.623872756958008
[TRAIN] Iter: 705300 Loss: 0.004126627463847399  PSNR: 29.14775848388672
[TRAIN] Iter: 705400 Loss: 0.005186011549085379  PSNR: 28.174108505249023
[TRAIN] Iter: 705500 Loss: 0.005267120897769928  PSNR: 27.510549545288086
[TRAIN] Iter: 705600 Loss: 0.004574303515255451  PSNR: 27.969499588012695
[TRAIN] Iter: 705700 Loss: 0.0053854892030358315  PSNR: 27.875024795532227
[TRAIN] Iter: 705800 Loss: 0.004156861454248428  PSNR: 28.800752639770508
[TRAIN] Iter: 705900 Loss: 0.005948556587100029  PSNR: 26.607778549194336
[TRAIN] Iter: 706000 Loss: 0.004611681215465069  PSNR: 28.038448333740234
[TRAIN] Iter: 706100 Loss: 0.0037084780633449554  PSNR: 30.277862548828125
[TRAIN] Iter: 706200 Loss: 0.005597534589469433  PSNR: 27.517242431640625
[TRAIN] Iter: 706300 Loss: 0.0055239275097846985  PSNR: 27.478635787963867
[TRAIN] Iter: 706400 Loss: 0.004826236516237259  PSNR: 28.405792236328125
[TRAIN] Iter: 706500 Loss: 0.0037032822147011757  PSNR: 29.743181228637695
[TRAIN] Iter: 706600 Loss: 0.0047306036576628685  PSNR: 28.751399993896484
[TRAIN] Iter: 706700 Loss: 0.004674579482525587  PSNR: 28.37816047668457
[TRAIN] Iter: 706800 Loss: 0.004673585295677185  PSNR: 28.32797622680664
[TRAIN] Iter: 706900 Loss: 0.005116564687341452  PSNR: 27.39137840270996
[TRAIN] Iter: 707000 Loss: 0.003833816386759281  PSNR: 29.38003921508789
[TRAIN] Iter: 707100 Loss: 0.0054031419567763805  PSNR: 27.917451858520508
[TRAIN] Iter: 707200 Loss: 0.004327458329498768  PSNR: 28.23150062561035
[TRAIN] Iter: 707300 Loss: 0.0048898025415837765  PSNR: 28.27025604248047
[TRAIN] Iter: 707400 Loss: 0.005115307867527008  PSNR: 27.9239501953125
[TRAIN] Iter: 707500 Loss: 0.005375235341489315  PSNR: 28.161298751831055
[TRAIN] Iter: 707600 Loss: 0.005710855592042208  PSNR: 26.96851921081543
[TRAIN] Iter: 707700 Loss: 0.004303076304495335  PSNR: 28.94811248779297
[TRAIN] Iter: 707800 Loss: 0.004459802061319351  PSNR: 27.937902450561523
[TRAIN] Iter: 707900 Loss: 0.0056860377080738544  PSNR: 27.677997589111328
[TRAIN] Iter: 708000 Loss: 0.0035270764492452145  PSNR: 30.441551208496094
[TRAIN] Iter: 708100 Loss: 0.004158033989369869  PSNR: 29.839881896972656
[TRAIN] Iter: 708200 Loss: 0.005599034018814564  PSNR: 27.316295623779297
[TRAIN] Iter: 708300 Loss: 0.004289943724870682  PSNR: 28.990406036376953
[TRAIN] Iter: 708400 Loss: 0.003113275393843651  PSNR: 30.205629348754883
[TRAIN] Iter: 708500 Loss: 0.005401996895670891  PSNR: 27.124792098999023
[TRAIN] Iter: 708600 Loss: 0.0043569388799369335  PSNR: 28.188648223876953
[TRAIN] Iter: 708700 Loss: 0.004297612234950066  PSNR: 28.46150016784668
[TRAIN] Iter: 708800 Loss: 0.005078646820038557  PSNR: 27.98226547241211
[TRAIN] Iter: 708900 Loss: 0.005121052265167236  PSNR: 27.43769073486328
[TRAIN] Iter: 709000 Loss: 0.003917743917554617  PSNR: 29.63360595703125
[TRAIN] Iter: 709100 Loss: 0.003944776952266693  PSNR: 28.61058235168457
[TRAIN] Iter: 709200 Loss: 0.004390317481011152  PSNR: 28.222238540649414
[TRAIN] Iter: 709300 Loss: 0.004281467292457819  PSNR: 28.746654510498047
[TRAIN] Iter: 709400 Loss: 0.003794676624238491  PSNR: 29.374624252319336
[TRAIN] Iter: 709500 Loss: 0.004420820623636246  PSNR: 28.91570472717285
[TRAIN] Iter: 709600 Loss: 0.003825350431725383  PSNR: 30.132741928100586
[TRAIN] Iter: 709700 Loss: 0.004413731396198273  PSNR: 27.73509979248047
[TRAIN] Iter: 709800 Loss: 0.004669027402997017  PSNR: 27.89742660522461
[TRAIN] Iter: 709900 Loss: 0.00332859018817544  PSNR: 30.247713088989258
Saved checkpoints at ./logs/TUT-KE101-nerf/710000.tar
[TRAIN] Iter: 710000 Loss: 0.005330550484359264  PSNR: 27.47872543334961
[TRAIN] Iter: 710100 Loss: 0.003346854355186224  PSNR: 30.43391990661621
[TRAIN] Iter: 710200 Loss: 0.004647791385650635  PSNR: 27.81756019592285
[TRAIN] Iter: 710300 Loss: 0.006347163114696741  PSNR: 26.46195411682129
[TRAIN] Iter: 710400 Loss: 0.004029800184071064  PSNR: 28.569408416748047
[TRAIN] Iter: 710500 Loss: 0.005260241683572531  PSNR: 27.235078811645508
[TRAIN] Iter: 710600 Loss: 0.0051372773014009  PSNR: 28.037872314453125
[TRAIN] Iter: 710700 Loss: 0.0038497508503496647  PSNR: 30.23766326904297
[TRAIN] Iter: 710800 Loss: 0.006064635701477528  PSNR: 26.51632308959961
[TRAIN] Iter: 710900 Loss: 0.0038797506131231785  PSNR: 29.596214294433594
[TRAIN] Iter: 711000 Loss: 0.004516908898949623  PSNR: 28.95701026916504
[TRAIN] Iter: 711100 Loss: 0.005059882067143917  PSNR: 27.69139289855957
[TRAIN] Iter: 711200 Loss: 0.004294134676456451  PSNR: 28.56667137145996
[TRAIN] Iter: 711300 Loss: 0.0047934469766914845  PSNR: 27.822778701782227
[TRAIN] Iter: 711400 Loss: 0.004316221922636032  PSNR: 28.311548233032227
[TRAIN] Iter: 711500 Loss: 0.003434128127992153  PSNR: 30.098669052124023
[TRAIN] Iter: 711600 Loss: 0.0037485056091099977  PSNR: 29.027408599853516
[TRAIN] Iter: 711700 Loss: 0.004476352594792843  PSNR: 28.340011596679688
[TRAIN] Iter: 711800 Loss: 0.0043272278271615505  PSNR: 27.78647804260254
[TRAIN] Iter: 711900 Loss: 0.0050757816061377525  PSNR: 27.07499122619629
[TRAIN] Iter: 712000 Loss: 0.004505134653300047  PSNR: 27.990787506103516
[TRAIN] Iter: 712100 Loss: 0.004183518700301647  PSNR: 28.729700088500977
[TRAIN] Iter: 712200 Loss: 0.003991176374256611  PSNR: 28.515531539916992
[TRAIN] Iter: 712300 Loss: 0.00464770570397377  PSNR: 28.546833038330078
[TRAIN] Iter: 712400 Loss: 0.003761694300919771  PSNR: 29.928953170776367
[TRAIN] Iter: 712500 Loss: 0.003659571520984173  PSNR: 29.73359489440918
[TRAIN] Iter: 712600 Loss: 0.00424045929685235  PSNR: 28.591716766357422
[TRAIN] Iter: 712700 Loss: 0.005207833833992481  PSNR: 27.977235794067383
[TRAIN] Iter: 712800 Loss: 0.005853456445038319  PSNR: 27.038185119628906
[TRAIN] Iter: 712900 Loss: 0.0061078500002622604  PSNR: 26.539907455444336
[TRAIN] Iter: 713000 Loss: 0.005024605896323919  PSNR: 27.73605728149414
[TRAIN] Iter: 713100 Loss: 0.00455200532451272  PSNR: 28.21535301208496
[TRAIN] Iter: 713200 Loss: 0.003589717671275139  PSNR: 30.178138732910156
[TRAIN] Iter: 713300 Loss: 0.003002062439918518  PSNR: 30.687904357910156
[TRAIN] Iter: 713400 Loss: 0.004522467963397503  PSNR: 28.607276916503906
[TRAIN] Iter: 713500 Loss: 0.005239398684352636  PSNR: 26.801773071289062
[TRAIN] Iter: 713600 Loss: 0.004633237607777119  PSNR: 28.042030334472656
[TRAIN] Iter: 713700 Loss: 0.004385537933558226  PSNR: 28.7164363861084
[TRAIN] Iter: 713800 Loss: 0.005083537194877863  PSNR: 27.716039657592773
[TRAIN] Iter: 713900 Loss: 0.0037883694749325514  PSNR: 29.067760467529297
[TRAIN] Iter: 714000 Loss: 0.003962109796702862  PSNR: 29.145309448242188
[TRAIN] Iter: 714100 Loss: 0.003143525682389736  PSNR: 30.424808502197266
[TRAIN] Iter: 714200 Loss: 0.005550427362322807  PSNR: 27.011682510375977
[TRAIN] Iter: 714300 Loss: 0.004102315288037062  PSNR: 30.55205535888672
[TRAIN] Iter: 714400 Loss: 0.004803100135177374  PSNR: 27.50258445739746
[TRAIN] Iter: 714500 Loss: 0.004842862952500582  PSNR: 28.770660400390625
[TRAIN] Iter: 714600 Loss: 0.005057027563452721  PSNR: 28.565935134887695
[TRAIN] Iter: 714700 Loss: 0.004901980049908161  PSNR: 29.07216453552246
[TRAIN] Iter: 714800 Loss: 0.0047335755079984665  PSNR: 28.304584503173828
[TRAIN] Iter: 714900 Loss: 0.0033620712347328663  PSNR: 30.488359451293945
[TRAIN] Iter: 715000 Loss: 0.00521225668489933  PSNR: 27.826555252075195
[TRAIN] Iter: 715100 Loss: 0.0035401307977735996  PSNR: 30.1949462890625
[TRAIN] Iter: 715200 Loss: 0.004577565938234329  PSNR: 28.017274856567383
[TRAIN] Iter: 715300 Loss: 0.004317639395594597  PSNR: 28.77875518798828
[TRAIN] Iter: 715400 Loss: 0.004872591234743595  PSNR: 28.016155242919922
[TRAIN] Iter: 715500 Loss: 0.0033929806668311357  PSNR: 30.00486946105957
[TRAIN] Iter: 715600 Loss: 0.0040851798839867115  PSNR: 29.29080581665039
[TRAIN] Iter: 715700 Loss: 0.0049584535881876945  PSNR: 28.74368667602539
[TRAIN] Iter: 715800 Loss: 0.003838827135041356  PSNR: 30.310226440429688
[TRAIN] Iter: 715900 Loss: 0.00506170280277729  PSNR: 28.5671443939209
[TRAIN] Iter: 716000 Loss: 0.0034946873784065247  PSNR: 29.60824966430664
[TRAIN] Iter: 716100 Loss: 0.0038031120784580708  PSNR: 29.728164672851562
[TRAIN] Iter: 716200 Loss: 0.005283074453473091  PSNR: 26.96306610107422
[TRAIN] Iter: 716300 Loss: 0.0035283081233501434  PSNR: 30.205957412719727
[TRAIN] Iter: 716400 Loss: 0.004343617241829634  PSNR: 28.4268856048584
[TRAIN] Iter: 716500 Loss: 0.004383486229926348  PSNR: 28.013534545898438
[TRAIN] Iter: 716600 Loss: 0.00499675702303648  PSNR: 28.300233840942383
[TRAIN] Iter: 716700 Loss: 0.0051858508959412575  PSNR: 26.8351993560791
[TRAIN] Iter: 716800 Loss: 0.004855277948081493  PSNR: 27.926740646362305
[TRAIN] Iter: 716900 Loss: 0.0038199550472199917  PSNR: 28.753849029541016
[TRAIN] Iter: 717000 Loss: 0.004150794818997383  PSNR: 28.49349021911621
[TRAIN] Iter: 717100 Loss: 0.004240982700139284  PSNR: 28.65137481689453
[TRAIN] Iter: 717200 Loss: 0.004310121294111013  PSNR: 29.187986373901367
[TRAIN] Iter: 717300 Loss: 0.004715106450021267  PSNR: 27.73561668395996
[TRAIN] Iter: 717400 Loss: 0.0036627412773668766  PSNR: 29.968887329101562
[TRAIN] Iter: 717500 Loss: 0.005202800035476685  PSNR: 27.259387969970703
[TRAIN] Iter: 717600 Loss: 0.0038815652951598167  PSNR: 28.9752197265625
[TRAIN] Iter: 717700 Loss: 0.004869626834988594  PSNR: 27.29787254333496
[TRAIN] Iter: 717800 Loss: 0.003273448906838894  PSNR: 31.165950775146484
[TRAIN] Iter: 717900 Loss: 0.003793856827542186  PSNR: 29.98015594482422
[TRAIN] Iter: 718000 Loss: 0.0033327811397612095  PSNR: 30.001842498779297
[TRAIN] Iter: 718100 Loss: 0.003935737069696188  PSNR: 29.55586051940918
[TRAIN] Iter: 718200 Loss: 0.005635260138660669  PSNR: 27.876544952392578
[TRAIN] Iter: 718300 Loss: 0.004458052106201649  PSNR: 28.37108039855957
[TRAIN] Iter: 718400 Loss: 0.003911601845175028  PSNR: 30.0015869140625
[TRAIN] Iter: 718500 Loss: 0.0037718545645475388  PSNR: 29.76949691772461
[TRAIN] Iter: 718600 Loss: 0.003480304963886738  PSNR: 30.66968536376953
[TRAIN] Iter: 718700 Loss: 0.0037258334923535585  PSNR: 29.701934814453125
[TRAIN] Iter: 718800 Loss: 0.004996982868760824  PSNR: 28.06655502319336
[TRAIN] Iter: 718900 Loss: 0.0051975250244140625  PSNR: 27.801828384399414
[TRAIN] Iter: 719000 Loss: 0.0033296141773462296  PSNR: 30.81636619567871
[TRAIN] Iter: 719100 Loss: 0.003972746431827545  PSNR: 28.69059181213379
[TRAIN] Iter: 719200 Loss: 0.0037520374171435833  PSNR: 30.020771026611328
[TRAIN] Iter: 719300 Loss: 0.004429525695741177  PSNR: 28.099342346191406
[TRAIN] Iter: 719400 Loss: 0.003974409308284521  PSNR: 29.871788024902344
[TRAIN] Iter: 719500 Loss: 0.0038502090610563755  PSNR: 29.922426223754883
[TRAIN] Iter: 719600 Loss: 0.005110219120979309  PSNR: 27.871707916259766
[TRAIN] Iter: 719700 Loss: 0.005060387775301933  PSNR: 27.786649703979492
[TRAIN] Iter: 719800 Loss: 0.004937019199132919  PSNR: 28.386051177978516
[TRAIN] Iter: 719900 Loss: 0.004461673088371754  PSNR: 27.956430435180664
Saved checkpoints at ./logs/TUT-KE101-nerf/720000.tar
[TRAIN] Iter: 720000 Loss: 0.004156238399446011  PSNR: 28.389005661010742
[TRAIN] Iter: 720100 Loss: 0.004741334356367588  PSNR: 27.5447998046875
[TRAIN] Iter: 720200 Loss: 0.004545410629361868  PSNR: 28.180437088012695
[TRAIN] Iter: 720300 Loss: 0.0031619747169315815  PSNR: 30.768712997436523
[TRAIN] Iter: 720400 Loss: 0.004599481355398893  PSNR: 27.882022857666016
[TRAIN] Iter: 720500 Loss: 0.004911300726234913  PSNR: 28.41780662536621
[TRAIN] Iter: 720600 Loss: 0.004721830599009991  PSNR: 28.7354679107666
[TRAIN] Iter: 720700 Loss: 0.0050172461196780205  PSNR: 28.48890495300293
[TRAIN] Iter: 720800 Loss: 0.004721445497125387  PSNR: 28.48337173461914
[TRAIN] Iter: 720900 Loss: 0.004986428655683994  PSNR: 27.53904151916504
[TRAIN] Iter: 721000 Loss: 0.004246018826961517  PSNR: 28.21512222290039
[TRAIN] Iter: 721100 Loss: 0.0039094737730920315  PSNR: 29.386552810668945
[TRAIN] Iter: 721200 Loss: 0.0051652053371071815  PSNR: 27.568538665771484
[TRAIN] Iter: 721300 Loss: 0.0040182312950491905  PSNR: 30.1610050201416
[TRAIN] Iter: 721400 Loss: 0.004545710980892181  PSNR: 28.22515869140625
[TRAIN] Iter: 721500 Loss: 0.003380388952791691  PSNR: 29.92974281311035
[TRAIN] Iter: 721600 Loss: 0.004430708009749651  PSNR: 27.79456329345703
[TRAIN] Iter: 721700 Loss: 0.006321985740214586  PSNR: 26.206491470336914
[TRAIN] Iter: 721800 Loss: 0.00561636034399271  PSNR: 27.352174758911133
[TRAIN] Iter: 721900 Loss: 0.003879158990457654  PSNR: 29.594100952148438
[TRAIN] Iter: 722000 Loss: 0.004818982910364866  PSNR: 28.849939346313477
[TRAIN] Iter: 722100 Loss: 0.0034891837276518345  PSNR: 29.506776809692383
[TRAIN] Iter: 722200 Loss: 0.004156826995313168  PSNR: 28.913875579833984
[TRAIN] Iter: 722300 Loss: 0.004011253826320171  PSNR: 28.804466247558594
[TRAIN] Iter: 722400 Loss: 0.00414593331515789  PSNR: 28.732698440551758
[TRAIN] Iter: 722500 Loss: 0.004410257562994957  PSNR: 28.24659538269043
[TRAIN] Iter: 722600 Loss: 0.004594916477799416  PSNR: 29.431230545043945
[TRAIN] Iter: 722700 Loss: 0.004598760511726141  PSNR: 28.0986328125
[TRAIN] Iter: 722800 Loss: 0.003500499064102769  PSNR: 29.984283447265625
[TRAIN] Iter: 722900 Loss: 0.004350282251834869  PSNR: 27.969499588012695
[TRAIN] Iter: 723000 Loss: 0.004065534099936485  PSNR: 28.559215545654297
[TRAIN] Iter: 723100 Loss: 0.00601805979385972  PSNR: 27.227088928222656
[TRAIN] Iter: 723200 Loss: 0.004461629316210747  PSNR: 28.270336151123047
[TRAIN] Iter: 723300 Loss: 0.004231700673699379  PSNR: 27.924875259399414
[TRAIN] Iter: 723400 Loss: 0.00383833353407681  PSNR: 30.69138526916504
[TRAIN] Iter: 723500 Loss: 0.004715772345662117  PSNR: 28.14470863342285
[TRAIN] Iter: 723600 Loss: 0.0041924589313566685  PSNR: 28.26918601989746
[TRAIN] Iter: 723700 Loss: 0.004672403912991285  PSNR: 28.35234260559082
[TRAIN] Iter: 723800 Loss: 0.003605125704780221  PSNR: 30.195775985717773
[TRAIN] Iter: 723900 Loss: 0.005604663863778114  PSNR: 26.99995994567871
[TRAIN] Iter: 724000 Loss: 0.005327245220541954  PSNR: 27.41971778869629
[TRAIN] Iter: 724100 Loss: 0.004312057979404926  PSNR: 29.782567977905273
[TRAIN] Iter: 724200 Loss: 0.005076257511973381  PSNR: 28.065547943115234
[TRAIN] Iter: 724300 Loss: 0.004309948533773422  PSNR: 29.15512466430664
[TRAIN] Iter: 724400 Loss: 0.004419407807290554  PSNR: 29.869239807128906
[TRAIN] Iter: 724500 Loss: 0.003937781322747469  PSNR: 28.59785270690918
[TRAIN] Iter: 724600 Loss: 0.0049695950001478195  PSNR: 28.11405372619629
[TRAIN] Iter: 724700 Loss: 0.0036648446694016457  PSNR: 29.778892517089844
[TRAIN] Iter: 724800 Loss: 0.0036993520334362984  PSNR: 29.949033737182617
[TRAIN] Iter: 724900 Loss: 0.005041160620748997  PSNR: 27.944149017333984
[TRAIN] Iter: 725000 Loss: 0.005778873339295387  PSNR: 27.12221336364746
[TRAIN] Iter: 725100 Loss: 0.00586217176169157  PSNR: 27.338796615600586
[TRAIN] Iter: 725200 Loss: 0.0043299030512571335  PSNR: 28.410093307495117
[TRAIN] Iter: 725300 Loss: 0.004655436147004366  PSNR: 28.532039642333984
[TRAIN] Iter: 725400 Loss: 0.004862389527261257  PSNR: 27.59477996826172
[TRAIN] Iter: 725500 Loss: 0.003860341850668192  PSNR: 29.396997451782227
[TRAIN] Iter: 725600 Loss: 0.005580980330705643  PSNR: 27.642608642578125
[TRAIN] Iter: 725700 Loss: 0.0051147653721272945  PSNR: 27.477602005004883
[TRAIN] Iter: 725800 Loss: 0.0036916392855346203  PSNR: 29.84661293029785
[TRAIN] Iter: 725900 Loss: 0.0038763172924518585  PSNR: 29.8602237701416
[TRAIN] Iter: 726000 Loss: 0.004673168528825045  PSNR: 27.528182983398438
[TRAIN] Iter: 726100 Loss: 0.0034873702097684145  PSNR: 30.097301483154297
[TRAIN] Iter: 726200 Loss: 0.0042612794786691666  PSNR: 30.329818725585938
[TRAIN] Iter: 726300 Loss: 0.004636828321963549  PSNR: 28.713895797729492
[TRAIN] Iter: 726400 Loss: 0.004046193324029446  PSNR: 29.493091583251953
[TRAIN] Iter: 726500 Loss: 0.003949490375816822  PSNR: 29.08407211303711
[TRAIN] Iter: 726600 Loss: 0.004640580620616674  PSNR: 27.954069137573242
[TRAIN] Iter: 726700 Loss: 0.00520668737590313  PSNR: 27.04791259765625
[TRAIN] Iter: 726800 Loss: 0.0035977705847471952  PSNR: 30.035486221313477
[TRAIN] Iter: 726900 Loss: 0.005444590002298355  PSNR: 26.609161376953125
[TRAIN] Iter: 727000 Loss: 0.004765723366290331  PSNR: 28.25453758239746
[TRAIN] Iter: 727100 Loss: 0.005293341353535652  PSNR: 27.055349349975586
[TRAIN] Iter: 727200 Loss: 0.00408190069720149  PSNR: 29.398746490478516
[TRAIN] Iter: 727300 Loss: 0.004699208773672581  PSNR: 28.265453338623047
[TRAIN] Iter: 727400 Loss: 0.004824391566216946  PSNR: 27.929716110229492
[TRAIN] Iter: 727500 Loss: 0.004386227577924728  PSNR: 28.16279411315918
[TRAIN] Iter: 727600 Loss: 0.0037222402170300484  PSNR: 30.124935150146484
[TRAIN] Iter: 727700 Loss: 0.004195822402834892  PSNR: 28.85452651977539
[TRAIN] Iter: 727800 Loss: 0.004184071905910969  PSNR: 29.78333282470703
[TRAIN] Iter: 727900 Loss: 0.004171659238636494  PSNR: 29.302133560180664
[TRAIN] Iter: 728000 Loss: 0.003270962042734027  PSNR: 31.2363224029541
[TRAIN] Iter: 728100 Loss: 0.004513284657150507  PSNR: 27.68597412109375
[TRAIN] Iter: 728200 Loss: 0.0037878374569118023  PSNR: 29.92034912109375
[TRAIN] Iter: 728300 Loss: 0.003486413974314928  PSNR: 30.199121475219727
[TRAIN] Iter: 728400 Loss: 0.0037668426521122456  PSNR: 29.922359466552734
[TRAIN] Iter: 728500 Loss: 0.00437763100489974  PSNR: 28.662364959716797
[TRAIN] Iter: 728600 Loss: 0.0039894492365419865  PSNR: 29.666601181030273
[TRAIN] Iter: 728700 Loss: 0.0050672683864831924  PSNR: 27.9124755859375
[TRAIN] Iter: 728800 Loss: 0.004237394314259291  PSNR: 27.9105167388916
[TRAIN] Iter: 728900 Loss: 0.003638834459707141  PSNR: 29.84739112854004
[TRAIN] Iter: 729000 Loss: 0.0045129829086363316  PSNR: 28.030256271362305
[TRAIN] Iter: 729100 Loss: 0.005642568226903677  PSNR: 26.737403869628906
[TRAIN] Iter: 729200 Loss: 0.005346345715224743  PSNR: 27.576635360717773
[TRAIN] Iter: 729300 Loss: 0.004348260350525379  PSNR: 27.82516098022461
[TRAIN] Iter: 729400 Loss: 0.0043680137023329735  PSNR: 28.46932601928711
[TRAIN] Iter: 729500 Loss: 0.003723355708643794  PSNR: 29.38421630859375
[TRAIN] Iter: 729600 Loss: 0.005660464987158775  PSNR: 27.155550003051758
[TRAIN] Iter: 729700 Loss: 0.0038762055337429047  PSNR: 29.995820999145508
[TRAIN] Iter: 729800 Loss: 0.005493162199854851  PSNR: 27.74163818359375
[TRAIN] Iter: 729900 Loss: 0.005680050700902939  PSNR: 27.2701416015625
Saved checkpoints at ./logs/TUT-KE101-nerf/730000.tar
[TRAIN] Iter: 730000 Loss: 0.005351214203983545  PSNR: 28.05779457092285
[TRAIN] Iter: 730100 Loss: 0.003953453619033098  PSNR: 30.309051513671875
[TRAIN] Iter: 730200 Loss: 0.005742816254496574  PSNR: 27.542003631591797
[TRAIN] Iter: 730300 Loss: 0.00360696529969573  PSNR: 29.672056198120117
[TRAIN] Iter: 730400 Loss: 0.006285678595304489  PSNR: 27.32231330871582
[TRAIN] Iter: 730500 Loss: 0.0037785498425364494  PSNR: 29.846698760986328
[TRAIN] Iter: 730600 Loss: 0.0034096064046025276  PSNR: 30.630268096923828
[TRAIN] Iter: 730700 Loss: 0.005692960694432259  PSNR: 28.170095443725586
[TRAIN] Iter: 730800 Loss: 0.005545490887016058  PSNR: 27.38565444946289
[TRAIN] Iter: 730900 Loss: 0.00502269621938467  PSNR: 27.195371627807617
[TRAIN] Iter: 731000 Loss: 0.005179325584322214  PSNR: 27.487295150756836
[TRAIN] Iter: 731100 Loss: 0.004788815975189209  PSNR: 28.02923583984375
[TRAIN] Iter: 731200 Loss: 0.004125874489545822  PSNR: 28.477304458618164
[TRAIN] Iter: 731300 Loss: 0.0050549693405628204  PSNR: 28.355772018432617
[TRAIN] Iter: 731400 Loss: 0.004371185787022114  PSNR: 29.8616943359375
[TRAIN] Iter: 731500 Loss: 0.003933421801775694  PSNR: 29.561227798461914
[TRAIN] Iter: 731600 Loss: 0.004597594030201435  PSNR: 28.256874084472656
[TRAIN] Iter: 731700 Loss: 0.005056886468082666  PSNR: 28.51646614074707
[TRAIN] Iter: 731800 Loss: 0.0036734133027493954  PSNR: 29.502431869506836
[TRAIN] Iter: 731900 Loss: 0.003541226964443922  PSNR: 30.145387649536133
[TRAIN] Iter: 732000 Loss: 0.004199361894279718  PSNR: 27.943157196044922
[TRAIN] Iter: 732100 Loss: 0.006036873906850815  PSNR: 26.906064987182617
[TRAIN] Iter: 732200 Loss: 0.0052650910802185535  PSNR: 27.89045524597168
[TRAIN] Iter: 732300 Loss: 0.004500871524214745  PSNR: 28.13874053955078
[TRAIN] Iter: 732400 Loss: 0.004878334701061249  PSNR: 27.778833389282227
[TRAIN] Iter: 732500 Loss: 0.00536278635263443  PSNR: 27.48189926147461
[TRAIN] Iter: 732600 Loss: 0.004181362688541412  PSNR: 28.49785041809082
[TRAIN] Iter: 732700 Loss: 0.0035647358745336533  PSNR: 30.588611602783203
[TRAIN] Iter: 732800 Loss: 0.004242281429469585  PSNR: 28.15662384033203
[TRAIN] Iter: 732900 Loss: 0.0037003159523010254  PSNR: 30.032106399536133
[TRAIN] Iter: 733000 Loss: 0.005362688563764095  PSNR: 27.378786087036133
[TRAIN] Iter: 733100 Loss: 0.005323972087353468  PSNR: 27.664566040039062
[TRAIN] Iter: 733200 Loss: 0.003877759212628007  PSNR: 30.147581100463867
[TRAIN] Iter: 733300 Loss: 0.00545510184019804  PSNR: 27.41534996032715
[TRAIN] Iter: 733400 Loss: 0.0046106912195682526  PSNR: 27.63941192626953
[TRAIN] Iter: 733500 Loss: 0.004347430542111397  PSNR: 29.141450881958008
[TRAIN] Iter: 733600 Loss: 0.0034784208983182907  PSNR: 29.870830535888672
[TRAIN] Iter: 733700 Loss: 0.005534125491976738  PSNR: 27.30470085144043
[TRAIN] Iter: 733800 Loss: 0.004473306704312563  PSNR: 28.663488388061523
[TRAIN] Iter: 733900 Loss: 0.003938872367143631  PSNR: 29.564327239990234
[TRAIN] Iter: 734000 Loss: 0.004487457685172558  PSNR: 28.310108184814453
[TRAIN] Iter: 734100 Loss: 0.004210291896015406  PSNR: 28.922401428222656
[TRAIN] Iter: 734200 Loss: 0.004133745562285185  PSNR: 29.064359664916992
[TRAIN] Iter: 734300 Loss: 0.004162692464888096  PSNR: 28.738414764404297
[TRAIN] Iter: 734400 Loss: 0.0040161581709980965  PSNR: 28.51658821105957
[TRAIN] Iter: 734500 Loss: 0.0041706557385623455  PSNR: 28.2802791595459
[TRAIN] Iter: 734600 Loss: 0.004899352788925171  PSNR: 27.838659286499023
[TRAIN] Iter: 734700 Loss: 0.0034752851352095604  PSNR: 30.177536010742188
[TRAIN] Iter: 734800 Loss: 0.004647593479603529  PSNR: 28.145225524902344
[TRAIN] Iter: 734900 Loss: 0.005218822974711657  PSNR: 27.276792526245117
[TRAIN] Iter: 735000 Loss: 0.003882333170622587  PSNR: 30.074607849121094
[TRAIN] Iter: 735100 Loss: 0.004185366444289684  PSNR: 28.002086639404297
[TRAIN] Iter: 735200 Loss: 0.004849453456699848  PSNR: 28.390254974365234
[TRAIN] Iter: 735300 Loss: 0.003765020752325654  PSNR: 29.51923370361328
[TRAIN] Iter: 735400 Loss: 0.005098970606923103  PSNR: 27.8323974609375
[TRAIN] Iter: 735500 Loss: 0.0063083297573029995  PSNR: 26.17552375793457
[TRAIN] Iter: 735600 Loss: 0.0033331639133393764  PSNR: 30.059860229492188
[TRAIN] Iter: 735700 Loss: 0.0034030303359031677  PSNR: 29.49466896057129
[TRAIN] Iter: 735800 Loss: 0.0033250516280531883  PSNR: 30.408403396606445
[TRAIN] Iter: 735900 Loss: 0.003804328152909875  PSNR: 30.28014373779297
[TRAIN] Iter: 736000 Loss: 0.005037772469222546  PSNR: 27.859060287475586
[TRAIN] Iter: 736100 Loss: 0.005066434852778912  PSNR: 28.09456443786621
[TRAIN] Iter: 736200 Loss: 0.004242237191647291  PSNR: 28.244173049926758
[TRAIN] Iter: 736300 Loss: 0.005055663641542196  PSNR: 27.251955032348633
[TRAIN] Iter: 736400 Loss: 0.0038662964943796396  PSNR: 28.841182708740234
[TRAIN] Iter: 736500 Loss: 0.005916797090321779  PSNR: 26.83872413635254
[TRAIN] Iter: 736600 Loss: 0.003546563209965825  PSNR: 30.202686309814453
[TRAIN] Iter: 736700 Loss: 0.00392511859536171  PSNR: 30.196046829223633
[TRAIN] Iter: 736800 Loss: 0.005520986393094063  PSNR: 27.411945343017578
[TRAIN] Iter: 736900 Loss: 0.004461788572371006  PSNR: 28.20113182067871
[TRAIN] Iter: 737000 Loss: 0.0038208849728107452  PSNR: 29.52898597717285
[TRAIN] Iter: 737100 Loss: 0.005011093802750111  PSNR: 28.036598205566406
[TRAIN] Iter: 737200 Loss: 0.003864194732159376  PSNR: 29.041292190551758
[TRAIN] Iter: 737300 Loss: 0.004708709195256233  PSNR: 28.493288040161133
[TRAIN] Iter: 737400 Loss: 0.004972734488546848  PSNR: 27.63596534729004
[TRAIN] Iter: 737500 Loss: 0.004047679714858532  PSNR: 29.380489349365234
[TRAIN] Iter: 737600 Loss: 0.004929660353809595  PSNR: 27.518512725830078
[TRAIN] Iter: 737700 Loss: 0.004216792061924934  PSNR: 28.502655029296875
[TRAIN] Iter: 737800 Loss: 0.004071702249348164  PSNR: 30.08013343811035
[TRAIN] Iter: 737900 Loss: 0.005618388298898935  PSNR: 26.707088470458984
[TRAIN] Iter: 738000 Loss: 0.004825879819691181  PSNR: 28.314010620117188
[TRAIN] Iter: 738100 Loss: 0.003237390425056219  PSNR: 30.434831619262695
[TRAIN] Iter: 738200 Loss: 0.004556077532470226  PSNR: 28.321962356567383
[TRAIN] Iter: 738300 Loss: 0.005366597790271044  PSNR: 27.412500381469727
[TRAIN] Iter: 738400 Loss: 0.004280134104192257  PSNR: 28.31061363220215
[TRAIN] Iter: 738500 Loss: 0.004176987335085869  PSNR: 28.818435668945312
[TRAIN] Iter: 738600 Loss: 0.00383434584364295  PSNR: 28.659168243408203
[TRAIN] Iter: 738700 Loss: 0.004109985660761595  PSNR: 29.295251846313477
[TRAIN] Iter: 738800 Loss: 0.004171780310571194  PSNR: 29.161447525024414
[TRAIN] Iter: 738900 Loss: 0.00394326401874423  PSNR: 28.62015151977539
[TRAIN] Iter: 739000 Loss: 0.0037266938015818596  PSNR: 29.152301788330078
[TRAIN] Iter: 739100 Loss: 0.004577868618071079  PSNR: 28.191160202026367
[TRAIN] Iter: 739200 Loss: 0.005104621406644583  PSNR: 27.049602508544922
[TRAIN] Iter: 739300 Loss: 0.005241846665740013  PSNR: 27.205448150634766
[TRAIN] Iter: 739400 Loss: 0.004923241212964058  PSNR: 28.2479248046875
[TRAIN] Iter: 739500 Loss: 0.003328355262055993  PSNR: 30.29159164428711
[TRAIN] Iter: 739600 Loss: 0.0035840002819895744  PSNR: 29.68998146057129
[TRAIN] Iter: 739700 Loss: 0.005072776228189468  PSNR: 27.748376846313477
[TRAIN] Iter: 739800 Loss: 0.004041445907205343  PSNR: 30.346973419189453
[TRAIN] Iter: 739900 Loss: 0.004937954246997833  PSNR: 27.612590789794922
Saved checkpoints at ./logs/TUT-KE101-nerf/740000.tar
[TRAIN] Iter: 740000 Loss: 0.004013529978692532  PSNR: 30.007844924926758
[TRAIN] Iter: 740100 Loss: 0.004136381670832634  PSNR: 28.62152099609375
[TRAIN] Iter: 740200 Loss: 0.0035861139185726643  PSNR: 28.785573959350586
[TRAIN] Iter: 740300 Loss: 0.0034423652105033398  PSNR: 29.607606887817383
[TRAIN] Iter: 740400 Loss: 0.0044369944371283054  PSNR: 28.62299156188965
[TRAIN] Iter: 740500 Loss: 0.004000920802354813  PSNR: 28.68172264099121
[TRAIN] Iter: 740600 Loss: 0.005229620262980461  PSNR: 28.360225677490234
[TRAIN] Iter: 740700 Loss: 0.005498496349900961  PSNR: 28.043119430541992
[TRAIN] Iter: 740800 Loss: 0.004462003242224455  PSNR: 29.313127517700195
[TRAIN] Iter: 740900 Loss: 0.0032140309922397137  PSNR: 30.351991653442383
[TRAIN] Iter: 741000 Loss: 0.0052413721568882465  PSNR: 27.604860305786133
[TRAIN] Iter: 741100 Loss: 0.0033455616794526577  PSNR: 30.228776931762695
[TRAIN] Iter: 741200 Loss: 0.006017118692398071  PSNR: 27.31574821472168
[TRAIN] Iter: 741300 Loss: 0.004629394039511681  PSNR: 28.094402313232422
[TRAIN] Iter: 741400 Loss: 0.004217733629047871  PSNR: 28.956148147583008
[TRAIN] Iter: 741500 Loss: 0.005190170370042324  PSNR: 27.053464889526367
[TRAIN] Iter: 741600 Loss: 0.004302387125790119  PSNR: 29.271650314331055
[TRAIN] Iter: 741700 Loss: 0.003877907758578658  PSNR: 29.496997833251953
[TRAIN] Iter: 741800 Loss: 0.0037317578680813313  PSNR: 30.062328338623047
[TRAIN] Iter: 741900 Loss: 0.0033794408664107323  PSNR: 30.331890106201172
[TRAIN] Iter: 742000 Loss: 0.004284684546291828  PSNR: 28.422948837280273
[TRAIN] Iter: 742100 Loss: 0.004176171496510506  PSNR: 28.594606399536133
[TRAIN] Iter: 742200 Loss: 0.005777029320597649  PSNR: 27.203475952148438
[TRAIN] Iter: 742300 Loss: 0.0038916277699172497  PSNR: 29.557514190673828
[TRAIN] Iter: 742400 Loss: 0.004272027872502804  PSNR: 30.036142349243164
[TRAIN] Iter: 742500 Loss: 0.00657134922221303  PSNR: 26.444442749023438
[TRAIN] Iter: 742600 Loss: 0.004065783694386482  PSNR: 29.76218605041504
[TRAIN] Iter: 742700 Loss: 0.0048162853345274925  PSNR: 27.89793586730957
[TRAIN] Iter: 742800 Loss: 0.004989225417375565  PSNR: 27.56875228881836
[TRAIN] Iter: 742900 Loss: 0.005946039687842131  PSNR: 27.16181755065918
[TRAIN] Iter: 743000 Loss: 0.004401316866278648  PSNR: 28.200353622436523
[TRAIN] Iter: 743100 Loss: 0.0044458662159740925  PSNR: 29.54787254333496
[TRAIN] Iter: 743200 Loss: 0.0047894627787172794  PSNR: 27.75920867919922
[TRAIN] Iter: 743300 Loss: 0.004175079986453056  PSNR: 28.7603816986084
[TRAIN] Iter: 743400 Loss: 0.004839940927922726  PSNR: 28.208106994628906
[TRAIN] Iter: 743500 Loss: 0.004582691006362438  PSNR: 28.316024780273438
[TRAIN] Iter: 743600 Loss: 0.00540075683966279  PSNR: 27.918291091918945
[TRAIN] Iter: 743700 Loss: 0.004068750888109207  PSNR: 30.288898468017578
[TRAIN] Iter: 743800 Loss: 0.003084347117692232  PSNR: 30.260299682617188
[TRAIN] Iter: 743900 Loss: 0.006101435050368309  PSNR: 27.418371200561523
[TRAIN] Iter: 744000 Loss: 0.004242578521370888  PSNR: 28.336708068847656
[TRAIN] Iter: 744100 Loss: 0.0043462421745061874  PSNR: 28.441911697387695
[TRAIN] Iter: 744200 Loss: 0.004344926215708256  PSNR: 28.109987258911133
[TRAIN] Iter: 744300 Loss: 0.0051498208194971085  PSNR: 28.523746490478516
[TRAIN] Iter: 744400 Loss: 0.004639485385268927  PSNR: 28.907756805419922
[TRAIN] Iter: 744500 Loss: 0.003445597132667899  PSNR: 30.32160186767578
[TRAIN] Iter: 744600 Loss: 0.004770861007273197  PSNR: 28.139211654663086
[TRAIN] Iter: 744700 Loss: 0.002893457654863596  PSNR: 30.558616638183594
[TRAIN] Iter: 744800 Loss: 0.004211908206343651  PSNR: 28.931543350219727
[TRAIN] Iter: 744900 Loss: 0.0033975220285356045  PSNR: 30.507295608520508
[TRAIN] Iter: 745000 Loss: 0.0043665883131325245  PSNR: 28.53339385986328
[TRAIN] Iter: 745100 Loss: 0.006396849639713764  PSNR: 26.496307373046875
[TRAIN] Iter: 745200 Loss: 0.0044839512556791306  PSNR: 28.11864471435547
[TRAIN] Iter: 745300 Loss: 0.003927952144294977  PSNR: 28.27159309387207
[TRAIN] Iter: 745400 Loss: 0.0036917407996952534  PSNR: 29.43514633178711
[TRAIN] Iter: 745500 Loss: 0.0030651679262518883  PSNR: 30.419923782348633
[TRAIN] Iter: 745600 Loss: 0.005100500304251909  PSNR: 27.74876594543457
[TRAIN] Iter: 745700 Loss: 0.0053980909287929535  PSNR: 26.996458053588867
[TRAIN] Iter: 745800 Loss: 0.005590737331658602  PSNR: 27.66154670715332
[TRAIN] Iter: 745900 Loss: 0.0038559690583497286  PSNR: 29.36316680908203
[TRAIN] Iter: 746000 Loss: 0.0046765743754804134  PSNR: 28.119314193725586
[TRAIN] Iter: 746100 Loss: 0.004388241097331047  PSNR: 28.242971420288086
[TRAIN] Iter: 746200 Loss: 0.005157414358109236  PSNR: 26.630388259887695
[TRAIN] Iter: 746300 Loss: 0.004173532128334045  PSNR: 28.17648696899414
[TRAIN] Iter: 746400 Loss: 0.004270799923688173  PSNR: 28.981508255004883
[TRAIN] Iter: 746500 Loss: 0.003933792933821678  PSNR: 29.700904846191406
[TRAIN] Iter: 746600 Loss: 0.0036100666038691998  PSNR: 29.77157974243164
[TRAIN] Iter: 746700 Loss: 0.004637382924556732  PSNR: 28.826242446899414
[TRAIN] Iter: 746800 Loss: 0.004054214805364609  PSNR: 28.659914016723633
[TRAIN] Iter: 746900 Loss: 0.004118198063224554  PSNR: 28.943222045898438
[TRAIN] Iter: 747000 Loss: 0.004900978412479162  PSNR: 27.750144958496094
[TRAIN] Iter: 747100 Loss: 0.003791274968534708  PSNR: 29.44397735595703
[TRAIN] Iter: 747200 Loss: 0.004543313756585121  PSNR: 28.974802017211914
[TRAIN] Iter: 747300 Loss: 0.003414520062506199  PSNR: 29.99057960510254
[TRAIN] Iter: 747400 Loss: 0.00343373348005116  PSNR: 30.360614776611328
[TRAIN] Iter: 747500 Loss: 0.003712022677063942  PSNR: 29.350006103515625
[TRAIN] Iter: 747600 Loss: 0.004820189438760281  PSNR: 28.302148818969727
[TRAIN] Iter: 747700 Loss: 0.004109440371394157  PSNR: 28.847087860107422
[TRAIN] Iter: 747800 Loss: 0.004462374374270439  PSNR: 28.63923454284668
[TRAIN] Iter: 747900 Loss: 0.004878670908510685  PSNR: 27.577375411987305
[TRAIN] Iter: 748000 Loss: 0.005273248068988323  PSNR: 26.915159225463867
[TRAIN] Iter: 748100 Loss: 0.0048550707288086414  PSNR: 27.770410537719727
[TRAIN] Iter: 748200 Loss: 0.004306894727051258  PSNR: 28.480661392211914
[TRAIN] Iter: 748300 Loss: 0.0049329474568367004  PSNR: 27.188648223876953
[TRAIN] Iter: 748400 Loss: 0.004712797701358795  PSNR: 28.412534713745117
[TRAIN] Iter: 748500 Loss: 0.004457656294107437  PSNR: 27.914745330810547
[TRAIN] Iter: 748600 Loss: 0.004496605601161718  PSNR: 28.084732055664062
[TRAIN] Iter: 748700 Loss: 0.0045040929690003395  PSNR: 28.42683219909668
[TRAIN] Iter: 748800 Loss: 0.0048298826441168785  PSNR: 28.24721908569336
[TRAIN] Iter: 748900 Loss: 0.003939783200621605  PSNR: 29.026607513427734
[TRAIN] Iter: 749000 Loss: 0.004161285236477852  PSNR: 28.1478271484375
[TRAIN] Iter: 749100 Loss: 0.003633616492152214  PSNR: 30.07240104675293
[TRAIN] Iter: 749200 Loss: 0.004730244632810354  PSNR: 28.841798782348633
[TRAIN] Iter: 749300 Loss: 0.005886458326131105  PSNR: 26.446685791015625
[TRAIN] Iter: 749400 Loss: 0.004080317914485931  PSNR: 29.19086265563965
[TRAIN] Iter: 749500 Loss: 0.0035215956158936024  PSNR: 29.60898208618164
[TRAIN] Iter: 749600 Loss: 0.005339955911040306  PSNR: 27.01479148864746
[TRAIN] Iter: 749700 Loss: 0.003364007920026779  PSNR: 29.80704689025879
[TRAIN] Iter: 749800 Loss: 0.004828678909689188  PSNR: 27.656564712524414
[TRAIN] Iter: 749900 Loss: 0.003998791333287954  PSNR: 28.937145233154297
Saved checkpoints at ./logs/TUT-KE101-nerf/750000.tar
0 0.0004534721374511719
torch.Size([320, 640, 3]) torch.Size([320, 640])
1 14.190739393234253
2 14.123388051986694
3 14.067094326019287
4 14.202352046966553
5 14.18362021446228
6 14.178176879882812
7 14.164793491363525
8 14.17504072189331
9 14.133203268051147
10 14.160177946090698
11 14.146239995956421
12 14.180763244628906
13 14.147900581359863
14 14.164541721343994
15 14.175203323364258
16 14.160621881484985
17 14.142161130905151
18 14.127924680709839
19 14.205783605575562
20 14.167302370071411
21 14.169739484786987
22 14.169845581054688
23 14.114409446716309
24 14.098504066467285
25 14.142240047454834
26 14.179409980773926
27 14.187718629837036
28 14.169531106948853
29 14.163362741470337
30 14.128976106643677
31 14.156871318817139
32 14.22825312614441
33 14.154497385025024
34 14.211928367614746
35 14.205107688903809
36 14.17344045639038
37 14.176238775253296
38 14.16198468208313
39 14.184335708618164
40 14.171988725662231
41 14.13042402267456
42 14.190693855285645
43 14.156431674957275
44 14.203064441680908
45 14.17383885383606
46 14.187734365463257
47 14.16826581954956
48 14.16182017326355
49 14.054049968719482
50 14.140416622161865
51 14.16938829421997
52 14.150888442993164
53 14.161278247833252
54 14.143728971481323
55 14.181550741195679
56 14.182843923568726
57 14.181889772415161
58 14.21392035484314
59 14.169695138931274
60 14.188198566436768
61 14.1743745803833
62 14.117378950119019
63 14.129836082458496
64 14.209381580352783
65 14.190603971481323
66 14.193953275680542
67 14.173998832702637
68 14.20254635810852
69 14.18558382987976
70 14.201804399490356
71 14.184471368789673
72 14.137033939361572
73 14.203603029251099
74 14.10354232788086
75 14.155233144760132
76 14.21784257888794
77 14.242115497589111
78 14.196600437164307
79 14.196762323379517
80 14.196090459823608
81 14.172648191452026
82 14.184051275253296
83 14.212013244628906
84 14.188531160354614
85 14.197457075119019
86 14.213752031326294
87 14.185771942138672
88 14.221246004104614
89 14.139843463897705
90 14.213670015335083
91 14.157936334609985
92 14.17470383644104
93 14.114025354385376
94 14.166656255722046
95 14.19385814666748
96 14.226784944534302
97 14.186136960983276
98 14.16936731338501
99 14.160037279129028
100 14.111145973205566
101 14.143306970596313
102 14.20029067993164
103 14.196590423583984
104 14.084344148635864
105 14.178490400314331
106 14.166516780853271
107 14.150709867477417
108 14.22697114944458
109 14.139226198196411
110 14.161219358444214
111 14.161263942718506
112 14.12362813949585
113 14.14812159538269
114 14.12113356590271
115 14.196168422698975
116 14.168967723846436
117 14.16347622871399
118 14.176258087158203
119 14.15972638130188
Done, saving (120, 320, 640, 3) (120, 320, 640)
extras:{'raw': tensor([[[ 6.2410e+00,  7.1925e+00,  9.6875e+00, -6.9888e+01],
         [-9.8635e-01,  3.2997e-01,  3.3522e+00, -9.9805e+01],
         [-2.2886e+00, -9.7752e-01,  1.9166e+00, -9.7989e+01],
         ...,
         [ 1.4187e-01, -1.3539e+00, -5.5407e+00,  3.1839e+02],
         [ 5.7697e-01, -1.0071e+00, -5.4448e+00,  3.0796e+02],
         [ 3.9794e-01, -1.2359e+00, -5.5201e+00,  3.8763e+02]],

        [[-4.8670e+00, -6.0164e+00, -9.0218e+00, -7.9196e+01],
         [ 3.2955e-01,  5.7365e-01,  5.0412e-01, -3.7160e+01],
         [-8.5744e-01, -1.1164e+00, -1.5789e+00,  7.9145e+00],
         ...,
         [-9.1077e+00, -8.4860e+00, -6.9402e+00, -2.4117e+02],
         [-1.3924e+01, -1.2550e+01, -9.7947e+00, -2.0772e+02],
         [-1.2777e+01, -1.1598e+01, -9.3896e+00, -1.4179e+02]],

        [[ 6.8673e-01,  1.7528e+00,  3.3610e+00, -6.1037e+01],
         [-7.9463e-01, -4.9608e-02,  1.4792e+00, -5.1445e+01],
         [-4.3243e-01,  1.6865e-01,  1.4363e+00, -5.2899e+01],
         ...,
         [-7.2932e+00, -6.6005e+00, -7.0712e+00, -1.0783e+02],
         [-7.9224e+00, -7.8754e+00, -8.8859e+00, -1.1348e+01],
         [-6.9228e+00, -6.6989e+00, -7.5124e+00, -5.5183e+01]],

        ...,

        [[ 3.4381e-01,  8.9819e-01,  2.1323e+00, -7.7841e+01],
         [-1.1425e-01, -2.8001e-01, -5.5989e-01,  2.2218e+01],
         [-7.6590e-02, -2.5780e-01, -5.5221e-01,  1.9785e+01],
         ...,
         [ 5.7960e+00,  6.8542e+00,  9.6866e+00,  5.0619e+02],
         [ 5.9387e+00,  6.4806e+00,  9.1148e+00,  4.8973e+02],
         [ 6.1608e+00,  7.2637e+00,  1.0234e+01,  5.0498e+02]],

        [[-7.7866e-01, -1.3706e-01,  1.4290e+00, -6.6664e+01],
         [ 1.0320e-01,  1.5340e-01,  4.4117e-01, -2.5269e+01],
         [ 1.4555e-01,  1.2664e-02,  6.6389e-02,  2.2334e+01],
         ...,
         [ 3.8343e-01, -1.4162e+00, -5.7769e+00, -3.0678e+01],
         [ 5.9059e-01, -1.3242e+00, -5.9224e+00, -6.5034e+01],
         [ 3.8341e-01, -1.3032e+00, -5.3218e+00,  3.7242e+01]],

        [[ 7.6622e-02, -1.0440e-01, -2.9619e-01, -4.2742e+01],
         [ 1.0243e-01, -1.0200e-01, -2.5948e-01, -3.0527e+01],
         [ 1.2868e-01, -1.1075e-01, -2.8811e-01, -2.7942e+01],
         ...,
         [-1.6434e+00, -2.0669e+00, -8.5371e+00,  5.9988e+02],
         [-1.9527e+00, -2.3352e+00, -8.3329e+00,  5.0560e+02],
         [-2.1353e+00, -2.3990e+00, -8.9186e+00,  6.9361e+02]]],
       grad_fn=<ReshapeAliasBackward0>), 'rgb0': tensor([[0.4480, 0.4315, 0.3778],
        [0.4644, 0.3452, 0.1712],
        [0.6258, 0.5629, 0.4879],
        ...,
        [0.4765, 0.4344, 0.3786],
        [0.4725, 0.4509, 0.4741],
        [0.9892, 0.9876, 0.9920]], grad_fn=<ReshapeAliasBackward0>), 'disp0': tensor([226.1914,  54.0929, 182.6161,  ...,  96.2839,  62.5388, 136.6332],
       grad_fn=<ReshapeAliasBackward0>), 'acc0': tensor([1., 1., 1.,  ..., 1., 1., 1.], grad_fn=<ReshapeAliasBackward0>), 'z_std': tensor([0.0018, 0.0034, 0.0020,  ..., 0.0015, 0.0164, 0.0014])}
0 0.0004780292510986328
torch.Size([320, 640, 3]) torch.Size([320, 640])
1 14.16470742225647
2 14.125784158706665
3 14.15637755393982
4 14.118664026260376
5 14.088452100753784
6 14.102283954620361
7 14.138872623443604
8 14.17806100845337
9 14.138646841049194
10 14.092920064926147
11 14.15351939201355
12 14.162333726882935
13 14.1709566116333
14 14.156379222869873
15 14.144069194793701
16 14.166801691055298
17 14.162853002548218
18 14.16326904296875
19 14.144452810287476
20 14.144470453262329
21 14.110726594924927
22 14.19257378578186
23 14.211365461349487
24 14.17775845527649
25 14.169615268707275
26 14.165839433670044
27 14.170012712478638
28 14.198258876800537
29 14.196353912353516
30 14.153055906295776
31 14.120136976242065
32 14.164705753326416
33 14.169538021087646
34 14.177114009857178
35 14.179117202758789
36 14.106245517730713
37 14.154371738433838
38 14.127259731292725
39 14.155024766921997
40 14.181904554367065
41 14.164020776748657
42 14.161166906356812
43 14.188416957855225
44 14.102108001708984
45 14.155459642410278
46 14.188700199127197
47 14.227136135101318
48 14.182005643844604
49 14.197165727615356
50 14.164982557296753
51 14.134300470352173
52 14.160337448120117
53 14.169101476669312
54 14.191300630569458
55 14.134583473205566
56 14.15163779258728
57 14.065191507339478
58 14.166842460632324
59 14.136555671691895
60 14.17891240119934
61 14.144230127334595
62 14.171217679977417
63 14.16587233543396
64 14.167967319488525
65 14.149269104003906
66 14.184698820114136
67 14.150732517242432
68 14.17638611793518
69 14.172780752182007
70 14.14498519897461
71 14.145852088928223
72 14.231863260269165
73 14.169904470443726
74 14.149071455001831
75 14.151334762573242
76 14.164109706878662
77 14.28954005241394
78 16.12060809135437
79 16.18643617630005
80 16.150747060775757
81 16.147742748260498
82 16.167118310928345
83 16.0481858253479
84 16.12035322189331
85 16.10177993774414
86 16.15679430961609
87 16.11950969696045
88 16.121803522109985
89 16.115519046783447
90 16.141924381256104
91 16.131347179412842
92 16.219831466674805
93 16.178316831588745
94 16.191657066345215
95 16.07463502883911
96 16.152820348739624
97 16.128430366516113
98 16.15505051612854
99 16.139375686645508
100 16.165969610214233
101 16.159597396850586
102 16.056862115859985
103 16.110610008239746
104 16.16698670387268
105 16.15594172477722
106 16.12891125679016
107 16.150044441223145
108 16.157056093215942
109 16.0499529838562
110 16.115467309951782
111 16.224689483642578
112 16.295525312423706
113 16.417367219924927
114 16.487063884735107
115 16.574553966522217
116 16.64472246170044
117 16.476369619369507
118 16.506184816360474
119 16.37926745414734
test poses shape torch.Size([4, 3, 4])
0 0.0006248950958251953
torch.Size([320, 640, 3]) torch.Size([320, 640])
1 16.183740377426147
2 16.184610843658447
3 16.20706558227539
Saved test set
[TRAIN] Iter: 750000 Loss: 0.004581571090966463  PSNR: 27.530792236328125
[TRAIN] Iter: 750100 Loss: 0.004903290420770645  PSNR: 27.408397674560547
[TRAIN] Iter: 750200 Loss: 0.0035820018965750933  PSNR: 28.988142013549805
[TRAIN] Iter: 750300 Loss: 0.005159267224371433  PSNR: 27.64731788635254
[TRAIN] Iter: 750400 Loss: 0.004724569618701935  PSNR: 28.029836654663086
[TRAIN] Iter: 750500 Loss: 0.00520591763779521  PSNR: 26.996828079223633
[TRAIN] Iter: 750600 Loss: 0.0043397084809839725  PSNR: 28.180688858032227
[TRAIN] Iter: 750700 Loss: 0.004041576758027077  PSNR: 28.532608032226562
[TRAIN] Iter: 750800 Loss: 0.00551609555259347  PSNR: 27.29096794128418
[TRAIN] Iter: 750900 Loss: 0.00479656457901001  PSNR: 28.37619972229004
[TRAIN] Iter: 751000 Loss: 0.005262451246380806  PSNR: 27.3126163482666
[TRAIN] Iter: 751100 Loss: 0.003460496664047241  PSNR: 30.797056198120117
[TRAIN] Iter: 751200 Loss: 0.004252048209309578  PSNR: 28.9055118560791
[TRAIN] Iter: 751300 Loss: 0.003960061818361282  PSNR: 28.94536590576172
[TRAIN] Iter: 751400 Loss: 0.003988008480519056  PSNR: 30.44626235961914
[TRAIN] Iter: 751500 Loss: 0.004166125785559416  PSNR: 28.947845458984375
[TRAIN] Iter: 751600 Loss: 0.00337214395403862  PSNR: 29.85359001159668
[TRAIN] Iter: 751700 Loss: 0.0038924277760088444  PSNR: 29.30747413635254
[TRAIN] Iter: 751800 Loss: 0.0035933596082031727  PSNR: 29.9729061126709
[TRAIN] Iter: 751900 Loss: 0.004058158025145531  PSNR: 28.76890754699707
[TRAIN] Iter: 752000 Loss: 0.0048394352197647095  PSNR: 28.381364822387695
[TRAIN] Iter: 752100 Loss: 0.0046320026740431786  PSNR: 29.27300262451172
[TRAIN] Iter: 752200 Loss: 0.0035202079452574253  PSNR: 30.485380172729492
[TRAIN] Iter: 752300 Loss: 0.004024427384138107  PSNR: 28.140380859375
[TRAIN] Iter: 752400 Loss: 0.004279056563973427  PSNR: 28.708866119384766
[TRAIN] Iter: 752500 Loss: 0.0035514070186764  PSNR: 29.93687629699707
[TRAIN] Iter: 752600 Loss: 0.004169706720858812  PSNR: 28.39781951904297
[TRAIN] Iter: 752700 Loss: 0.0036123786121606827  PSNR: 29.9307918548584
[TRAIN] Iter: 752800 Loss: 0.004949608352035284  PSNR: 28.363937377929688
[TRAIN] Iter: 752900 Loss: 0.004484333097934723  PSNR: 27.801931381225586
[TRAIN] Iter: 753000 Loss: 0.0035237669944763184  PSNR: 29.578170776367188
[TRAIN] Iter: 753100 Loss: 0.0049658603966236115  PSNR: 27.642684936523438
[TRAIN] Iter: 753200 Loss: 0.003346347250044346  PSNR: 30.297245025634766
[TRAIN] Iter: 753300 Loss: 0.005585147999227047  PSNR: 27.294090270996094
[TRAIN] Iter: 753400 Loss: 0.003975586034357548  PSNR: 29.517047882080078
[TRAIN] Iter: 753500 Loss: 0.0046237558126449585  PSNR: 28.043643951416016
[TRAIN] Iter: 753600 Loss: 0.004395525436848402  PSNR: 27.825511932373047
[TRAIN] Iter: 753700 Loss: 0.0037152019795030355  PSNR: 29.08209800720215
[TRAIN] Iter: 753800 Loss: 0.003385222749784589  PSNR: 30.107906341552734
[TRAIN] Iter: 753900 Loss: 0.004978186450898647  PSNR: 27.81584358215332
[TRAIN] Iter: 754000 Loss: 0.003953639417886734  PSNR: 29.56283950805664
[TRAIN] Iter: 754100 Loss: 0.00425271550193429  PSNR: 28.756282806396484
[TRAIN] Iter: 754200 Loss: 0.004691175185143948  PSNR: 27.626972198486328
[TRAIN] Iter: 754300 Loss: 0.005412092432379723  PSNR: 27.242767333984375
[TRAIN] Iter: 754400 Loss: 0.004668928682804108  PSNR: 28.652854919433594
[TRAIN] Iter: 754500 Loss: 0.004993730690330267  PSNR: 27.67029571533203
[TRAIN] Iter: 754600 Loss: 0.0043329037725925446  PSNR: 29.336559295654297
[TRAIN] Iter: 754700 Loss: 0.004924307111650705  PSNR: 28.33454704284668
[TRAIN] Iter: 754800 Loss: 0.003907342441380024  PSNR: 29.848865509033203
[TRAIN] Iter: 754900 Loss: 0.003974057734012604  PSNR: 29.983015060424805
[TRAIN] Iter: 755000 Loss: 0.004528576508164406  PSNR: 28.673669815063477
[TRAIN] Iter: 755100 Loss: 0.0035606911405920982  PSNR: 30.59930419921875
[TRAIN] Iter: 755200 Loss: 0.003544756444171071  PSNR: 30.62894630432129
[TRAIN] Iter: 755300 Loss: 0.005251712165772915  PSNR: 28.224939346313477
[TRAIN] Iter: 755400 Loss: 0.004254508297890425  PSNR: 28.27707862854004
[TRAIN] Iter: 755500 Loss: 0.004499029368162155  PSNR: 28.392375946044922
[TRAIN] Iter: 755600 Loss: 0.004720098339021206  PSNR: 28.127458572387695
[TRAIN] Iter: 755700 Loss: 0.003925691358745098  PSNR: 28.924671173095703
[TRAIN] Iter: 755800 Loss: 0.004622941836714745  PSNR: 28.040184020996094
[TRAIN] Iter: 755900 Loss: 0.005041065625846386  PSNR: 27.536069869995117
[TRAIN] Iter: 756000 Loss: 0.003743340028449893  PSNR: 29.677814483642578
[TRAIN] Iter: 756100 Loss: 0.004336732439696789  PSNR: 28.683452606201172
[TRAIN] Iter: 756200 Loss: 0.004049000795930624  PSNR: 28.519031524658203
[TRAIN] Iter: 756300 Loss: 0.005136304534971714  PSNR: 28.01938247680664
[TRAIN] Iter: 756400 Loss: 0.0044286176562309265  PSNR: 28.362607955932617
[TRAIN] Iter: 756500 Loss: 0.004987568594515324  PSNR: 27.855712890625
[TRAIN] Iter: 756600 Loss: 0.0038662219885736704  PSNR: 29.68376922607422
[TRAIN] Iter: 756700 Loss: 0.004420817829668522  PSNR: 28.78744888305664
[TRAIN] Iter: 756800 Loss: 0.005260411184281111  PSNR: 28.178274154663086
[TRAIN] Iter: 756900 Loss: 0.005134994629770517  PSNR: 27.529829025268555
[TRAIN] Iter: 757000 Loss: 0.00393033679574728  PSNR: 30.399076461791992
[TRAIN] Iter: 757100 Loss: 0.004797500092536211  PSNR: 28.365758895874023
[TRAIN] Iter: 757200 Loss: 0.004994574002921581  PSNR: 28.201391220092773
[TRAIN] Iter: 757300 Loss: 0.004705219529569149  PSNR: 28.168102264404297
[TRAIN] Iter: 757400 Loss: 0.0048030661419034  PSNR: 28.056442260742188
[TRAIN] Iter: 757500 Loss: 0.0038962785620242357  PSNR: 28.752227783203125
[TRAIN] Iter: 757600 Loss: 0.004281774163246155  PSNR: 28.530277252197266
[TRAIN] Iter: 757700 Loss: 0.0047019729390740395  PSNR: 27.589813232421875
[TRAIN] Iter: 757800 Loss: 0.004895550198853016  PSNR: 27.9510498046875
[TRAIN] Iter: 757900 Loss: 0.00509405555203557  PSNR: 27.583887100219727
[TRAIN] Iter: 758000 Loss: 0.003363492665812373  PSNR: 30.852746963500977
[TRAIN] Iter: 758100 Loss: 0.003487849608063698  PSNR: 30.164304733276367
[TRAIN] Iter: 758200 Loss: 0.0046043554320931435  PSNR: 27.94430160522461
[TRAIN] Iter: 758300 Loss: 0.004939002450555563  PSNR: 28.436918258666992
[TRAIN] Iter: 758400 Loss: 0.00428655045107007  PSNR: 29.756481170654297
[TRAIN] Iter: 758500 Loss: 0.004308050498366356  PSNR: 28.57248878479004
[TRAIN] Iter: 758600 Loss: 0.004138996824622154  PSNR: 29.922483444213867
[TRAIN] Iter: 758700 Loss: 0.0035123247653245926  PSNR: 29.374771118164062
[TRAIN] Iter: 758800 Loss: 0.0057692816480994225  PSNR: 27.679340362548828
[TRAIN] Iter: 758900 Loss: 0.005422331392765045  PSNR: 27.052026748657227
[TRAIN] Iter: 759000 Loss: 0.005865670740604401  PSNR: 27.132396697998047
[TRAIN] Iter: 759100 Loss: 0.0038475836627185345  PSNR: 30.18926429748535
[TRAIN] Iter: 759200 Loss: 0.00515416543930769  PSNR: 27.669172286987305
[TRAIN] Iter: 759300 Loss: 0.003748062066733837  PSNR: 29.966487884521484
[TRAIN] Iter: 759400 Loss: 0.004731582012027502  PSNR: 28.132020950317383
[TRAIN] Iter: 759500 Loss: 0.005295217502862215  PSNR: 27.962745666503906
[TRAIN] Iter: 759600 Loss: 0.004716992378234863  PSNR: 27.890676498413086
[TRAIN] Iter: 759700 Loss: 0.005435764789581299  PSNR: 27.135841369628906
[TRAIN] Iter: 759800 Loss: 0.005045926198363304  PSNR: 27.882009506225586
[TRAIN] Iter: 759900 Loss: 0.003544323146343231  PSNR: 30.191251754760742
Saved checkpoints at ./logs/TUT-KE101-nerf/760000.tar
[TRAIN] Iter: 760000 Loss: 0.0036900315899401903  PSNR: 30.461069107055664
[TRAIN] Iter: 760100 Loss: 0.005269289016723633  PSNR: 27.329389572143555
[TRAIN] Iter: 760200 Loss: 0.0043300483375787735  PSNR: 29.19883918762207
[TRAIN] Iter: 760300 Loss: 0.0040016258135437965  PSNR: 28.88147735595703
[TRAIN] Iter: 760400 Loss: 0.004055590834468603  PSNR: 29.2774715423584
[TRAIN] Iter: 760500 Loss: 0.005358394235372543  PSNR: 27.608407974243164
[TRAIN] Iter: 760600 Loss: 0.004546352196484804  PSNR: 27.488361358642578
[TRAIN] Iter: 760700 Loss: 0.004423667676746845  PSNR: 27.99103546142578
[TRAIN] Iter: 760800 Loss: 0.004543260671198368  PSNR: 28.046926498413086
[TRAIN] Iter: 760900 Loss: 0.005177641287446022  PSNR: 27.900291442871094
[TRAIN] Iter: 761000 Loss: 0.005225266329944134  PSNR: 27.751686096191406
[TRAIN] Iter: 761100 Loss: 0.003998542204499245  PSNR: 28.912139892578125
[TRAIN] Iter: 761200 Loss: 0.00467083090916276  PSNR: 27.362186431884766
[TRAIN] Iter: 761300 Loss: 0.0037417381536215544  PSNR: 28.928415298461914
[TRAIN] Iter: 761400 Loss: 0.004809602629393339  PSNR: 27.615055084228516
[TRAIN] Iter: 761500 Loss: 0.003297304268926382  PSNR: 30.7371883392334
[TRAIN] Iter: 761600 Loss: 0.003794415621086955  PSNR: 29.82025146484375
[TRAIN] Iter: 761700 Loss: 0.0047546494752168655  PSNR: 27.912830352783203
[TRAIN] Iter: 761800 Loss: 0.0033320945221930742  PSNR: 30.3000545501709
[TRAIN] Iter: 761900 Loss: 0.005198635160923004  PSNR: 27.15719223022461
[TRAIN] Iter: 762000 Loss: 0.0037102005444467068  PSNR: 29.674354553222656
[TRAIN] Iter: 762100 Loss: 0.00541807571426034  PSNR: 28.20599937438965
[TRAIN] Iter: 762200 Loss: 0.004585593938827515  PSNR: 27.571956634521484
[TRAIN] Iter: 762300 Loss: 0.004600474610924721  PSNR: 28.06266975402832
[TRAIN] Iter: 762400 Loss: 0.004991949535906315  PSNR: 27.552684783935547
[TRAIN] Iter: 762500 Loss: 0.0035744193010032177  PSNR: 30.28012466430664
[TRAIN] Iter: 762600 Loss: 0.004010419361293316  PSNR: 29.695541381835938
[TRAIN] Iter: 762700 Loss: 0.004236951470375061  PSNR: 29.65423583984375
[TRAIN] Iter: 762800 Loss: 0.0039612902328372  PSNR: 29.049938201904297
[TRAIN] Iter: 762900 Loss: 0.004790767561644316  PSNR: 28.374568939208984
[TRAIN] Iter: 763000 Loss: 0.004260896705091  PSNR: 28.711864471435547
[TRAIN] Iter: 763100 Loss: 0.003708174219354987  PSNR: 30.79535484313965
[TRAIN] Iter: 763200 Loss: 0.003458551596850157  PSNR: 30.731536865234375
[TRAIN] Iter: 763300 Loss: 0.003924897871911526  PSNR: 30.66943359375
[TRAIN] Iter: 763400 Loss: 0.005131088197231293  PSNR: 27.628915786743164
[TRAIN] Iter: 763500 Loss: 0.003749549388885498  PSNR: 30.108381271362305
[TRAIN] Iter: 763600 Loss: 0.004710107110440731  PSNR: 27.934539794921875
[TRAIN] Iter: 763700 Loss: 0.004467964172363281  PSNR: 28.32815933227539
[TRAIN] Iter: 763800 Loss: 0.004267991986125708  PSNR: 28.988149642944336
[TRAIN] Iter: 763900 Loss: 0.0036088768392801285  PSNR: 30.18646812438965
[TRAIN] Iter: 764000 Loss: 0.005435117520391941  PSNR: 27.76378059387207
[TRAIN] Iter: 764100 Loss: 0.00579079519957304  PSNR: 26.948827743530273
[TRAIN] Iter: 764200 Loss: 0.00426033278927207  PSNR: 28.405088424682617
[TRAIN] Iter: 764300 Loss: 0.004801812581717968  PSNR: 28.03558921813965
[TRAIN] Iter: 764400 Loss: 0.004050038289278746  PSNR: 28.649578094482422
[TRAIN] Iter: 764500 Loss: 0.004073265008628368  PSNR: 28.96105194091797
[TRAIN] Iter: 764600 Loss: 0.0048726447857916355  PSNR: 27.95059585571289
[TRAIN] Iter: 764700 Loss: 0.004935873672366142  PSNR: 27.52797508239746
[TRAIN] Iter: 764800 Loss: 0.00447880057618022  PSNR: 28.055492401123047
[TRAIN] Iter: 764900 Loss: 0.005078264512121677  PSNR: 27.634437561035156
[TRAIN] Iter: 765000 Loss: 0.0037772455252707005  PSNR: 29.874059677124023
[TRAIN] Iter: 765100 Loss: 0.0049164146184921265  PSNR: 27.6807804107666
[TRAIN] Iter: 765200 Loss: 0.004363005980849266  PSNR: 28.966251373291016
[TRAIN] Iter: 765300 Loss: 0.004601638298481703  PSNR: 27.420745849609375
[TRAIN] Iter: 765400 Loss: 0.004559215158224106  PSNR: 28.363475799560547
[TRAIN] Iter: 765500 Loss: 0.003740494605153799  PSNR: 29.316587448120117
[TRAIN] Iter: 765600 Loss: 0.004608753137290478  PSNR: 28.579280853271484
[TRAIN] Iter: 765700 Loss: 0.003878317540511489  PSNR: 30.028860092163086
[TRAIN] Iter: 765800 Loss: 0.0052313534542918205  PSNR: 27.854228973388672
[TRAIN] Iter: 765900 Loss: 0.00544377788901329  PSNR: 27.851293563842773
[TRAIN] Iter: 766000 Loss: 0.004490484483540058  PSNR: 29.60854148864746
[TRAIN] Iter: 766100 Loss: 0.004052940756082535  PSNR: 29.5029354095459
[TRAIN] Iter: 766200 Loss: 0.003897774498909712  PSNR: 29.53070831298828
[TRAIN] Iter: 766300 Loss: 0.006287502124905586  PSNR: 26.271451950073242
[TRAIN] Iter: 766400 Loss: 0.004133779089897871  PSNR: 29.840068817138672
[TRAIN] Iter: 766500 Loss: 0.004331859759986401  PSNR: 29.09609603881836
[TRAIN] Iter: 766600 Loss: 0.0035154172219336033  PSNR: 30.090774536132812
[TRAIN] Iter: 766700 Loss: 0.004126174375414848  PSNR: 29.783124923706055
[TRAIN] Iter: 766800 Loss: 0.003729419317096472  PSNR: 30.179933547973633
[TRAIN] Iter: 766900 Loss: 0.004964860156178474  PSNR: 28.177690505981445
[TRAIN] Iter: 767000 Loss: 0.0032758114393800497  PSNR: 30.408353805541992
[TRAIN] Iter: 767100 Loss: 0.005098720081150532  PSNR: 27.92741584777832
[TRAIN] Iter: 767200 Loss: 0.0031501397024840117  PSNR: 30.031850814819336
[TRAIN] Iter: 767300 Loss: 0.005402023904025555  PSNR: 26.71687126159668
[TRAIN] Iter: 767400 Loss: 0.0058765108697116375  PSNR: 27.05298614501953
[TRAIN] Iter: 767500 Loss: 0.0053602103143930435  PSNR: 27.86199951171875
[TRAIN] Iter: 767600 Loss: 0.0039968229830265045  PSNR: 29.661611557006836
[TRAIN] Iter: 767700 Loss: 0.004558810032904148  PSNR: 28.26039695739746
[TRAIN] Iter: 767800 Loss: 0.005090105812996626  PSNR: 27.242361068725586
[TRAIN] Iter: 767900 Loss: 0.0038135203067213297  PSNR: 29.189043045043945
[TRAIN] Iter: 768000 Loss: 0.004790104925632477  PSNR: 27.844865798950195
[TRAIN] Iter: 768100 Loss: 0.004341568797826767  PSNR: 28.673315048217773
[TRAIN] Iter: 768200 Loss: 0.0046768831089138985  PSNR: 28.473430633544922
[TRAIN] Iter: 768300 Loss: 0.0038373139686882496  PSNR: 29.775022506713867
[TRAIN] Iter: 768400 Loss: 0.0038643134757876396  PSNR: 29.335834503173828
[TRAIN] Iter: 768500 Loss: 0.005152364727109671  PSNR: 27.69656753540039
[TRAIN] Iter: 768600 Loss: 0.003649798221886158  PSNR: 30.574934005737305
[TRAIN] Iter: 768700 Loss: 0.004605908878147602  PSNR: 28.08385467529297
[TRAIN] Iter: 768800 Loss: 0.004514620173722506  PSNR: 29.253374099731445
[TRAIN] Iter: 768900 Loss: 0.003569826250895858  PSNR: 30.6409912109375
[TRAIN] Iter: 769000 Loss: 0.0038508703000843525  PSNR: 29.940031051635742
[TRAIN] Iter: 769100 Loss: 0.0054724449291825294  PSNR: 27.90282440185547
[TRAIN] Iter: 769200 Loss: 0.0035887029953300953  PSNR: 29.948314666748047
[TRAIN] Iter: 769300 Loss: 0.0036845614667981863  PSNR: 29.88005828857422
[TRAIN] Iter: 769400 Loss: 0.004138397052884102  PSNR: 28.544872283935547
[TRAIN] Iter: 769500 Loss: 0.005356495268642902  PSNR: 27.67925453186035
[TRAIN] Iter: 769600 Loss: 0.003580165561288595  PSNR: 30.136770248413086
[TRAIN] Iter: 769700 Loss: 0.005159445106983185  PSNR: 28.32692527770996
[TRAIN] Iter: 769800 Loss: 0.004631856456398964  PSNR: 28.577028274536133
[TRAIN] Iter: 769900 Loss: 0.0046606967225670815  PSNR: 28.904781341552734
Saved checkpoints at ./logs/TUT-KE101-nerf/770000.tar
[TRAIN] Iter: 770000 Loss: 0.004526485688984394  PSNR: 28.32814598083496
[TRAIN] Iter: 770100 Loss: 0.00532515998929739  PSNR: 27.700786590576172
[TRAIN] Iter: 770200 Loss: 0.004404714331030846  PSNR: 28.775928497314453
[TRAIN] Iter: 770300 Loss: 0.005265452899038792  PSNR: 27.367433547973633
[TRAIN] Iter: 770400 Loss: 0.005263147875666618  PSNR: 27.5130672454834
[TRAIN] Iter: 770500 Loss: 0.0034529617987573147  PSNR: 30.433256149291992
[TRAIN] Iter: 770600 Loss: 0.003326178528368473  PSNR: 30.568235397338867
[TRAIN] Iter: 770700 Loss: 0.0041121444664895535  PSNR: 29.364965438842773
[TRAIN] Iter: 770800 Loss: 0.004749934189021587  PSNR: 27.64286994934082
[TRAIN] Iter: 770900 Loss: 0.004138665273785591  PSNR: 28.935148239135742
[TRAIN] Iter: 771000 Loss: 0.00510176969692111  PSNR: 27.748104095458984
[TRAIN] Iter: 771100 Loss: 0.004278701264411211  PSNR: 27.99747085571289
[TRAIN] Iter: 771200 Loss: 0.003289894200861454  PSNR: 30.448598861694336
[TRAIN] Iter: 771300 Loss: 0.005301946308463812  PSNR: 27.630401611328125
[TRAIN] Iter: 771400 Loss: 0.004128162283450365  PSNR: 28.732698440551758
[TRAIN] Iter: 771500 Loss: 0.004748392850160599  PSNR: 28.215456008911133
[TRAIN] Iter: 771600 Loss: 0.006158606614917517  PSNR: 27.254894256591797
[TRAIN] Iter: 771700 Loss: 0.004084816202521324  PSNR: 28.216400146484375
[TRAIN] Iter: 771800 Loss: 0.005381847731769085  PSNR: 26.87956428527832
[TRAIN] Iter: 771900 Loss: 0.004748333711177111  PSNR: 27.88481903076172
[TRAIN] Iter: 772000 Loss: 0.005214981734752655  PSNR: 27.079919815063477
[TRAIN] Iter: 772100 Loss: 0.003729431424289942  PSNR: 29.907432556152344
[TRAIN] Iter: 772200 Loss: 0.0037335373926907778  PSNR: 30.66276741027832
[TRAIN] Iter: 772300 Loss: 0.005598088260740042  PSNR: 26.81157112121582
[TRAIN] Iter: 772400 Loss: 0.004603269975632429  PSNR: 27.83249282836914
[TRAIN] Iter: 772500 Loss: 0.00490961316972971  PSNR: 27.52082633972168
[TRAIN] Iter: 772600 Loss: 0.004141452722251415  PSNR: 28.82449722290039
[TRAIN] Iter: 772700 Loss: 0.005422982387244701  PSNR: 27.278202056884766
[TRAIN] Iter: 772800 Loss: 0.004708210937678814  PSNR: 28.14312171936035
[TRAIN] Iter: 772900 Loss: 0.004150809720158577  PSNR: 29.58831214904785
[TRAIN] Iter: 773000 Loss: 0.004651511553674936  PSNR: 28.3095645904541
[TRAIN] Iter: 773100 Loss: 0.004914298187941313  PSNR: 27.57514762878418
[TRAIN] Iter: 773200 Loss: 0.004386603832244873  PSNR: 28.37330436706543
[TRAIN] Iter: 773300 Loss: 0.003794750664383173  PSNR: 30.24675941467285
[TRAIN] Iter: 773400 Loss: 0.005089828744530678  PSNR: 27.075122833251953
[TRAIN] Iter: 773500 Loss: 0.0044403476640582085  PSNR: 28.68543243408203
[TRAIN] Iter: 773600 Loss: 0.006006759125739336  PSNR: 26.663110733032227
[TRAIN] Iter: 773700 Loss: 0.004786089528352022  PSNR: 27.971914291381836
[TRAIN] Iter: 773800 Loss: 0.005598332732915878  PSNR: 27.234020233154297
[TRAIN] Iter: 773900 Loss: 0.0031727200839668512  PSNR: 30.560640335083008
[TRAIN] Iter: 774000 Loss: 0.003288128413259983  PSNR: 30.466663360595703
[TRAIN] Iter: 774100 Loss: 0.004923434462398291  PSNR: 27.735944747924805
[TRAIN] Iter: 774200 Loss: 0.0055388594046235085  PSNR: 27.386688232421875
[TRAIN] Iter: 774300 Loss: 0.004382215440273285  PSNR: 28.64532470703125
[TRAIN] Iter: 774400 Loss: 0.00394347682595253  PSNR: 29.505001068115234
[TRAIN] Iter: 774500 Loss: 0.004115065559744835  PSNR: 28.46047592163086
[TRAIN] Iter: 774600 Loss: 0.0042197443544864655  PSNR: 29.50530433654785
[TRAIN] Iter: 774700 Loss: 0.004375751130282879  PSNR: 28.979385375976562
[TRAIN] Iter: 774800 Loss: 0.004958656150847673  PSNR: 28.133800506591797
[TRAIN] Iter: 774900 Loss: 0.005086932331323624  PSNR: 27.726364135742188
[TRAIN] Iter: 775000 Loss: 0.0034664454869925976  PSNR: 30.708425521850586
[TRAIN] Iter: 775100 Loss: 0.005136902444064617  PSNR: 27.923851013183594
[TRAIN] Iter: 775200 Loss: 0.0057901619002223015  PSNR: 27.161151885986328
[TRAIN] Iter: 775300 Loss: 0.004857526160776615  PSNR: 28.083345413208008
[TRAIN] Iter: 775400 Loss: 0.005104396492242813  PSNR: 27.323530197143555
[TRAIN] Iter: 775500 Loss: 0.003130145836621523  PSNR: 30.438814163208008
[TRAIN] Iter: 775600 Loss: 0.005248146131634712  PSNR: 27.655168533325195
[TRAIN] Iter: 775700 Loss: 0.004484694451093674  PSNR: 28.161298751831055
[TRAIN] Iter: 775800 Loss: 0.005407729186117649  PSNR: 27.329030990600586
[TRAIN] Iter: 775900 Loss: 0.0037278304807841778  PSNR: 29.318723678588867
[TRAIN] Iter: 776000 Loss: 0.003872472094371915  PSNR: 29.78424072265625
[TRAIN] Iter: 776100 Loss: 0.0054380763322114944  PSNR: 27.054271697998047
[TRAIN] Iter: 776200 Loss: 0.004385374486446381  PSNR: 27.90586280822754
[TRAIN] Iter: 776300 Loss: 0.003918639849871397  PSNR: 29.404935836791992
[TRAIN] Iter: 776400 Loss: 0.0032980942633002996  PSNR: 30.654953002929688
[TRAIN] Iter: 776500 Loss: 0.005121062509715557  PSNR: 28.00402069091797
[TRAIN] Iter: 776600 Loss: 0.00599433109164238  PSNR: 26.82977867126465
[TRAIN] Iter: 776700 Loss: 0.005174992606043816  PSNR: 28.24355125427246
[TRAIN] Iter: 776800 Loss: 0.005002985708415508  PSNR: 27.02629852294922
[TRAIN] Iter: 776900 Loss: 0.004017190542072058  PSNR: 28.62769317626953
[TRAIN] Iter: 777000 Loss: 0.0041035073809325695  PSNR: 29.34231948852539
[TRAIN] Iter: 777100 Loss: 0.004193134605884552  PSNR: 28.847103118896484
[TRAIN] Iter: 777200 Loss: 0.003671316895633936  PSNR: 30.11576271057129
[TRAIN] Iter: 777300 Loss: 0.003868649946525693  PSNR: 29.009721755981445
[TRAIN] Iter: 777400 Loss: 0.003317736554890871  PSNR: 30.589954376220703
[TRAIN] Iter: 777500 Loss: 0.005622110795229673  PSNR: 27.1015625
[TRAIN] Iter: 777600 Loss: 0.004024113528430462  PSNR: 29.083608627319336
[TRAIN] Iter: 777700 Loss: 0.005155791528522968  PSNR: 27.812297821044922
[TRAIN] Iter: 777800 Loss: 0.004038707818835974  PSNR: 28.626708984375
[TRAIN] Iter: 777900 Loss: 0.004986985120922327  PSNR: 28.471195220947266
[TRAIN] Iter: 778000 Loss: 0.0036262706853449345  PSNR: 30.354476928710938
[TRAIN] Iter: 778100 Loss: 0.0037975276354700327  PSNR: 29.891040802001953
[TRAIN] Iter: 778200 Loss: 0.0056451354175806046  PSNR: 27.232288360595703
[TRAIN] Iter: 778300 Loss: 0.0038290596567094326  PSNR: 30.942493438720703
[TRAIN] Iter: 778400 Loss: 0.003370553720742464  PSNR: 30.732370376586914
[TRAIN] Iter: 778500 Loss: 0.005594860762357712  PSNR: 27.787155151367188
[TRAIN] Iter: 778600 Loss: 0.003700139932334423  PSNR: 30.960466384887695
[TRAIN] Iter: 778700 Loss: 0.0037088009994477034  PSNR: 29.945358276367188
[TRAIN] Iter: 778800 Loss: 0.004316607490181923  PSNR: 28.17391586303711
[TRAIN] Iter: 778900 Loss: 0.003570345463231206  PSNR: 28.989904403686523
[TRAIN] Iter: 779000 Loss: 0.0036718747578561306  PSNR: 30.17076873779297
[TRAIN] Iter: 779100 Loss: 0.003986596595495939  PSNR: 28.436546325683594
[TRAIN] Iter: 779200 Loss: 0.003760321531444788  PSNR: 30.39154815673828
[TRAIN] Iter: 779300 Loss: 0.0041524493135511875  PSNR: 30.358516693115234
[TRAIN] Iter: 779400 Loss: 0.005039656534790993  PSNR: 28.074663162231445
[TRAIN] Iter: 779500 Loss: 0.004036299884319305  PSNR: 28.434293746948242
[TRAIN] Iter: 779600 Loss: 0.004330590832978487  PSNR: 28.567909240722656
[TRAIN] Iter: 779700 Loss: 0.00512247160077095  PSNR: 27.451017379760742
[TRAIN] Iter: 779800 Loss: 0.003698631888255477  PSNR: 30.12772560119629
[TRAIN] Iter: 779900 Loss: 0.004525457508862019  PSNR: 28.606712341308594
Saved checkpoints at ./logs/TUT-KE101-nerf/780000.tar
[TRAIN] Iter: 780000 Loss: 0.005937628913670778  PSNR: 26.358314514160156
[TRAIN] Iter: 780100 Loss: 0.00410474743694067  PSNR: 28.60883140563965
[TRAIN] Iter: 780200 Loss: 0.005437763873487711  PSNR: 28.363969802856445
[TRAIN] Iter: 780300 Loss: 0.0043835509568452835  PSNR: 28.58061408996582
[TRAIN] Iter: 780400 Loss: 0.00400158204138279  PSNR: 29.911849975585938
[TRAIN] Iter: 780500 Loss: 0.005154043436050415  PSNR: 27.842182159423828
[TRAIN] Iter: 780600 Loss: 0.004976706113666296  PSNR: 28.459144592285156
[TRAIN] Iter: 780700 Loss: 0.0044756666757166386  PSNR: 28.671533584594727
[TRAIN] Iter: 780800 Loss: 0.005457786843180656  PSNR: 26.923446655273438
[TRAIN] Iter: 780900 Loss: 0.004747397731989622  PSNR: 28.25656509399414
[TRAIN] Iter: 781000 Loss: 0.00669492781162262  PSNR: 27.38852882385254
[TRAIN] Iter: 781100 Loss: 0.004551699385046959  PSNR: 28.961101531982422
[TRAIN] Iter: 781200 Loss: 0.004128776490688324  PSNR: 30.014249801635742
[TRAIN] Iter: 781300 Loss: 0.0048898011445999146  PSNR: 27.991477966308594
[TRAIN] Iter: 781400 Loss: 0.00482284976169467  PSNR: 28.5552921295166
[TRAIN] Iter: 781500 Loss: 0.004887696355581284  PSNR: 27.472360610961914
[TRAIN] Iter: 781600 Loss: 0.0041262684389948845  PSNR: 29.011810302734375
[TRAIN] Iter: 781700 Loss: 0.004109898582100868  PSNR: 30.308813095092773
[TRAIN] Iter: 781800 Loss: 0.003839418524876237  PSNR: 29.198116302490234
[TRAIN] Iter: 781900 Loss: 0.005715946666896343  PSNR: 27.3818302154541
[TRAIN] Iter: 782000 Loss: 0.004509972408413887  PSNR: 28.148578643798828
[TRAIN] Iter: 782100 Loss: 0.00475724320858717  PSNR: 27.457548141479492
[TRAIN] Iter: 782200 Loss: 0.004851565696299076  PSNR: 28.652381896972656
[TRAIN] Iter: 782300 Loss: 0.003973814193159342  PSNR: 29.380966186523438
[TRAIN] Iter: 782400 Loss: 0.0033063567243516445  PSNR: 29.559310913085938
[TRAIN] Iter: 782500 Loss: 0.0038891215808689594  PSNR: 28.926969528198242
[TRAIN] Iter: 782600 Loss: 0.003645784454420209  PSNR: 29.98787498474121
[TRAIN] Iter: 782700 Loss: 0.004049078095704317  PSNR: 29.292306900024414
[TRAIN] Iter: 782800 Loss: 0.005180174484848976  PSNR: 28.196531295776367
[TRAIN] Iter: 782900 Loss: 0.004714467562735081  PSNR: 27.471826553344727
[TRAIN] Iter: 783000 Loss: 0.00337670324370265  PSNR: 30.681106567382812
[TRAIN] Iter: 783100 Loss: 0.005070098210126162  PSNR: 27.4638671875
[TRAIN] Iter: 783200 Loss: 0.004781479947268963  PSNR: 28.293214797973633
[TRAIN] Iter: 783300 Loss: 0.005409493111073971  PSNR: 27.672780990600586
[TRAIN] Iter: 783400 Loss: 0.005483213812112808  PSNR: 27.18136215209961
[TRAIN] Iter: 783500 Loss: 0.0047982181422412395  PSNR: 27.722688674926758
[TRAIN] Iter: 783600 Loss: 0.005968119017779827  PSNR: 26.57413101196289
[TRAIN] Iter: 783700 Loss: 0.003985753748565912  PSNR: 29.074907302856445
[TRAIN] Iter: 783800 Loss: 0.004120085388422012  PSNR: 29.934410095214844
[TRAIN] Iter: 783900 Loss: 0.005467189475893974  PSNR: 27.420345306396484
[TRAIN] Iter: 784000 Loss: 0.005501794628798962  PSNR: 28.213090896606445
[TRAIN] Iter: 784100 Loss: 0.0037398033309727907  PSNR: 29.691497802734375
[TRAIN] Iter: 784200 Loss: 0.0046158526092767715  PSNR: 28.524017333984375
[TRAIN] Iter: 784300 Loss: 0.004529976285994053  PSNR: 28.039037704467773
[TRAIN] Iter: 784400 Loss: 0.003740902990102768  PSNR: 29.411794662475586
[TRAIN] Iter: 784500 Loss: 0.005718494765460491  PSNR: 27.518484115600586
[TRAIN] Iter: 784600 Loss: 0.0033250884152948856  PSNR: 30.123334884643555
[TRAIN] Iter: 784700 Loss: 0.003867895808070898  PSNR: 29.369535446166992
[TRAIN] Iter: 784800 Loss: 0.004622966982424259  PSNR: 27.722332000732422
[TRAIN] Iter: 784900 Loss: 0.00517903221771121  PSNR: 28.521257400512695
[TRAIN] Iter: 785000 Loss: 0.005237715318799019  PSNR: 27.748830795288086
[TRAIN] Iter: 785100 Loss: 0.004554910585284233  PSNR: 29.344539642333984
[TRAIN] Iter: 785200 Loss: 0.004801655188202858  PSNR: 28.523473739624023
[TRAIN] Iter: 785300 Loss: 0.004863307345658541  PSNR: 27.710798263549805
[TRAIN] Iter: 785400 Loss: 0.003923283889889717  PSNR: 28.941226959228516
[TRAIN] Iter: 785500 Loss: 0.004397270269691944  PSNR: 28.22018814086914
[TRAIN] Iter: 785600 Loss: 0.004514440894126892  PSNR: 28.663124084472656
[TRAIN] Iter: 785700 Loss: 0.005125316791236401  PSNR: 27.140869140625
[TRAIN] Iter: 785800 Loss: 0.004499363247305155  PSNR: 28.706939697265625
[TRAIN] Iter: 785900 Loss: 0.005667782388627529  PSNR: 28.022991180419922
[TRAIN] Iter: 786000 Loss: 0.005510507617145777  PSNR: 27.123594284057617
[TRAIN] Iter: 786100 Loss: 0.004334354307502508  PSNR: 27.698402404785156
[TRAIN] Iter: 786200 Loss: 0.005001676734536886  PSNR: 28.192922592163086
[TRAIN] Iter: 786300 Loss: 0.0031556827016174793  PSNR: 30.607627868652344
[TRAIN] Iter: 786400 Loss: 0.0054128123447299  PSNR: 27.475616455078125
[TRAIN] Iter: 786500 Loss: 0.003566627623513341  PSNR: 29.8123722076416
[TRAIN] Iter: 786600 Loss: 0.0045173089019954205  PSNR: 27.915302276611328
[TRAIN] Iter: 786700 Loss: 0.003463981207460165  PSNR: 29.693790435791016
[TRAIN] Iter: 786800 Loss: 0.0038133077323436737  PSNR: 30.07610511779785
[TRAIN] Iter: 786900 Loss: 0.0036708060652017593  PSNR: 30.07625389099121
[TRAIN] Iter: 787000 Loss: 0.0042555611580610275  PSNR: 28.660234451293945
[TRAIN] Iter: 787100 Loss: 0.004596174694597721  PSNR: 28.099384307861328
[TRAIN] Iter: 787200 Loss: 0.003985149785876274  PSNR: 29.115243911743164
[TRAIN] Iter: 787300 Loss: 0.0034524034708738327  PSNR: 30.14157485961914
[TRAIN] Iter: 787400 Loss: 0.003778622252866626  PSNR: 29.79273223876953
[TRAIN] Iter: 787500 Loss: 0.0047879028134047985  PSNR: 28.205900192260742
[TRAIN] Iter: 787600 Loss: 0.004624686203896999  PSNR: 27.428630828857422
[TRAIN] Iter: 787700 Loss: 0.004597930237650871  PSNR: 28.419170379638672
[TRAIN] Iter: 787800 Loss: 0.004027949180454016  PSNR: 28.597190856933594
[TRAIN] Iter: 787900 Loss: 0.0046013821847736835  PSNR: 27.743799209594727
[TRAIN] Iter: 788000 Loss: 0.005363944917917252  PSNR: 26.75351905822754
[TRAIN] Iter: 788100 Loss: 0.004482075572013855  PSNR: 28.36033821105957
[TRAIN] Iter: 788200 Loss: 0.00451572285965085  PSNR: 28.25031089782715
[TRAIN] Iter: 788300 Loss: 0.004414608236402273  PSNR: 28.05626678466797
[TRAIN] Iter: 788400 Loss: 0.004389188718050718  PSNR: 28.371871948242188
[TRAIN] Iter: 788500 Loss: 0.005535561125725508  PSNR: 27.848854064941406
[TRAIN] Iter: 788600 Loss: 0.00335888285189867  PSNR: 29.959138870239258
[TRAIN] Iter: 788700 Loss: 0.005158868618309498  PSNR: 27.6068115234375
[TRAIN] Iter: 788800 Loss: 0.0042555914260447025  PSNR: 29.21222496032715
[TRAIN] Iter: 788900 Loss: 0.003606456331908703  PSNR: 30.644357681274414
[TRAIN] Iter: 789000 Loss: 0.0038870954886078835  PSNR: 29.5643367767334
[TRAIN] Iter: 789100 Loss: 0.003713825950399041  PSNR: 30.0727481842041
[TRAIN] Iter: 789200 Loss: 0.004462170414626598  PSNR: 27.524213790893555
[TRAIN] Iter: 789300 Loss: 0.0038321497850120068  PSNR: 30.387216567993164
[TRAIN] Iter: 789400 Loss: 0.003563235979527235  PSNR: 31.096067428588867
[TRAIN] Iter: 789500 Loss: 0.003566003404557705  PSNR: 30.202856063842773
[TRAIN] Iter: 789600 Loss: 0.00337508344091475  PSNR: 30.081575393676758
[TRAIN] Iter: 789700 Loss: 0.00554105918854475  PSNR: 27.576807022094727
[TRAIN] Iter: 789800 Loss: 0.003705571172758937  PSNR: 30.266061782836914
[TRAIN] Iter: 789900 Loss: 0.00411840109154582  PSNR: 29.275920867919922
Saved checkpoints at ./logs/TUT-KE101-nerf/790000.tar
[TRAIN] Iter: 790000 Loss: 0.003830336034297943  PSNR: 29.82769012451172
[TRAIN] Iter: 790100 Loss: 0.005564768798649311  PSNR: 26.953493118286133
[TRAIN] Iter: 790200 Loss: 0.0034010685048997402  PSNR: 30.037195205688477
[TRAIN] Iter: 790300 Loss: 0.005906517617404461  PSNR: 27.202232360839844
[TRAIN] Iter: 790400 Loss: 0.0062754410319030285  PSNR: 26.986099243164062
[TRAIN] Iter: 790500 Loss: 0.004260843154042959  PSNR: 28.838253021240234
[TRAIN] Iter: 790600 Loss: 0.0053942520171403885  PSNR: 27.482500076293945
[TRAIN] Iter: 790700 Loss: 0.003937260247766972  PSNR: 29.244739532470703
[TRAIN] Iter: 790800 Loss: 0.004222323186695576  PSNR: 28.989198684692383
[TRAIN] Iter: 790900 Loss: 0.0035160458646714687  PSNR: 29.73076820373535
[TRAIN] Iter: 791000 Loss: 0.005595293827354908  PSNR: 27.128070831298828
[TRAIN] Iter: 791100 Loss: 0.0034811682999134064  PSNR: 29.99041748046875
[TRAIN] Iter: 791200 Loss: 0.005442291032522917  PSNR: 27.604358673095703
[TRAIN] Iter: 791300 Loss: 0.004672274924814701  PSNR: 28.32200813293457
[TRAIN] Iter: 791400 Loss: 0.004435177892446518  PSNR: 28.21353530883789
[TRAIN] Iter: 791500 Loss: 0.0042601339519023895  PSNR: 28.519018173217773
[TRAIN] Iter: 791600 Loss: 0.004676429554820061  PSNR: 28.135454177856445
[TRAIN] Iter: 791700 Loss: 0.003952839877456427  PSNR: 28.606508255004883
[TRAIN] Iter: 791800 Loss: 0.0053512598387897015  PSNR: 27.675796508789062
[TRAIN] Iter: 791900 Loss: 0.004576667211949825  PSNR: 27.990413665771484
[TRAIN] Iter: 792000 Loss: 0.005082275718450546  PSNR: 27.841238021850586
[TRAIN] Iter: 792100 Loss: 0.004220320377498865  PSNR: 28.384599685668945
[TRAIN] Iter: 792200 Loss: 0.0038922340609133244  PSNR: 28.84059715270996
[TRAIN] Iter: 792300 Loss: 0.004628824535757303  PSNR: 28.461280822753906
[TRAIN] Iter: 792400 Loss: 0.005426968447864056  PSNR: 27.5106201171875
[TRAIN] Iter: 792500 Loss: 0.00437330873683095  PSNR: 29.655467987060547
[TRAIN] Iter: 792600 Loss: 0.003210813505575061  PSNR: 30.96109962463379
[TRAIN] Iter: 792700 Loss: 0.003762813750654459  PSNR: 29.70705795288086
[TRAIN] Iter: 792800 Loss: 0.005162016488611698  PSNR: 27.841773986816406
[TRAIN] Iter: 792900 Loss: 0.004773897118866444  PSNR: 28.307619094848633
[TRAIN] Iter: 793000 Loss: 0.003701501525938511  PSNR: 29.814611434936523
[TRAIN] Iter: 793100 Loss: 0.005272131413221359  PSNR: 27.812652587890625
[TRAIN] Iter: 793200 Loss: 0.003401166060939431  PSNR: 30.10077667236328
[TRAIN] Iter: 793300 Loss: 0.0037209969013929367  PSNR: 28.90485954284668
[TRAIN] Iter: 793400 Loss: 0.005057745613157749  PSNR: 27.845998764038086
[TRAIN] Iter: 793500 Loss: 0.0035413606092333794  PSNR: 29.232425689697266
[TRAIN] Iter: 793600 Loss: 0.0036017494276165962  PSNR: 29.968416213989258
[TRAIN] Iter: 793700 Loss: 0.00550405029207468  PSNR: 27.078767776489258
[TRAIN] Iter: 793800 Loss: 0.004169509746134281  PSNR: 28.3572998046875
[TRAIN] Iter: 793900 Loss: 0.00541863776743412  PSNR: 27.560447692871094
[TRAIN] Iter: 794000 Loss: 0.004536889493465424  PSNR: 29.251020431518555
[TRAIN] Iter: 794100 Loss: 0.004039333201944828  PSNR: 28.63196563720703
[TRAIN] Iter: 794200 Loss: 0.005018588155508041  PSNR: 28.112648010253906
[TRAIN] Iter: 794300 Loss: 0.005220931023359299  PSNR: 27.574840545654297
[TRAIN] Iter: 794400 Loss: 0.0035291346721351147  PSNR: 29.82750129699707
[TRAIN] Iter: 794500 Loss: 0.0049203261733055115  PSNR: 27.59819984436035
[TRAIN] Iter: 794600 Loss: 0.0031794984824955463  PSNR: 30.583417892456055
[TRAIN] Iter: 794700 Loss: 0.004683464765548706  PSNR: 28.239011764526367
[TRAIN] Iter: 794800 Loss: 0.003974403720349073  PSNR: 29.80227279663086
[TRAIN] Iter: 794900 Loss: 0.005136407446116209  PSNR: 27.502790451049805
[TRAIN] Iter: 795000 Loss: 0.004515492357313633  PSNR: 28.36113739013672
[TRAIN] Iter: 795100 Loss: 0.003521527163684368  PSNR: 29.950990676879883
[TRAIN] Iter: 795200 Loss: 0.00477233063429594  PSNR: 28.019962310791016
[TRAIN] Iter: 795300 Loss: 0.005755385383963585  PSNR: 27.55388641357422
[TRAIN] Iter: 795400 Loss: 0.0034869923256337643  PSNR: 29.812471389770508
[TRAIN] Iter: 795500 Loss: 0.004696597345173359  PSNR: 28.3555965423584
[TRAIN] Iter: 795600 Loss: 0.005217075813561678  PSNR: 27.806121826171875
[TRAIN] Iter: 795700 Loss: 0.004622177220880985  PSNR: 27.67457389831543
[TRAIN] Iter: 795800 Loss: 0.004234241787344217  PSNR: 29.133895874023438
[TRAIN] Iter: 795900 Loss: 0.003349589416757226  PSNR: 31.092458724975586
[TRAIN] Iter: 796000 Loss: 0.005178125575184822  PSNR: 28.00297737121582
[TRAIN] Iter: 796100 Loss: 0.005061411764472723  PSNR: 27.62285614013672
[TRAIN] Iter: 796200 Loss: 0.004569400567561388  PSNR: 28.217531204223633
[TRAIN] Iter: 796300 Loss: 0.0032319417223334312  PSNR: 30.576982498168945
[TRAIN] Iter: 796400 Loss: 0.003980380482971668  PSNR: 29.332841873168945
[TRAIN] Iter: 796500 Loss: 0.0042070611380040646  PSNR: 28.41684913635254
[TRAIN] Iter: 796600 Loss: 0.006148128770291805  PSNR: 26.706192016601562
[TRAIN] Iter: 796700 Loss: 0.0038345667999237776  PSNR: 29.867755889892578
[TRAIN] Iter: 796800 Loss: 0.004437018185853958  PSNR: 28.490230560302734
[TRAIN] Iter: 796900 Loss: 0.005538944620639086  PSNR: 27.181081771850586
[TRAIN] Iter: 797000 Loss: 0.003431522287428379  PSNR: 30.81154441833496
[TRAIN] Iter: 797100 Loss: 0.004159771837294102  PSNR: 28.69133758544922
[TRAIN] Iter: 797200 Loss: 0.004514746833592653  PSNR: 28.12050437927246
[TRAIN] Iter: 797300 Loss: 0.004906867630779743  PSNR: 27.37776756286621
[TRAIN] Iter: 797400 Loss: 0.004144554026424885  PSNR: 28.49875831604004
[TRAIN] Iter: 797500 Loss: 0.004441741853952408  PSNR: 28.47733497619629
[TRAIN] Iter: 797600 Loss: 0.004166724160313606  PSNR: 29.40492820739746
[TRAIN] Iter: 797700 Loss: 0.004851558245718479  PSNR: 28.073043823242188
[TRAIN] Iter: 797800 Loss: 0.00528031075373292  PSNR: 27.367027282714844
[TRAIN] Iter: 797900 Loss: 0.0047604055143892765  PSNR: 28.229021072387695
[TRAIN] Iter: 798000 Loss: 0.0043287258595228195  PSNR: 28.66813087463379
[TRAIN] Iter: 798100 Loss: 0.003870728425681591  PSNR: 28.711469650268555
[TRAIN] Iter: 798200 Loss: 0.004144929349422455  PSNR: 28.709186553955078
[TRAIN] Iter: 798300 Loss: 0.005501735955476761  PSNR: 27.450693130493164
[TRAIN] Iter: 798400 Loss: 0.004549543373286724  PSNR: 27.94305419921875
[TRAIN] Iter: 798500 Loss: 0.0038706096820533276  PSNR: 28.699878692626953
[TRAIN] Iter: 798600 Loss: 0.004162152297794819  PSNR: 28.724239349365234
[TRAIN] Iter: 798700 Loss: 0.004835683852434158  PSNR: 27.931352615356445
[TRAIN] Iter: 798800 Loss: 0.005512855015695095  PSNR: 27.332365036010742
[TRAIN] Iter: 798900 Loss: 0.00445521529763937  PSNR: 28.381017684936523
[TRAIN] Iter: 799000 Loss: 0.003560322569683194  PSNR: 28.98929786682129
[TRAIN] Iter: 799100 Loss: 0.004120221361517906  PSNR: 29.76071548461914
[TRAIN] Iter: 799200 Loss: 0.003977374639362097  PSNR: 28.470670700073242
[TRAIN] Iter: 799300 Loss: 0.0037028847727924585  PSNR: 29.97601318359375
[TRAIN] Iter: 799400 Loss: 0.006837966851890087  PSNR: 26.639362335205078
[TRAIN] Iter: 799500 Loss: 0.004098313860595226  PSNR: 30.059833526611328
[TRAIN] Iter: 799600 Loss: 0.004809204488992691  PSNR: 27.810306549072266
[TRAIN] Iter: 799700 Loss: 0.003856135066598654  PSNR: 29.414133071899414
[TRAIN] Iter: 799800 Loss: 0.004489017650485039  PSNR: 28.260316848754883
[TRAIN] Iter: 799900 Loss: 0.005029883701354265  PSNR: 27.877254486083984
Saved checkpoints at ./logs/TUT-KE101-nerf/800000.tar
0 0.00043129920959472656
torch.Size([320, 640, 3]) torch.Size([320, 640])
1 14.197597742080688
2 14.087375402450562
3 14.292033433914185
4 14.069514513015747
5 14.292220830917358
6 14.077763080596924
7 14.225552320480347
8 14.09722900390625
9 14.208326578140259
10 14.220151662826538
11 14.015363693237305
12 14.324641466140747
13 14.061658382415771
14 14.24269986152649
15 14.046501636505127
16 14.192162275314331
17 14.15381908416748
18 14.035077571868896
19 14.268141746520996
20 14.052675724029541
21 14.326609373092651
22 14.103420734405518
23 14.234926223754883
24 14.114546298980713
25 14.181618928909302
26 14.18356466293335
27 14.083844423294067
28 14.299211978912354
29 14.071388006210327
30 14.306878566741943
31 14.064058542251587
32 14.260326623916626
33 14.120024919509888
34 14.151264667510986
35 14.256134748458862
36 14.030688524246216
37 14.284045696258545
38 14.040756464004517
39 14.220781087875366
40 14.081892967224121
41 14.186139822006226
42 14.142436504364014
43 14.050670623779297
44 14.305575847625732
45 14.004533052444458
46 14.286374807357788
47 14.125209331512451
48 14.233484506607056
49 14.088253736495972
50 14.16374158859253
51 14.226970434188843
52 14.023530721664429
53 14.400907278060913
54 14.01302695274353
55 14.246638059616089
56 14.08761715888977
57 14.163170099258423
58 14.150390148162842
59 14.106628656387329
60 14.136075735092163
61 14.049756288528442
62 14.275291919708252
63 14.055273294448853
64 14.314977407455444
65 14.099563598632812
66 14.427746534347534
67 13.963707208633423
68 14.023303270339966
69 14.31002688407898
70 14.002654790878296
71 14.271732330322266
72 14.100170135498047
73 14.29635763168335
74 14.166315793991089
75 14.414156913757324
76 13.97542119026184
77 13.970845460891724
78 14.289505958557129
79 14.097437143325806
80 14.411700010299683
81 13.969752311706543
82 14.147637367248535
83 14.224372386932373
84 14.019431591033936
85 14.369921922683716
86 14.055575132369995
87 14.406059503555298
88 13.830667495727539
89 14.604772090911865
90 13.95139455795288
91 14.33497405052185
92 13.805359601974487
93 13.994226694107056
94 14.549671649932861
95 13.890267372131348
96 14.513968467712402
97 13.843480587005615
98 14.561040163040161
99 13.877119541168213
100 13.961222887039185
101 14.429037094116211
102 13.901204109191895
103 14.590518474578857
104 13.759405612945557
105 14.575742721557617
106 13.861688375473022
107 14.014455556869507
108 14.382165431976318
109 13.980258226394653
110 14.538792133331299
111 13.883090019226074
112 14.506179809570312
113 13.831475257873535
114 14.014737129211426
115 14.586358547210693
116 13.815521717071533
117 14.45828890800476
118 13.846195220947266
119 14.588028192520142
Done, saving (120, 320, 640, 3) (120, 320, 640)
extras:{'raw': tensor([[[ 1.4520e+01,  1.5024e+01,  1.6837e+01, -2.6821e+01],
         [-2.3501e-02, -2.1496e-01, -5.4258e-01, -8.6769e+00],
         [-3.8968e-02, -2.5183e-01, -5.3149e-01,  7.9180e+00],
         ...,
         [ 7.4811e+00,  8.7515e+00,  1.3703e+01,  4.2762e+02],
         [ 9.2777e+00,  1.0773e+01,  1.6155e+01,  3.8120e+02],
         [ 8.0703e+00,  9.6978e+00,  1.6090e+01,  4.6628e+02]],

        [[ 7.9556e-01,  6.7798e-01,  8.8065e-01, -3.8522e+01],
         [-1.8165e+00, -1.8700e+00, -2.2191e+00, -6.5360e+01],
         [-1.8623e+00, -1.9285e+00, -2.3550e+00, -4.7182e+01],
         ...,
         [-1.1503e+00, -1.5126e+00, -2.1041e+00,  5.5803e+01],
         [-3.3752e+00, -3.7664e+00, -4.5307e+00,  2.0950e+01],
         [-3.0277e+00, -3.3440e+00, -4.1005e+00,  1.0596e+00]],

        [[-8.6227e-01, -9.5296e-01, -8.8832e-01, -3.8292e+01],
         [-8.2171e-02, -2.9503e-01, -6.0500e-01, -1.0044e+01],
         [-1.9125e-01, -3.6101e-01, -7.1240e-01,  1.8517e+01],
         ...,
         [ 7.4556e-01, -2.4289e+00, -1.2730e+01,  7.9297e+02],
         [ 5.9680e-01, -2.5145e+00, -1.2571e+01,  8.4896e+02],
         [ 4.4883e-01, -2.6098e+00, -1.2844e+01,  9.1704e+02]],

        ...,

        [[-1.1747e+00, -1.2624e+00, -9.8266e-01, -3.9701e+01],
         [ 2.0517e+00,  2.0679e+00,  2.3553e+00, -2.6342e+01],
         [ 3.9440e-01,  2.4140e-01,  1.1261e-01,  7.9581e+00],
         ...,
         [ 3.0470e+00,  2.4524e+00,  1.1793e+00,  6.8137e+02],
         [ 2.6943e+00,  2.4067e+00,  1.9969e+00,  6.5343e+02],
         [ 3.4599e+00,  3.2663e+00,  3.0745e+00,  6.0105e+02]],

        [[-1.1752e+00, -6.0267e-01,  9.3554e-01, -4.5078e+01],
         [-1.3689e-01,  6.2370e-02,  5.4065e-01, -2.2770e+01],
         [ 1.8375e-01,  2.5082e-01,  4.1210e-01, -1.5828e+01],
         ...,
         [-3.2779e-01, -2.2586e+00, -4.9008e+00,  1.2180e+02],
         [-2.6125e-01, -2.0566e+00, -4.4313e+00,  1.3145e+02],
         [-2.2752e-01, -2.1592e+00, -4.7855e+00,  1.0384e+02]],

        [[ 3.7212e+00,  3.5467e+00,  4.4260e+00, -4.9003e+01],
         [-6.3525e+00, -6.6108e+00, -6.7601e+00, -6.7979e+01],
         [-6.2347e+00, -6.3559e+00, -6.2685e+00, -2.2666e+01],
         ...,
         [-3.8575e+00, -3.7867e+00, -4.1387e+00, -3.0663e+01],
         [-4.1222e+00, -3.9544e+00, -4.2648e+00, -5.2307e+01],
         [-2.5782e+00, -2.3670e+00, -2.5516e+00, -4.1572e+01]]],
       grad_fn=<ReshapeAliasBackward0>), 'rgb0': tensor([[0.4724, 0.4268, 0.3641],
        [0.5696, 0.5709, 0.5885],
        [0.4646, 0.4257, 0.3351],
        ...,
        [0.5610, 0.5297, 0.5103],
        [0.5162, 0.5068, 0.4925],
        [0.4459, 0.2707, 0.0785]], grad_fn=<ReshapeAliasBackward0>), 'disp0': tensor([54.4522, 71.3455, 51.7500,  ..., 53.0868, 51.8613, 37.2560],
       grad_fn=<ReshapeAliasBackward0>), 'acc0': tensor([1., 1., 1.,  ..., 1., 1., 1.], grad_fn=<ReshapeAliasBackward0>), 'z_std': tensor([0.0023, 0.0041, 0.0020,  ..., 0.0026, 0.0018, 0.0028])}
0 0.0005700588226318359
torch.Size([320, 640, 3]) torch.Size([320, 640])
1 14.556660413742065
2 13.835503101348877
3 13.95841383934021
4 14.340206384658813
5 14.064400911331177
6 14.374603748321533
7 13.967663288116455
8 14.331937074661255
9 14.198086738586426
10 14.097655534744263
11 14.274550437927246
12 14.068352460861206
13 14.402625322341919
14 13.976990461349487
15 14.18891429901123
16 14.140460968017578
17 14.108334064483643
18 14.298549175262451
19 14.04089093208313
20 14.39010763168335
21 13.986701011657715
22 14.345356941223145
23 13.92520785331726
24 14.416122198104858
25 13.974987506866455
26 13.953036069869995
27 14.366621732711792
28 13.967816352844238
29 14.43525743484497
30 14.019740343093872
31 14.456553936004639
32 14.023860454559326
33 14.007713556289673
34 14.379704236984253
35 14.017080783843994
36 14.430492401123047
37 13.99057936668396
38 14.441095113754272
39 13.988590717315674
40 14.405474662780762
41 14.0045325756073
42 14.030216693878174
43 14.425538778305054
44 13.969880104064941
45 14.385982275009155
46 13.96865177154541
47 14.416542768478394
48 13.943328619003296
49 13.97150206565857
50 14.452178716659546
51 13.880581617355347
52 14.613049030303955
53 13.861246824264526
54 14.441962242126465
55 13.727437257766724
56 14.028522253036499
57 14.62748098373413
58 13.830293893814087
59 14.52317762374878
60 13.80696702003479
61 14.579581499099731
62 13.864191055297852
63 13.99760913848877
64 14.41521954536438
65 13.935701847076416
66 14.57887077331543
67 13.826151132583618
68 14.593300342559814
69 13.837052583694458
70 14.54202651977539
71 14.081319093704224
72 13.84752106666565
73 14.300152063369751
74 13.901556253433228
75 14.629271745681763
76 13.77484679222107
77 14.509410858154297
78 13.84570574760437
79 13.966009616851807
80 14.440992593765259
81 14.01943325996399
82 14.588853359222412
83 13.980994939804077
84 14.384569883346558
85 13.974383354187012
86 13.925144672393799
87 14.442779779434204
88 14.803338050842285
89 16.42273497581482
90 15.815112590789795
91 16.36654567718506
92 15.893932580947876
93 16.3893404006958
94 15.944570064544678
95 16.414764404296875
96 15.937296867370605
97 16.436200380325317
98 15.957326650619507
99 16.343353271484375
100 15.892846822738647
101 16.395589351654053
102 15.883915662765503
103 16.411920309066772
104 15.903727054595947
105 16.396543264389038
106 15.853218793869019
107 16.42191219329834
108 15.91154432296753
109 16.34387469291687
110 15.887080192565918
111 16.399170398712158
112 15.903725385665894
113 16.366739511489868
114 15.90431261062622
115 16.43325114250183
116 15.907371044158936
117 16.387169361114502
118 15.875568866729736
119 16.325677633285522
test poses shape torch.Size([4, 3, 4])
0 0.0006902217864990234
torch.Size([320, 640, 3]) torch.Size([320, 640])
1 16.446662187576294
2 15.903563737869263
3 16.450522899627686
Saved test set
[TRAIN] Iter: 800000 Loss: 0.0040045203641057014  PSNR: 29.58382225036621
[TRAIN] Iter: 800100 Loss: 0.005246635060757399  PSNR: 28.30389404296875
[TRAIN] Iter: 800200 Loss: 0.004858391359448433  PSNR: 27.985029220581055
[TRAIN] Iter: 800300 Loss: 0.004087802022695541  PSNR: 28.481473922729492
[TRAIN] Iter: 800400 Loss: 0.0033619478344917297  PSNR: 30.55531883239746
[TRAIN] Iter: 800500 Loss: 0.004826235584914684  PSNR: 27.95108985900879
[TRAIN] Iter: 800600 Loss: 0.003915197215974331  PSNR: 29.84367561340332
[TRAIN] Iter: 800700 Loss: 0.0052556125447154045  PSNR: 27.12099838256836
[TRAIN] Iter: 800800 Loss: 0.003087093587964773  PSNR: 29.81195640563965
[TRAIN] Iter: 800900 Loss: 0.005038772244006395  PSNR: 27.81513786315918
[TRAIN] Iter: 801000 Loss: 0.003256872296333313  PSNR: 30.72789764404297
[TRAIN] Iter: 801100 Loss: 0.004213355947285891  PSNR: 29.157791137695312
[TRAIN] Iter: 801200 Loss: 0.004239696078002453  PSNR: 28.910810470581055
[TRAIN] Iter: 801300 Loss: 0.004998663906008005  PSNR: 27.92605972290039
[TRAIN] Iter: 801400 Loss: 0.0046784961596131325  PSNR: 28.19268035888672
[TRAIN] Iter: 801500 Loss: 0.005294244736433029  PSNR: 27.26862907409668
[TRAIN] Iter: 801600 Loss: 0.005838165059685707  PSNR: 27.182632446289062
[TRAIN] Iter: 801700 Loss: 0.004543050657957792  PSNR: 27.715383529663086
[TRAIN] Iter: 801800 Loss: 0.004849914461374283  PSNR: 28.21760368347168
[TRAIN] Iter: 801900 Loss: 0.004194975830614567  PSNR: 28.41476058959961
[TRAIN] Iter: 802000 Loss: 0.0030759687069803476  PSNR: 30.97476577758789
[TRAIN] Iter: 802100 Loss: 0.0035522975958883762  PSNR: 30.69396209716797
[TRAIN] Iter: 802200 Loss: 0.0037631175946444273  PSNR: 28.90041732788086
[TRAIN] Iter: 802300 Loss: 0.004068952519446611  PSNR: 29.73343276977539
[TRAIN] Iter: 802400 Loss: 0.004933470394462347  PSNR: 28.025230407714844
[TRAIN] Iter: 802500 Loss: 0.0043271370232105255  PSNR: 28.752168655395508
[TRAIN] Iter: 802600 Loss: 0.0041708070784807205  PSNR: 28.56273078918457
[TRAIN] Iter: 802700 Loss: 0.0044943722896277905  PSNR: 28.368881225585938
[TRAIN] Iter: 802800 Loss: 0.005492726340889931  PSNR: 27.626296997070312
[TRAIN] Iter: 802900 Loss: 0.0040769269689917564  PSNR: 29.460527420043945
[TRAIN] Iter: 803000 Loss: 0.003922342788428068  PSNR: 29.944297790527344
[TRAIN] Iter: 803100 Loss: 0.005338799208402634  PSNR: 27.843936920166016
[TRAIN] Iter: 803200 Loss: 0.00338350678794086  PSNR: 29.597251892089844
[TRAIN] Iter: 803300 Loss: 0.0047140601091086864  PSNR: 27.852476119995117
[TRAIN] Iter: 803400 Loss: 0.004101910628378391  PSNR: 29.626131057739258
[TRAIN] Iter: 803500 Loss: 0.004788242280483246  PSNR: 28.03797721862793
[TRAIN] Iter: 803600 Loss: 0.004244566895067692  PSNR: 28.70408058166504
[TRAIN] Iter: 803700 Loss: 0.0042648520320653915  PSNR: 28.23737144470215
[TRAIN] Iter: 803800 Loss: 0.004597309976816177  PSNR: 28.59555435180664
[TRAIN] Iter: 803900 Loss: 0.005402427166700363  PSNR: 27.88578224182129
[TRAIN] Iter: 804000 Loss: 0.005892664194107056  PSNR: 26.865543365478516
[TRAIN] Iter: 804100 Loss: 0.003878718940541148  PSNR: 29.334339141845703
[TRAIN] Iter: 804200 Loss: 0.005295578856021166  PSNR: 27.727428436279297
[TRAIN] Iter: 804300 Loss: 0.0034170725848525763  PSNR: 30.135637283325195
[TRAIN] Iter: 804400 Loss: 0.00353682367131114  PSNR: 29.505443572998047
[TRAIN] Iter: 804500 Loss: 0.0036514156963676214  PSNR: 29.944795608520508
[TRAIN] Iter: 804600 Loss: 0.004053084179759026  PSNR: 28.859966278076172
[TRAIN] Iter: 804700 Loss: 0.004847434349358082  PSNR: 28.292539596557617
[TRAIN] Iter: 804800 Loss: 0.0038724695332348347  PSNR: 30.000595092773438
[TRAIN] Iter: 804900 Loss: 0.0035252561792731285  PSNR: 29.61624526977539
[TRAIN] Iter: 805000 Loss: 0.005388303659856319  PSNR: 27.764511108398438
[TRAIN] Iter: 805100 Loss: 0.004667803645133972  PSNR: 28.285078048706055
[TRAIN] Iter: 805200 Loss: 0.003606067504733801  PSNR: 29.18865394592285
[TRAIN] Iter: 805300 Loss: 0.0060394639149308205  PSNR: 27.068254470825195
[TRAIN] Iter: 805400 Loss: 0.0032253556419163942  PSNR: 30.439054489135742
[TRAIN] Iter: 805500 Loss: 0.00483910646289587  PSNR: 27.271202087402344
[TRAIN] Iter: 805600 Loss: 0.005492892116308212  PSNR: 27.499475479125977
[TRAIN] Iter: 805700 Loss: 0.004517686553299427  PSNR: 28.45030975341797
[TRAIN] Iter: 805800 Loss: 0.005478216335177422  PSNR: 27.26711654663086
[TRAIN] Iter: 805900 Loss: 0.003684757510200143  PSNR: 29.855167388916016
[TRAIN] Iter: 806000 Loss: 0.0038279779255390167  PSNR: 29.792570114135742
[TRAIN] Iter: 806100 Loss: 0.005556140094995499  PSNR: 27.213436126708984
[TRAIN] Iter: 806200 Loss: 0.003760864958167076  PSNR: 29.5838623046875
[TRAIN] Iter: 806300 Loss: 0.004654611460864544  PSNR: 28.53782844543457
[TRAIN] Iter: 806400 Loss: 0.005097322631627321  PSNR: 27.696178436279297
[TRAIN] Iter: 806500 Loss: 0.007359020411968231  PSNR: 26.21833610534668
[TRAIN] Iter: 806600 Loss: 0.004429682623594999  PSNR: 27.789777755737305
[TRAIN] Iter: 806700 Loss: 0.0035949295852333307  PSNR: 29.876462936401367
[TRAIN] Iter: 806800 Loss: 0.005588942673057318  PSNR: 27.520410537719727
[TRAIN] Iter: 806900 Loss: 0.0035303947515785694  PSNR: 31.043853759765625
[TRAIN] Iter: 807000 Loss: 0.003964043688029051  PSNR: 30.464479446411133
[TRAIN] Iter: 807100 Loss: 0.004650401882827282  PSNR: 28.548824310302734
[TRAIN] Iter: 807200 Loss: 0.005377156659960747  PSNR: 27.571929931640625
[TRAIN] Iter: 807300 Loss: 0.004470242187380791  PSNR: 29.08505630493164
[TRAIN] Iter: 807400 Loss: 0.003408502321690321  PSNR: 29.59124755859375
[TRAIN] Iter: 807500 Loss: 0.004986813757568598  PSNR: 28.368694305419922
[TRAIN] Iter: 807600 Loss: 0.005678740330040455  PSNR: 27.028287887573242
[TRAIN] Iter: 807700 Loss: 0.003500961000099778  PSNR: 29.729135513305664
[TRAIN] Iter: 807800 Loss: 0.0033336919732391834  PSNR: 29.997507095336914
[TRAIN] Iter: 807900 Loss: 0.004037264734506607  PSNR: 28.937599182128906
[TRAIN] Iter: 808000 Loss: 0.003434322541579604  PSNR: 29.75033187866211
[TRAIN] Iter: 808100 Loss: 0.003682914189994335  PSNR: 29.38212013244629
[TRAIN] Iter: 808200 Loss: 0.006067174486815929  PSNR: 26.867412567138672
[TRAIN] Iter: 808300 Loss: 0.005372592248022556  PSNR: 27.487443923950195
[TRAIN] Iter: 808400 Loss: 0.004861719440668821  PSNR: 27.920135498046875
[TRAIN] Iter: 808500 Loss: 0.00432767067104578  PSNR: 28.413230895996094
[TRAIN] Iter: 808600 Loss: 0.004497363232076168  PSNR: 28.164827346801758
[TRAIN] Iter: 808700 Loss: 0.005426496267318726  PSNR: 27.527904510498047
[TRAIN] Iter: 808800 Loss: 0.004298214800655842  PSNR: 29.513484954833984
[TRAIN] Iter: 808900 Loss: 0.004227958619594574  PSNR: 28.174449920654297
[TRAIN] Iter: 809000 Loss: 0.005638553760945797  PSNR: 27.377042770385742
[TRAIN] Iter: 809100 Loss: 0.004045396111905575  PSNR: 29.508283615112305
[TRAIN] Iter: 809200 Loss: 0.003369253594428301  PSNR: 30.299964904785156
[TRAIN] Iter: 809300 Loss: 0.004116176161915064  PSNR: 30.252193450927734
[TRAIN] Iter: 809400 Loss: 0.004996286239475012  PSNR: 27.603984832763672
[TRAIN] Iter: 809500 Loss: 0.004130584187805653  PSNR: 29.352312088012695
[TRAIN] Iter: 809600 Loss: 0.0046816435642540455  PSNR: 27.798538208007812
[TRAIN] Iter: 809700 Loss: 0.004559194669127464  PSNR: 27.29689598083496
[TRAIN] Iter: 809800 Loss: 0.0036884413566440344  PSNR: 29.85402488708496
[TRAIN] Iter: 809900 Loss: 0.002661786274984479  PSNR: 31.465330123901367
Saved checkpoints at ./logs/TUT-KE101-nerf/810000.tar
[TRAIN] Iter: 810000 Loss: 0.00401204451918602  PSNR: 29.1065673828125
[TRAIN] Iter: 810100 Loss: 0.004612610209733248  PSNR: 28.412229537963867
[TRAIN] Iter: 810200 Loss: 0.003872289089486003  PSNR: 30.242080688476562
[TRAIN] Iter: 810300 Loss: 0.0043197148479521275  PSNR: 28.026573181152344
[TRAIN] Iter: 810400 Loss: 0.0046806978061795235  PSNR: 27.731971740722656
[TRAIN] Iter: 810500 Loss: 0.004572184756398201  PSNR: 27.933528900146484
[TRAIN] Iter: 810600 Loss: 0.004095478914678097  PSNR: 28.247255325317383
[TRAIN] Iter: 810700 Loss: 0.005188061855733395  PSNR: 27.85889434814453
[TRAIN] Iter: 810800 Loss: 0.00516957463696599  PSNR: 28.39768409729004
[TRAIN] Iter: 810900 Loss: 0.003935054410248995  PSNR: 30.699478149414062
[TRAIN] Iter: 811000 Loss: 0.004727490711957216  PSNR: 28.777536392211914
[TRAIN] Iter: 811100 Loss: 0.005321078933775425  PSNR: 27.610361099243164
[TRAIN] Iter: 811200 Loss: 0.005474791396409273  PSNR: 27.612337112426758
[TRAIN] Iter: 811300 Loss: 0.005259825848042965  PSNR: 27.441862106323242
[TRAIN] Iter: 811400 Loss: 0.005304127931594849  PSNR: 27.287324905395508
[TRAIN] Iter: 811500 Loss: 0.005319010466337204  PSNR: 27.628097534179688
[TRAIN] Iter: 811600 Loss: 0.00487101636826992  PSNR: 28.26560401916504
[TRAIN] Iter: 811700 Loss: 0.003543524071574211  PSNR: 29.764347076416016
[TRAIN] Iter: 811800 Loss: 0.0045148106291890144  PSNR: 28.68852424621582
[TRAIN] Iter: 811900 Loss: 0.005582969635725021  PSNR: 27.277809143066406
[TRAIN] Iter: 812000 Loss: 0.006082866340875626  PSNR: 27.213897705078125
[TRAIN] Iter: 812100 Loss: 0.004435303620994091  PSNR: 28.273014068603516
[TRAIN] Iter: 812200 Loss: 0.004013759084045887  PSNR: 28.504974365234375
[TRAIN] Iter: 812300 Loss: 0.0038523152470588684  PSNR: 30.12807273864746
[TRAIN] Iter: 812400 Loss: 0.006184357218444347  PSNR: 26.28772735595703
[TRAIN] Iter: 812500 Loss: 0.00399527233093977  PSNR: 29.05295181274414
[TRAIN] Iter: 812600 Loss: 0.004759324248880148  PSNR: 27.974403381347656
[TRAIN] Iter: 812700 Loss: 0.0060655465349555016  PSNR: 26.760099411010742
[TRAIN] Iter: 812800 Loss: 0.004679699428379536  PSNR: 28.169330596923828
[TRAIN] Iter: 812900 Loss: 0.004584834910929203  PSNR: 28.350475311279297
[TRAIN] Iter: 813000 Loss: 0.004102359060198069  PSNR: 29.65830421447754
[TRAIN] Iter: 813100 Loss: 0.0050775716081261635  PSNR: 28.07160758972168
[TRAIN] Iter: 813200 Loss: 0.004806117154657841  PSNR: 28.184877395629883
[TRAIN] Iter: 813300 Loss: 0.0031182775273919106  PSNR: 30.474395751953125
[TRAIN] Iter: 813400 Loss: 0.0043988218531012535  PSNR: 28.101886749267578
[TRAIN] Iter: 813500 Loss: 0.004403664730489254  PSNR: 28.661592483520508
[TRAIN] Iter: 813600 Loss: 0.0049920473247766495  PSNR: 27.630889892578125
[TRAIN] Iter: 813700 Loss: 0.005300484132021666  PSNR: 28.647024154663086
[TRAIN] Iter: 813800 Loss: 0.0054071517661213875  PSNR: 26.85622787475586
[TRAIN] Iter: 813900 Loss: 0.0046898312866687775  PSNR: 28.29106903076172
[TRAIN] Iter: 814000 Loss: 0.004427316598594189  PSNR: 27.976993560791016
[TRAIN] Iter: 814100 Loss: 0.003786848159506917  PSNR: 28.79766845703125
[TRAIN] Iter: 814200 Loss: 0.0056168437004089355  PSNR: 27.181005477905273
[TRAIN] Iter: 814300 Loss: 0.0046877129934728146  PSNR: 28.372108459472656
[TRAIN] Iter: 814400 Loss: 0.005476882681250572  PSNR: 27.545106887817383
[TRAIN] Iter: 814500 Loss: 0.0036640744656324387  PSNR: 30.050134658813477
[TRAIN] Iter: 814600 Loss: 0.005008491687476635  PSNR: 27.964876174926758
[TRAIN] Iter: 814700 Loss: 0.004114186391234398  PSNR: 29.197731018066406
[TRAIN] Iter: 814800 Loss: 0.004084472078830004  PSNR: 28.75440216064453
[TRAIN] Iter: 814900 Loss: 0.005520418286323547  PSNR: 27.51984977722168
[TRAIN] Iter: 815000 Loss: 0.003304774174466729  PSNR: 30.181020736694336
[TRAIN] Iter: 815100 Loss: 0.0036499262787401676  PSNR: 29.919836044311523
[TRAIN] Iter: 815200 Loss: 0.005003873724490404  PSNR: 28.310216903686523
[TRAIN] Iter: 815300 Loss: 0.004532948136329651  PSNR: 27.814348220825195
[TRAIN] Iter: 815400 Loss: 0.0040091462433338165  PSNR: 29.598743438720703
[TRAIN] Iter: 815500 Loss: 0.005419749766588211  PSNR: 27.289352416992188
[TRAIN] Iter: 815600 Loss: 0.005468329880386591  PSNR: 27.1118106842041
[TRAIN] Iter: 815700 Loss: 0.003518499666824937  PSNR: 29.59332847595215
[TRAIN] Iter: 815800 Loss: 0.004922008141875267  PSNR: 28.657909393310547
[TRAIN] Iter: 815900 Loss: 0.0036441341508179903  PSNR: 29.883968353271484
[TRAIN] Iter: 816000 Loss: 0.0039110505022108555  PSNR: 29.823745727539062
[TRAIN] Iter: 816100 Loss: 0.003589377738535404  PSNR: 29.614013671875
[TRAIN] Iter: 816200 Loss: 0.004912732634693384  PSNR: 28.203256607055664
[TRAIN] Iter: 816300 Loss: 0.004864060319960117  PSNR: 28.011768341064453
[TRAIN] Iter: 816400 Loss: 0.003789324779063463  PSNR: 30.298521041870117
[TRAIN] Iter: 816500 Loss: 0.0040323687717318535  PSNR: 29.480857849121094
[TRAIN] Iter: 816600 Loss: 0.006493670865893364  PSNR: 26.530881881713867
[TRAIN] Iter: 816700 Loss: 0.005002832971513271  PSNR: 28.044355392456055
[TRAIN] Iter: 816800 Loss: 0.004476278088986874  PSNR: 28.397687911987305
[TRAIN] Iter: 816900 Loss: 0.004381516017019749  PSNR: 29.226173400878906
[TRAIN] Iter: 817000 Loss: 0.005731002893298864  PSNR: 27.067846298217773
[TRAIN] Iter: 817100 Loss: 0.005300522781908512  PSNR: 27.88743019104004
[TRAIN] Iter: 817200 Loss: 0.0036924052983522415  PSNR: 30.63176918029785
[TRAIN] Iter: 817300 Loss: 0.004347528796643019  PSNR: 28.385921478271484
[TRAIN] Iter: 817400 Loss: 0.0037826825864613056  PSNR: 30.502073287963867
[TRAIN] Iter: 817500 Loss: 0.004497600253671408  PSNR: 28.3884220123291
[TRAIN] Iter: 817600 Loss: 0.004968668799847364  PSNR: 28.36586570739746
[TRAIN] Iter: 817700 Loss: 0.0037995127495378256  PSNR: 30.292287826538086
[TRAIN] Iter: 817800 Loss: 0.00410130899399519  PSNR: 28.69232177734375
[TRAIN] Iter: 817900 Loss: 0.0040064724162220955  PSNR: 29.04193878173828
[TRAIN] Iter: 818000 Loss: 0.004367065615952015  PSNR: 28.877424240112305
[TRAIN] Iter: 818100 Loss: 0.005795822944492102  PSNR: 27.46662712097168
[TRAIN] Iter: 818200 Loss: 0.0042084320448338985  PSNR: 28.33588218688965
[TRAIN] Iter: 818300 Loss: 0.004984682891517878  PSNR: 27.92105484008789
[TRAIN] Iter: 818400 Loss: 0.004837920889258385  PSNR: 28.143896102905273
[TRAIN] Iter: 818500 Loss: 0.004594775382429361  PSNR: 27.709383010864258
[TRAIN] Iter: 818600 Loss: 0.005034381523728371  PSNR: 28.232576370239258
[TRAIN] Iter: 818700 Loss: 0.0038990434259176254  PSNR: 29.323379516601562
[TRAIN] Iter: 818800 Loss: 0.00383640150539577  PSNR: 29.77267074584961
[TRAIN] Iter: 818900 Loss: 0.004849557299166918  PSNR: 28.332565307617188
[TRAIN] Iter: 819000 Loss: 0.004091811832040548  PSNR: 28.14909553527832
[TRAIN] Iter: 819100 Loss: 0.005210992880165577  PSNR: 27.175413131713867
[TRAIN] Iter: 819200 Loss: 0.005007797386497259  PSNR: 27.649330139160156
[TRAIN] Iter: 819300 Loss: 0.0036663152277469635  PSNR: 31.1690616607666
[TRAIN] Iter: 819400 Loss: 0.004306106362491846  PSNR: 29.40143394470215
[TRAIN] Iter: 819500 Loss: 0.004311838652938604  PSNR: 29.13544273376465
[TRAIN] Iter: 819600 Loss: 0.0033259347546845675  PSNR: 30.80622100830078
[TRAIN] Iter: 819700 Loss: 0.005063539370894432  PSNR: 28.197965621948242
[TRAIN] Iter: 819800 Loss: 0.003949025180190802  PSNR: 30.0518741607666
[TRAIN] Iter: 819900 Loss: 0.005230730399489403  PSNR: 27.493101119995117
Saved checkpoints at ./logs/TUT-KE101-nerf/820000.tar
[TRAIN] Iter: 820000 Loss: 0.005277364049106836  PSNR: 27.365266799926758
[TRAIN] Iter: 820100 Loss: 0.0037828218191862106  PSNR: 29.773189544677734
[TRAIN] Iter: 820200 Loss: 0.004706398583948612  PSNR: 28.111011505126953
[TRAIN] Iter: 820300 Loss: 0.004566097166389227  PSNR: 29.418699264526367
[TRAIN] Iter: 820400 Loss: 0.004796891938894987  PSNR: 28.027706146240234
[TRAIN] Iter: 820500 Loss: 0.004917556419968605  PSNR: 28.214323043823242
[TRAIN] Iter: 820600 Loss: 0.0030662452336400747  PSNR: 30.216407775878906
[TRAIN] Iter: 820700 Loss: 0.0038323283661156893  PSNR: 29.100038528442383
[TRAIN] Iter: 820800 Loss: 0.0038458588533103466  PSNR: 29.279420852661133
[TRAIN] Iter: 820900 Loss: 0.0035389643162488937  PSNR: 30.527069091796875
[TRAIN] Iter: 821000 Loss: 0.0032325705979019403  PSNR: 30.600248336791992
[TRAIN] Iter: 821100 Loss: 0.006182283163070679  PSNR: 26.929096221923828
[TRAIN] Iter: 821200 Loss: 0.0035858340561389923  PSNR: 30.40140151977539
[TRAIN] Iter: 821300 Loss: 0.004065089859068394  PSNR: 28.3226261138916
[TRAIN] Iter: 821400 Loss: 0.004175305832177401  PSNR: 30.210691452026367
[TRAIN] Iter: 821500 Loss: 0.004120357800275087  PSNR: 30.113122940063477
[TRAIN] Iter: 821600 Loss: 0.004016992636024952  PSNR: 28.495359420776367
[TRAIN] Iter: 821700 Loss: 0.005560432095080614  PSNR: 27.685585021972656
[TRAIN] Iter: 821800 Loss: 0.003538308199495077  PSNR: 30.47334861755371
[TRAIN] Iter: 821900 Loss: 0.005253276787698269  PSNR: 27.246801376342773
[TRAIN] Iter: 822000 Loss: 0.004399764351546764  PSNR: 28.252891540527344
[TRAIN] Iter: 822100 Loss: 0.003306399565190077  PSNR: 30.454965591430664
[TRAIN] Iter: 822200 Loss: 0.004383152350783348  PSNR: 28.38808250427246
[TRAIN] Iter: 822300 Loss: 0.003829894121736288  PSNR: 29.45137596130371
[TRAIN] Iter: 822400 Loss: 0.0037698615342378616  PSNR: 29.704360961914062
[TRAIN] Iter: 822500 Loss: 0.003887575352564454  PSNR: 30.379533767700195
[TRAIN] Iter: 822600 Loss: 0.004860132001340389  PSNR: 27.924179077148438
[TRAIN] Iter: 822700 Loss: 0.004700657445937395  PSNR: 28.26905059814453
[TRAIN] Iter: 822800 Loss: 0.004550526384264231  PSNR: 27.255626678466797
[TRAIN] Iter: 822900 Loss: 0.005092065781354904  PSNR: 27.4013729095459
[TRAIN] Iter: 823000 Loss: 0.003951864317059517  PSNR: 28.343137741088867
[TRAIN] Iter: 823100 Loss: 0.005645420402288437  PSNR: 27.522294998168945
[TRAIN] Iter: 823200 Loss: 0.004983119200915098  PSNR: 28.017772674560547
[TRAIN] Iter: 823300 Loss: 0.0037900127936154604  PSNR: 29.345182418823242
[TRAIN] Iter: 823400 Loss: 0.004079374950379133  PSNR: 29.317289352416992
[TRAIN] Iter: 823500 Loss: 0.0033369390293955803  PSNR: 30.182767868041992
[TRAIN] Iter: 823600 Loss: 0.005378959700465202  PSNR: 27.496450424194336
[TRAIN] Iter: 823700 Loss: 0.003876230213791132  PSNR: 29.29515266418457
[TRAIN] Iter: 823800 Loss: 0.005392273887991905  PSNR: 26.72753143310547
[TRAIN] Iter: 823900 Loss: 0.005807544104754925  PSNR: 26.729110717773438
[TRAIN] Iter: 824000 Loss: 0.004630222450941801  PSNR: 28.111825942993164
[TRAIN] Iter: 824100 Loss: 0.005243969149887562  PSNR: 27.70654296875
[TRAIN] Iter: 824200 Loss: 0.005017891060560942  PSNR: 27.651639938354492
[TRAIN] Iter: 824300 Loss: 0.0034087542444467545  PSNR: 30.966821670532227
[TRAIN] Iter: 824400 Loss: 0.004332575481384993  PSNR: 27.860843658447266
[TRAIN] Iter: 824500 Loss: 0.004993848502635956  PSNR: 27.755002975463867
[TRAIN] Iter: 824600 Loss: 0.004759166855365038  PSNR: 28.326982498168945
[TRAIN] Iter: 824700 Loss: 0.004305616021156311  PSNR: 27.655839920043945
[TRAIN] Iter: 824800 Loss: 0.004541265312582254  PSNR: 28.634658813476562
[TRAIN] Iter: 824900 Loss: 0.0034411251544952393  PSNR: 30.280384063720703
[TRAIN] Iter: 825000 Loss: 0.004199806600809097  PSNR: 28.329761505126953
[TRAIN] Iter: 825100 Loss: 0.004875054117292166  PSNR: 27.83720588684082
[TRAIN] Iter: 825200 Loss: 0.0035227315966039896  PSNR: 29.614004135131836
[TRAIN] Iter: 825300 Loss: 0.005302937235683203  PSNR: 28.465402603149414
[TRAIN] Iter: 825400 Loss: 0.004028665367513895  PSNR: 29.45953369140625
[TRAIN] Iter: 825500 Loss: 0.004845485556870699  PSNR: 27.4711856842041
[TRAIN] Iter: 825600 Loss: 0.004344511311501265  PSNR: 28.821321487426758
[TRAIN] Iter: 825700 Loss: 0.00486863125115633  PSNR: 27.353418350219727
[TRAIN] Iter: 825800 Loss: 0.004695349372923374  PSNR: 27.936647415161133
[TRAIN] Iter: 825900 Loss: 0.004836511332541704  PSNR: 28.350278854370117
[TRAIN] Iter: 826000 Loss: 0.005060369148850441  PSNR: 27.55230140686035
[TRAIN] Iter: 826100 Loss: 0.004747990518808365  PSNR: 28.27567481994629
[TRAIN] Iter: 826200 Loss: 0.0034177075140178204  PSNR: 30.761140823364258
[TRAIN] Iter: 826300 Loss: 0.00468807527795434  PSNR: 27.938840866088867
[TRAIN] Iter: 826400 Loss: 0.0037895464338362217  PSNR: 29.521577835083008
[TRAIN] Iter: 826500 Loss: 0.003743818961083889  PSNR: 30.17017936706543
[TRAIN] Iter: 826600 Loss: 0.004152159206569195  PSNR: 28.574880599975586
[TRAIN] Iter: 826700 Loss: 0.004206827841699123  PSNR: 28.6223087310791
[TRAIN] Iter: 826800 Loss: 0.0031335698440670967  PSNR: 31.301513671875
[TRAIN] Iter: 826900 Loss: 0.004232258535921574  PSNR: 28.78843116760254
[TRAIN] Iter: 827000 Loss: 0.003648117883130908  PSNR: 30.021224975585938
[TRAIN] Iter: 827100 Loss: 0.005319847259670496  PSNR: 27.67658233642578
[TRAIN] Iter: 827200 Loss: 0.004273009952157736  PSNR: 29.33912467956543
[TRAIN] Iter: 827300 Loss: 0.003814087947830558  PSNR: 30.854787826538086
[TRAIN] Iter: 827400 Loss: 0.0032337126322090626  PSNR: 30.404359817504883
[TRAIN] Iter: 827500 Loss: 0.004200358875095844  PSNR: 28.467266082763672
[TRAIN] Iter: 827600 Loss: 0.0035212254151701927  PSNR: 29.90955352783203
[TRAIN] Iter: 827700 Loss: 0.003979992121458054  PSNR: 28.37922477722168
[TRAIN] Iter: 827800 Loss: 0.0037340964190661907  PSNR: 29.24016571044922
[TRAIN] Iter: 827900 Loss: 0.0038198595866560936  PSNR: 29.085464477539062
[TRAIN] Iter: 828000 Loss: 0.0051200296729803085  PSNR: 27.526119232177734
[TRAIN] Iter: 828100 Loss: 0.004092592746019363  PSNR: 29.20576286315918
[TRAIN] Iter: 828200 Loss: 0.004860780201852322  PSNR: 28.407258987426758
[TRAIN] Iter: 828300 Loss: 0.00383984437212348  PSNR: 29.39293670654297
[TRAIN] Iter: 828400 Loss: 0.004316185135394335  PSNR: 29.048419952392578
[TRAIN] Iter: 828500 Loss: 0.004264818970113993  PSNR: 28.798542022705078
[TRAIN] Iter: 828600 Loss: 0.004937127698212862  PSNR: 27.447988510131836
[TRAIN] Iter: 828700 Loss: 0.0044060517102479935  PSNR: 28.372940063476562
[TRAIN] Iter: 828800 Loss: 0.00609020609408617  PSNR: 26.773130416870117
[TRAIN] Iter: 828900 Loss: 0.004396442789584398  PSNR: 28.14596176147461
[TRAIN] Iter: 829000 Loss: 0.005203315056860447  PSNR: 27.9185791015625
[TRAIN] Iter: 829100 Loss: 0.0061268555000424385  PSNR: 27.45833969116211
[TRAIN] Iter: 829200 Loss: 0.004295791499316692  PSNR: 28.783172607421875
[TRAIN] Iter: 829300 Loss: 0.004315799102187157  PSNR: 28.22953224182129
[TRAIN] Iter: 829400 Loss: 0.0036061739083379507  PSNR: 29.30284309387207
[TRAIN] Iter: 829500 Loss: 0.004061028361320496  PSNR: 28.37526512145996
[TRAIN] Iter: 829600 Loss: 0.004045153968036175  PSNR: 29.287643432617188
[TRAIN] Iter: 829700 Loss: 0.005366099998354912  PSNR: 27.42290496826172
[TRAIN] Iter: 829800 Loss: 0.003919891081750393  PSNR: 30.183502197265625
[TRAIN] Iter: 829900 Loss: 0.004085898399353027  PSNR: 28.769775390625
Saved checkpoints at ./logs/TUT-KE101-nerf/830000.tar
[TRAIN] Iter: 830000 Loss: 0.0037479009479284286  PSNR: 30.258909225463867
[TRAIN] Iter: 830100 Loss: 0.0034390660002827644  PSNR: 29.876850128173828
[TRAIN] Iter: 830200 Loss: 0.005375918000936508  PSNR: 27.40816307067871
[TRAIN] Iter: 830300 Loss: 0.0048164608888328075  PSNR: 28.26050567626953
[TRAIN] Iter: 830400 Loss: 0.004635822959244251  PSNR: 28.2829647064209
[TRAIN] Iter: 830500 Loss: 0.004819435998797417  PSNR: 28.220497131347656
[TRAIN] Iter: 830600 Loss: 0.00458884984254837  PSNR: 28.389284133911133
[TRAIN] Iter: 830700 Loss: 0.0037453765980899334  PSNR: 29.53919219970703
[TRAIN] Iter: 830800 Loss: 0.005090827122330666  PSNR: 27.657543182373047
[TRAIN] Iter: 830900 Loss: 0.006135484203696251  PSNR: 26.7479190826416
[TRAIN] Iter: 831000 Loss: 0.005400372669100761  PSNR: 27.49927520751953
[TRAIN] Iter: 831100 Loss: 0.005133359227329493  PSNR: 28.13317108154297
[TRAIN] Iter: 831200 Loss: 0.005266797263175249  PSNR: 27.12575340270996
[TRAIN] Iter: 831300 Loss: 0.0047742766328155994  PSNR: 28.17931365966797
[TRAIN] Iter: 831400 Loss: 0.004531866405159235  PSNR: 28.189062118530273
[TRAIN] Iter: 831500 Loss: 0.005149411037564278  PSNR: 27.0624942779541
[TRAIN] Iter: 831600 Loss: 0.005846111569553614  PSNR: 26.732690811157227
[TRAIN] Iter: 831700 Loss: 0.005174380727112293  PSNR: 27.353288650512695
[TRAIN] Iter: 831800 Loss: 0.003961076959967613  PSNR: 29.22185707092285
[TRAIN] Iter: 831900 Loss: 0.003751936601474881  PSNR: 28.51287841796875
[TRAIN] Iter: 832000 Loss: 0.004272878170013428  PSNR: 28.71501922607422
[TRAIN] Iter: 832100 Loss: 0.005363430827856064  PSNR: 27.487003326416016
[TRAIN] Iter: 832200 Loss: 0.004067356698215008  PSNR: 29.010974884033203
[TRAIN] Iter: 832300 Loss: 0.003941281698644161  PSNR: 28.527725219726562
[TRAIN] Iter: 832400 Loss: 0.005148099269717932  PSNR: 28.039560317993164
[TRAIN] Iter: 832500 Loss: 0.004302246496081352  PSNR: 29.704172134399414
[TRAIN] Iter: 832600 Loss: 0.003802215214818716  PSNR: 29.454063415527344
[TRAIN] Iter: 832700 Loss: 0.003463456407189369  PSNR: 29.713552474975586
[TRAIN] Iter: 832800 Loss: 0.005516593344509602  PSNR: 27.32047462463379
[TRAIN] Iter: 832900 Loss: 0.0045654443092644215  PSNR: 28.60765838623047
[TRAIN] Iter: 833000 Loss: 0.004043164663016796  PSNR: 28.5411376953125
[TRAIN] Iter: 833100 Loss: 0.005332238040864468  PSNR: 27.749160766601562
[TRAIN] Iter: 833200 Loss: 0.004875687882304192  PSNR: 28.083967208862305
[TRAIN] Iter: 833300 Loss: 0.004751372151076794  PSNR: 27.495439529418945
[TRAIN] Iter: 833400 Loss: 0.004748905077576637  PSNR: 27.50066375732422
[TRAIN] Iter: 833500 Loss: 0.003861798206344247  PSNR: 29.014116287231445
[TRAIN] Iter: 833600 Loss: 0.00448202807456255  PSNR: 28.522708892822266
[TRAIN] Iter: 833700 Loss: 0.0041221934370696545  PSNR: 28.423465728759766
[TRAIN] Iter: 833800 Loss: 0.004733411595225334  PSNR: 27.764083862304688
[TRAIN] Iter: 833900 Loss: 0.005847637075930834  PSNR: 26.97846221923828
[TRAIN] Iter: 834000 Loss: 0.003651182632893324  PSNR: 29.794340133666992
[TRAIN] Iter: 834100 Loss: 0.0037448310758918524  PSNR: 30.0439453125
[TRAIN] Iter: 834200 Loss: 0.005065447650849819  PSNR: 27.72063636779785
[TRAIN] Iter: 834300 Loss: 0.0034246137365698814  PSNR: 29.93277359008789
[TRAIN] Iter: 834400 Loss: 0.004598094150424004  PSNR: 27.78499412536621
[TRAIN] Iter: 834500 Loss: 0.003909444436430931  PSNR: 28.850154876708984
[TRAIN] Iter: 834600 Loss: 0.004118861630558968  PSNR: 29.171899795532227
[TRAIN] Iter: 834700 Loss: 0.00511076720431447  PSNR: 27.733165740966797
[TRAIN] Iter: 834800 Loss: 0.005345014855265617  PSNR: 27.208694458007812
[TRAIN] Iter: 834900 Loss: 0.00516566401347518  PSNR: 27.43210220336914
[TRAIN] Iter: 835000 Loss: 0.004507085308432579  PSNR: 27.716157913208008
[TRAIN] Iter: 835100 Loss: 0.0050629135221242905  PSNR: 27.642047882080078
[TRAIN] Iter: 835200 Loss: 0.003615614026784897  PSNR: 30.707984924316406
[TRAIN] Iter: 835300 Loss: 0.0048833489418029785  PSNR: 28.149274826049805
[TRAIN] Iter: 835400 Loss: 0.0056246062740683556  PSNR: 26.886045455932617
[TRAIN] Iter: 835500 Loss: 0.005153672304004431  PSNR: 28.106624603271484
[TRAIN] Iter: 835600 Loss: 0.0046487790532410145  PSNR: 28.7358341217041
[TRAIN] Iter: 835700 Loss: 0.005256611853837967  PSNR: 27.729087829589844
[TRAIN] Iter: 835800 Loss: 0.005117758177220821  PSNR: 27.63820457458496
[TRAIN] Iter: 835900 Loss: 0.004287892021238804  PSNR: 28.23410415649414
[TRAIN] Iter: 836000 Loss: 0.004529627971351147  PSNR: 28.465967178344727
[TRAIN] Iter: 836100 Loss: 0.004602400586009026  PSNR: 28.331167221069336
[TRAIN] Iter: 836200 Loss: 0.005728836636990309  PSNR: 26.97719383239746
[TRAIN] Iter: 836300 Loss: 0.003423497546464205  PSNR: 30.417999267578125
[TRAIN] Iter: 836400 Loss: 0.0038548926822841167  PSNR: 29.91423225402832
[TRAIN] Iter: 836500 Loss: 0.0035424907691776752  PSNR: 30.06559181213379
[TRAIN] Iter: 836600 Loss: 0.004042412620037794  PSNR: 28.110727310180664
[TRAIN] Iter: 836700 Loss: 0.006272186525166035  PSNR: 26.44667625427246
[TRAIN] Iter: 836800 Loss: 0.003160144668072462  PSNR: 30.62937355041504
[TRAIN] Iter: 836900 Loss: 0.004102033097296953  PSNR: 28.815269470214844
[TRAIN] Iter: 837000 Loss: 0.004842476919293404  PSNR: 28.329805374145508
[TRAIN] Iter: 837100 Loss: 0.004013117868453264  PSNR: 29.155698776245117
[TRAIN] Iter: 837200 Loss: 0.005155027844011784  PSNR: 28.09250259399414
[TRAIN] Iter: 837300 Loss: 0.0034174243919551373  PSNR: 31.17422103881836
[TRAIN] Iter: 837400 Loss: 0.0035126707516610622  PSNR: 30.178363800048828
[TRAIN] Iter: 837500 Loss: 0.00359065062366426  PSNR: 30.371061325073242
[TRAIN] Iter: 837600 Loss: 0.004632253665477037  PSNR: 28.683273315429688
[TRAIN] Iter: 837700 Loss: 0.004795287735760212  PSNR: 27.56365203857422
[TRAIN] Iter: 837800 Loss: 0.004766674246639013  PSNR: 27.762861251831055
[TRAIN] Iter: 837900 Loss: 0.004862637724727392  PSNR: 27.925249099731445
[TRAIN] Iter: 838000 Loss: 0.005006065592169762  PSNR: 27.64980125427246
[TRAIN] Iter: 838100 Loss: 0.004926944151520729  PSNR: 27.772865295410156
[TRAIN] Iter: 838200 Loss: 0.003551656845957041  PSNR: 29.858295440673828
[TRAIN] Iter: 838300 Loss: 0.003989573568105698  PSNR: 28.992406845092773
[TRAIN] Iter: 838400 Loss: 0.004900461062788963  PSNR: 27.66493034362793
[TRAIN] Iter: 838500 Loss: 0.0037083602510392666  PSNR: 30.234691619873047
[TRAIN] Iter: 838600 Loss: 0.004763878881931305  PSNR: 28.24175262451172
[TRAIN] Iter: 838700 Loss: 0.004400019533932209  PSNR: 28.48369598388672
[TRAIN] Iter: 838800 Loss: 0.004683811217546463  PSNR: 27.751516342163086
[TRAIN] Iter: 838900 Loss: 0.004951441660523415  PSNR: 27.889925003051758
[TRAIN] Iter: 839000 Loss: 0.003982885740697384  PSNR: 29.13951873779297
[TRAIN] Iter: 839100 Loss: 0.0052013518288731575  PSNR: 28.024776458740234
[TRAIN] Iter: 839200 Loss: 0.00489012710750103  PSNR: 27.530691146850586
[TRAIN] Iter: 839300 Loss: 0.003379330039024353  PSNR: 29.906763076782227
[TRAIN] Iter: 839400 Loss: 0.004299520514905453  PSNR: 28.82828140258789
[TRAIN] Iter: 839500 Loss: 0.00371814938262105  PSNR: 30.035423278808594
[TRAIN] Iter: 839600 Loss: 0.004799338057637215  PSNR: 27.714908599853516
[TRAIN] Iter: 839700 Loss: 0.0038511017337441444  PSNR: 29.18768310546875
[TRAIN] Iter: 839800 Loss: 0.004846829455345869  PSNR: 28.91169548034668
[TRAIN] Iter: 839900 Loss: 0.0034631993621587753  PSNR: 30.321569442749023
Saved checkpoints at ./logs/TUT-KE101-nerf/840000.tar
[TRAIN] Iter: 840000 Loss: 0.004281274974346161  PSNR: 28.50908660888672
[TRAIN] Iter: 840100 Loss: 0.00451228953897953  PSNR: 28.422849655151367
[TRAIN] Iter: 840200 Loss: 0.004212896339595318  PSNR: 28.73462677001953
[TRAIN] Iter: 840300 Loss: 0.006021897308528423  PSNR: 26.1549072265625
[TRAIN] Iter: 840400 Loss: 0.004584883339703083  PSNR: 28.084524154663086
[TRAIN] Iter: 840500 Loss: 0.005349432583898306  PSNR: 27.203941345214844
[TRAIN] Iter: 840600 Loss: 0.0038736413698643446  PSNR: 28.276723861694336
[TRAIN] Iter: 840700 Loss: 0.0039847237057983875  PSNR: 29.506729125976562
[TRAIN] Iter: 840800 Loss: 0.00542762316763401  PSNR: 27.86992835998535
[TRAIN] Iter: 840900 Loss: 0.004835927858948708  PSNR: 27.2208194732666
[TRAIN] Iter: 841000 Loss: 0.0044366829097270966  PSNR: 27.721851348876953
[TRAIN] Iter: 841100 Loss: 0.004466626793146133  PSNR: 28.172475814819336
[TRAIN] Iter: 841200 Loss: 0.004252759274095297  PSNR: 28.28568458557129
[TRAIN] Iter: 841300 Loss: 0.003490020986646414  PSNR: 29.93979263305664
[TRAIN] Iter: 841400 Loss: 0.004059553612023592  PSNR: 29.298343658447266
[TRAIN] Iter: 841500 Loss: 0.0033980293665081263  PSNR: 30.315183639526367
[TRAIN] Iter: 841600 Loss: 0.005516652949154377  PSNR: 27.991600036621094
[TRAIN] Iter: 841700 Loss: 0.0057896701619029045  PSNR: 27.71759796142578
[TRAIN] Iter: 841800 Loss: 0.003818156663328409  PSNR: 28.397193908691406
[TRAIN] Iter: 841900 Loss: 0.00354229798540473  PSNR: 30.07254981994629
[TRAIN] Iter: 842000 Loss: 0.0033656719606369734  PSNR: 30.981224060058594
[TRAIN] Iter: 842100 Loss: 0.004933769814670086  PSNR: 28.454416275024414
[TRAIN] Iter: 842200 Loss: 0.004818481393158436  PSNR: 27.695951461791992
[TRAIN] Iter: 842300 Loss: 0.0041434187442064285  PSNR: 29.474580764770508
[TRAIN] Iter: 842400 Loss: 0.003921720199286938  PSNR: 28.689292907714844
[TRAIN] Iter: 842500 Loss: 0.005017215386033058  PSNR: 28.2432861328125
[TRAIN] Iter: 842600 Loss: 0.004483136348426342  PSNR: 28.70574188232422
[TRAIN] Iter: 842700 Loss: 0.0041595883667469025  PSNR: 30.1777400970459
[TRAIN] Iter: 842800 Loss: 0.004966691602021456  PSNR: 28.347734451293945
[TRAIN] Iter: 842900 Loss: 0.004653538577258587  PSNR: 28.231285095214844
[TRAIN] Iter: 843000 Loss: 0.004484373144805431  PSNR: 28.265966415405273
[TRAIN] Iter: 843100 Loss: 0.005353941582143307  PSNR: 27.428709030151367
[TRAIN] Iter: 843200 Loss: 0.005672055296599865  PSNR: 26.76378631591797
[TRAIN] Iter: 843300 Loss: 0.004065298475325108  PSNR: 28.447071075439453
[TRAIN] Iter: 843400 Loss: 0.0036842881236225367  PSNR: 29.34552574157715
[TRAIN] Iter: 843500 Loss: 0.00420682318508625  PSNR: 28.300514221191406
[TRAIN] Iter: 843600 Loss: 0.004085821099579334  PSNR: 29.53282356262207
[TRAIN] Iter: 843700 Loss: 0.005201399326324463  PSNR: 27.541513442993164
[TRAIN] Iter: 843800 Loss: 0.004432918503880501  PSNR: 27.83590316772461
[TRAIN] Iter: 843900 Loss: 0.004121876321732998  PSNR: 29.8720760345459
[TRAIN] Iter: 844000 Loss: 0.005510872229933739  PSNR: 27.45399284362793
[TRAIN] Iter: 844100 Loss: 0.005095686763525009  PSNR: 27.527868270874023
[TRAIN] Iter: 844200 Loss: 0.004745694808661938  PSNR: 28.183734893798828
[TRAIN] Iter: 844300 Loss: 0.005628845654428005  PSNR: 27.670223236083984
[TRAIN] Iter: 844400 Loss: 0.0039940387941896915  PSNR: 29.553688049316406
[TRAIN] Iter: 844500 Loss: 0.005103610455989838  PSNR: 28.40697479248047
[TRAIN] Iter: 844600 Loss: 0.005495807155966759  PSNR: 28.072742462158203
[TRAIN] Iter: 844700 Loss: 0.004429548047482967  PSNR: 28.739465713500977
[TRAIN] Iter: 844800 Loss: 0.002831460675224662  PSNR: 30.994491577148438
[TRAIN] Iter: 844900 Loss: 0.003993075806647539  PSNR: 29.83237075805664
[TRAIN] Iter: 845000 Loss: 0.005370375234633684  PSNR: 28.058149337768555
[TRAIN] Iter: 845100 Loss: 0.004029370378702879  PSNR: 28.88738250732422
[TRAIN] Iter: 845200 Loss: 0.005157659761607647  PSNR: 27.630176544189453
[TRAIN] Iter: 845300 Loss: 0.0033671888522803783  PSNR: 30.06190299987793
[TRAIN] Iter: 845400 Loss: 0.004876038059592247  PSNR: 27.50970458984375
[TRAIN] Iter: 845500 Loss: 0.004385235253721476  PSNR: 27.75299072265625
[TRAIN] Iter: 845600 Loss: 0.0054036774672567844  PSNR: 28.51422882080078
[TRAIN] Iter: 845700 Loss: 0.0032998332753777504  PSNR: 30.978504180908203
[TRAIN] Iter: 845800 Loss: 0.0043834177777171135  PSNR: 29.535367965698242
[TRAIN] Iter: 845900 Loss: 0.005006477236747742  PSNR: 27.535247802734375
[TRAIN] Iter: 846000 Loss: 0.003750408301129937  PSNR: 29.711849212646484
[TRAIN] Iter: 846100 Loss: 0.005010283552110195  PSNR: 28.246980667114258
[TRAIN] Iter: 846200 Loss: 0.005625005345791578  PSNR: 26.693153381347656
[TRAIN] Iter: 846300 Loss: 0.0032357240561395884  PSNR: 30.838973999023438
[TRAIN] Iter: 846400 Loss: 0.004641429986804724  PSNR: 28.308725357055664
[TRAIN] Iter: 846500 Loss: 0.005209464579820633  PSNR: 27.491436004638672
[TRAIN] Iter: 846600 Loss: 0.0036401720717549324  PSNR: 30.450151443481445
[TRAIN] Iter: 846700 Loss: 0.004418054595589638  PSNR: 28.363933563232422
[TRAIN] Iter: 846800 Loss: 0.004715793766081333  PSNR: 28.021211624145508
[TRAIN] Iter: 846900 Loss: 0.004731537774205208  PSNR: 27.729263305664062
[TRAIN] Iter: 847000 Loss: 0.004045161884278059  PSNR: 29.649686813354492
[TRAIN] Iter: 847100 Loss: 0.004959926474839449  PSNR: 27.56167984008789
[TRAIN] Iter: 847200 Loss: 0.003987164236605167  PSNR: 28.927444458007812
[TRAIN] Iter: 847300 Loss: 0.004597289487719536  PSNR: 28.740163803100586
[TRAIN] Iter: 847400 Loss: 0.0036038062535226345  PSNR: 29.471607208251953
[TRAIN] Iter: 847500 Loss: 0.004314516671001911  PSNR: 28.79850959777832
[TRAIN] Iter: 847600 Loss: 0.005589132662862539  PSNR: 27.136255264282227
[TRAIN] Iter: 847700 Loss: 0.0038444409146904945  PSNR: 29.157569885253906
[TRAIN] Iter: 847800 Loss: 0.0044221431016922  PSNR: 27.945777893066406
[TRAIN] Iter: 847900 Loss: 0.003760099411010742  PSNR: 29.990543365478516
[TRAIN] Iter: 848000 Loss: 0.005811575334519148  PSNR: 27.040359497070312
[TRAIN] Iter: 848100 Loss: 0.005901736207306385  PSNR: 27.572988510131836
[TRAIN] Iter: 848200 Loss: 0.004739466588944197  PSNR: 28.67552947998047
[TRAIN] Iter: 848300 Loss: 0.004376848228275776  PSNR: 28.65141487121582
[TRAIN] Iter: 848400 Loss: 0.004088441841304302  PSNR: 30.48722267150879
[TRAIN] Iter: 848500 Loss: 0.003964493982493877  PSNR: 29.227718353271484
[TRAIN] Iter: 848600 Loss: 0.003293032757937908  PSNR: 30.841434478759766
[TRAIN] Iter: 848700 Loss: 0.003982079681009054  PSNR: 29.46463394165039
[TRAIN] Iter: 848800 Loss: 0.00495326379314065  PSNR: 28.551647186279297
[TRAIN] Iter: 848900 Loss: 0.005341678392142057  PSNR: 26.786584854125977
[TRAIN] Iter: 849000 Loss: 0.004587823990732431  PSNR: 28.77886199951172
[TRAIN] Iter: 849100 Loss: 0.003117326647043228  PSNR: 31.242210388183594
[TRAIN] Iter: 849200 Loss: 0.0036287710536271334  PSNR: 29.976219177246094
[TRAIN] Iter: 849300 Loss: 0.003763774409890175  PSNR: 29.964529037475586
[TRAIN] Iter: 849400 Loss: 0.003884621663019061  PSNR: 29.80840492248535
[TRAIN] Iter: 849500 Loss: 0.003934354055672884  PSNR: 28.35948371887207
[TRAIN] Iter: 849600 Loss: 0.0057683661580085754  PSNR: 27.051490783691406
[TRAIN] Iter: 849700 Loss: 0.003537046490237117  PSNR: 29.971450805664062
[TRAIN] Iter: 849800 Loss: 0.004253390245139599  PSNR: 28.07769775390625
[TRAIN] Iter: 849900 Loss: 0.004884054884314537  PSNR: 27.802133560180664
Saved checkpoints at ./logs/TUT-KE101-nerf/850000.tar
0 0.0004391670227050781
torch.Size([320, 640, 3]) torch.Size([320, 640])
1 13.967514276504517
2 14.532660722732544
3 13.517993688583374
4 14.923691749572754
5 13.607079029083252
6 13.890389919281006
7 14.832121133804321
8 13.881131172180176
9 14.495223760604858
10 13.728058099746704
11 14.573721408843994
12 13.82234239578247
13 13.793201923370361
14 14.599339962005615
15 13.790568113327026
16 14.67597222328186
17 13.785161972045898
18 14.630847454071045
19 13.826376914978027
20 13.85695195198059
21 14.554837703704834
22 13.76735234260559
23 14.665928840637207
24 13.725820302963257
25 14.640902280807495
26 13.725822925567627
27 13.857526540756226
28 14.774113655090332
29 13.712112188339233
30 14.684011936187744
31 13.708230972290039
32 14.592779159545898
33 13.865694522857666
34 13.715680599212646
35 14.669790029525757
36 13.786375284194946
37 14.80802845954895
38 13.645526647567749
39 14.760956764221191
40 13.766730308532715
41 13.682780265808105
42 14.8500394821167
43 13.560029745101929
44 14.909295082092285
45 13.50603199005127
46 14.796470165252686
47 13.798095464706421
48 13.624597549438477
49 14.861597299575806
50 13.622815608978271
51 14.904510974884033
52 13.668127059936523
53 14.646147727966309
54 13.803379774093628
55 13.74760365486145
56 14.641166925430298
57 13.767982721328735
58 14.67739987373352
59 13.749890804290771
60 14.648791551589966
61 13.816925764083862
62 13.773832082748413
63 14.616018533706665
64 13.710123062133789
65 14.680347919464111
66 14.087805271148682
67 15.005131006240845
68 14.090085744857788
69 14.061781167984009
70 15.00319766998291
71 13.95242190361023
72 14.939117908477783
73 14.062064409255981
74 14.920441389083862
75 14.105205774307251
76 14.01924991607666
77 14.947605609893799
78 14.0078866481781
79 14.991097688674927
80 13.929929733276367
81 15.081075668334961
82 14.117858648300171
83 14.151825189590454
84 14.74089503288269
85 14.052575826644897
86 15.005842447280884
87 13.939197063446045
88 15.10697865486145
89 14.052386283874512
90 14.930154085159302
91 14.06657099723816
92 14.034454345703125
93 15.004575252532959
94 14.041675806045532
95 14.913879871368408
96 14.047441720962524
97 14.96765398979187
98 14.012699842453003
99 14.038763046264648
100 15.000061988830566
101 14.017499923706055
102 15.006701231002808
103 14.01019024848938
104 15.054708480834961
105 13.897859811782837
106 14.96789836883545
107 14.176055669784546
108 13.944793701171875
109 15.057060480117798
110 13.891290426254272
111 15.183985948562622
112 13.880099296569824
113 15.039227485656738
114 14.081234455108643
115 13.860366582870483
116 15.195792198181152
117 13.800608396530151
118 15.251227140426636
119 13.781222343444824
Done, saving (120, 320, 640, 3) (120, 320, 640)
extras:{'raw': tensor([[[ 1.3492e-01, -5.2866e-03, -5.7172e-01, -4.1335e+01],
         [ 6.3087e-01,  4.0716e-01, -1.9667e-01, -3.0934e+01],
         [ 8.6164e-01,  5.1206e-01, -2.7119e-01, -2.9693e+01],
         ...,
         [-2.5207e+01, -1.9577e+01, -1.6346e+01, -1.7313e+02],
         [-2.3844e+01, -1.8311e+01, -1.5634e+01, -1.9762e+02],
         [-2.3025e+01, -1.8433e+01, -1.6428e+01, -1.8757e+02]],

        [[ 6.1728e+00,  7.0979e+00,  9.1664e+00, -7.2558e+01],
         [ 3.2487e-02,  4.6715e-01,  1.5856e+00, -2.4001e+01],
         [ 8.1798e-03,  4.2687e-01,  1.5059e+00, -2.2551e+01],
         ...,
         [ 1.1432e+02,  1.1477e+02,  1.2855e+02, -1.1739e+02],
         [ 1.2099e+02,  1.2029e+02,  1.3289e+02, -1.3794e+02],
         [ 9.7094e+01,  9.5279e+01,  1.0046e+02, -5.2651e+01]],

        [[-1.4425e+00, -8.4269e-01,  7.3357e-02, -7.7815e+01],
         [-2.0339e-01, -8.8151e-01, -2.3952e+00, -3.4511e+01],
         [-7.6801e-01, -1.0620e+00, -1.6462e+00,  3.1923e+01],
         ...,
         [-1.3919e+00, -2.8174e+00, -4.8753e+00,  8.2851e+01],
         [-1.6149e+00, -3.1027e+00, -5.2675e+00,  9.3478e+01],
         [-9.0010e-01, -2.2979e+00, -4.2341e+00,  1.2063e+02]],

        ...,

        [[-5.2742e-01, -1.2200e-01,  1.2882e+00, -4.7700e+01],
         [-7.1127e-02, -8.2547e-02, -4.7618e-02, -5.1985e+01],
         [-6.9180e-02, -7.2282e-02, -1.3337e-02, -5.2791e+01],
         ...,
         [-2.8730e+00, -3.0307e+00, -1.2180e+01,  8.3550e+02],
         [-2.2595e+00, -2.9743e+00, -1.3433e+01,  9.1900e+02],
         [-2.8335e+00, -3.0704e+00, -1.2794e+01,  9.6235e+02]],

        [[-1.0254e+01, -1.0508e+01, -1.0910e+01, -6.4707e+01],
         [-8.1655e-02, -3.4808e-02, -1.1250e-03, -3.3178e+01],
         [-3.0976e-01, -8.1153e-01, -2.2116e+00,  4.0985e+00],
         ...,
         [-9.0447e+00, -8.5141e+00, -8.9828e+00, -1.9298e+02],
         [-7.4376e+00, -6.9934e+00, -7.9124e+00, -1.2787e+02],
         [-8.5325e+00, -8.5527e+00, -9.6394e+00, -1.4716e+02]],

        [[-1.2202e+00, -1.6795e+00, -2.4467e+00, -7.5133e+01],
         [-1.5263e+00, -1.7438e+00, -2.0980e+00, -3.3016e+01],
         [ 1.9362e-01, -3.6354e-01, -1.3622e+00, -8.7486e+00],
         ...,
         [-8.0601e+00, -8.1460e+00, -9.2331e+00, -2.1602e+02],
         [-8.3131e+00, -7.9257e+00, -8.3691e+00, -1.9004e+02],
         [-6.3665e+00, -6.0568e+00, -6.5253e+00, -1.4659e+02]]],
       grad_fn=<ReshapeAliasBackward0>), 'rgb0': tensor([[0.0525, 0.0553, 0.0488],
        [0.5071, 0.4722, 0.4573],
        [0.3918, 0.3017, 0.1610],
        ...,
        [0.4718, 0.4375, 0.3878],
        [0.4146, 0.3108, 0.1092],
        [0.2690, 0.2016, 0.1001]], grad_fn=<ReshapeAliasBackward0>), 'disp0': tensor([ 46.8315, 120.4368,  56.1571,  ..., 250.5798,  56.4385,  68.3805],
       grad_fn=<ReshapeAliasBackward0>), 'acc0': tensor([1., 1., 1.,  ..., 1., 1., 1.], grad_fn=<ReshapeAliasBackward0>), 'z_std': tensor([0.0029, 0.0021, 0.0010,  ..., 0.0023, 0.0025, 0.0021])}
0 0.0004677772521972656
torch.Size([320, 640, 3]) torch.Size([320, 640])
1 13.816886901855469
2 15.111672401428223
3 14.01088571548462
4 13.968504905700684
5 15.167186260223389
6 13.865606307983398
7 15.041762828826904
8 13.7584867477417
9 14.85141658782959
10 13.764065265655518
11 13.656907796859741
12 14.870160579681396
13 13.621920108795166
14 14.927544116973877
15 13.617090940475464
16 14.750792980194092
17 13.66653299331665
18 13.740678310394287
19 14.853676557540894
20 13.585942268371582
21 14.889689683914185
22 13.569854497909546
23 14.854736804962158
24 13.610068559646606
25 13.728811740875244
26 14.744890928268433
27 13.761118173599243
28 14.939594507217407
29 13.478816032409668
30 14.935487270355225
31 13.549572467803955
32 13.92592978477478
33 14.621904611587524
34 13.818404912948608
35 14.880800008773804
36 13.500384330749512
37 14.178059101104736
38 14.131521701812744
39 14.115353345870972
40 14.363993406295776
41 14.107500076293945
42 14.718404769897461
43 13.557348728179932
44 14.27277398109436
45 14.069746732711792
46 14.183515071868896
47 14.325904369354248
48 14.139631986618042
49 14.718135833740234
50 14.437881231307983
51 14.197233438491821
52 13.366832971572876
53 14.062313795089722
54 14.937392950057983
55 13.744932889938354
56 14.570066213607788
57 13.658145666122437
58 14.683088541030884
59 13.826873302459717
60 13.924081087112427
61 14.569198608398438
62 13.823176622390747
63 14.520773887634277
64 13.921683549880981
65 14.616581439971924
66 13.772189378738403
67 13.97053074836731
68 14.553583860397339
69 13.7691330909729
70 14.61878752708435
71 13.832824945449829
72 14.663424253463745
73 13.804133653640747
74 13.813754081726074
75 14.586174726486206
76 13.919655323028564
77 14.603426456451416
78 13.808857679367065
79 14.569899797439575
80 13.77286148071289
81 13.838042497634888
82 14.600295543670654
83 13.824798107147217
84 14.758638858795166
85 13.576288223266602
86 14.854790210723877
87 13.60846996307373
88 13.852418422698975
89 14.705805540084839
90 13.696998119354248
91 14.934540510177612
92 13.588732242584229
93 14.735881328582764
94 13.618004560470581
95 13.82398796081543
96 14.986881017684937
97 13.64278793334961
98 14.438579797744751
99 15.512002229690552
100 16.7980797290802
101 15.608439683914185
102 16.64686942100525
103 15.540422201156616
104 16.79240107536316
105 15.700525999069214
106 16.57321834564209
107 15.632029056549072
108 16.577288150787354
109 15.759151935577393
110 16.720781087875366
111 15.954038858413696
112 16.08261799812317
113 17.061819076538086
114 16.17639684677124
115 17.14387273788452
116 16.01595449447632
117 16.751092195510864
118 15.760321617126465
119 16.798267602920532
test poses shape torch.Size([4, 3, 4])
0 0.0005636215209960938
torch.Size([320, 640, 3]) torch.Size([320, 640])
1 16.07081151008606
2 16.99784755706787
3 16.09999108314514
Saved test set
[TRAIN] Iter: 850000 Loss: 0.005422702990472317  PSNR: 27.442628860473633
[TRAIN] Iter: 850100 Loss: 0.004457922652363777  PSNR: 28.43272590637207
[TRAIN] Iter: 850200 Loss: 0.005034053698182106  PSNR: 28.07339096069336
[TRAIN] Iter: 850300 Loss: 0.0060025835409760475  PSNR: 26.732131958007812
[TRAIN] Iter: 850400 Loss: 0.0044232201762497425  PSNR: 28.488245010375977
[TRAIN] Iter: 850500 Loss: 0.0035868745762854815  PSNR: 29.81431007385254
[TRAIN] Iter: 850600 Loss: 0.004159126430749893  PSNR: 28.826295852661133
[TRAIN] Iter: 850700 Loss: 0.004600320942699909  PSNR: 28.608108520507812
[TRAIN] Iter: 850800 Loss: 0.003724269801750779  PSNR: 30.395471572875977
[TRAIN] Iter: 850900 Loss: 0.004216658882796764  PSNR: 29.022781372070312
[TRAIN] Iter: 851000 Loss: 0.004419258330017328  PSNR: 28.355972290039062
[TRAIN] Iter: 851100 Loss: 0.004469981882721186  PSNR: 28.671560287475586
[TRAIN] Iter: 851200 Loss: 0.0052270470187067986  PSNR: 28.4171142578125
[TRAIN] Iter: 851300 Loss: 0.005818234756588936  PSNR: 27.209388732910156
[TRAIN] Iter: 851400 Loss: 0.005609325133264065  PSNR: 27.179323196411133
[TRAIN] Iter: 851500 Loss: 0.004823720082640648  PSNR: 27.974138259887695
[TRAIN] Iter: 851600 Loss: 0.004545397590845823  PSNR: 27.409767150878906
[TRAIN] Iter: 851700 Loss: 0.004698426462709904  PSNR: 28.17003631591797
[TRAIN] Iter: 851800 Loss: 0.004067169036716223  PSNR: 28.679929733276367
[TRAIN] Iter: 851900 Loss: 0.005325259640812874  PSNR: 27.932552337646484
[TRAIN] Iter: 852000 Loss: 0.004396744538098574  PSNR: 28.576438903808594
[TRAIN] Iter: 852100 Loss: 0.004795100074261427  PSNR: 28.17207145690918
[TRAIN] Iter: 852200 Loss: 0.0037727192975580692  PSNR: 30.224681854248047
[TRAIN] Iter: 852300 Loss: 0.0036371909081935883  PSNR: 29.905981063842773
[TRAIN] Iter: 852400 Loss: 0.003855253802612424  PSNR: 29.327123641967773
[TRAIN] Iter: 852500 Loss: 0.004044177010655403  PSNR: 28.44593048095703
[TRAIN] Iter: 852600 Loss: 0.004479282535612583  PSNR: 28.123586654663086
[TRAIN] Iter: 852700 Loss: 0.0044603352434933186  PSNR: 28.58193588256836
[TRAIN] Iter: 852800 Loss: 0.004373262170702219  PSNR: 28.00902557373047
[TRAIN] Iter: 852900 Loss: 0.005913999397307634  PSNR: 27.513038635253906
[TRAIN] Iter: 853000 Loss: 0.0033080331049859524  PSNR: 29.925106048583984
[TRAIN] Iter: 853100 Loss: 0.005672640632838011  PSNR: 27.78086280822754
[TRAIN] Iter: 853200 Loss: 0.005100196693092585  PSNR: 27.244720458984375
[TRAIN] Iter: 853300 Loss: 0.005458102561533451  PSNR: 27.641529083251953
[TRAIN] Iter: 853400 Loss: 0.004502801224589348  PSNR: 28.228872299194336
[TRAIN] Iter: 853500 Loss: 0.0035758567973971367  PSNR: 30.798294067382812
[TRAIN] Iter: 853600 Loss: 0.004601435735821724  PSNR: 27.83241081237793
[TRAIN] Iter: 853700 Loss: 0.005897222552448511  PSNR: 27.04574966430664
[TRAIN] Iter: 853800 Loss: 0.005568472668528557  PSNR: 27.617185592651367
[TRAIN] Iter: 853900 Loss: 0.005512497387826443  PSNR: 27.3806095123291
[TRAIN] Iter: 854000 Loss: 0.0056347656063735485  PSNR: 26.902568817138672
[TRAIN] Iter: 854100 Loss: 0.00579115841537714  PSNR: 27.37735366821289
[TRAIN] Iter: 854200 Loss: 0.0046894424594938755  PSNR: 27.830379486083984
[TRAIN] Iter: 854300 Loss: 0.004252665676176548  PSNR: 28.23649024963379
[TRAIN] Iter: 854400 Loss: 0.004144381731748581  PSNR: 28.108049392700195
[TRAIN] Iter: 854500 Loss: 0.004825718235224485  PSNR: 27.970664978027344
[TRAIN] Iter: 854600 Loss: 0.003672756487503648  PSNR: 29.3168888092041
[TRAIN] Iter: 854700 Loss: 0.005591794848442078  PSNR: 27.310068130493164
[TRAIN] Iter: 854800 Loss: 0.0038243927992880344  PSNR: 29.704980850219727
[TRAIN] Iter: 854900 Loss: 0.005535148549824953  PSNR: 26.828956604003906
[TRAIN] Iter: 855000 Loss: 0.0036046409513801336  PSNR: 30.290103912353516
[TRAIN] Iter: 855100 Loss: 0.0036339040379971266  PSNR: 30.43524742126465
[TRAIN] Iter: 855200 Loss: 0.004627795424312353  PSNR: 28.312400817871094
[TRAIN] Iter: 855300 Loss: 0.005623077042400837  PSNR: 27.286909103393555
[TRAIN] Iter: 855400 Loss: 0.005587845109403133  PSNR: 27.718896865844727
[TRAIN] Iter: 855500 Loss: 0.004269015043973923  PSNR: 27.904451370239258
[TRAIN] Iter: 855600 Loss: 0.0036284932866692543  PSNR: 29.654281616210938
[TRAIN] Iter: 855700 Loss: 0.00520405313000083  PSNR: 27.234172821044922
[TRAIN] Iter: 855800 Loss: 0.0037391697987914085  PSNR: 30.10296630859375
[TRAIN] Iter: 855900 Loss: 0.005025822203606367  PSNR: 27.994718551635742
[TRAIN] Iter: 856000 Loss: 0.0037379281129688025  PSNR: 29.911678314208984
[TRAIN] Iter: 856100 Loss: 0.005029021762311459  PSNR: 27.641368865966797
[TRAIN] Iter: 856200 Loss: 0.005171601660549641  PSNR: 28.090312957763672
[TRAIN] Iter: 856300 Loss: 0.005402794573456049  PSNR: 27.242197036743164
[TRAIN] Iter: 856400 Loss: 0.005289449356496334  PSNR: 27.417984008789062
[TRAIN] Iter: 856500 Loss: 0.003455088473856449  PSNR: 30.108238220214844
[TRAIN] Iter: 856600 Loss: 0.0033256900496780872  PSNR: 30.376361846923828
[TRAIN] Iter: 856700 Loss: 0.004319524858146906  PSNR: 28.71518325805664
[TRAIN] Iter: 856800 Loss: 0.003803487867116928  PSNR: 28.861799240112305
[TRAIN] Iter: 856900 Loss: 0.004667185712605715  PSNR: 28.329105377197266
[TRAIN] Iter: 857000 Loss: 0.0039306944236159325  PSNR: 30.1708984375
[TRAIN] Iter: 857100 Loss: 0.005260812118649483  PSNR: 27.51051902770996
[TRAIN] Iter: 857200 Loss: 0.0042747571133077145  PSNR: 28.038007736206055
[TRAIN] Iter: 857300 Loss: 0.004738337825983763  PSNR: 27.61905288696289
[TRAIN] Iter: 857400 Loss: 0.004290489479899406  PSNR: 28.350894927978516
[TRAIN] Iter: 857500 Loss: 0.00443655252456665  PSNR: 28.6303768157959
[TRAIN] Iter: 857600 Loss: 0.005116354674100876  PSNR: 27.882213592529297
[TRAIN] Iter: 857700 Loss: 0.003732544369995594  PSNR: 30.46208953857422
[TRAIN] Iter: 857800 Loss: 0.0053275884129107  PSNR: 27.502111434936523
[TRAIN] Iter: 857900 Loss: 0.0037731570191681385  PSNR: 30.02825355529785
[TRAIN] Iter: 858000 Loss: 0.0056662531569600105  PSNR: 27.07912254333496
[TRAIN] Iter: 858100 Loss: 0.004437294322997332  PSNR: 28.521137237548828
[TRAIN] Iter: 858200 Loss: 0.004366359673440456  PSNR: 27.507028579711914
[TRAIN] Iter: 858300 Loss: 0.004974791780114174  PSNR: 28.16342544555664
[TRAIN] Iter: 858400 Loss: 0.0041365111246705055  PSNR: 29.295623779296875
[TRAIN] Iter: 858500 Loss: 0.003648078069090843  PSNR: 29.857139587402344
[TRAIN] Iter: 858600 Loss: 0.0043716030195355415  PSNR: 28.697629928588867
[TRAIN] Iter: 858700 Loss: 0.003930813167244196  PSNR: 30.116392135620117
[TRAIN] Iter: 858800 Loss: 0.0063065397553145885  PSNR: 26.760873794555664
[TRAIN] Iter: 858900 Loss: 0.0033359138760715723  PSNR: 30.177104949951172
[TRAIN] Iter: 859000 Loss: 0.004512215033173561  PSNR: 28.244369506835938
[TRAIN] Iter: 859100 Loss: 0.005320730619132519  PSNR: 27.51652717590332
[TRAIN] Iter: 859200 Loss: 0.004787459969520569  PSNR: 28.0136661529541
[TRAIN] Iter: 859300 Loss: 0.004923289641737938  PSNR: 27.39515495300293
[TRAIN] Iter: 859400 Loss: 0.003435224760323763  PSNR: 30.581279754638672
[TRAIN] Iter: 859500 Loss: 0.0037660570815205574  PSNR: 30.18878746032715
[TRAIN] Iter: 859600 Loss: 0.004681848455220461  PSNR: 28.19755744934082
[TRAIN] Iter: 859700 Loss: 0.0051909275352954865  PSNR: 28.4710693359375
[TRAIN] Iter: 859800 Loss: 0.005023455247282982  PSNR: 27.245759963989258
[TRAIN] Iter: 859900 Loss: 0.004052203148603439  PSNR: 29.200925827026367
Saved checkpoints at ./logs/TUT-KE101-nerf/860000.tar
[TRAIN] Iter: 860000 Loss: 0.0051534525118768215  PSNR: 27.282983779907227
[TRAIN] Iter: 860100 Loss: 0.00493991281837225  PSNR: 27.596553802490234
[TRAIN] Iter: 860200 Loss: 0.003870707005262375  PSNR: 29.68239402770996
[TRAIN] Iter: 860300 Loss: 0.0037749866023659706  PSNR: 29.268104553222656
[TRAIN] Iter: 860400 Loss: 0.004597931168973446  PSNR: 27.827960968017578
[TRAIN] Iter: 860500 Loss: 0.004604354966431856  PSNR: 28.18695831298828
[TRAIN] Iter: 860600 Loss: 0.0034818616695702076  PSNR: 30.483829498291016
[TRAIN] Iter: 860700 Loss: 0.0034755391534417868  PSNR: 30.531463623046875
[TRAIN] Iter: 860800 Loss: 0.004068313632160425  PSNR: 28.48626708984375
[TRAIN] Iter: 860900 Loss: 0.004897654056549072  PSNR: 27.80902099609375
[TRAIN] Iter: 861000 Loss: 0.004376973956823349  PSNR: 28.473596572875977
[TRAIN] Iter: 861100 Loss: 0.005492840427905321  PSNR: 28.032865524291992
[TRAIN] Iter: 861200 Loss: 0.004346903413534164  PSNR: 28.55887794494629
[TRAIN] Iter: 861300 Loss: 0.00403007585555315  PSNR: 29.84211540222168
[TRAIN] Iter: 861400 Loss: 0.00602931622415781  PSNR: 28.003952026367188
[TRAIN] Iter: 861500 Loss: 0.003406783565878868  PSNR: 30.27255630493164
[TRAIN] Iter: 861600 Loss: 0.0035615519154816866  PSNR: 30.5408992767334
[TRAIN] Iter: 861700 Loss: 0.004239225760102272  PSNR: 28.59830093383789
[TRAIN] Iter: 861800 Loss: 0.0047460258938372135  PSNR: 28.123214721679688
[TRAIN] Iter: 861900 Loss: 0.0044559272937476635  PSNR: 28.681346893310547
[TRAIN] Iter: 862000 Loss: 0.004724217113107443  PSNR: 28.478036880493164
[TRAIN] Iter: 862100 Loss: 0.005048874765634537  PSNR: 27.93726348876953
[TRAIN] Iter: 862200 Loss: 0.0035009821876883507  PSNR: 30.42512321472168
[TRAIN] Iter: 862300 Loss: 0.005501501727849245  PSNR: 27.37603187561035
[TRAIN] Iter: 862400 Loss: 0.004160093143582344  PSNR: 28.588380813598633
[TRAIN] Iter: 862500 Loss: 0.004770658444613218  PSNR: 28.082834243774414
[TRAIN] Iter: 862600 Loss: 0.003893166547641158  PSNR: 28.623741149902344
[TRAIN] Iter: 862700 Loss: 0.004700938705354929  PSNR: 27.965234756469727
[TRAIN] Iter: 862800 Loss: 0.004517631605267525  PSNR: 28.52012825012207
[TRAIN] Iter: 862900 Loss: 0.004170842468738556  PSNR: 27.923873901367188
[TRAIN] Iter: 863000 Loss: 0.00394910154864192  PSNR: 28.9020938873291
[TRAIN] Iter: 863100 Loss: 0.0038490458391606808  PSNR: 28.73247718811035
[TRAIN] Iter: 863200 Loss: 0.0037391900550574064  PSNR: 29.892704010009766
[TRAIN] Iter: 863300 Loss: 0.004303443245589733  PSNR: 28.0104923248291
[TRAIN] Iter: 863400 Loss: 0.00582435168325901  PSNR: 27.067543029785156
[TRAIN] Iter: 863500 Loss: 0.0060164788737893105  PSNR: 27.04108428955078
[TRAIN] Iter: 863600 Loss: 0.004301668610423803  PSNR: 28.279895782470703
[TRAIN] Iter: 863700 Loss: 0.004733974114060402  PSNR: 28.41941261291504
[TRAIN] Iter: 863800 Loss: 0.005376257002353668  PSNR: 27.36958122253418
[TRAIN] Iter: 863900 Loss: 0.00419518630951643  PSNR: 29.079416275024414
[TRAIN] Iter: 864000 Loss: 0.004028601106256247  PSNR: 28.577136993408203
[TRAIN] Iter: 864100 Loss: 0.004676056560128927  PSNR: 28.6119327545166
[TRAIN] Iter: 864200 Loss: 0.004823990166187286  PSNR: 27.9735050201416
[TRAIN] Iter: 864300 Loss: 0.004816003143787384  PSNR: 27.809146881103516
[TRAIN] Iter: 864400 Loss: 0.005941360257565975  PSNR: 26.66579818725586
[TRAIN] Iter: 864500 Loss: 0.005100926384329796  PSNR: 27.985595703125
[TRAIN] Iter: 864600 Loss: 0.005655436776578426  PSNR: 27.257369995117188
[TRAIN] Iter: 864700 Loss: 0.004260034300386906  PSNR: 28.973766326904297
[TRAIN] Iter: 864800 Loss: 0.005210611037909985  PSNR: 28.058420181274414
[TRAIN] Iter: 864900 Loss: 0.004290636628866196  PSNR: 28.654048919677734
[TRAIN] Iter: 865000 Loss: 0.005539211444556713  PSNR: 27.696842193603516
[TRAIN] Iter: 865100 Loss: 0.003880550619214773  PSNR: 28.826080322265625
[TRAIN] Iter: 865200 Loss: 0.0041709658689796925  PSNR: 28.91545295715332
[TRAIN] Iter: 865300 Loss: 0.004915652330964804  PSNR: 28.749279022216797
[TRAIN] Iter: 865400 Loss: 0.005041924770921469  PSNR: 27.549924850463867
[TRAIN] Iter: 865500 Loss: 0.0043317098170518875  PSNR: 28.319639205932617
[TRAIN] Iter: 865600 Loss: 0.004922192543745041  PSNR: 27.49931526184082
[TRAIN] Iter: 865700 Loss: 0.0055204941891133785  PSNR: 27.236146926879883
[TRAIN] Iter: 865800 Loss: 0.004025163594633341  PSNR: 28.47853660583496
[TRAIN] Iter: 865900 Loss: 0.005200947169214487  PSNR: 27.508554458618164
[TRAIN] Iter: 866000 Loss: 0.0034644026309251785  PSNR: 29.834918975830078
[TRAIN] Iter: 866100 Loss: 0.004058321006596088  PSNR: 29.711885452270508
[TRAIN] Iter: 866200 Loss: 0.004008899908512831  PSNR: 28.7073917388916
[TRAIN] Iter: 866300 Loss: 0.0035826824605464935  PSNR: 31.009571075439453
[TRAIN] Iter: 866400 Loss: 0.005839196499437094  PSNR: 27.552152633666992
[TRAIN] Iter: 866500 Loss: 0.004356425255537033  PSNR: 28.114572525024414
[TRAIN] Iter: 866600 Loss: 0.004739799536764622  PSNR: 27.881744384765625
[TRAIN] Iter: 866700 Loss: 0.003010781481862068  PSNR: 31.07246971130371
[TRAIN] Iter: 866800 Loss: 0.0035269437357783318  PSNR: 29.596830368041992
[TRAIN] Iter: 866900 Loss: 0.004200476687401533  PSNR: 29.262252807617188
[TRAIN] Iter: 867000 Loss: 0.004978702403604984  PSNR: 27.46782112121582
[TRAIN] Iter: 867100 Loss: 0.005459919106215239  PSNR: 26.860952377319336
[TRAIN] Iter: 867200 Loss: 0.003772383090108633  PSNR: 29.313419342041016
[TRAIN] Iter: 867300 Loss: 0.004638534039258957  PSNR: 27.848880767822266
[TRAIN] Iter: 867400 Loss: 0.005004464648663998  PSNR: 27.39476776123047
[TRAIN] Iter: 867500 Loss: 0.003945688251405954  PSNR: 28.103897094726562
[TRAIN] Iter: 867600 Loss: 0.004745148122310638  PSNR: 28.59506607055664
[TRAIN] Iter: 867700 Loss: 0.005147664342075586  PSNR: 26.854042053222656
[TRAIN] Iter: 867800 Loss: 0.0052296919748187065  PSNR: 26.722885131835938
[TRAIN] Iter: 867900 Loss: 0.005109106190502644  PSNR: 27.711013793945312
[TRAIN] Iter: 868000 Loss: 0.003957253880798817  PSNR: 29.793750762939453
[TRAIN] Iter: 868100 Loss: 0.003455137135460973  PSNR: 30.134244918823242
[TRAIN] Iter: 868200 Loss: 0.005572875030338764  PSNR: 26.852155685424805
[TRAIN] Iter: 868300 Loss: 0.004803919233381748  PSNR: 27.71768569946289
[TRAIN] Iter: 868400 Loss: 0.004233825486153364  PSNR: 28.932945251464844
[TRAIN] Iter: 868500 Loss: 0.005092502571642399  PSNR: 28.446182250976562
[TRAIN] Iter: 868600 Loss: 0.00414165947586298  PSNR: 28.859664916992188
[TRAIN] Iter: 868700 Loss: 0.0034243417903780937  PSNR: 30.51967430114746
[TRAIN] Iter: 868800 Loss: 0.005619551055133343  PSNR: 27.48302459716797
[TRAIN] Iter: 868900 Loss: 0.005032739136368036  PSNR: 28.28777503967285
[TRAIN] Iter: 869000 Loss: 0.005151245277374983  PSNR: 27.755632400512695
[TRAIN] Iter: 869100 Loss: 0.005052117630839348  PSNR: 27.050411224365234
[TRAIN] Iter: 869200 Loss: 0.003836812451481819  PSNR: 29.521038055419922
[TRAIN] Iter: 869300 Loss: 0.004275272600352764  PSNR: 28.824420928955078
[TRAIN] Iter: 869400 Loss: 0.0050598978996276855  PSNR: 27.32426643371582
[TRAIN] Iter: 869500 Loss: 0.00563577376306057  PSNR: 27.425119400024414
[TRAIN] Iter: 869600 Loss: 0.006486174650490284  PSNR: 26.728134155273438
[TRAIN] Iter: 869700 Loss: 0.00473168957978487  PSNR: 28.116666793823242
[TRAIN] Iter: 869800 Loss: 0.005533830262720585  PSNR: 27.14192771911621
[TRAIN] Iter: 869900 Loss: 0.005052101798355579  PSNR: 27.57342529296875
Saved checkpoints at ./logs/TUT-KE101-nerf/870000.tar
[TRAIN] Iter: 870000 Loss: 0.0036763278767466545  PSNR: 30.459253311157227
[TRAIN] Iter: 870100 Loss: 0.0037657679058611393  PSNR: 28.54965591430664
[TRAIN] Iter: 870200 Loss: 0.0046851746737957  PSNR: 28.562713623046875
[TRAIN] Iter: 870300 Loss: 0.004635998979210854  PSNR: 27.627460479736328
[TRAIN] Iter: 870400 Loss: 0.004224669188261032  PSNR: 27.944442749023438
[TRAIN] Iter: 870500 Loss: 0.003889417741447687  PSNR: 29.339004516601562
[TRAIN] Iter: 870600 Loss: 0.003263105172663927  PSNR: 30.757761001586914
[TRAIN] Iter: 870700 Loss: 0.004368860274553299  PSNR: 29.152450561523438
[TRAIN] Iter: 870800 Loss: 0.0037987977266311646  PSNR: 28.57468605041504
[TRAIN] Iter: 870900 Loss: 0.005487971939146519  PSNR: 27.710411071777344
[TRAIN] Iter: 871000 Loss: 0.004035685211420059  PSNR: 28.484806060791016
[TRAIN] Iter: 871100 Loss: 0.005375580862164497  PSNR: 27.494569778442383
[TRAIN] Iter: 871200 Loss: 0.003535153344273567  PSNR: 30.028358459472656
[TRAIN] Iter: 871300 Loss: 0.00434830691665411  PSNR: 28.766759872436523
[TRAIN] Iter: 871400 Loss: 0.0037570451386272907  PSNR: 30.110862731933594
[TRAIN] Iter: 871500 Loss: 0.0037930866237729788  PSNR: 29.82628059387207
[TRAIN] Iter: 871600 Loss: 0.0043205879628658295  PSNR: 29.56987953186035
[TRAIN] Iter: 871700 Loss: 0.004416484385728836  PSNR: 28.08195686340332
[TRAIN] Iter: 871800 Loss: 0.004414403345435858  PSNR: 28.360387802124023
[TRAIN] Iter: 871900 Loss: 0.004286670126020908  PSNR: 28.839046478271484
[TRAIN] Iter: 872000 Loss: 0.004047720693051815  PSNR: 28.91124153137207
[TRAIN] Iter: 872100 Loss: 0.003635769709944725  PSNR: 29.43714141845703
[TRAIN] Iter: 872200 Loss: 0.00473361648619175  PSNR: 28.157594680786133
[TRAIN] Iter: 872300 Loss: 0.0042059230618178844  PSNR: 28.906940460205078
[TRAIN] Iter: 872400 Loss: 0.005090910941362381  PSNR: 27.548484802246094
[TRAIN] Iter: 872500 Loss: 0.003931385464966297  PSNR: 29.692058563232422
[TRAIN] Iter: 872600 Loss: 0.0037922048941254616  PSNR: 29.67978858947754
[TRAIN] Iter: 872700 Loss: 0.004393431358039379  PSNR: 28.78725814819336
[TRAIN] Iter: 872800 Loss: 0.003713529324159026  PSNR: 29.28693199157715
[TRAIN] Iter: 872900 Loss: 0.0047630975022912025  PSNR: 28.32048225402832
[TRAIN] Iter: 873000 Loss: 0.0038657523691654205  PSNR: 29.50437355041504
[TRAIN] Iter: 873100 Loss: 0.005105570890009403  PSNR: 27.935964584350586
[TRAIN] Iter: 873200 Loss: 0.004702665377408266  PSNR: 27.729381561279297
[TRAIN] Iter: 873300 Loss: 0.004468502011150122  PSNR: 28.78487205505371
[TRAIN] Iter: 873400 Loss: 0.004270268604159355  PSNR: 28.431884765625
[TRAIN] Iter: 873500 Loss: 0.004057068377733231  PSNR: 28.695072174072266
[TRAIN] Iter: 873600 Loss: 0.004951370880007744  PSNR: 27.741878509521484
[TRAIN] Iter: 873700 Loss: 0.004065657500177622  PSNR: 29.427099227905273
[TRAIN] Iter: 873800 Loss: 0.005514547228813171  PSNR: 26.754425048828125
[TRAIN] Iter: 873900 Loss: 0.003680281573906541  PSNR: 30.216466903686523
[TRAIN] Iter: 874000 Loss: 0.003937508910894394  PSNR: 29.80767250061035
[TRAIN] Iter: 874100 Loss: 0.0035628306213766336  PSNR: 30.835294723510742
[TRAIN] Iter: 874200 Loss: 0.004502508789300919  PSNR: 28.310598373413086
[TRAIN] Iter: 874300 Loss: 0.004636779427528381  PSNR: 28.246505737304688
[TRAIN] Iter: 874400 Loss: 0.004346523433923721  PSNR: 28.96910285949707
[TRAIN] Iter: 874500 Loss: 0.004611607640981674  PSNR: 28.315528869628906
[TRAIN] Iter: 874600 Loss: 0.005321073345839977  PSNR: 27.875919342041016
[TRAIN] Iter: 874700 Loss: 0.003968798089772463  PSNR: 29.505199432373047
[TRAIN] Iter: 874800 Loss: 0.0036779725924134254  PSNR: 29.9974422454834
[TRAIN] Iter: 874900 Loss: 0.003548302687704563  PSNR: 29.908344268798828
[TRAIN] Iter: 875000 Loss: 0.0050890748389065266  PSNR: 27.450725555419922
[TRAIN] Iter: 875100 Loss: 0.0037014931440353394  PSNR: 30.35845375061035
[TRAIN] Iter: 875200 Loss: 0.004756439011543989  PSNR: 28.13283348083496
[TRAIN] Iter: 875300 Loss: 0.0045076459646224976  PSNR: 28.336746215820312
[TRAIN] Iter: 875400 Loss: 0.0038493697065860033  PSNR: 29.85807991027832
[TRAIN] Iter: 875500 Loss: 0.005007187835872173  PSNR: 27.71983528137207
[TRAIN] Iter: 875600 Loss: 0.0030062932055443525  PSNR: 30.31009864807129
[TRAIN] Iter: 875700 Loss: 0.003546488471329212  PSNR: 29.942174911499023
[TRAIN] Iter: 875800 Loss: 0.00428913626819849  PSNR: 28.501102447509766
[TRAIN] Iter: 875900 Loss: 0.0047898320481181145  PSNR: 27.471771240234375
[TRAIN] Iter: 876000 Loss: 0.004535841289907694  PSNR: 28.71844482421875
[TRAIN] Iter: 876100 Loss: 0.0034957341849803925  PSNR: 30.124446868896484
[TRAIN] Iter: 876200 Loss: 0.0045033893547952175  PSNR: 28.029216766357422
[TRAIN] Iter: 876300 Loss: 0.0038425030652433634  PSNR: 29.100984573364258
[TRAIN] Iter: 876400 Loss: 0.004491291008889675  PSNR: 29.184371948242188
[TRAIN] Iter: 876500 Loss: 0.004241222515702248  PSNR: 28.353443145751953
[TRAIN] Iter: 876600 Loss: 0.005183366592973471  PSNR: 27.618804931640625
[TRAIN] Iter: 876700 Loss: 0.004652680829167366  PSNR: 28.4090518951416
[TRAIN] Iter: 876800 Loss: 0.003800836857408285  PSNR: 29.833534240722656
[TRAIN] Iter: 876900 Loss: 0.006310661323368549  PSNR: 27.067747116088867
[TRAIN] Iter: 877000 Loss: 0.003950292244553566  PSNR: 29.769960403442383
[TRAIN] Iter: 877100 Loss: 0.004389981273561716  PSNR: 29.06892967224121
[TRAIN] Iter: 877200 Loss: 0.004765818361192942  PSNR: 28.54591941833496
[TRAIN] Iter: 877300 Loss: 0.004782163538038731  PSNR: 28.164005279541016
[TRAIN] Iter: 877400 Loss: 0.0046896375715732574  PSNR: 28.066944122314453
[TRAIN] Iter: 877500 Loss: 0.00410469900816679  PSNR: 28.772768020629883
[TRAIN] Iter: 877600 Loss: 0.004938043188303709  PSNR: 28.251201629638672
[TRAIN] Iter: 877700 Loss: 0.005191987846046686  PSNR: 27.95111656188965
[TRAIN] Iter: 877800 Loss: 0.005971326492726803  PSNR: 27.34492301940918
[TRAIN] Iter: 877900 Loss: 0.0036506252363324165  PSNR: 30.583599090576172
[TRAIN] Iter: 878000 Loss: 0.0037134706508368254  PSNR: 29.666889190673828
[TRAIN] Iter: 878100 Loss: 0.004932792857289314  PSNR: 28.376033782958984
[TRAIN] Iter: 878200 Loss: 0.0037997968029230833  PSNR: 29.152936935424805
[TRAIN] Iter: 878300 Loss: 0.005583128426223993  PSNR: 27.807729721069336
[TRAIN] Iter: 878400 Loss: 0.005400742404162884  PSNR: 28.516263961791992
[TRAIN] Iter: 878500 Loss: 0.005293192341923714  PSNR: 27.729907989501953
[TRAIN] Iter: 878600 Loss: 0.003416591789573431  PSNR: 30.823619842529297
[TRAIN] Iter: 878700 Loss: 0.004194680601358414  PSNR: 28.92913818359375
[TRAIN] Iter: 878800 Loss: 0.005379040259867907  PSNR: 27.83527946472168
[TRAIN] Iter: 878900 Loss: 0.00411586370319128  PSNR: 28.663740158081055
[TRAIN] Iter: 879000 Loss: 0.00479827169328928  PSNR: 27.89494514465332
[TRAIN] Iter: 879100 Loss: 0.003593247616663575  PSNR: 29.196584701538086
[TRAIN] Iter: 879200 Loss: 0.0038945861160755157  PSNR: 28.822708129882812
[TRAIN] Iter: 879300 Loss: 0.00426553376019001  PSNR: 28.65066909790039
[TRAIN] Iter: 879400 Loss: 0.004182235337793827  PSNR: 29.378183364868164
[TRAIN] Iter: 879500 Loss: 0.004189847037196159  PSNR: 29.149730682373047
[TRAIN] Iter: 879600 Loss: 0.0032850238494575024  PSNR: 30.705738067626953
[TRAIN] Iter: 879700 Loss: 0.005036528687924147  PSNR: 27.490503311157227
[TRAIN] Iter: 879800 Loss: 0.0039995163679122925  PSNR: 29.570091247558594
[TRAIN] Iter: 879900 Loss: 0.004921378567814827  PSNR: 28.297327041625977
Saved checkpoints at ./logs/TUT-KE101-nerf/880000.tar
[TRAIN] Iter: 880000 Loss: 0.0038950492162257433  PSNR: 30.564729690551758
[TRAIN] Iter: 880100 Loss: 0.005099882371723652  PSNR: 27.957256317138672
[TRAIN] Iter: 880200 Loss: 0.005652847699820995  PSNR: 27.820323944091797
[TRAIN] Iter: 880300 Loss: 0.0038596622180193663  PSNR: 28.670379638671875
[TRAIN] Iter: 880400 Loss: 0.0037619760259985924  PSNR: 29.279422760009766
[TRAIN] Iter: 880500 Loss: 0.004755140747874975  PSNR: 27.87110710144043
[TRAIN] Iter: 880600 Loss: 0.0058279382064938545  PSNR: 27.223115921020508
[TRAIN] Iter: 880700 Loss: 0.005613601300865412  PSNR: 27.216760635375977
[TRAIN] Iter: 880800 Loss: 0.0035112660843878984  PSNR: 30.759750366210938
[TRAIN] Iter: 880900 Loss: 0.003927433863282204  PSNR: 28.54256820678711
[TRAIN] Iter: 881000 Loss: 0.00477448059245944  PSNR: 28.05906105041504
[TRAIN] Iter: 881100 Loss: 0.005121336784213781  PSNR: 27.482019424438477
[TRAIN] Iter: 881200 Loss: 0.003935959190130234  PSNR: 29.805702209472656
[TRAIN] Iter: 881300 Loss: 0.004971685353666544  PSNR: 27.361953735351562
[TRAIN] Iter: 881400 Loss: 0.004061501007527113  PSNR: 29.433761596679688
[TRAIN] Iter: 881500 Loss: 0.005267833359539509  PSNR: 27.636272430419922
[TRAIN] Iter: 881600 Loss: 0.00455602677538991  PSNR: 28.778648376464844
[TRAIN] Iter: 881700 Loss: 0.004990565124899149  PSNR: 27.781944274902344
[TRAIN] Iter: 881800 Loss: 0.0041905352845788  PSNR: 29.543787002563477
[TRAIN] Iter: 881900 Loss: 0.0034148639533668756  PSNR: 30.523643493652344
[TRAIN] Iter: 882000 Loss: 0.005697884596884251  PSNR: 27.370065689086914
[TRAIN] Iter: 882100 Loss: 0.004805377684533596  PSNR: 27.867006301879883
[TRAIN] Iter: 882200 Loss: 0.00551606947556138  PSNR: 27.298635482788086
[TRAIN] Iter: 882300 Loss: 0.003839938435703516  PSNR: 29.028669357299805
[TRAIN] Iter: 882400 Loss: 0.0040424903854727745  PSNR: 28.80374526977539
[TRAIN] Iter: 882500 Loss: 0.004759296774864197  PSNR: 27.721675872802734
[TRAIN] Iter: 882600 Loss: 0.0036834427155554295  PSNR: 29.78577423095703
[TRAIN] Iter: 882700 Loss: 0.004829193465411663  PSNR: 27.751806259155273
[TRAIN] Iter: 882800 Loss: 0.004299916792660952  PSNR: 28.787839889526367
[TRAIN] Iter: 882900 Loss: 0.0037846225313842297  PSNR: 29.68600845336914
[TRAIN] Iter: 883000 Loss: 0.0057239830493927  PSNR: 26.5649471282959
[TRAIN] Iter: 883100 Loss: 0.005111110396683216  PSNR: 27.342609405517578
[TRAIN] Iter: 883200 Loss: 0.0039185574278235435  PSNR: 29.207361221313477
[TRAIN] Iter: 883300 Loss: 0.003484961111098528  PSNR: 30.030717849731445
[TRAIN] Iter: 883400 Loss: 0.005412098951637745  PSNR: 27.38348388671875
[TRAIN] Iter: 883500 Loss: 0.004488340578973293  PSNR: 28.369831085205078
[TRAIN] Iter: 883600 Loss: 0.004438807722181082  PSNR: 27.69449234008789
[TRAIN] Iter: 883700 Loss: 0.00432724691927433  PSNR: 29.2108154296875
[TRAIN] Iter: 883800 Loss: 0.004943656735122204  PSNR: 28.45671844482422
[TRAIN] Iter: 883900 Loss: 0.0052417488768696785  PSNR: 27.58104133605957
[TRAIN] Iter: 884000 Loss: 0.005989670753479004  PSNR: 26.101842880249023
[TRAIN] Iter: 884100 Loss: 0.004707464016973972  PSNR: 27.771480560302734
[TRAIN] Iter: 884200 Loss: 0.0037840232253074646  PSNR: 28.60376739501953
[TRAIN] Iter: 884300 Loss: 0.005171667784452438  PSNR: 27.685644149780273
[TRAIN] Iter: 884400 Loss: 0.0037714848294854164  PSNR: 30.46573829650879
[TRAIN] Iter: 884500 Loss: 0.004648481495678425  PSNR: 27.5355167388916
[TRAIN] Iter: 884600 Loss: 0.003533037845045328  PSNR: 28.701725006103516
[TRAIN] Iter: 884700 Loss: 0.004353690892457962  PSNR: 28.3562068939209
[TRAIN] Iter: 884800 Loss: 0.004607180133461952  PSNR: 28.522499084472656
[TRAIN] Iter: 884900 Loss: 0.003265613690018654  PSNR: 30.915542602539062
[TRAIN] Iter: 885000 Loss: 0.003919361624866724  PSNR: 29.111785888671875
[TRAIN] Iter: 885100 Loss: 0.004486344754695892  PSNR: 27.88003158569336
[TRAIN] Iter: 885200 Loss: 0.0037630023434758186  PSNR: 29.775157928466797
[TRAIN] Iter: 885300 Loss: 0.0060114930383861065  PSNR: 27.393341064453125
[TRAIN] Iter: 885400 Loss: 0.004288052208721638  PSNR: 28.53314208984375
[TRAIN] Iter: 885500 Loss: 0.003932976629585028  PSNR: 30.137462615966797
[TRAIN] Iter: 885600 Loss: 0.00338700320571661  PSNR: 30.027599334716797
[TRAIN] Iter: 885700 Loss: 0.004874561447650194  PSNR: 28.07962989807129
[TRAIN] Iter: 885800 Loss: 0.003412616439163685  PSNR: 30.410058975219727
[TRAIN] Iter: 885900 Loss: 0.003685761010274291  PSNR: 30.39379119873047
[TRAIN] Iter: 886000 Loss: 0.005117145832628012  PSNR: 27.841201782226562
[TRAIN] Iter: 886100 Loss: 0.004437614232301712  PSNR: 28.31797981262207
[TRAIN] Iter: 886200 Loss: 0.005166674964129925  PSNR: 28.039941787719727
[TRAIN] Iter: 886300 Loss: 0.0042794072069227695  PSNR: 28.491989135742188
[TRAIN] Iter: 886400 Loss: 0.003910000901669264  PSNR: 28.57279396057129
[TRAIN] Iter: 886500 Loss: 0.005157609470188618  PSNR: 27.897274017333984
[TRAIN] Iter: 886600 Loss: 0.004930989816784859  PSNR: 27.654277801513672
[TRAIN] Iter: 886700 Loss: 0.004482689779251814  PSNR: 28.46747398376465
[TRAIN] Iter: 886800 Loss: 0.003939102869480848  PSNR: 29.390634536743164
[TRAIN] Iter: 886900 Loss: 0.004099290817975998  PSNR: 29.795513153076172
[TRAIN] Iter: 887000 Loss: 0.004377884790301323  PSNR: 28.101438522338867
[TRAIN] Iter: 887100 Loss: 0.004085519351065159  PSNR: 28.457059860229492
[TRAIN] Iter: 887200 Loss: 0.004756144247949123  PSNR: 27.729251861572266
[TRAIN] Iter: 887300 Loss: 0.003831882495433092  PSNR: 29.170167922973633
[TRAIN] Iter: 887400 Loss: 0.005365620367228985  PSNR: 27.171966552734375
[TRAIN] Iter: 887500 Loss: 0.004166509956121445  PSNR: 28.64982795715332
[TRAIN] Iter: 887600 Loss: 0.004042044281959534  PSNR: 29.719194412231445
[TRAIN] Iter: 887700 Loss: 0.003933328669518232  PSNR: 29.968585968017578
[TRAIN] Iter: 887800 Loss: 0.0049505759961903095  PSNR: 27.339542388916016
[TRAIN] Iter: 887900 Loss: 0.004853732883930206  PSNR: 27.347665786743164
[TRAIN] Iter: 888000 Loss: 0.004879698157310486  PSNR: 27.562814712524414
[TRAIN] Iter: 888100 Loss: 0.005386913660913706  PSNR: 27.248872756958008
[TRAIN] Iter: 888200 Loss: 0.006268138997256756  PSNR: 27.01140594482422
[TRAIN] Iter: 888300 Loss: 0.0039836042560637  PSNR: 29.392284393310547
[TRAIN] Iter: 888400 Loss: 0.005553897004574537  PSNR: 26.79125213623047
[TRAIN] Iter: 888500 Loss: 0.004566873423755169  PSNR: 28.391212463378906
[TRAIN] Iter: 888600 Loss: 0.006356206256896257  PSNR: 26.206424713134766
[TRAIN] Iter: 888700 Loss: 0.004187723621726036  PSNR: 28.864879608154297
[TRAIN] Iter: 888800 Loss: 0.004426892846822739  PSNR: 28.386388778686523
[TRAIN] Iter: 888900 Loss: 0.005317539907991886  PSNR: 27.073278427124023
[TRAIN] Iter: 889000 Loss: 0.004211332183331251  PSNR: 28.192909240722656
[TRAIN] Iter: 889100 Loss: 0.0036419399548321962  PSNR: 30.18567657470703
[TRAIN] Iter: 889200 Loss: 0.005545360967516899  PSNR: 27.55512237548828
[TRAIN] Iter: 889300 Loss: 0.003518363693729043  PSNR: 30.436689376831055
[TRAIN] Iter: 889400 Loss: 0.0033630337566137314  PSNR: 30.05267906188965
[TRAIN] Iter: 889500 Loss: 0.004996268078684807  PSNR: 27.706514358520508
[TRAIN] Iter: 889600 Loss: 0.005205805413424969  PSNR: 27.562633514404297
[TRAIN] Iter: 889700 Loss: 0.005355987697839737  PSNR: 26.88926887512207
[TRAIN] Iter: 889800 Loss: 0.00306203356012702  PSNR: 30.02825927734375
[TRAIN] Iter: 889900 Loss: 0.003971650265157223  PSNR: 29.289674758911133
Saved checkpoints at ./logs/TUT-KE101-nerf/890000.tar
[TRAIN] Iter: 890000 Loss: 0.005169875919818878  PSNR: 27.783458709716797
[TRAIN] Iter: 890100 Loss: 0.004255513660609722  PSNR: 28.744020462036133
[TRAIN] Iter: 890200 Loss: 0.003989564720541239  PSNR: 28.875446319580078
[TRAIN] Iter: 890300 Loss: 0.005862732417881489  PSNR: 27.488100051879883
[TRAIN] Iter: 890400 Loss: 0.004558178596198559  PSNR: 28.026161193847656
[TRAIN] Iter: 890500 Loss: 0.004508155398070812  PSNR: 28.559831619262695
[TRAIN] Iter: 890600 Loss: 0.004238874651491642  PSNR: 28.135435104370117
[TRAIN] Iter: 890700 Loss: 0.0056208535097539425  PSNR: 27.448684692382812
[TRAIN] Iter: 890800 Loss: 0.004855175968259573  PSNR: 27.992813110351562
[TRAIN] Iter: 890900 Loss: 0.005470340605825186  PSNR: 27.27425765991211
[TRAIN] Iter: 891000 Loss: 0.003656596876680851  PSNR: 30.590404510498047
[TRAIN] Iter: 891100 Loss: 0.004262417554855347  PSNR: 28.33684730529785
[TRAIN] Iter: 891200 Loss: 0.006082933396100998  PSNR: 27.59939956665039
[TRAIN] Iter: 891300 Loss: 0.004150673747062683  PSNR: 29.27373695373535
[TRAIN] Iter: 891400 Loss: 0.005246094428002834  PSNR: 27.93024253845215
[TRAIN] Iter: 891500 Loss: 0.005219985730946064  PSNR: 28.305767059326172
[TRAIN] Iter: 891600 Loss: 0.005653281230479479  PSNR: 27.066070556640625
[TRAIN] Iter: 891700 Loss: 0.005274753086268902  PSNR: 27.985990524291992
[TRAIN] Iter: 891800 Loss: 0.004568868316709995  PSNR: 27.95224952697754
[TRAIN] Iter: 891900 Loss: 0.003984909504652023  PSNR: 29.298982620239258
[TRAIN] Iter: 892000 Loss: 0.003108275355771184  PSNR: 30.76970672607422
[TRAIN] Iter: 892100 Loss: 0.00443692784756422  PSNR: 28.389787673950195
[TRAIN] Iter: 892200 Loss: 0.0037808488123118877  PSNR: 30.275671005249023
[TRAIN] Iter: 892300 Loss: 0.003128936979919672  PSNR: 30.814720153808594
[TRAIN] Iter: 892400 Loss: 0.005211315117776394  PSNR: 28.339984893798828
[TRAIN] Iter: 892500 Loss: 0.0037827459163963795  PSNR: 29.856672286987305
[TRAIN] Iter: 892600 Loss: 0.005220411345362663  PSNR: 27.87529182434082
[TRAIN] Iter: 892700 Loss: 0.0032221695873886347  PSNR: 30.267637252807617
[TRAIN] Iter: 892800 Loss: 0.005278372205793858  PSNR: 27.68619155883789
[TRAIN] Iter: 892900 Loss: 0.003599666990339756  PSNR: 30.00128936767578
[TRAIN] Iter: 893000 Loss: 0.0056831869296729565  PSNR: 27.44493865966797
[TRAIN] Iter: 893100 Loss: 0.0033897024113684893  PSNR: 30.55351448059082
[TRAIN] Iter: 893200 Loss: 0.005878099240362644  PSNR: 26.71234703063965
[TRAIN] Iter: 893300 Loss: 0.0035963295958936214  PSNR: 30.636796951293945
[TRAIN] Iter: 893400 Loss: 0.005145195871591568  PSNR: 27.79204559326172
[TRAIN] Iter: 893500 Loss: 0.004431090783327818  PSNR: 28.50018310546875
[TRAIN] Iter: 893600 Loss: 0.005024199839681387  PSNR: 27.543537139892578
[TRAIN] Iter: 893700 Loss: 0.0035203760489821434  PSNR: 30.400941848754883
[TRAIN] Iter: 893800 Loss: 0.0049541667103767395  PSNR: 28.072818756103516
[TRAIN] Iter: 893900 Loss: 0.003782105166465044  PSNR: 29.846641540527344
[TRAIN] Iter: 894000 Loss: 0.005361884366720915  PSNR: 27.85360336303711
[TRAIN] Iter: 894100 Loss: 0.003578550647944212  PSNR: 30.012195587158203
[TRAIN] Iter: 894200 Loss: 0.00477597676217556  PSNR: 28.628995895385742
[TRAIN] Iter: 894300 Loss: 0.005368929356336594  PSNR: 27.371530532836914
[TRAIN] Iter: 894400 Loss: 0.005005054175853729  PSNR: 28.271350860595703
[TRAIN] Iter: 894500 Loss: 0.004891273565590382  PSNR: 27.711870193481445
[TRAIN] Iter: 894600 Loss: 0.005382610484957695  PSNR: 26.89836311340332
[TRAIN] Iter: 894700 Loss: 0.003945686388760805  PSNR: 29.710657119750977
[TRAIN] Iter: 894800 Loss: 0.003829540451988578  PSNR: 28.761890411376953
[TRAIN] Iter: 894900 Loss: 0.0035189781337976456  PSNR: 30.49964714050293
[TRAIN] Iter: 895000 Loss: 0.003842974314466119  PSNR: 29.646041870117188
[TRAIN] Iter: 895100 Loss: 0.005082130432128906  PSNR: 27.90899658203125
[TRAIN] Iter: 895200 Loss: 0.003697741776704788  PSNR: 30.53517723083496
[TRAIN] Iter: 895300 Loss: 0.0054967827163636684  PSNR: 26.94532012939453
[TRAIN] Iter: 895400 Loss: 0.0046340953558683395  PSNR: 28.92372703552246
[TRAIN] Iter: 895500 Loss: 0.004785550758242607  PSNR: 27.99518394470215
[TRAIN] Iter: 895600 Loss: 0.00488281948491931  PSNR: 27.92645835876465
[TRAIN] Iter: 895700 Loss: 0.004852836020290852  PSNR: 27.834196090698242
[TRAIN] Iter: 895800 Loss: 0.004009959287941456  PSNR: 29.616268157958984
[TRAIN] Iter: 895900 Loss: 0.004370338749140501  PSNR: 28.377269744873047
[TRAIN] Iter: 896000 Loss: 0.004328074399381876  PSNR: 28.553821563720703
[TRAIN] Iter: 896100 Loss: 0.005025039426982403  PSNR: 28.60017204284668
[TRAIN] Iter: 896200 Loss: 0.0035091929603368044  PSNR: 30.65199089050293
[TRAIN] Iter: 896300 Loss: 0.0049337418749928474  PSNR: 28.889362335205078
[TRAIN] Iter: 896400 Loss: 0.005847347900271416  PSNR: 27.157974243164062
[TRAIN] Iter: 896500 Loss: 0.00415362324565649  PSNR: 29.616764068603516
[TRAIN] Iter: 896600 Loss: 0.0035296485293656588  PSNR: 29.628652572631836
[TRAIN] Iter: 896700 Loss: 0.004461273550987244  PSNR: 28.376567840576172
[TRAIN] Iter: 896800 Loss: 0.003317619673907757  PSNR: 31.07678985595703
[TRAIN] Iter: 896900 Loss: 0.0037332752253860235  PSNR: 29.751209259033203
[TRAIN] Iter: 897000 Loss: 0.003615038003772497  PSNR: 30.508249282836914
[TRAIN] Iter: 897100 Loss: 0.005007011815905571  PSNR: 27.783802032470703
[TRAIN] Iter: 897200 Loss: 0.004651316441595554  PSNR: 28.390682220458984
[TRAIN] Iter: 897300 Loss: 0.0053602419793605804  PSNR: 27.596040725708008
[TRAIN] Iter: 897400 Loss: 0.003872340079396963  PSNR: 30.51660919189453
[TRAIN] Iter: 897500 Loss: 0.0051711127161979675  PSNR: 27.931961059570312
[TRAIN] Iter: 897600 Loss: 0.003938612062484026  PSNR: 28.39107322692871
[TRAIN] Iter: 897700 Loss: 0.005175493657588959  PSNR: 27.52741241455078
[TRAIN] Iter: 897800 Loss: 0.0061438074335455894  PSNR: 26.661043167114258
[TRAIN] Iter: 897900 Loss: 0.003695641178637743  PSNR: 29.838518142700195
[TRAIN] Iter: 898000 Loss: 0.0054699573665857315  PSNR: 26.678586959838867
[TRAIN] Iter: 898100 Loss: 0.00307206017896533  PSNR: 30.7158145904541
[TRAIN] Iter: 898200 Loss: 0.004186676815152168  PSNR: 29.92728042602539
[TRAIN] Iter: 898300 Loss: 0.0035951496101915836  PSNR: 29.49408531188965
[TRAIN] Iter: 898400 Loss: 0.005326129496097565  PSNR: 27.671031951904297
[TRAIN] Iter: 898500 Loss: 0.004300052300095558  PSNR: 28.28804588317871
[TRAIN] Iter: 898600 Loss: 0.004213298205286264  PSNR: 28.877288818359375
[TRAIN] Iter: 898700 Loss: 0.004513835068792105  PSNR: 28.42414093017578
[TRAIN] Iter: 898800 Loss: 0.0050983428955078125  PSNR: 28.06902313232422
[TRAIN] Iter: 898900 Loss: 0.004728951491415501  PSNR: 28.012102127075195
[TRAIN] Iter: 899000 Loss: 0.0036587093491107225  PSNR: 29.903738021850586
[TRAIN] Iter: 899100 Loss: 0.004840300418436527  PSNR: 27.31183433532715
[TRAIN] Iter: 899200 Loss: 0.004029472358524799  PSNR: 29.46344757080078
[TRAIN] Iter: 899300 Loss: 0.005378931760787964  PSNR: 27.181224822998047
[TRAIN] Iter: 899400 Loss: 0.0031056359875947237  PSNR: 30.685056686401367
[TRAIN] Iter: 899500 Loss: 0.004953341092914343  PSNR: 27.62494468688965
[TRAIN] Iter: 899600 Loss: 0.0046280124224722385  PSNR: 28.848783493041992
[TRAIN] Iter: 899700 Loss: 0.004901522304862738  PSNR: 28.4038143157959
[TRAIN] Iter: 899800 Loss: 0.0035674511454999447  PSNR: 30.552734375
[TRAIN] Iter: 899900 Loss: 0.0051158335991203785  PSNR: 27.374231338500977
Saved checkpoints at ./logs/TUT-KE101-nerf/900000.tar
0 0.0003845691680908203
torch.Size([320, 640, 3]) torch.Size([320, 640])
1 15.203261137008667
2 13.573390007019043
3 13.595317125320435
4 14.965988636016846
5 13.556155681610107
6 15.096176862716675
7 13.449603080749512
8 14.977506637573242
9 13.659510135650635
10 13.467342615127563
11 15.099366664886475
12 13.469987869262695
13 15.12753939628601
14 13.498482465744019
15 13.553761959075928
16 14.944849729537964
17 13.566194534301758
18 15.123076438903809
19 13.324134111404419
20 15.15957760810852
21 13.303016424179077
22 13.544527292251587
23 14.992915391921997
24 13.35483717918396
25 15.144663333892822
26 13.276636600494385
27 15.096084356307983
28 13.653918504714966
29 13.265363216400146
30 15.286493062973022
31 13.260449647903442
32 15.280612707138062
33 13.277191877365112
34 13.544033527374268
35 15.033727407455444
36 13.414285898208618
37 15.150160551071167
38 13.32944655418396
39 15.201635122299194
40 13.337112426757812
41 13.486344575881958
42 14.947299003601074
43 13.483078002929688
44 15.237128496170044
45 13.16987657546997
46 13.802688837051392
47 14.841262102127075
48 13.448333263397217
49 14.903853416442871
50 13.309507846832275
51 15.423360347747803
52 13.57477593421936
53 13.587491273880005
54 14.978609561920166
55 13.612690687179565
56 14.955655336380005
57 13.501442670822144
58 14.961887121200562
59 13.557199954986572
60 13.604380369186401
61 14.911292314529419
62 13.556894063949585
63 14.970736026763916
64 13.621621370315552
65 13.55354642868042
66 14.920367956161499
67 13.463850975036621
68 14.959381580352783
69 13.539518117904663
70 14.90841794013977
71 13.538295030593872
72 13.546249628067017
73 13.587725162506104
74 15.567835330963135
75 13.536566734313965
76 14.946022510528564
77 13.559585332870483
78 13.5109703540802
79 14.971845388412476
80 13.6733078956604
81 14.810656070709229
82 13.440633773803711
83 14.941277742385864
84 13.544757843017578
85 13.528686046600342
86 14.949105739593506
87 13.552692651748657
88 14.886051893234253
89 13.544645071029663
90 13.557318210601807
91 14.88818073272705
92 13.546691417694092
93 14.93931531906128
94 13.50376033782959
95 14.969197750091553
96 13.547075748443604
97 13.528400897979736
98 15.012006998062134
99 13.416058778762817
100 15.14858865737915
101 13.46005892753601
102 14.865746021270752
103 13.6360764503479
104 13.436764240264893
105 15.1205415725708
106 13.472955465316772
107 14.926828622817993
108 13.499134063720703
109 13.425098657608032
110 14.97273874282837
111 13.442863464355469
112 15.160456657409668
113 13.279931783676147
114 15.155649423599243
115 13.366666078567505
116 13.440187931060791
117 15.272154092788696
118 13.119709968566895
119 15.369717836380005
Done, saving (120, 320, 640, 3) (120, 320, 640)
extras:{'raw': tensor([[[-5.9676e+00, -5.7597e+00, -5.2485e+00, -6.5996e+01],
         [-1.8935e+00, -1.4937e+00, -7.2446e-01, -4.9504e+01],
         [-1.6555e+00, -1.7689e+00, -1.8911e+00, -9.6615e+00],
         ...,
         [-5.2774e+00, -4.8784e+00, -5.2496e+00, -2.0983e+02],
         [-5.4671e+00, -5.4402e+00, -6.3687e+00, -1.4520e+02],
         [-6.2081e+00, -6.0780e+00, -6.7624e+00, -1.5159e+02]],

        [[ 1.7349e-01,  1.2380e-01,  2.5743e-01, -5.6773e+01],
         [-1.3794e-01, -3.1642e-01, -5.9506e-01,  1.7710e+01],
         [-1.1584e-01, -2.9391e-01, -6.1034e-01,  1.7913e+00],
         ...,
         [-1.3413e+00, -2.9147e+00, -1.1076e+01,  7.7733e+02],
         [-1.6462e+00, -3.0931e+00, -1.0891e+01,  8.3853e+02],
         [-1.2358e+00, -3.1005e+00, -1.1991e+01,  8.5300e+02]],

        [[ 1.0586e+00,  9.3608e-01,  3.0184e-01, -4.7772e+01],
         [-3.4835e+00, -4.3133e+00, -6.0350e+00, -2.2271e+01],
         [ 1.1800e+00,  1.0415e+00,  4.6385e-01,  1.2817e+01],
         ...,
         [-1.5544e+01, -1.4517e+01, -1.1966e+01, -3.2884e+02],
         [-1.5823e+01, -1.4391e+01, -1.1036e+01, -2.5190e+02],
         [-1.6456e+01, -1.5386e+01, -1.2384e+01, -2.2010e+02]],

        ...,

        [[ 5.4550e-01,  3.9975e-01,  3.6551e-01, -6.5179e+01],
         [-6.9105e-02, -2.2727e-01, -3.8942e-01, -1.4382e+01],
         [-6.9363e-02, -2.4553e-01, -4.3649e-01, -1.4402e+01],
         ...,
         [-1.5986e+00, -2.9356e+00, -1.1026e+01,  7.8520e+02],
         [-2.1867e+00, -2.9662e+00, -1.0591e+01,  7.7800e+02],
         [-2.0316e+00, -3.0392e+00, -1.2250e+01,  9.3105e+02]],

        [[ 7.1428e-01,  8.0009e-01,  1.4152e+00, -5.6984e+01],
         [-2.2036e-01, -4.1966e-01, -8.7097e-01, -1.8696e+01],
         [-2.1925e-01, -4.7358e-01, -9.0851e-01, -1.2186e+01],
         ...,
         [ 1.0668e+00, -1.9266e+00, -1.1232e+01,  6.6024e+02],
         [ 8.7471e-01, -2.1423e+00, -1.1314e+01,  7.2951e+02],
         [ 6.7493e-01, -2.1776e+00, -1.1197e+01,  8.3840e+02]],

        [[-5.9037e+00, -6.4796e+00, -8.3875e+00, -6.1635e+01],
         [-1.1606e-01, -2.7815e-01, -7.8576e-01, -2.9330e+01],
         [-5.0596e-01, -4.5242e-01, -7.6152e-01, -7.6176e+00],
         ...,
         [-1.9850e+01, -1.8975e+01, -1.6403e+01, -3.9871e+02],
         [-1.8943e+01, -1.7684e+01, -1.4703e+01, -3.4656e+02],
         [-1.9519e+01, -1.8687e+01, -1.6553e+01, -2.5197e+02]]],
       grad_fn=<ReshapeAliasBackward0>), 'rgb0': tensor([[0.2303, 0.2127, 0.1811],
        [0.4694, 0.4197, 0.3434],
        [0.6098, 0.5321, 0.3557],
        ...,
        [0.4690, 0.4267, 0.3642],
        [0.4647, 0.4155, 0.3287],
        [0.3806, 0.3723, 0.3081]], grad_fn=<ReshapeAliasBackward0>), 'disp0': tensor([ 55.2347, 305.7209, 259.6726,  ..., 397.7770, 374.4002,  65.1011],
       grad_fn=<ReshapeAliasBackward0>), 'acc0': tensor([1., 1., 1.,  ..., 1., 1., 1.], grad_fn=<ReshapeAliasBackward0>), 'z_std': tensor([0.0026, 0.2973, 0.2873,  ..., 0.3356, 0.0908, 0.0028])}
0 0.00044345855712890625
torch.Size([320, 640, 3]) torch.Size([320, 640])
1 15.233904361724854
2 13.334702968597412
3 13.451012134552002
4 15.092236042022705
5 13.43172550201416
6 15.104641914367676
7 13.520370960235596
8 15.02969741821289
9 13.564541816711426
10 13.472443342208862
11 15.102084398269653
12 13.569122552871704
13 14.958512306213379
14 13.535086631774902
15 13.584879636764526
16 14.968310594558716
17 13.570242881774902
18 14.9880690574646
19 13.55580472946167
20 14.967857360839844
21 13.545503377914429
22 13.594789743423462
23 14.944039583206177
24 13.58909559249878
25 14.990729808807373
26 13.57634687423706
27 14.989916324615479
28 13.592372179031372
29 13.575064420700073
30 15.012340784072876
31 13.569175720214844
32 15.036317586898804
33 13.561134099960327
34 13.570268630981445
35 15.00464415550232
36 13.523162841796875
37 15.051008462905884
38 13.489847898483276
39 15.111026763916016
40 13.625590562820435
41 13.514025926589966
42 14.958062648773193
43 13.55870795249939
44 15.037697315216064
45 13.561731100082397
46 13.522490978240967
47 15.058311462402344
48 13.56693720817566
49 15.004795789718628
50 13.564012050628662
51 15.026273727416992
52 13.521842956542969
53 13.544898986816406
54 15.059741973876953
55 13.554027795791626
56 15.035070180892944
57 13.558814525604248
58 15.021590948104858
59 13.536027431488037
60 13.608967781066895
61 15.019490242004395
62 13.46528434753418
63 15.138314485549927
64 13.502181053161621
65 13.594234704971313
66 15.134070873260498
67 13.4460768699646
68 15.187485456466675
69 13.413499593734741
70 15.108722686767578
71 13.549337387084961
72 13.632199048995972
73 15.039887189865112
74 13.525312900543213
75 15.151966571807861
76 13.317243576049805
77 15.111827611923218
78 13.418823480606079
79 13.598272562026978
80 15.144623517990112
81 13.318243503570557
82 15.375763177871704
83 13.252303123474121
84 13.473017930984497
85 15.066384553909302
86 13.463548421859741
87 15.314719438552856
88 13.242345571517944
89 15.295874118804932
90 13.259612083435059
91 13.536860704421997
92 15.138410329818726
93 13.472328424453735
94 15.287135362625122
95 13.246958017349243
96 15.197941780090332
97 13.364538431167603
98 13.592479944229126
99 15.231183290481567
100 13.569422483444214
101 15.006590127944946
102 13.545757532119751
103 13.554322481155396
104 14.979413986206055
105 13.558705568313599
106 15.010064601898193
107 13.577486515045166
108 15.009808540344238
109 14.059158325195312
110 15.34864854812622
111 16.87746787071228
112 15.42152738571167
113 16.884141206741333
114 15.410331726074219
115 16.862263441085815
116 15.406233787536621
117 16.85979986190796
118 15.478366374969482
119 16.892189025878906
test poses shape torch.Size([4, 3, 4])
0 0.0006449222564697266
torch.Size([320, 640, 3]) torch.Size([320, 640])
1 16.881023406982422
2 15.522176265716553
3 16.95846724510193
Saved test set
[TRAIN] Iter: 900000 Loss: 0.005023354664444923  PSNR: 27.534866333007812
[TRAIN] Iter: 900100 Loss: 0.0050311884842813015  PSNR: 27.576332092285156
[TRAIN] Iter: 900200 Loss: 0.003710035467520356  PSNR: 30.410371780395508
[TRAIN] Iter: 900300 Loss: 0.005078510846942663  PSNR: 27.620433807373047
[TRAIN] Iter: 900400 Loss: 0.005241541191935539  PSNR: 27.50303077697754
[TRAIN] Iter: 900500 Loss: 0.004544142633676529  PSNR: 27.915699005126953
[TRAIN] Iter: 900600 Loss: 0.004454722162336111  PSNR: 28.463226318359375
[TRAIN] Iter: 900700 Loss: 0.005089638754725456  PSNR: 28.196212768554688
[TRAIN] Iter: 900800 Loss: 0.004083604086190462  PSNR: 29.079235076904297
[TRAIN] Iter: 900900 Loss: 0.004111021291464567  PSNR: 28.69143295288086
[TRAIN] Iter: 901000 Loss: 0.003635549917817116  PSNR: 29.899242401123047
[TRAIN] Iter: 901100 Loss: 0.005339886527508497  PSNR: 27.131515502929688
[TRAIN] Iter: 901200 Loss: 0.0040178727358579636  PSNR: 29.48567008972168
[TRAIN] Iter: 901300 Loss: 0.004305969923734665  PSNR: 28.604530334472656
[TRAIN] Iter: 901400 Loss: 0.004729400388896465  PSNR: 27.728336334228516
[TRAIN] Iter: 901500 Loss: 0.0035087568685412407  PSNR: 30.517383575439453
[TRAIN] Iter: 901600 Loss: 0.006195442285388708  PSNR: 26.949443817138672
[TRAIN] Iter: 901700 Loss: 0.005631019826978445  PSNR: 26.92688751220703
[TRAIN] Iter: 901800 Loss: 0.003762453328818083  PSNR: 29.791900634765625
[TRAIN] Iter: 901900 Loss: 0.004043939523398876  PSNR: 29.09062957763672
[TRAIN] Iter: 902000 Loss: 0.003924018237739801  PSNR: 29.76668357849121
[TRAIN] Iter: 902100 Loss: 0.005226508714258671  PSNR: 28.440231323242188
[TRAIN] Iter: 902200 Loss: 0.004327577073127031  PSNR: 28.724912643432617
[TRAIN] Iter: 902300 Loss: 0.005139635875821114  PSNR: 27.502586364746094
[TRAIN] Iter: 902400 Loss: 0.0030043639708310366  PSNR: 30.71890640258789
[TRAIN] Iter: 902500 Loss: 0.006190643645823002  PSNR: 26.916139602661133
[TRAIN] Iter: 902600 Loss: 0.004008307121694088  PSNR: 28.8433837890625
[TRAIN] Iter: 902700 Loss: 0.005098693538457155  PSNR: 28.072059631347656
[TRAIN] Iter: 902800 Loss: 0.00458372151479125  PSNR: 28.318986892700195
[TRAIN] Iter: 902900 Loss: 0.005038085859268904  PSNR: 27.907764434814453
[TRAIN] Iter: 903000 Loss: 0.004026799462735653  PSNR: 29.037776947021484
[TRAIN] Iter: 903100 Loss: 0.005344591103494167  PSNR: 27.824644088745117
[TRAIN] Iter: 903200 Loss: 0.00491731334477663  PSNR: 27.691112518310547
[TRAIN] Iter: 903300 Loss: 0.005172311794012785  PSNR: 27.404891967773438
[TRAIN] Iter: 903400 Loss: 0.003994095139205456  PSNR: 29.50462532043457
[TRAIN] Iter: 903500 Loss: 0.0036072414368391037  PSNR: 30.80565071105957
[TRAIN] Iter: 903600 Loss: 0.004966991953551769  PSNR: 27.775163650512695
[TRAIN] Iter: 903700 Loss: 0.003605165518820286  PSNR: 30.673419952392578
[TRAIN] Iter: 903800 Loss: 0.0037931781262159348  PSNR: 29.81060791015625
[TRAIN] Iter: 903900 Loss: 0.003819745033979416  PSNR: 30.484521865844727
[TRAIN] Iter: 904000 Loss: 0.004332534968852997  PSNR: 29.89116668701172
[TRAIN] Iter: 904100 Loss: 0.005375330336391926  PSNR: 27.911598205566406
[TRAIN] Iter: 904200 Loss: 0.003994917497038841  PSNR: 30.29669189453125
[TRAIN] Iter: 904300 Loss: 0.004470604471862316  PSNR: 28.705554962158203
[TRAIN] Iter: 904400 Loss: 0.0055047981441020966  PSNR: 27.7443904876709
[TRAIN] Iter: 904500 Loss: 0.003165074624121189  PSNR: 30.7236270904541
[TRAIN] Iter: 904600 Loss: 0.0042166514322161674  PSNR: 28.68436050415039
[TRAIN] Iter: 904700 Loss: 0.0035798344761133194  PSNR: 30.57148551940918
[TRAIN] Iter: 904800 Loss: 0.005265724845230579  PSNR: 27.953250885009766
[TRAIN] Iter: 904900 Loss: 0.005010630469769239  PSNR: 27.904123306274414
[TRAIN] Iter: 905000 Loss: 0.005528855137526989  PSNR: 28.00117301940918
[TRAIN] Iter: 905100 Loss: 0.00526817562058568  PSNR: 27.538114547729492
[TRAIN] Iter: 905200 Loss: 0.003752098884433508  PSNR: 28.364042282104492
[TRAIN] Iter: 905300 Loss: 0.005728837102651596  PSNR: 26.72354507446289
[TRAIN] Iter: 905400 Loss: 0.0041370149701833725  PSNR: 28.3618221282959
[TRAIN] Iter: 905500 Loss: 0.005282689351588488  PSNR: 27.690261840820312
[TRAIN] Iter: 905600 Loss: 0.0032953177578747272  PSNR: 30.741748809814453
[TRAIN] Iter: 905700 Loss: 0.0048992568626999855  PSNR: 27.53398895263672
[TRAIN] Iter: 905800 Loss: 0.00472870422527194  PSNR: 28.68258285522461
[TRAIN] Iter: 905900 Loss: 0.0039015847723931074  PSNR: 28.6263484954834
[TRAIN] Iter: 906000 Loss: 0.005104497540742159  PSNR: 27.44803810119629
[TRAIN] Iter: 906100 Loss: 0.0037318577524274588  PSNR: 29.79568099975586
[TRAIN] Iter: 906200 Loss: 0.004916775971651077  PSNR: 28.32825469970703
[TRAIN] Iter: 906300 Loss: 0.00469700014218688  PSNR: 27.825435638427734
[TRAIN] Iter: 906400 Loss: 0.005840348079800606  PSNR: 26.77281379699707
[TRAIN] Iter: 906500 Loss: 0.004700340796262026  PSNR: 28.570241928100586
[TRAIN] Iter: 906600 Loss: 0.00327095459215343  PSNR: 30.33277130126953
[TRAIN] Iter: 906700 Loss: 0.005840006750077009  PSNR: 27.984983444213867
[TRAIN] Iter: 906800 Loss: 0.004381444305181503  PSNR: 28.29217529296875
[TRAIN] Iter: 906900 Loss: 0.003228717250749469  PSNR: 29.818336486816406
[TRAIN] Iter: 907000 Loss: 0.005157547537237406  PSNR: 28.107851028442383
[TRAIN] Iter: 907100 Loss: 0.0055753313936293125  PSNR: 27.43899154663086
[TRAIN] Iter: 907200 Loss: 0.00429181382060051  PSNR: 28.59217071533203
[TRAIN] Iter: 907300 Loss: 0.0032437907066196203  PSNR: 30.144609451293945
[TRAIN] Iter: 907400 Loss: 0.004665052518248558  PSNR: 28.437040328979492
[TRAIN] Iter: 907500 Loss: 0.0050700572319328785  PSNR: 28.05445098876953
[TRAIN] Iter: 907600 Loss: 0.005744726397097111  PSNR: 27.813474655151367
[TRAIN] Iter: 907700 Loss: 0.004188705235719681  PSNR: 28.525680541992188
[TRAIN] Iter: 907800 Loss: 0.0054512787610292435  PSNR: 27.06612205505371
[TRAIN] Iter: 907900 Loss: 0.004266559146344662  PSNR: 29.944311141967773
[TRAIN] Iter: 908000 Loss: 0.005211878567934036  PSNR: 26.975980758666992
[TRAIN] Iter: 908100 Loss: 0.003726428374648094  PSNR: 29.59517478942871
[TRAIN] Iter: 908200 Loss: 0.004395131021738052  PSNR: 28.912683486938477
[TRAIN] Iter: 908300 Loss: 0.004606414586305618  PSNR: 28.461181640625
[TRAIN] Iter: 908400 Loss: 0.004064576700329781  PSNR: 28.77716064453125
[TRAIN] Iter: 908500 Loss: 0.0048145935870707035  PSNR: 28.362333297729492
[TRAIN] Iter: 908600 Loss: 0.005403620190918446  PSNR: 27.191041946411133
[TRAIN] Iter: 908700 Loss: 0.004362862091511488  PSNR: 28.65020179748535
[TRAIN] Iter: 908800 Loss: 0.004179614596068859  PSNR: 28.79314613342285
[TRAIN] Iter: 908900 Loss: 0.004652364645153284  PSNR: 29.070606231689453
[TRAIN] Iter: 909000 Loss: 0.004230619873851538  PSNR: 29.482839584350586
[TRAIN] Iter: 909100 Loss: 0.0037205731496214867  PSNR: 28.752479553222656
[TRAIN] Iter: 909200 Loss: 0.0034027262590825558  PSNR: 30.558517456054688
[TRAIN] Iter: 909300 Loss: 0.00418792013078928  PSNR: 28.302532196044922
[TRAIN] Iter: 909400 Loss: 0.003634385298937559  PSNR: 29.316272735595703
[TRAIN] Iter: 909500 Loss: 0.003915283363312483  PSNR: 29.214567184448242
[TRAIN] Iter: 909600 Loss: 0.004071615170687437  PSNR: 28.919090270996094
[TRAIN] Iter: 909700 Loss: 0.006192980799823999  PSNR: 26.566240310668945
[TRAIN] Iter: 909800 Loss: 0.004434871021658182  PSNR: 28.081340789794922
[TRAIN] Iter: 909900 Loss: 0.0037403879687190056  PSNR: 29.92310905456543
Saved checkpoints at ./logs/TUT-KE101-nerf/910000.tar
[TRAIN] Iter: 910000 Loss: 0.004210024140775204  PSNR: 29.045385360717773
[TRAIN] Iter: 910100 Loss: 0.003635501954704523  PSNR: 30.25080108642578
[TRAIN] Iter: 910200 Loss: 0.00418747216463089  PSNR: 27.9085750579834
[TRAIN] Iter: 910300 Loss: 0.003930657636374235  PSNR: 28.957393646240234
[TRAIN] Iter: 910400 Loss: 0.004814989864826202  PSNR: 28.21004867553711
[TRAIN] Iter: 910500 Loss: 0.00393675547093153  PSNR: 28.878969192504883
[TRAIN] Iter: 910600 Loss: 0.006041611544787884  PSNR: 26.699840545654297
[TRAIN] Iter: 910700 Loss: 0.004406871274113655  PSNR: 28.763532638549805
[TRAIN] Iter: 910800 Loss: 0.0054420894011855125  PSNR: 27.67816925048828
[TRAIN] Iter: 910900 Loss: 0.005267319269478321  PSNR: 27.6426944732666
[TRAIN] Iter: 911000 Loss: 0.003914585802704096  PSNR: 29.2379207611084
[TRAIN] Iter: 911100 Loss: 0.00523700937628746  PSNR: 27.34644889831543
[TRAIN] Iter: 911200 Loss: 0.004971172660589218  PSNR: 28.3764705657959
[TRAIN] Iter: 911300 Loss: 0.004974060226231813  PSNR: 28.112794876098633
[TRAIN] Iter: 911400 Loss: 0.0032062046229839325  PSNR: 29.751155853271484
[TRAIN] Iter: 911500 Loss: 0.004284446127712727  PSNR: 28.931297302246094
[TRAIN] Iter: 911600 Loss: 0.004276980180293322  PSNR: 28.773744583129883
[TRAIN] Iter: 911700 Loss: 0.005445703398436308  PSNR: 27.060623168945312
[TRAIN] Iter: 911800 Loss: 0.00468149920925498  PSNR: 28.309947967529297
[TRAIN] Iter: 911900 Loss: 0.0033622111659497023  PSNR: 30.52635383605957
[TRAIN] Iter: 912000 Loss: 0.00397054199129343  PSNR: 29.391437530517578
[TRAIN] Iter: 912100 Loss: 0.005349696613848209  PSNR: 27.38688850402832
[TRAIN] Iter: 912200 Loss: 0.0037787826731801033  PSNR: 29.83611488342285
[TRAIN] Iter: 912300 Loss: 0.004466190934181213  PSNR: 28.20631217956543
[TRAIN] Iter: 912400 Loss: 0.005284189246594906  PSNR: 27.904268264770508
[TRAIN] Iter: 912500 Loss: 0.003555696690455079  PSNR: 29.481727600097656
[TRAIN] Iter: 912600 Loss: 0.003360180649906397  PSNR: 29.82597541809082
[TRAIN] Iter: 912700 Loss: 0.005421164445579052  PSNR: 27.67545509338379
[TRAIN] Iter: 912800 Loss: 0.005984775722026825  PSNR: 26.65184211730957
[TRAIN] Iter: 912900 Loss: 0.00304231746122241  PSNR: 31.12250518798828
[TRAIN] Iter: 913000 Loss: 0.005147366784512997  PSNR: 28.00428581237793
[TRAIN] Iter: 913100 Loss: 0.0046159010380506516  PSNR: 28.30142593383789
[TRAIN] Iter: 913200 Loss: 0.003925066441297531  PSNR: 30.213701248168945
[TRAIN] Iter: 913300 Loss: 0.0037250081077218056  PSNR: 29.28264045715332
[TRAIN] Iter: 913400 Loss: 0.005021062679588795  PSNR: 27.871915817260742
[TRAIN] Iter: 913500 Loss: 0.0035990255419164896  PSNR: 29.820085525512695
[TRAIN] Iter: 913600 Loss: 0.004092793446034193  PSNR: 29.478397369384766
[TRAIN] Iter: 913700 Loss: 0.005450732074677944  PSNR: 27.03057289123535
[TRAIN] Iter: 913800 Loss: 0.004760308191180229  PSNR: 27.973989486694336
[TRAIN] Iter: 913900 Loss: 0.0033698768820613623  PSNR: 29.712797164916992
[TRAIN] Iter: 914000 Loss: 0.005448430776596069  PSNR: 26.994997024536133
[TRAIN] Iter: 914100 Loss: 0.003777191275730729  PSNR: 30.301300048828125
[TRAIN] Iter: 914200 Loss: 0.004717541858553886  PSNR: 27.66950798034668
[TRAIN] Iter: 914300 Loss: 0.006140512879937887  PSNR: 26.199827194213867
[TRAIN] Iter: 914400 Loss: 0.004230189137160778  PSNR: 27.8718204498291
[TRAIN] Iter: 914500 Loss: 0.004544776398688555  PSNR: 28.86665916442871
[TRAIN] Iter: 914600 Loss: 0.006325930822640657  PSNR: 26.964736938476562
[TRAIN] Iter: 914700 Loss: 0.005126761272549629  PSNR: 27.967275619506836
[TRAIN] Iter: 914800 Loss: 0.0029674640391021967  PSNR: 30.703899383544922
[TRAIN] Iter: 914900 Loss: 0.005231394898146391  PSNR: 27.506975173950195
[TRAIN] Iter: 915000 Loss: 0.0047315130941569805  PSNR: 27.867483139038086
[TRAIN] Iter: 915100 Loss: 0.005558397155255079  PSNR: 27.494260787963867
[TRAIN] Iter: 915200 Loss: 0.00503236148506403  PSNR: 27.82305335998535
[TRAIN] Iter: 915300 Loss: 0.005072607193142176  PSNR: 28.084945678710938
[TRAIN] Iter: 915400 Loss: 0.005071592982858419  PSNR: 27.42535972595215
[TRAIN] Iter: 915500 Loss: 0.00345314247533679  PSNR: 29.973058700561523
[TRAIN] Iter: 915600 Loss: 0.00473115174099803  PSNR: 27.945341110229492
[TRAIN] Iter: 915700 Loss: 0.004152611363679171  PSNR: 28.874893188476562
[TRAIN] Iter: 915800 Loss: 0.004910827614367008  PSNR: 27.7950382232666
[TRAIN] Iter: 915900 Loss: 0.005261802580207586  PSNR: 27.824800491333008
[TRAIN] Iter: 916000 Loss: 0.0036979643628001213  PSNR: 30.461503982543945
[TRAIN] Iter: 916100 Loss: 0.004345451481640339  PSNR: 28.406513214111328
[TRAIN] Iter: 916200 Loss: 0.004033675882965326  PSNR: 29.226221084594727
[TRAIN] Iter: 916300 Loss: 0.004830969497561455  PSNR: 28.531658172607422
[TRAIN] Iter: 916400 Loss: 0.0043207742273807526  PSNR: 27.96718978881836
[TRAIN] Iter: 916500 Loss: 0.0040637655183672905  PSNR: 28.999338150024414
[TRAIN] Iter: 916600 Loss: 0.005195894278585911  PSNR: 27.25495147705078
[TRAIN] Iter: 916700 Loss: 0.005322909913957119  PSNR: 27.020193099975586
[TRAIN] Iter: 916800 Loss: 0.0035780933685600758  PSNR: 30.353357315063477
[TRAIN] Iter: 916900 Loss: 0.0037971034180372953  PSNR: 29.655899047851562
[TRAIN] Iter: 917000 Loss: 0.003359984140843153  PSNR: 30.086448669433594
[TRAIN] Iter: 917100 Loss: 0.005533426068723202  PSNR: 27.676239013671875
[TRAIN] Iter: 917200 Loss: 0.0033950156066566706  PSNR: 30.13099479675293
[TRAIN] Iter: 917300 Loss: 0.004465061239898205  PSNR: 28.173887252807617
[TRAIN] Iter: 917400 Loss: 0.00326886516995728  PSNR: 29.561159133911133
[TRAIN] Iter: 917500 Loss: 0.005974956322461367  PSNR: 26.667970657348633
[TRAIN] Iter: 917600 Loss: 0.004465362057089806  PSNR: 27.89105796813965
[TRAIN] Iter: 917700 Loss: 0.005106796510517597  PSNR: 27.835403442382812
[TRAIN] Iter: 917800 Loss: 0.0033193109557032585  PSNR: 30.201778411865234
[TRAIN] Iter: 917900 Loss: 0.004825552925467491  PSNR: 27.871313095092773
[TRAIN] Iter: 918000 Loss: 0.005135351326316595  PSNR: 27.28618049621582
[TRAIN] Iter: 918100 Loss: 0.00490150973200798  PSNR: 27.833450317382812
[TRAIN] Iter: 918200 Loss: 0.004794624168425798  PSNR: 28.602323532104492
[TRAIN] Iter: 918300 Loss: 0.004589171148836613  PSNR: 28.311349868774414
[TRAIN] Iter: 918400 Loss: 0.004681697115302086  PSNR: 27.884815216064453
[TRAIN] Iter: 918500 Loss: 0.004135008901357651  PSNR: 29.106884002685547
[TRAIN] Iter: 918600 Loss: 0.0049403938464820385  PSNR: 28.160106658935547
[TRAIN] Iter: 918700 Loss: 0.006078196689486504  PSNR: 26.77594566345215
[TRAIN] Iter: 918800 Loss: 0.005228228867053986  PSNR: 27.90704917907715
[TRAIN] Iter: 918900 Loss: 0.0037994931917637587  PSNR: 29.541440963745117
[TRAIN] Iter: 919000 Loss: 0.004103789571672678  PSNR: 28.740283966064453
[TRAIN] Iter: 919100 Loss: 0.005229356698691845  PSNR: 28.124258041381836
[TRAIN] Iter: 919200 Loss: 0.003991045989096165  PSNR: 29.97701072692871
[TRAIN] Iter: 919300 Loss: 0.004253952763974667  PSNR: 28.53433609008789
[TRAIN] Iter: 919400 Loss: 0.006170709617435932  PSNR: 26.63398551940918
[TRAIN] Iter: 919500 Loss: 0.005135207436978817  PSNR: 27.412267684936523
[TRAIN] Iter: 919600 Loss: 0.0037506907247006893  PSNR: 30.067651748657227
[TRAIN] Iter: 919700 Loss: 0.004171651788055897  PSNR: 28.50468635559082
[TRAIN] Iter: 919800 Loss: 0.005112144630402327  PSNR: 28.111572265625
[TRAIN] Iter: 919900 Loss: 0.005605509039014578  PSNR: 27.502878189086914
Saved checkpoints at ./logs/TUT-KE101-nerf/920000.tar
[TRAIN] Iter: 920000 Loss: 0.004873444326221943  PSNR: 27.689098358154297
[TRAIN] Iter: 920100 Loss: 0.00412644911557436  PSNR: 30.294925689697266
[TRAIN] Iter: 920200 Loss: 0.0037294309586286545  PSNR: 30.736555099487305
[TRAIN] Iter: 920300 Loss: 0.004123985767364502  PSNR: 29.0130615234375
[TRAIN] Iter: 920400 Loss: 0.00387295288965106  PSNR: 29.476306915283203
[TRAIN] Iter: 920500 Loss: 0.0048380205407738686  PSNR: 27.550724029541016
[TRAIN] Iter: 920600 Loss: 0.004729899112135172  PSNR: 28.201974868774414
[TRAIN] Iter: 920700 Loss: 0.004915232770144939  PSNR: 27.62433624267578
[TRAIN] Iter: 920800 Loss: 0.003727063536643982  PSNR: 29.79916763305664
[TRAIN] Iter: 920900 Loss: 0.005245502106845379  PSNR: 27.76732635498047
[TRAIN] Iter: 921000 Loss: 0.004381854552775621  PSNR: 29.130781173706055
[TRAIN] Iter: 921100 Loss: 0.0033216297160834074  PSNR: 30.504348754882812
[TRAIN] Iter: 921200 Loss: 0.004624095745384693  PSNR: 27.717144012451172
[TRAIN] Iter: 921300 Loss: 0.0038175596855580807  PSNR: 28.832521438598633
[TRAIN] Iter: 921400 Loss: 0.004161931574344635  PSNR: 28.746654510498047
[TRAIN] Iter: 921500 Loss: 0.004709620960056782  PSNR: 27.86382484436035
[TRAIN] Iter: 921600 Loss: 0.0034567643888294697  PSNR: 29.664505004882812
[TRAIN] Iter: 921700 Loss: 0.004501056857407093  PSNR: 28.53792953491211
[TRAIN] Iter: 921800 Loss: 0.004291839897632599  PSNR: 28.084707260131836
[TRAIN] Iter: 921900 Loss: 0.005618823226541281  PSNR: 26.909879684448242
[TRAIN] Iter: 922000 Loss: 0.005225154105573893  PSNR: 27.620508193969727
[TRAIN] Iter: 922100 Loss: 0.0049664778634905815  PSNR: 27.661300659179688
[TRAIN] Iter: 922200 Loss: 0.005452889017760754  PSNR: 26.4052791595459
[TRAIN] Iter: 922300 Loss: 0.004094974137842655  PSNR: 30.03609848022461
[TRAIN] Iter: 922400 Loss: 0.005010577384382486  PSNR: 28.04767608642578
[TRAIN] Iter: 922500 Loss: 0.005962103605270386  PSNR: 26.689558029174805
[TRAIN] Iter: 922600 Loss: 0.004971290472894907  PSNR: 28.208847045898438
[TRAIN] Iter: 922700 Loss: 0.005335943773388863  PSNR: 26.24165916442871
[TRAIN] Iter: 922800 Loss: 0.0037590903230011463  PSNR: 29.212139129638672
[TRAIN] Iter: 922900 Loss: 0.005536189768463373  PSNR: 27.74754524230957
[TRAIN] Iter: 923000 Loss: 0.0037950449623167515  PSNR: 29.872419357299805
[TRAIN] Iter: 923100 Loss: 0.004750124178826809  PSNR: 28.475860595703125
[TRAIN] Iter: 923200 Loss: 0.004638207610696554  PSNR: 27.265148162841797
[TRAIN] Iter: 923300 Loss: 0.0034188495483249426  PSNR: 29.469865798950195
[TRAIN] Iter: 923400 Loss: 0.003119599772617221  PSNR: 30.958375930786133
[TRAIN] Iter: 923500 Loss: 0.004443752579391003  PSNR: 28.179929733276367
[TRAIN] Iter: 923600 Loss: 0.0054871691390872  PSNR: 27.429649353027344
[TRAIN] Iter: 923700 Loss: 0.004342115018516779  PSNR: 29.682538986206055
[TRAIN] Iter: 923800 Loss: 0.005372724961489439  PSNR: 27.491178512573242
[TRAIN] Iter: 923900 Loss: 0.0035840722266584635  PSNR: 29.931703567504883
[TRAIN] Iter: 924000 Loss: 0.0050354646518826485  PSNR: 28.295520782470703
[TRAIN] Iter: 924100 Loss: 0.004778580274432898  PSNR: 27.59430694580078
[TRAIN] Iter: 924200 Loss: 0.005349877756088972  PSNR: 28.405122756958008
[TRAIN] Iter: 924300 Loss: 0.0038207797333598137  PSNR: 28.895389556884766
[TRAIN] Iter: 924400 Loss: 0.004144421778619289  PSNR: 28.354324340820312
[TRAIN] Iter: 924500 Loss: 0.005057607311755419  PSNR: 27.64751434326172
[TRAIN] Iter: 924600 Loss: 0.0063950298354029655  PSNR: 26.75368309020996
[TRAIN] Iter: 924700 Loss: 0.003912594169378281  PSNR: 29.487058639526367
[TRAIN] Iter: 924800 Loss: 0.004928961396217346  PSNR: 27.59276580810547
[TRAIN] Iter: 924900 Loss: 0.004916563630104065  PSNR: 28.365842819213867
[TRAIN] Iter: 925000 Loss: 0.00549791706725955  PSNR: 27.495454788208008
[TRAIN] Iter: 925100 Loss: 0.004923432134091854  PSNR: 29.042034149169922
[TRAIN] Iter: 925200 Loss: 0.003606978803873062  PSNR: 29.38323211669922
[TRAIN] Iter: 925300 Loss: 0.0034165000542998314  PSNR: 30.28280258178711
[TRAIN] Iter: 925400 Loss: 0.0032017456833273172  PSNR: 30.524272918701172
[TRAIN] Iter: 925500 Loss: 0.003539334749802947  PSNR: 30.30723762512207
[TRAIN] Iter: 925600 Loss: 0.0050516435876488686  PSNR: 28.445234298706055
[TRAIN] Iter: 925700 Loss: 0.004565260838717222  PSNR: 28.47062873840332
[TRAIN] Iter: 925800 Loss: 0.0038589765317738056  PSNR: 29.513233184814453
[TRAIN] Iter: 925900 Loss: 0.004290524870157242  PSNR: 28.91183853149414
[TRAIN] Iter: 926000 Loss: 0.0047470275312662125  PSNR: 27.512184143066406
[TRAIN] Iter: 926100 Loss: 0.0049498677253723145  PSNR: 28.087635040283203
[TRAIN] Iter: 926200 Loss: 0.004196478519588709  PSNR: 29.31353759765625
[TRAIN] Iter: 926300 Loss: 0.00360320508480072  PSNR: 30.14581298828125
[TRAIN] Iter: 926400 Loss: 0.004626335576176643  PSNR: 27.54217529296875
[TRAIN] Iter: 926500 Loss: 0.0037162185180932283  PSNR: 29.582944869995117
[TRAIN] Iter: 926600 Loss: 0.003724426031112671  PSNR: 29.19462776184082
[TRAIN] Iter: 926700 Loss: 0.002586252987384796  PSNR: 30.837181091308594
[TRAIN] Iter: 926800 Loss: 0.003554163035005331  PSNR: 30.193784713745117
[TRAIN] Iter: 926900 Loss: 0.004475906491279602  PSNR: 29.256772994995117
[TRAIN] Iter: 927000 Loss: 0.0035714833065867424  PSNR: 29.643596649169922
[TRAIN] Iter: 927100 Loss: 0.0051457989029586315  PSNR: 28.57133674621582
[TRAIN] Iter: 927200 Loss: 0.004711394663900137  PSNR: 28.055952072143555
[TRAIN] Iter: 927300 Loss: 0.005277434829622507  PSNR: 27.281017303466797
[TRAIN] Iter: 927400 Loss: 0.005697144661098719  PSNR: 27.24288558959961
[TRAIN] Iter: 927500 Loss: 0.004638588987290859  PSNR: 28.469057083129883
[TRAIN] Iter: 927600 Loss: 0.003929255064576864  PSNR: 29.554906845092773
[TRAIN] Iter: 927700 Loss: 0.004710899665951729  PSNR: 28.338966369628906
[TRAIN] Iter: 927800 Loss: 0.0029593859799206257  PSNR: 30.708906173706055
[TRAIN] Iter: 927900 Loss: 0.004207042045891285  PSNR: 29.3913631439209
[TRAIN] Iter: 928000 Loss: 0.004730631597340107  PSNR: 28.195947647094727
[TRAIN] Iter: 928100 Loss: 0.004520620219409466  PSNR: 28.164592742919922
[TRAIN] Iter: 928200 Loss: 0.0033163554035127163  PSNR: 30.524995803833008
[TRAIN] Iter: 928300 Loss: 0.005363027565181255  PSNR: 27.025049209594727
[TRAIN] Iter: 928400 Loss: 0.0032914848998188972  PSNR: 30.71588706970215
[TRAIN] Iter: 928500 Loss: 0.004421485587954521  PSNR: 28.33112144470215
[TRAIN] Iter: 928600 Loss: 0.005673442035913467  PSNR: 27.858449935913086
[TRAIN] Iter: 928700 Loss: 0.005135406274348497  PSNR: 27.597000122070312
[TRAIN] Iter: 928800 Loss: 0.005098540335893631  PSNR: 28.188976287841797
[TRAIN] Iter: 928900 Loss: 0.005198693834245205  PSNR: 27.81290054321289
[TRAIN] Iter: 929000 Loss: 0.004613555036485195  PSNR: 27.69089126586914
[TRAIN] Iter: 929100 Loss: 0.004335368517786264  PSNR: 28.65723991394043
[TRAIN] Iter: 929200 Loss: 0.005105846095830202  PSNR: 27.224580764770508
[TRAIN] Iter: 929300 Loss: 0.004975270479917526  PSNR: 28.032394409179688
[TRAIN] Iter: 929400 Loss: 0.0038105982821434736  PSNR: 30.06570816040039
[TRAIN] Iter: 929500 Loss: 0.00511180330067873  PSNR: 28.33367156982422
[TRAIN] Iter: 929600 Loss: 0.0047988868318498135  PSNR: 28.157419204711914
[TRAIN] Iter: 929700 Loss: 0.004920076113194227  PSNR: 28.177696228027344
[TRAIN] Iter: 929800 Loss: 0.004352117422968149  PSNR: 27.989788055419922
[TRAIN] Iter: 929900 Loss: 0.004005237948149443  PSNR: 29.797487258911133
Saved checkpoints at ./logs/TUT-KE101-nerf/930000.tar
[TRAIN] Iter: 930000 Loss: 0.003848574822768569  PSNR: 29.131732940673828
[TRAIN] Iter: 930100 Loss: 0.003776508616283536  PSNR: 29.49884033203125
[TRAIN] Iter: 930200 Loss: 0.0034906140062958  PSNR: 30.67653465270996
[TRAIN] Iter: 930300 Loss: 0.0041170381009578705  PSNR: 28.816448211669922
[TRAIN] Iter: 930400 Loss: 0.0031874659471213818  PSNR: 30.142860412597656
[TRAIN] Iter: 930500 Loss: 0.004693126305937767  PSNR: 28.08940887451172
[TRAIN] Iter: 930600 Loss: 0.004134817980229855  PSNR: 29.66817283630371
[TRAIN] Iter: 930700 Loss: 0.004268129356205463  PSNR: 28.54201889038086
[TRAIN] Iter: 930800 Loss: 0.0036379308439791203  PSNR: 29.776945114135742
[TRAIN] Iter: 930900 Loss: 0.003407936543226242  PSNR: 30.100379943847656
[TRAIN] Iter: 931000 Loss: 0.002992805326357484  PSNR: 30.44790267944336
[TRAIN] Iter: 931100 Loss: 0.003384321928024292  PSNR: 30.192800521850586
[TRAIN] Iter: 931200 Loss: 0.00564885139465332  PSNR: 26.565563201904297
[TRAIN] Iter: 931300 Loss: 0.005442626308649778  PSNR: 27.905643463134766
[TRAIN] Iter: 931400 Loss: 0.004227818921208382  PSNR: 28.85381317138672
[TRAIN] Iter: 931500 Loss: 0.0039015570655465126  PSNR: 30.286296844482422
[TRAIN] Iter: 931600 Loss: 0.004570137709379196  PSNR: 27.986122131347656
[TRAIN] Iter: 931700 Loss: 0.003517582081258297  PSNR: 30.363313674926758
[TRAIN] Iter: 931800 Loss: 0.005483301822096109  PSNR: 27.538442611694336
[TRAIN] Iter: 931900 Loss: 0.004559334367513657  PSNR: 28.324275970458984
[TRAIN] Iter: 932000 Loss: 0.005067828577011824  PSNR: 28.134618759155273
[TRAIN] Iter: 932100 Loss: 0.004251424223184586  PSNR: 28.154178619384766
[TRAIN] Iter: 932200 Loss: 0.0051195151172578335  PSNR: 28.68272590637207
[TRAIN] Iter: 932300 Loss: 0.004211551044136286  PSNR: 28.543079376220703
[TRAIN] Iter: 932400 Loss: 0.004264949355274439  PSNR: 28.827396392822266
[TRAIN] Iter: 932500 Loss: 0.006087992340326309  PSNR: 26.39964485168457
[TRAIN] Iter: 932600 Loss: 0.003498510457575321  PSNR: 30.248159408569336
[TRAIN] Iter: 932700 Loss: 0.004138421267271042  PSNR: 29.06026268005371
[TRAIN] Iter: 932800 Loss: 0.0052661653608083725  PSNR: 28.57895278930664
[TRAIN] Iter: 932900 Loss: 0.004507046192884445  PSNR: 27.849822998046875
[TRAIN] Iter: 933000 Loss: 0.00456595653668046  PSNR: 28.33587646484375
[TRAIN] Iter: 933100 Loss: 0.005188132170587778  PSNR: 28.462921142578125
[TRAIN] Iter: 933200 Loss: 0.00440446101129055  PSNR: 28.077930450439453
[TRAIN] Iter: 933300 Loss: 0.004523838870227337  PSNR: 28.8002986907959
[TRAIN] Iter: 933400 Loss: 0.0033350414596498013  PSNR: 30.621795654296875
[TRAIN] Iter: 933500 Loss: 0.0064261117950081825  PSNR: 27.094125747680664
[TRAIN] Iter: 933600 Loss: 0.005663362331688404  PSNR: 26.9541072845459
[TRAIN] Iter: 933700 Loss: 0.004291858058422804  PSNR: 27.855989456176758
[TRAIN] Iter: 933800 Loss: 0.005291971378028393  PSNR: 27.28229522705078
[TRAIN] Iter: 933900 Loss: 0.0037234239280223846  PSNR: 30.241540908813477
[TRAIN] Iter: 934000 Loss: 0.0038111130706965923  PSNR: 30.01702117919922
[TRAIN] Iter: 934100 Loss: 0.003776212688535452  PSNR: 29.092262268066406
[TRAIN] Iter: 934200 Loss: 0.003810415742918849  PSNR: 30.11735725402832
[TRAIN] Iter: 934300 Loss: 0.0036227505188435316  PSNR: 30.449857711791992
[TRAIN] Iter: 934400 Loss: 0.005490881856530905  PSNR: 27.60839080810547
[TRAIN] Iter: 934500 Loss: 0.0046101342886686325  PSNR: 28.09697914123535
[TRAIN] Iter: 934600 Loss: 0.0034720073454082012  PSNR: 29.084409713745117
[TRAIN] Iter: 934700 Loss: 0.004760900512337685  PSNR: 26.9123592376709
[TRAIN] Iter: 934800 Loss: 0.004541485104709864  PSNR: 27.967702865600586
[TRAIN] Iter: 934900 Loss: 0.003397903172299266  PSNR: 30.650205612182617
[TRAIN] Iter: 935000 Loss: 0.005200929008424282  PSNR: 27.488323211669922
[TRAIN] Iter: 935100 Loss: 0.005130651406943798  PSNR: 27.70355796813965
[TRAIN] Iter: 935200 Loss: 0.005027507431805134  PSNR: 27.401134490966797
[TRAIN] Iter: 935300 Loss: 0.005961326416581869  PSNR: 26.85192108154297
[TRAIN] Iter: 935400 Loss: 0.004162156954407692  PSNR: 28.813396453857422
[TRAIN] Iter: 935500 Loss: 0.0038869227282702923  PSNR: 30.035114288330078
[TRAIN] Iter: 935600 Loss: 0.00575490016490221  PSNR: 26.5711727142334
[TRAIN] Iter: 935700 Loss: 0.004708507098257542  PSNR: 27.878189086914062
[TRAIN] Iter: 935800 Loss: 0.005164165981113911  PSNR: 27.964231491088867
[TRAIN] Iter: 935900 Loss: 0.004699667915701866  PSNR: 27.622053146362305
[TRAIN] Iter: 936000 Loss: 0.004813867621123791  PSNR: 27.60064697265625
[TRAIN] Iter: 936100 Loss: 0.004449972417205572  PSNR: 28.534862518310547
[TRAIN] Iter: 936200 Loss: 0.0039191171526908875  PSNR: 28.90087127685547
[TRAIN] Iter: 936300 Loss: 0.004407698754221201  PSNR: 27.88313865661621
[TRAIN] Iter: 936400 Loss: 0.003481517545878887  PSNR: 29.312286376953125
[TRAIN] Iter: 936500 Loss: 0.003866552608087659  PSNR: 29.21925926208496
[TRAIN] Iter: 936600 Loss: 0.004412837326526642  PSNR: 29.515363693237305
[TRAIN] Iter: 936700 Loss: 0.006007749121636152  PSNR: 26.64836883544922
[TRAIN] Iter: 936800 Loss: 0.003723148023709655  PSNR: 30.129615783691406
[TRAIN] Iter: 936900 Loss: 0.0037424673791974783  PSNR: 30.291662216186523
[TRAIN] Iter: 937000 Loss: 0.005689184181392193  PSNR: 27.877897262573242
[TRAIN] Iter: 937100 Loss: 0.004043327644467354  PSNR: 29.668794631958008
[TRAIN] Iter: 937200 Loss: 0.004133089445531368  PSNR: 27.742534637451172
[TRAIN] Iter: 937300 Loss: 0.004982628859579563  PSNR: 27.498371124267578
[TRAIN] Iter: 937400 Loss: 0.0044225482270121574  PSNR: 27.74593734741211
[TRAIN] Iter: 937500 Loss: 0.0038454774767160416  PSNR: 30.475990295410156
[TRAIN] Iter: 937600 Loss: 0.003417693078517914  PSNR: 30.086366653442383
[TRAIN] Iter: 937700 Loss: 0.0035855229943990707  PSNR: 29.01873779296875
[TRAIN] Iter: 937800 Loss: 0.0042524319142103195  PSNR: 29.526464462280273
[TRAIN] Iter: 937900 Loss: 0.004518757574260235  PSNR: 29.287919998168945
[TRAIN] Iter: 938000 Loss: 0.0036842881236225367  PSNR: 30.720983505249023
[TRAIN] Iter: 938100 Loss: 0.00426468113437295  PSNR: 28.607677459716797
[TRAIN] Iter: 938200 Loss: 0.0040402039885520935  PSNR: 29.120914459228516
[TRAIN] Iter: 938300 Loss: 0.003914711531251669  PSNR: 29.678340911865234
[TRAIN] Iter: 938400 Loss: 0.0027721659280359745  PSNR: 31.116519927978516
[TRAIN] Iter: 938500 Loss: 0.004682712256908417  PSNR: 28.569496154785156
[TRAIN] Iter: 938600 Loss: 0.005156161263585091  PSNR: 27.09217643737793
[TRAIN] Iter: 938700 Loss: 0.005109267309308052  PSNR: 27.72494888305664
[TRAIN] Iter: 938800 Loss: 0.004086388275027275  PSNR: 29.903167724609375
[TRAIN] Iter: 938900 Loss: 0.005006236024200916  PSNR: 27.810630798339844
[TRAIN] Iter: 939000 Loss: 0.0034871515817940235  PSNR: 30.655733108520508
[TRAIN] Iter: 939100 Loss: 0.004886562004685402  PSNR: 27.57520866394043
[TRAIN] Iter: 939200 Loss: 0.004016062244772911  PSNR: 29.745227813720703
[TRAIN] Iter: 939300 Loss: 0.004089164547622204  PSNR: 28.898527145385742
[TRAIN] Iter: 939400 Loss: 0.005254257470369339  PSNR: 27.45134925842285
[TRAIN] Iter: 939500 Loss: 0.004097883589565754  PSNR: 29.355403900146484
[TRAIN] Iter: 939600 Loss: 0.005081031005829573  PSNR: 27.935766220092773
[TRAIN] Iter: 939700 Loss: 0.005425860174000263  PSNR: 27.157411575317383
[TRAIN] Iter: 939800 Loss: 0.003688506782054901  PSNR: 30.00941276550293
[TRAIN] Iter: 939900 Loss: 0.005108986049890518  PSNR: 27.39781379699707
Saved checkpoints at ./logs/TUT-KE101-nerf/940000.tar
[TRAIN] Iter: 940000 Loss: 0.004746444057673216  PSNR: 27.64759635925293
[TRAIN] Iter: 940100 Loss: 0.004712519235908985  PSNR: 27.97138786315918
[TRAIN] Iter: 940200 Loss: 0.0032408125698566437  PSNR: 30.20759391784668
[TRAIN] Iter: 940300 Loss: 0.004886430688202381  PSNR: 28.188644409179688
[TRAIN] Iter: 940400 Loss: 0.0044694095849990845  PSNR: 28.129587173461914
[TRAIN] Iter: 940500 Loss: 0.0032721012830734253  PSNR: 30.334402084350586
[TRAIN] Iter: 940600 Loss: 0.0038598510436713696  PSNR: 29.80997657775879
[TRAIN] Iter: 940700 Loss: 0.005530747584998608  PSNR: 27.017005920410156
[TRAIN] Iter: 940800 Loss: 0.0053119538351893425  PSNR: 27.731822967529297
[TRAIN] Iter: 940900 Loss: 0.006066969595849514  PSNR: 26.66836166381836
[TRAIN] Iter: 941000 Loss: 0.0037261543329805136  PSNR: 30.009483337402344
[TRAIN] Iter: 941100 Loss: 0.004447093233466148  PSNR: 27.94902229309082
[TRAIN] Iter: 941200 Loss: 0.005063924007117748  PSNR: 27.41858673095703
[TRAIN] Iter: 941300 Loss: 0.004945158027112484  PSNR: 27.365163803100586
[TRAIN] Iter: 941400 Loss: 0.005168111063539982  PSNR: 27.438383102416992
[TRAIN] Iter: 941500 Loss: 0.0037222434766590595  PSNR: 30.498815536499023
[TRAIN] Iter: 941600 Loss: 0.0039440179243683815  PSNR: 29.86630630493164
[TRAIN] Iter: 941700 Loss: 0.0037087476812303066  PSNR: 30.319822311401367
[TRAIN] Iter: 941800 Loss: 0.005226225592195988  PSNR: 28.202808380126953
[TRAIN] Iter: 941900 Loss: 0.004959283862262964  PSNR: 28.341007232666016
[TRAIN] Iter: 942000 Loss: 0.004790564067661762  PSNR: 27.97576904296875
[TRAIN] Iter: 942100 Loss: 0.0036811851896345615  PSNR: 29.54102325439453
[TRAIN] Iter: 942200 Loss: 0.003546234453096986  PSNR: 30.24854850769043
[TRAIN] Iter: 942300 Loss: 0.0039995936676859856  PSNR: 29.687725067138672
[TRAIN] Iter: 942400 Loss: 0.00412863539531827  PSNR: 29.61567497253418
[TRAIN] Iter: 942500 Loss: 0.005288098007440567  PSNR: 27.644636154174805
[TRAIN] Iter: 942600 Loss: 0.005283316597342491  PSNR: 27.00377655029297
[TRAIN] Iter: 942700 Loss: 0.004734937567263842  PSNR: 28.079090118408203
[TRAIN] Iter: 942800 Loss: 0.00518510676920414  PSNR: 27.690990447998047
[TRAIN] Iter: 942900 Loss: 0.003936856519430876  PSNR: 28.45772361755371
[TRAIN] Iter: 943000 Loss: 0.0036427408922463655  PSNR: 29.17401123046875
[TRAIN] Iter: 943100 Loss: 0.0032437078189104795  PSNR: 30.520376205444336
[TRAIN] Iter: 943200 Loss: 0.003991248086094856  PSNR: 29.851112365722656
[TRAIN] Iter: 943300 Loss: 0.004333280026912689  PSNR: 27.82254981994629
[TRAIN] Iter: 943400 Loss: 0.00448840344324708  PSNR: 29.069820404052734
[TRAIN] Iter: 943500 Loss: 0.0034318207763135433  PSNR: 29.062713623046875
[TRAIN] Iter: 943600 Loss: 0.0043075671419501305  PSNR: 28.596891403198242
[TRAIN] Iter: 943700 Loss: 0.005185058806091547  PSNR: 27.520553588867188
[TRAIN] Iter: 943800 Loss: 0.00374529673717916  PSNR: 29.915119171142578
[TRAIN] Iter: 943900 Loss: 0.0035495758056640625  PSNR: 30.10550880432129
[TRAIN] Iter: 944000 Loss: 0.005761938169598579  PSNR: 27.044424057006836
[TRAIN] Iter: 944100 Loss: 0.004554244689643383  PSNR: 28.064159393310547
[TRAIN] Iter: 944200 Loss: 0.005571823567152023  PSNR: 26.297277450561523
[TRAIN] Iter: 944300 Loss: 0.004219134338200092  PSNR: 28.79035186767578
[TRAIN] Iter: 944400 Loss: 0.00511650275439024  PSNR: 28.2545108795166
[TRAIN] Iter: 944500 Loss: 0.005257355980575085  PSNR: 27.58643341064453
[TRAIN] Iter: 944600 Loss: 0.005222355481237173  PSNR: 28.222570419311523
[TRAIN] Iter: 944700 Loss: 0.005456594750285149  PSNR: 27.221302032470703
[TRAIN] Iter: 944800 Loss: 0.004823474679142237  PSNR: 28.207637786865234
[TRAIN] Iter: 944900 Loss: 0.005596853792667389  PSNR: 27.441038131713867
[TRAIN] Iter: 945000 Loss: 0.005506773944944143  PSNR: 26.945653915405273
[TRAIN] Iter: 945100 Loss: 0.004707495681941509  PSNR: 27.998558044433594
[TRAIN] Iter: 945200 Loss: 0.004083589185029268  PSNR: 28.867475509643555
[TRAIN] Iter: 945300 Loss: 0.00481745321303606  PSNR: 28.001251220703125
[TRAIN] Iter: 945400 Loss: 0.0031656757928431034  PSNR: 30.22003936767578
[TRAIN] Iter: 945500 Loss: 0.0030405926518142223  PSNR: 30.88611602783203
[TRAIN] Iter: 945600 Loss: 0.005747674964368343  PSNR: 27.39232635498047
[TRAIN] Iter: 945700 Loss: 0.004058486316353083  PSNR: 29.424928665161133
[TRAIN] Iter: 945800 Loss: 0.0035750032402575016  PSNR: 29.717641830444336
[TRAIN] Iter: 945900 Loss: 0.0043446943163871765  PSNR: 28.02229881286621
[TRAIN] Iter: 946000 Loss: 0.0042433724738657475  PSNR: 28.1685791015625
[TRAIN] Iter: 946100 Loss: 0.004543795250356197  PSNR: 27.910062789916992
[TRAIN] Iter: 946200 Loss: 0.004490346182137728  PSNR: 28.75387191772461
[TRAIN] Iter: 946300 Loss: 0.005185683257877827  PSNR: 27.94007682800293
[TRAIN] Iter: 946400 Loss: 0.004438691306859255  PSNR: 28.20168685913086
[TRAIN] Iter: 946500 Loss: 0.004634048789739609  PSNR: 27.52570343017578
[TRAIN] Iter: 946600 Loss: 0.0043340944685041904  PSNR: 28.650609970092773
[TRAIN] Iter: 946700 Loss: 0.0032889521680772305  PSNR: 29.802387237548828
[TRAIN] Iter: 946800 Loss: 0.004833708517253399  PSNR: 27.902334213256836
[TRAIN] Iter: 946900 Loss: 0.0038365127984434366  PSNR: 29.430130004882812
[TRAIN] Iter: 947000 Loss: 0.005179385654628277  PSNR: 26.801589965820312
[TRAIN] Iter: 947100 Loss: 0.0037359436973929405  PSNR: 29.159912109375
[TRAIN] Iter: 947200 Loss: 0.003650778904557228  PSNR: 30.00400161743164
[TRAIN] Iter: 947300 Loss: 0.0037554684095084667  PSNR: 30.49250030517578
[TRAIN] Iter: 947400 Loss: 0.005118632223457098  PSNR: 27.57169532775879
[TRAIN] Iter: 947500 Loss: 0.004070869646966457  PSNR: 28.600435256958008
[TRAIN] Iter: 947600 Loss: 0.003561980091035366  PSNR: 30.236129760742188
[TRAIN] Iter: 947700 Loss: 0.0037265827413648367  PSNR: 30.3892879486084
[TRAIN] Iter: 947800 Loss: 0.004912907257676125  PSNR: 27.432079315185547
[TRAIN] Iter: 947900 Loss: 0.0051353746093809605  PSNR: 27.42241859436035
[TRAIN] Iter: 948000 Loss: 0.0038308552466332912  PSNR: 28.866374969482422
[TRAIN] Iter: 948100 Loss: 0.003811800619587302  PSNR: 30.103635787963867
[TRAIN] Iter: 948200 Loss: 0.004533680155873299  PSNR: 28.195993423461914
[TRAIN] Iter: 948300 Loss: 0.0035001463256776333  PSNR: 29.732704162597656
[TRAIN] Iter: 948400 Loss: 0.0031909276731312275  PSNR: 30.942441940307617
[TRAIN] Iter: 948500 Loss: 0.004744861274957657  PSNR: 28.15216636657715
[TRAIN] Iter: 948600 Loss: 0.003902837634086609  PSNR: 29.464324951171875
[TRAIN] Iter: 948700 Loss: 0.00356157124042511  PSNR: 30.16266441345215
[TRAIN] Iter: 948800 Loss: 0.003972710110247135  PSNR: 29.01909637451172
[TRAIN] Iter: 948900 Loss: 0.003521011443808675  PSNR: 30.72802734375
[TRAIN] Iter: 949000 Loss: 0.00478981202468276  PSNR: 27.83179473876953
[TRAIN] Iter: 949100 Loss: 0.0039741285145282745  PSNR: 28.460546493530273
[TRAIN] Iter: 949200 Loss: 0.004027272574603558  PSNR: 28.239269256591797
[TRAIN] Iter: 949300 Loss: 0.0034569057170301676  PSNR: 30.78763198852539
[TRAIN] Iter: 949400 Loss: 0.004293706268072128  PSNR: 28.500640869140625
[TRAIN] Iter: 949500 Loss: 0.005427361465990543  PSNR: 27.37468147277832
[TRAIN] Iter: 949600 Loss: 0.005004203412681818  PSNR: 27.89754295349121
[TRAIN] Iter: 949700 Loss: 0.004487987142056227  PSNR: 29.29998016357422
[TRAIN] Iter: 949800 Loss: 0.004737652838230133  PSNR: 27.300750732421875
[TRAIN] Iter: 949900 Loss: 0.004023665562272072  PSNR: 29.67829704284668
Saved checkpoints at ./logs/TUT-KE101-nerf/950000.tar
0 0.00037932395935058594
torch.Size([320, 640, 3]) torch.Size([320, 640])
1 13.562671184539795
2 15.06929874420166
3 13.956796169281006
4 13.891207933425903
5 14.277497291564941
6 13.853276252746582
7 15.175021409988403
8 13.736819982528687
9 14.859610080718994
10 13.714309692382812
11 13.83389401435852
12 14.881762266159058
13 13.824536800384521
14 14.820675373077393
15 13.782700061798096
16 15.18193244934082
17 13.53217339515686
18 13.951971292495728
19 14.966012716293335
20 13.615933656692505
21 15.080315113067627
22 13.678895950317383
23 15.196765422821045
24 13.424585819244385
25 13.852445363998413
26 15.15985894203186
27 13.73504090309143
28 14.930579423904419
29 13.862597227096558
30 14.985127210617065
31 13.650821685791016
32 13.811002016067505
33 15.012150526046753
34 13.746068000793457
35 14.875184774398804
36 13.853710174560547
37 14.98986005783081
38 13.68297028541565
39 13.753987312316895
40 15.099434852600098
41 13.71601152420044
42 15.077726364135742
43 13.649566650390625
44 15.069798231124878
45 13.609220504760742
46 13.804826498031616
47 15.097503662109375
48 13.740261316299438
49 14.998826265335083
50 13.733079433441162
51 15.041361570358276
52 13.556365728378296
53 13.859133958816528
54 15.016288757324219
55 13.675854206085205
56 14.952862739562988
57 13.649149179458618
58 13.710764646530151
59 15.283698558807373
60 13.190578699111938
61 15.960979700088501
62 12.817093849182129
63 15.70069169998169
64 13.695624828338623
65 13.259819269180298
66 15.32926893234253
67 13.323326826095581
68 15.804733037948608
69 13.187358140945435
70 15.360170602798462
71 13.33357548713684
72 13.564055442810059
73 15.777763366699219
74 13.22537899017334
75 15.486347198486328
76 13.209057569503784
77 13.672044038772583
78 15.448744773864746
79 13.675011396408081
80 15.202960014343262
81 13.380019903182983
82 15.499408960342407
83 13.712161540985107
84 13.620591402053833
85 14.998194456100464
86 13.609450340270996
87 15.186952352523804
88 13.613454818725586
89 15.15268588066101
90 13.595892906188965
91 13.64629077911377
92 15.083064317703247
93 13.62946605682373
94 15.206029176712036
95 13.604769945144653
96 13.590487003326416
97 15.18418574333191
98 13.669817924499512
99 15.185171842575073
100 13.624711990356445
101 15.201240539550781
102 13.638770341873169
103 13.602831363677979
104 15.152170181274414
105 13.66645359992981
106 15.189613580703735
107 13.631273746490479
108 15.151962757110596
109 13.622045755386353
110 13.630724668502808
111 15.060060739517212
112 13.57600712776184
113 15.282529592514038
114 13.58454179763794
115 15.19779634475708
116 13.580567359924316
117 13.537269592285156
118 15.295965909957886
119 13.584557294845581
Done, saving (120, 320, 640, 3) (120, 320, 640)
extras:{'raw': tensor([[[-8.1177e+00, -8.0152e+00, -7.8546e+00, -5.9433e+01],
         [-4.2891e-01, -3.6219e-01, -1.1967e-01, -1.8996e+01],
         [-4.0336e-01, -3.1322e-01, -8.1923e-02, -1.8347e+01],
         ...,
         [-1.8764e+01, -1.8794e+01, -2.0098e+01, -4.3082e+02],
         [-1.9775e+01, -1.9183e+01, -1.9325e+01, -2.5102e+02],
         [-1.7096e+01, -1.6928e+01, -1.7647e+01, -4.0818e+02]],

        [[ 1.8008e+01,  1.9467e+01,  2.4604e+01, -5.5298e+01],
         [-2.2342e-01, -4.0189e-01, -6.9134e-01, -7.2148e+00],
         [-1.1667e-01, -3.0681e-01, -7.2163e-01,  3.2710e-01],
         ...,
         [ 2.1908e+00,  3.0030e-02, -5.7128e+00,  6.7713e+02],
         [ 2.0099e+00, -2.8565e-01, -6.3658e+00,  6.9204e+02],
         [ 8.8925e-01, -1.8337e+00, -8.8269e+00,  7.9703e+02]],

        [[-2.6500e+00, -2.5642e+00, -2.1885e+00, -4.7681e+01],
         [-1.3138e-02, -3.2147e-01, -7.5217e-01, -4.3846e+00],
         [-1.1143e-01, -2.8747e-01, -5.9356e-01,  3.7900e+00],
         ...,
         [ 8.0810e+00,  8.3685e+00,  9.0281e+00,  1.0264e+03],
         [ 8.2486e+00,  8.9568e+00,  1.0879e+01,  9.8984e+02],
         [ 7.4735e+00,  7.9940e+00,  9.1969e+00,  1.0452e+03]],

        ...,

        [[ 1.1359e+00,  2.0438e+00,  4.1563e+00, -7.0127e+01],
         [-1.9460e-01, -4.2312e-01, -7.3115e-01, -3.4309e+00],
         [-1.4020e-01, -3.3846e-01, -5.6789e-01,  4.1627e+00],
         ...,
         [ 6.5169e+00,  1.8748e+00, -1.1800e+01,  1.5065e+02],
         [ 7.4371e+00,  2.7302e+00, -9.5496e+00,  1.7630e+02],
         [ 6.8620e+00,  2.2953e+00, -9.5699e+00,  1.5304e+02]],

        [[ 6.6587e-02,  1.6077e-01,  6.1481e-01, -6.3278e+01],
         [-1.4992e-01, -3.5322e-01, -6.1772e-01,  3.9699e+00],
         [-1.1677e-01, -3.1731e-01, -6.1953e-01,  6.0256e+00],
         ...,
         [-2.9909e+00, -2.1502e+00, -1.0729e+01,  7.7661e+02],
         [-3.0088e+00, -2.1545e+00, -1.0737e+01,  7.7959e+02],
         [-3.6217e+00, -2.3396e+00, -1.0562e+01,  8.0564e+02]],

        [[-5.3473e+00, -5.3710e+00, -6.2137e+00, -7.8240e+01],
         [-2.7269e-01, -9.0577e-02,  3.7080e-01, -3.5307e+01],
         [-6.1988e-01, -5.9455e-01, -5.4304e-01, -1.7730e+01],
         ...,
         [-1.9214e+01, -1.9179e+01, -1.9986e+01, -5.1370e+02],
         [-1.8375e+01, -1.9349e+01, -2.1723e+01, -5.2047e+02],
         [-2.2455e+01, -2.1310e+01, -2.0628e+01, -2.5689e+02]]],
       grad_fn=<ReshapeAliasBackward0>), 'rgb0': tensor([[0.3903, 0.3812, 0.3649],
        [0.4687, 0.4231, 0.3312],
        [0.4696, 0.4184, 0.3528],
        ...,
        [0.4679, 0.4220, 0.3691],
        [0.4566, 0.4119, 0.3541],
        [0.3748, 0.3620, 0.3340]], grad_fn=<ReshapeAliasBackward0>), 'disp0': tensor([  83.9763, 4001.6316,  733.7812,  ...,  158.1601,  338.6336,
          66.0843], grad_fn=<ReshapeAliasBackward0>), 'acc0': tensor([1., 1., 1.,  ..., 1., 1., 1.], grad_fn=<ReshapeAliasBackward0>), 'z_std': tensor([0.0026, 0.3061, 0.3057,  ..., 0.0963, 0.3157, 0.0018])}
0 0.0004336833953857422
torch.Size([320, 640, 3]) torch.Size([320, 640])
1 13.615137338638306
2 15.21150016784668
3 13.629688739776611
4 13.563465356826782
5 15.221993207931519
6 13.58501386642456
7 15.210753440856934
8 13.553741693496704
9 13.572847127914429
10 15.18951940536499
11 13.48543667793274
12 15.449586391448975
13 13.51315975189209
14 15.316168785095215
15 13.37562084197998
16 13.591535806655884
17 15.441712379455566
18 13.219224691390991
19 15.779120445251465
20 13.151285648345947
21 15.435073614120483
22 13.60990285873413
23 13.112130641937256
24 16.02889347076416
25 12.817984104156494
26 15.779625177383423
27 13.382996559143066
28 13.187777042388916
29 15.82379937171936
30 13.15071702003479
31 15.810970783233643
32 13.26075029373169
33 15.426055669784546
34 13.575925588607788
35 13.313130617141724
36 15.721169471740723
37 13.082579612731934
38 15.748035192489624
39 13.216284990310669
40 13.392558097839355
41 15.497134685516357
42 13.462435722351074
43 15.547566890716553
44 13.299427509307861
45 15.52713656425476
46 13.257634162902832
47 13.535995960235596
48 15.594215393066406
49 13.565552949905396
50 15.186858654022217
51 13.471506118774414
52 13.499354362487793
53 15.382127046585083
54 13.430019855499268
55 15.360368967056274
56 13.538886785507202
57 15.334966659545898
58 13.520113706588745
59 13.497294425964355
60 15.345988750457764
61 13.500561237335205
62 15.425927877426147
63 13.511175632476807
64 13.497884750366211
65 15.35721755027771
66 13.473144769668579
67 15.352121353149414
68 13.482739925384521
69 15.405751705169678
70 13.503313302993774
71 13.527020931243896
72 15.382782459259033
73 13.446736812591553
74 15.498914241790771
75 13.545099020004272
76 15.263519048690796
77 13.388905763626099
78 13.433970212936401
79 15.57014513015747
80 13.45033073425293
81 15.344827651977539
82 13.49749755859375
83 13.483638525009155
84 15.428156614303589
85 13.495248794555664
86 15.40761685371399
87 13.436026096343994
88 15.373252630233765
89 13.490436553955078
90 13.473760843276978
91 15.383506298065186
92 13.44702959060669
93 15.38132619857788
94 13.532458782196045
95 13.46936321258545
96 15.396600246429443
97 13.441267967224121
98 15.380146026611328
99 13.424787044525146
100 15.424103736877441
101 13.44429349899292
102 13.45189380645752
103 15.630540370941162
104 13.348398923873901
105 15.375141382217407
106 13.621183633804321
107 13.35367727279663
108 15.507152080535889
109 13.255736351013184
110 15.610548257827759
111 13.318058490753174
112 15.322334289550781
113 13.50485873222351
114 13.302678108215332
115 15.587227821350098
116 13.14634895324707
117 15.954933404922485
118 13.194973707199097
119 17.604801654815674
test poses shape torch.Size([4, 3, 4])
0 0.0005843639373779297
torch.Size([320, 640, 3]) torch.Size([320, 640])
1 17.432815074920654
2 15.210793733596802
3 15.404512882232666
Saved test set
[TRAIN] Iter: 950000 Loss: 0.005373002495616674  PSNR: 28.162818908691406
[TRAIN] Iter: 950100 Loss: 0.0051216501742601395  PSNR: 27.047412872314453
[TRAIN] Iter: 950200 Loss: 0.004084059968590736  PSNR: 29.922143936157227
[TRAIN] Iter: 950300 Loss: 0.0042495415546  PSNR: 28.15403175354004
[TRAIN] Iter: 950400 Loss: 0.0051281386986374855  PSNR: 27.293245315551758
[TRAIN] Iter: 950500 Loss: 0.003502726089209318  PSNR: 29.326255798339844
[TRAIN] Iter: 950600 Loss: 0.004908931441605091  PSNR: 28.533227920532227
[TRAIN] Iter: 950700 Loss: 0.005191659554839134  PSNR: 27.578887939453125
[TRAIN] Iter: 950800 Loss: 0.005147218704223633  PSNR: 28.142250061035156
[TRAIN] Iter: 950900 Loss: 0.005133768543601036  PSNR: 27.452024459838867
[TRAIN] Iter: 951000 Loss: 0.004015645012259483  PSNR: 30.41504669189453
[TRAIN] Iter: 951100 Loss: 0.00503168161958456  PSNR: 26.95746421813965
[TRAIN] Iter: 951200 Loss: 0.003896489506587386  PSNR: 29.756261825561523
[TRAIN] Iter: 951300 Loss: 0.0040541598573327065  PSNR: 29.247051239013672
[TRAIN] Iter: 951400 Loss: 0.005087898578494787  PSNR: 27.654930114746094
[TRAIN] Iter: 951500 Loss: 0.004779154434800148  PSNR: 28.373897552490234
[TRAIN] Iter: 951600 Loss: 0.004848669283092022  PSNR: 28.10439109802246
[TRAIN] Iter: 951700 Loss: 0.004204743541777134  PSNR: 28.817903518676758
[TRAIN] Iter: 951800 Loss: 0.004520921967923641  PSNR: 27.685245513916016
[TRAIN] Iter: 951900 Loss: 0.005212394520640373  PSNR: 28.115285873413086
[TRAIN] Iter: 952000 Loss: 0.004204810597002506  PSNR: 29.679784774780273
[TRAIN] Iter: 952100 Loss: 0.004798521753400564  PSNR: 28.467281341552734
[TRAIN] Iter: 952200 Loss: 0.0036127751227468252  PSNR: 30.229536056518555
[TRAIN] Iter: 952300 Loss: 0.005748950410634279  PSNR: 26.968355178833008
[TRAIN] Iter: 952400 Loss: 0.003691225778311491  PSNR: 30.219284057617188
[TRAIN] Iter: 952500 Loss: 0.004780060611665249  PSNR: 27.963071823120117
[TRAIN] Iter: 952600 Loss: 0.004237818997353315  PSNR: 28.466894149780273
[TRAIN] Iter: 952700 Loss: 0.005852728150784969  PSNR: 27.356372833251953
[TRAIN] Iter: 952800 Loss: 0.005498571787029505  PSNR: 27.56970977783203
[TRAIN] Iter: 952900 Loss: 0.0037091728299856186  PSNR: 28.915874481201172
[TRAIN] Iter: 953000 Loss: 0.00428804662078619  PSNR: 28.86994743347168
[TRAIN] Iter: 953100 Loss: 0.0037239238154143095  PSNR: 30.640268325805664
[TRAIN] Iter: 953200 Loss: 0.005587917752563953  PSNR: 27.34537696838379
[TRAIN] Iter: 953300 Loss: 0.004247657954692841  PSNR: 28.523590087890625
[TRAIN] Iter: 953400 Loss: 0.004191349260509014  PSNR: 29.134326934814453
[TRAIN] Iter: 953500 Loss: 0.004279305227100849  PSNR: 28.881595611572266
[TRAIN] Iter: 953600 Loss: 0.0036066449247300625  PSNR: 29.84740447998047
[TRAIN] Iter: 953700 Loss: 0.0054116747342050076  PSNR: 27.338232040405273
[TRAIN] Iter: 953800 Loss: 0.0043220194056630135  PSNR: 28.88220977783203
[TRAIN] Iter: 953900 Loss: 0.0028381349984556437  PSNR: 31.383365631103516
[TRAIN] Iter: 954000 Loss: 0.003423884976655245  PSNR: 30.397981643676758
[TRAIN] Iter: 954100 Loss: 0.004071709234267473  PSNR: 30.1717529296875
[TRAIN] Iter: 954200 Loss: 0.004041168838739395  PSNR: 29.945911407470703
[TRAIN] Iter: 954300 Loss: 0.003233826719224453  PSNR: 30.52414321899414
[TRAIN] Iter: 954400 Loss: 0.0041174376383423805  PSNR: 28.847570419311523
[TRAIN] Iter: 954500 Loss: 0.005251921713352203  PSNR: 27.471879959106445
[TRAIN] Iter: 954600 Loss: 0.005716003477573395  PSNR: 26.23874855041504
[TRAIN] Iter: 954700 Loss: 0.005673814564943314  PSNR: 26.888885498046875
[TRAIN] Iter: 954800 Loss: 0.006067590322345495  PSNR: 26.90202522277832
[TRAIN] Iter: 954900 Loss: 0.004863872192800045  PSNR: 28.071691513061523
[TRAIN] Iter: 955000 Loss: 0.004072569310665131  PSNR: 29.44839096069336
[TRAIN] Iter: 955100 Loss: 0.004441935569047928  PSNR: 28.691858291625977
[TRAIN] Iter: 955200 Loss: 0.003825192805379629  PSNR: 29.236698150634766
[TRAIN] Iter: 955300 Loss: 0.004533772822469473  PSNR: 30.075790405273438
[TRAIN] Iter: 955400 Loss: 0.004666907712817192  PSNR: 27.890546798706055
[TRAIN] Iter: 955500 Loss: 0.003364753210917115  PSNR: 30.260780334472656
[TRAIN] Iter: 955600 Loss: 0.00579418707638979  PSNR: 27.093276977539062
[TRAIN] Iter: 955700 Loss: 0.004950938280671835  PSNR: 27.897624969482422
[TRAIN] Iter: 955800 Loss: 0.005754468031227589  PSNR: 26.74233627319336
[TRAIN] Iter: 955900 Loss: 0.006229972466826439  PSNR: 26.21129608154297
[TRAIN] Iter: 956000 Loss: 0.004886241164058447  PSNR: 28.313297271728516
[TRAIN] Iter: 956100 Loss: 0.004397857468575239  PSNR: 28.550382614135742
[TRAIN] Iter: 956200 Loss: 0.003887886181473732  PSNR: 29.3875789642334
[TRAIN] Iter: 956300 Loss: 0.005079806782305241  PSNR: 27.54764747619629
[TRAIN] Iter: 956400 Loss: 0.004138799384236336  PSNR: 28.340606689453125
[TRAIN] Iter: 956500 Loss: 0.005602984223514795  PSNR: 27.345836639404297
[TRAIN] Iter: 956600 Loss: 0.0050365859642624855  PSNR: 27.446428298950195
[TRAIN] Iter: 956700 Loss: 0.005315691232681274  PSNR: 27.68953514099121
[TRAIN] Iter: 956800 Loss: 0.004685543477535248  PSNR: 28.343036651611328
[TRAIN] Iter: 956900 Loss: 0.0034797994885593653  PSNR: 30.780548095703125
[TRAIN] Iter: 957000 Loss: 0.0052511137910187244  PSNR: 27.141094207763672
[TRAIN] Iter: 957100 Loss: 0.005134226754307747  PSNR: 27.532405853271484
[TRAIN] Iter: 957200 Loss: 0.0037908952217549086  PSNR: 30.241010665893555
[TRAIN] Iter: 957300 Loss: 0.004613821394741535  PSNR: 27.841400146484375
[TRAIN] Iter: 957400 Loss: 0.005754676181823015  PSNR: 27.039613723754883
[TRAIN] Iter: 957500 Loss: 0.004917466081678867  PSNR: 28.70025634765625
[TRAIN] Iter: 957600 Loss: 0.0039009395986795425  PSNR: 30.172306060791016
[TRAIN] Iter: 957700 Loss: 0.004154853988438845  PSNR: 28.002159118652344
[TRAIN] Iter: 957800 Loss: 0.0041586244478821754  PSNR: 28.62602424621582
[TRAIN] Iter: 957900 Loss: 0.005640122108161449  PSNR: 27.36662483215332
[TRAIN] Iter: 958000 Loss: 0.003990355879068375  PSNR: 29.055709838867188
[TRAIN] Iter: 958100 Loss: 0.004258022643625736  PSNR: 29.08345603942871
[TRAIN] Iter: 958200 Loss: 0.0057311090640723705  PSNR: 27.189809799194336
[TRAIN] Iter: 958300 Loss: 0.003481493564322591  PSNR: 30.607200622558594
[TRAIN] Iter: 958400 Loss: 0.0036980807781219482  PSNR: 30.25318717956543
[TRAIN] Iter: 958500 Loss: 0.0047309510409832  PSNR: 28.56888771057129
[TRAIN] Iter: 958600 Loss: 0.004303301684558392  PSNR: 28.69373321533203
[TRAIN] Iter: 958700 Loss: 0.0050841206684708595  PSNR: 28.501102447509766
[TRAIN] Iter: 958800 Loss: 0.0034988573752343655  PSNR: 30.199256896972656
[TRAIN] Iter: 958900 Loss: 0.0038277884013950825  PSNR: 29.18319320678711
[TRAIN] Iter: 959000 Loss: 0.004073976073414087  PSNR: 29.497276306152344
[TRAIN] Iter: 959100 Loss: 0.004895700141787529  PSNR: 27.531023025512695
[TRAIN] Iter: 959200 Loss: 0.003722312394529581  PSNR: 30.249570846557617
[TRAIN] Iter: 959300 Loss: 0.00465050432831049  PSNR: 28.311452865600586
[TRAIN] Iter: 959400 Loss: 0.00523397047072649  PSNR: 27.77617835998535
[TRAIN] Iter: 959500 Loss: 0.005800305865705013  PSNR: 26.719919204711914
[TRAIN] Iter: 959600 Loss: 0.005774995312094688  PSNR: 26.618215560913086
[TRAIN] Iter: 959700 Loss: 0.004196408204734325  PSNR: 28.46484375
[TRAIN] Iter: 959800 Loss: 0.003697613487020135  PSNR: 29.646804809570312
[TRAIN] Iter: 959900 Loss: 0.004442028701305389  PSNR: 28.52861976623535
Saved checkpoints at ./logs/TUT-KE101-nerf/960000.tar
[TRAIN] Iter: 960000 Loss: 0.0041192686185240746  PSNR: 28.59693145751953
[TRAIN] Iter: 960100 Loss: 0.0032351890113204718  PSNR: 30.695255279541016
[TRAIN] Iter: 960200 Loss: 0.00420689070597291  PSNR: 28.14069938659668
[TRAIN] Iter: 960300 Loss: 0.00481797568500042  PSNR: 27.857791900634766
[TRAIN] Iter: 960400 Loss: 0.004522955045104027  PSNR: 28.579275131225586
[TRAIN] Iter: 960500 Loss: 0.004802740179002285  PSNR: 28.532920837402344
[TRAIN] Iter: 960600 Loss: 0.005012288689613342  PSNR: 27.370656967163086
[TRAIN] Iter: 960700 Loss: 0.003655019449070096  PSNR: 30.043292999267578
[TRAIN] Iter: 960800 Loss: 0.0037486522924154997  PSNR: 30.01494598388672
[TRAIN] Iter: 960900 Loss: 0.0038384669460356236  PSNR: 29.357763290405273
[TRAIN] Iter: 961000 Loss: 0.0036519202403724194  PSNR: 29.74571990966797
[TRAIN] Iter: 961100 Loss: 0.0040706247091293335  PSNR: 28.19837188720703
[TRAIN] Iter: 961200 Loss: 0.004968819674104452  PSNR: 28.603336334228516
[TRAIN] Iter: 961300 Loss: 0.004920767620205879  PSNR: 27.82909393310547
[TRAIN] Iter: 961400 Loss: 0.003672894788905978  PSNR: 29.758329391479492
[TRAIN] Iter: 961500 Loss: 0.005104916635900736  PSNR: 28.35063934326172
[TRAIN] Iter: 961600 Loss: 0.004642860032618046  PSNR: 28.045194625854492
[TRAIN] Iter: 961700 Loss: 0.004300007596611977  PSNR: 28.030803680419922
[TRAIN] Iter: 961800 Loss: 0.0033472497016191483  PSNR: 30.624725341796875
[TRAIN] Iter: 961900 Loss: 0.004139660857617855  PSNR: 28.84467887878418
[TRAIN] Iter: 962000 Loss: 0.004143423866480589  PSNR: 29.802417755126953
[TRAIN] Iter: 962100 Loss: 0.00398020725697279  PSNR: 29.309926986694336
[TRAIN] Iter: 962200 Loss: 0.004768183454871178  PSNR: 28.140769958496094
[TRAIN] Iter: 962300 Loss: 0.003616317408159375  PSNR: 30.43796157836914
[TRAIN] Iter: 962400 Loss: 0.005013209767639637  PSNR: 28.217483520507812
[TRAIN] Iter: 962500 Loss: 0.0036522161681205034  PSNR: 30.2193603515625
[TRAIN] Iter: 962600 Loss: 0.004019302316009998  PSNR: 28.3863525390625
[TRAIN] Iter: 962700 Loss: 0.0046178484335541725  PSNR: 29.57715606689453
[TRAIN] Iter: 962800 Loss: 0.003659617155790329  PSNR: 28.949703216552734
[TRAIN] Iter: 962900 Loss: 0.003810299327597022  PSNR: 29.546340942382812
[TRAIN] Iter: 963000 Loss: 0.005546486936509609  PSNR: 26.654630661010742
[TRAIN] Iter: 963100 Loss: 0.004622517619282007  PSNR: 28.285886764526367
[TRAIN] Iter: 963200 Loss: 0.00395993422716856  PSNR: 29.487491607666016
[TRAIN] Iter: 963300 Loss: 0.0034768092446029186  PSNR: 30.448585510253906
[TRAIN] Iter: 963400 Loss: 0.004956677556037903  PSNR: 28.02535057067871
[TRAIN] Iter: 963500 Loss: 0.004992623347789049  PSNR: 27.83720588684082
[TRAIN] Iter: 963600 Loss: 0.0053671784698963165  PSNR: 28.221811294555664
[TRAIN] Iter: 963700 Loss: 0.004744560457766056  PSNR: 28.618982315063477
[TRAIN] Iter: 963800 Loss: 0.0035857311449944973  PSNR: 29.7326602935791
[TRAIN] Iter: 963900 Loss: 0.0039368183352053165  PSNR: 28.69634437561035
[TRAIN] Iter: 964000 Loss: 0.005342105869203806  PSNR: 27.66622543334961
[TRAIN] Iter: 964100 Loss: 0.004821904003620148  PSNR: 28.04147720336914
[TRAIN] Iter: 964200 Loss: 0.003916732035577297  PSNR: 29.654003143310547
[TRAIN] Iter: 964300 Loss: 0.003725954797118902  PSNR: 30.613569259643555
[TRAIN] Iter: 964400 Loss: 0.0030500488355755806  PSNR: 31.14023780822754
[TRAIN] Iter: 964500 Loss: 0.00400931341573596  PSNR: 28.773088455200195
[TRAIN] Iter: 964600 Loss: 0.004956205375492573  PSNR: 27.684284210205078
[TRAIN] Iter: 964700 Loss: 0.005373571068048477  PSNR: 27.38966941833496
[TRAIN] Iter: 964800 Loss: 0.00369855435565114  PSNR: 30.43838882446289
[TRAIN] Iter: 964900 Loss: 0.004069007467478514  PSNR: 29.43749237060547
[TRAIN] Iter: 965000 Loss: 0.003135985229164362  PSNR: 30.550945281982422
[TRAIN] Iter: 965100 Loss: 0.004526103381067514  PSNR: 28.947158813476562
[TRAIN] Iter: 965200 Loss: 0.004981743637472391  PSNR: 27.932886123657227
[TRAIN] Iter: 965300 Loss: 0.003975120838731527  PSNR: 29.134624481201172
[TRAIN] Iter: 965400 Loss: 0.003419911488890648  PSNR: 30.712553024291992
[TRAIN] Iter: 965500 Loss: 0.004577846731990576  PSNR: 27.60449981689453
[TRAIN] Iter: 965600 Loss: 0.00403271708637476  PSNR: 29.95484161376953
[TRAIN] Iter: 965700 Loss: 0.004977748263627291  PSNR: 28.449819564819336
[TRAIN] Iter: 965800 Loss: 0.002990636508911848  PSNR: 30.14807891845703
[TRAIN] Iter: 965900 Loss: 0.004393073730170727  PSNR: 28.109060287475586
[TRAIN] Iter: 966000 Loss: 0.005499817430973053  PSNR: 27.42396354675293
[TRAIN] Iter: 966100 Loss: 0.0037820124998688698  PSNR: 29.280546188354492
[TRAIN] Iter: 966200 Loss: 0.0057076807133853436  PSNR: 27.215085983276367
[TRAIN] Iter: 966300 Loss: 0.004906910005956888  PSNR: 27.64899444580078
[TRAIN] Iter: 966400 Loss: 0.003302429337054491  PSNR: 30.439260482788086
[TRAIN] Iter: 966500 Loss: 0.0046762870624661446  PSNR: 28.24773597717285
[TRAIN] Iter: 966600 Loss: 0.003789068665355444  PSNR: 29.92405891418457
[TRAIN] Iter: 966700 Loss: 0.004620383959263563  PSNR: 28.182666778564453
[TRAIN] Iter: 966800 Loss: 0.005007235333323479  PSNR: 28.024246215820312
[TRAIN] Iter: 966900 Loss: 0.0060419379733502865  PSNR: 27.932716369628906
[TRAIN] Iter: 967000 Loss: 0.005618654191493988  PSNR: 27.33810043334961
[TRAIN] Iter: 967100 Loss: 0.004809308797121048  PSNR: 27.9061336517334
[TRAIN] Iter: 967200 Loss: 0.00454507302492857  PSNR: 27.42737579345703
[TRAIN] Iter: 967300 Loss: 0.005275798495858908  PSNR: 27.92420196533203
[TRAIN] Iter: 967400 Loss: 0.00500907190144062  PSNR: 28.007083892822266
[TRAIN] Iter: 967500 Loss: 0.004369708709418774  PSNR: 28.486019134521484
[TRAIN] Iter: 967600 Loss: 0.005537232384085655  PSNR: 27.04819107055664
[TRAIN] Iter: 967700 Loss: 0.0036175083369016647  PSNR: 29.837759017944336
[TRAIN] Iter: 967800 Loss: 0.004015476908534765  PSNR: 29.741146087646484
[TRAIN] Iter: 967900 Loss: 0.005041052587330341  PSNR: 28.562284469604492
[TRAIN] Iter: 968000 Loss: 0.004496270325034857  PSNR: 28.264707565307617
[TRAIN] Iter: 968100 Loss: 0.0047766980715096  PSNR: 27.77032470703125
[TRAIN] Iter: 968200 Loss: 0.004561065696179867  PSNR: 27.75674057006836
[TRAIN] Iter: 968300 Loss: 0.004114443436264992  PSNR: 28.526628494262695
[TRAIN] Iter: 968400 Loss: 0.0035156577359884977  PSNR: 30.140357971191406
[TRAIN] Iter: 968500 Loss: 0.00483342632651329  PSNR: 28.245115280151367
[TRAIN] Iter: 968600 Loss: 0.004092484246939421  PSNR: 29.530231475830078
[TRAIN] Iter: 968700 Loss: 0.0035066516138613224  PSNR: 30.660633087158203
[TRAIN] Iter: 968800 Loss: 0.0044477260671556  PSNR: 28.08963394165039
[TRAIN] Iter: 968900 Loss: 0.003935853485018015  PSNR: 30.363954544067383
[TRAIN] Iter: 969000 Loss: 0.004714183043688536  PSNR: 27.87143325805664
[TRAIN] Iter: 969100 Loss: 0.0038023958913981915  PSNR: 30.116962432861328
[TRAIN] Iter: 969200 Loss: 0.005315457470715046  PSNR: 27.673770904541016
[TRAIN] Iter: 969300 Loss: 0.00353636615909636  PSNR: 30.410722732543945
[TRAIN] Iter: 969400 Loss: 0.006068582180887461  PSNR: 27.300600051879883
[TRAIN] Iter: 969500 Loss: 0.00654692854732275  PSNR: 27.219133377075195
[TRAIN] Iter: 969600 Loss: 0.0046661775559186935  PSNR: 27.58244514465332
[TRAIN] Iter: 969700 Loss: 0.0037285571452230215  PSNR: 30.034276962280273
[TRAIN] Iter: 969800 Loss: 0.0050360290333628654  PSNR: 27.355911254882812
[TRAIN] Iter: 969900 Loss: 0.003979525528848171  PSNR: 28.516681671142578
Saved checkpoints at ./logs/TUT-KE101-nerf/970000.tar
[TRAIN] Iter: 970000 Loss: 0.005058105103671551  PSNR: 27.72528648376465
[TRAIN] Iter: 970100 Loss: 0.005390656180679798  PSNR: 27.93511199951172
[TRAIN] Iter: 970200 Loss: 0.0035081184469163418  PSNR: 30.276752471923828
[TRAIN] Iter: 970300 Loss: 0.005632588639855385  PSNR: 27.775821685791016
[TRAIN] Iter: 970400 Loss: 0.005830835551023483  PSNR: 26.729413986206055
[TRAIN] Iter: 970500 Loss: 0.0030783351976424456  PSNR: 30.74759864807129
[TRAIN] Iter: 970600 Loss: 0.004376795142889023  PSNR: 28.51966667175293
[TRAIN] Iter: 970700 Loss: 0.003710351884365082  PSNR: 29.271259307861328
[TRAIN] Iter: 970800 Loss: 0.0048580770380795  PSNR: 28.23512077331543
[TRAIN] Iter: 970900 Loss: 0.004008465446531773  PSNR: 28.864360809326172
[TRAIN] Iter: 971000 Loss: 0.0038468032144010067  PSNR: 29.958297729492188
[TRAIN] Iter: 971100 Loss: 0.00528573477640748  PSNR: 27.679506301879883
[TRAIN] Iter: 971200 Loss: 0.003457091050222516  PSNR: 30.201866149902344
[TRAIN] Iter: 971300 Loss: 0.005902471020817757  PSNR: 27.245759963989258
[TRAIN] Iter: 971400 Loss: 0.005598604213446379  PSNR: 27.3013916015625
[TRAIN] Iter: 971500 Loss: 0.005147700197994709  PSNR: 27.851612091064453
[TRAIN] Iter: 971600 Loss: 0.004171045962721109  PSNR: 29.18852996826172
[TRAIN] Iter: 971700 Loss: 0.004576428793370724  PSNR: 28.388118743896484
[TRAIN] Iter: 971800 Loss: 0.004134654067456722  PSNR: 28.125490188598633
[TRAIN] Iter: 971900 Loss: 0.004714654292911291  PSNR: 28.327396392822266
[TRAIN] Iter: 972000 Loss: 0.004818047862499952  PSNR: 28.078807830810547
[TRAIN] Iter: 972100 Loss: 0.004457907751202583  PSNR: 28.292306900024414
[TRAIN] Iter: 972200 Loss: 0.004479207564145327  PSNR: 28.652740478515625
[TRAIN] Iter: 972300 Loss: 0.005559188313782215  PSNR: 27.665897369384766
[TRAIN] Iter: 972400 Loss: 0.003741584252566099  PSNR: 29.085533142089844
[TRAIN] Iter: 972500 Loss: 0.004391815047711134  PSNR: 28.470760345458984
[TRAIN] Iter: 972600 Loss: 0.003718647174537182  PSNR: 30.22999382019043
[TRAIN] Iter: 972700 Loss: 0.003986933268606663  PSNR: 29.990341186523438
[TRAIN] Iter: 972800 Loss: 0.0038418674375861883  PSNR: 29.760974884033203
[TRAIN] Iter: 972900 Loss: 0.003689755452796817  PSNR: 29.614246368408203
[TRAIN] Iter: 973000 Loss: 0.004155625589191914  PSNR: 28.943647384643555
[TRAIN] Iter: 973100 Loss: 0.006172167137265205  PSNR: 27.173690795898438
[TRAIN] Iter: 973200 Loss: 0.005226174369454384  PSNR: 27.11187744140625
[TRAIN] Iter: 973300 Loss: 0.003909042105078697  PSNR: 28.690671920776367
[TRAIN] Iter: 973400 Loss: 0.0050311400555074215  PSNR: 28.190540313720703
[TRAIN] Iter: 973500 Loss: 0.00522203603759408  PSNR: 27.813039779663086
[TRAIN] Iter: 973600 Loss: 0.005045182071626186  PSNR: 27.84588623046875
[TRAIN] Iter: 973700 Loss: 0.004552407190203667  PSNR: 28.24864387512207
[TRAIN] Iter: 973800 Loss: 0.003913735970854759  PSNR: 29.219030380249023
[TRAIN] Iter: 973900 Loss: 0.003777961479499936  PSNR: 29.41033363342285
[TRAIN] Iter: 974000 Loss: 0.004458157811313868  PSNR: 28.218809127807617
[TRAIN] Iter: 974100 Loss: 0.004372525494545698  PSNR: 29.22406005859375
[TRAIN] Iter: 974200 Loss: 0.003686170559376478  PSNR: 29.424510955810547
[TRAIN] Iter: 974300 Loss: 0.004291582852602005  PSNR: 28.412939071655273
[TRAIN] Iter: 974400 Loss: 0.005696318112313747  PSNR: 26.71623992919922
[TRAIN] Iter: 974500 Loss: 0.005061594769358635  PSNR: 27.512096405029297
[TRAIN] Iter: 974600 Loss: 0.003999519161880016  PSNR: 28.663232803344727
[TRAIN] Iter: 974700 Loss: 0.0034482888877391815  PSNR: 30.54117774963379
[TRAIN] Iter: 974800 Loss: 0.004465451464056969  PSNR: 28.669780731201172
[TRAIN] Iter: 974900 Loss: 0.005116710439324379  PSNR: 27.713682174682617
[TRAIN] Iter: 975000 Loss: 0.00514854583889246  PSNR: 27.96722984313965
[TRAIN] Iter: 975100 Loss: 0.005672793835401535  PSNR: 27.021759033203125
[TRAIN] Iter: 975200 Loss: 0.004713496193289757  PSNR: 27.82835578918457
[TRAIN] Iter: 975300 Loss: 0.0035788915120065212  PSNR: 29.93732261657715
[TRAIN] Iter: 975400 Loss: 0.004207989666610956  PSNR: 28.57954978942871
[TRAIN] Iter: 975500 Loss: 0.0050720637664198875  PSNR: 28.32561492919922
[TRAIN] Iter: 975600 Loss: 0.005012156441807747  PSNR: 27.69999122619629
[TRAIN] Iter: 975700 Loss: 0.0039220405742526054  PSNR: 29.5081787109375
[TRAIN] Iter: 975800 Loss: 0.005169588141143322  PSNR: 27.780237197875977
[TRAIN] Iter: 975900 Loss: 0.004902467597275972  PSNR: 28.06949806213379
[TRAIN] Iter: 976000 Loss: 0.004118968266993761  PSNR: 28.76226806640625
[TRAIN] Iter: 976100 Loss: 0.005460962653160095  PSNR: 27.217979431152344
[TRAIN] Iter: 976200 Loss: 0.005012148525565863  PSNR: 27.642993927001953
[TRAIN] Iter: 976300 Loss: 0.005165585316717625  PSNR: 27.629743576049805
[TRAIN] Iter: 976400 Loss: 0.00573705742135644  PSNR: 27.31444549560547
[TRAIN] Iter: 976500 Loss: 0.004188011400401592  PSNR: 28.41393280029297
[TRAIN] Iter: 976600 Loss: 0.005606816615909338  PSNR: 27.2737979888916
[TRAIN] Iter: 976700 Loss: 0.004473284352570772  PSNR: 27.843318939208984
[TRAIN] Iter: 976800 Loss: 0.003544359002262354  PSNR: 30.07810401916504
[TRAIN] Iter: 976900 Loss: 0.004247420933097601  PSNR: 28.391395568847656
[TRAIN] Iter: 977000 Loss: 0.0030692261643707752  PSNR: 30.520204544067383
[TRAIN] Iter: 977100 Loss: 0.004859801381826401  PSNR: 28.018308639526367
[TRAIN] Iter: 977200 Loss: 0.0033981322776526213  PSNR: 30.386690139770508
[TRAIN] Iter: 977300 Loss: 0.004815377295017242  PSNR: 27.366592407226562
[TRAIN] Iter: 977400 Loss: 0.004421445075422525  PSNR: 28.309709548950195
[TRAIN] Iter: 977500 Loss: 0.0049632396548986435  PSNR: 27.746784210205078
[TRAIN] Iter: 977600 Loss: 0.003941505216062069  PSNR: 28.67315101623535
[TRAIN] Iter: 977700 Loss: 0.004137871786952019  PSNR: 29.041879653930664
[TRAIN] Iter: 977800 Loss: 0.004568565171211958  PSNR: 28.15578842163086
[TRAIN] Iter: 977900 Loss: 0.003704692702740431  PSNR: 29.499244689941406
[TRAIN] Iter: 978000 Loss: 0.004692445043474436  PSNR: 27.808813095092773
[TRAIN] Iter: 978100 Loss: 0.0038555413484573364  PSNR: 29.335119247436523
[TRAIN] Iter: 978200 Loss: 0.003375981468707323  PSNR: 30.29854393005371
[TRAIN] Iter: 978300 Loss: 0.0035941884852945805  PSNR: 29.565767288208008
[TRAIN] Iter: 978400 Loss: 0.004830165300518274  PSNR: 28.535720825195312
[TRAIN] Iter: 978500 Loss: 0.004959373734891415  PSNR: 27.457698822021484
[TRAIN] Iter: 978600 Loss: 0.005371756851673126  PSNR: 27.04393768310547
[TRAIN] Iter: 978700 Loss: 0.005884669255465269  PSNR: 27.381851196289062
[TRAIN] Iter: 978800 Loss: 0.004156437702476978  PSNR: 28.615259170532227
[TRAIN] Iter: 978900 Loss: 0.005010867491364479  PSNR: 28.120159149169922
[TRAIN] Iter: 979000 Loss: 0.005473054945468903  PSNR: 26.702531814575195
[TRAIN] Iter: 979100 Loss: 0.004965595435351133  PSNR: 28.286333084106445
[TRAIN] Iter: 979200 Loss: 0.0037541892379522324  PSNR: 29.82560157775879
[TRAIN] Iter: 979300 Loss: 0.004142352379858494  PSNR: 28.708818435668945
[TRAIN] Iter: 979400 Loss: 0.0031495296861976385  PSNR: 30.992145538330078
[TRAIN] Iter: 979500 Loss: 0.0045450301840901375  PSNR: 28.03565788269043
[TRAIN] Iter: 979600 Loss: 0.004209977574646473  PSNR: 28.577783584594727
[TRAIN] Iter: 979700 Loss: 0.004128148779273033  PSNR: 28.22678565979004
[TRAIN] Iter: 979800 Loss: 0.0048643676564097404  PSNR: 28.059680938720703
[TRAIN] Iter: 979900 Loss: 0.004166455008089542  PSNR: 28.361604690551758
Saved checkpoints at ./logs/TUT-KE101-nerf/980000.tar
[TRAIN] Iter: 980000 Loss: 0.0033533265814185143  PSNR: 29.458518981933594
[TRAIN] Iter: 980100 Loss: 0.005113792140036821  PSNR: 27.699575424194336
[TRAIN] Iter: 980200 Loss: 0.005293397698551416  PSNR: 27.73660659790039
[TRAIN] Iter: 980300 Loss: 0.005238963291049004  PSNR: 27.965539932250977
[TRAIN] Iter: 980400 Loss: 0.004302009008824825  PSNR: 28.488693237304688
[TRAIN] Iter: 980500 Loss: 0.004968931898474693  PSNR: 27.616445541381836
[TRAIN] Iter: 980600 Loss: 0.004342564381659031  PSNR: 29.326683044433594
[TRAIN] Iter: 980700 Loss: 0.0036242741625756025  PSNR: 30.040393829345703
[TRAIN] Iter: 980800 Loss: 0.004504930227994919  PSNR: 28.16265106201172
[TRAIN] Iter: 980900 Loss: 0.00485613476485014  PSNR: 27.651655197143555
[TRAIN] Iter: 981000 Loss: 0.0049253604374825954  PSNR: 27.483369827270508
[TRAIN] Iter: 981100 Loss: 0.0035354499705135822  PSNR: 30.789127349853516
[TRAIN] Iter: 981200 Loss: 0.004916834644973278  PSNR: 27.95220375061035
[TRAIN] Iter: 981300 Loss: 0.0050234668888151646  PSNR: 27.432069778442383
[TRAIN] Iter: 981400 Loss: 0.004341842141002417  PSNR: 29.2369384765625
[TRAIN] Iter: 981500 Loss: 0.004555435851216316  PSNR: 28.943378448486328
[TRAIN] Iter: 981600 Loss: 0.004388924688100815  PSNR: 27.783281326293945
[TRAIN] Iter: 981700 Loss: 0.0040536741726100445  PSNR: 27.75016975402832
[TRAIN] Iter: 981800 Loss: 0.003961537498980761  PSNR: 28.80052375793457
[TRAIN] Iter: 981900 Loss: 0.004208198748528957  PSNR: 28.223388671875
[TRAIN] Iter: 982000 Loss: 0.00456223962828517  PSNR: 28.275602340698242
[TRAIN] Iter: 982100 Loss: 0.004778292495757341  PSNR: 28.061147689819336
[TRAIN] Iter: 982200 Loss: 0.003751077689230442  PSNR: 30.225536346435547
[TRAIN] Iter: 982300 Loss: 0.0052603851072490215  PSNR: 27.846803665161133
[TRAIN] Iter: 982400 Loss: 0.004354467615485191  PSNR: 28.780561447143555
[TRAIN] Iter: 982500 Loss: 0.004471132531762123  PSNR: 27.67592430114746
[TRAIN] Iter: 982600 Loss: 0.006036787759512663  PSNR: 26.98308753967285
[TRAIN] Iter: 982700 Loss: 0.005762401968240738  PSNR: 27.266687393188477
[TRAIN] Iter: 982800 Loss: 0.0049438150599598885  PSNR: 27.8041934967041
[TRAIN] Iter: 982900 Loss: 0.004824738018214703  PSNR: 27.86879539489746
[TRAIN] Iter: 983000 Loss: 0.005757179576903582  PSNR: 27.454833984375
[TRAIN] Iter: 983100 Loss: 0.00412834994494915  PSNR: 28.48961067199707
[TRAIN] Iter: 983200 Loss: 0.004712084773927927  PSNR: 28.199453353881836
[TRAIN] Iter: 983300 Loss: 0.005575716961175203  PSNR: 26.646522521972656
[TRAIN] Iter: 983400 Loss: 0.00312235439196229  PSNR: 30.599693298339844
[TRAIN] Iter: 983500 Loss: 0.002963216509670019  PSNR: 31.412656784057617
[TRAIN] Iter: 983600 Loss: 0.0031399254221469164  PSNR: 30.541519165039062
[TRAIN] Iter: 983700 Loss: 0.004694254137575626  PSNR: 28.137075424194336
[TRAIN] Iter: 983800 Loss: 0.004948643036186695  PSNR: 27.929439544677734
[TRAIN] Iter: 983900 Loss: 0.003902143333107233  PSNR: 28.861839294433594
[TRAIN] Iter: 984000 Loss: 0.003869541920721531  PSNR: 28.390460968017578
[TRAIN] Iter: 984100 Loss: 0.004645626991987228  PSNR: 27.936613082885742
[TRAIN] Iter: 984200 Loss: 0.0049005672335624695  PSNR: 27.61342430114746
[TRAIN] Iter: 984300 Loss: 0.005874908063560724  PSNR: 26.356014251708984
[TRAIN] Iter: 984400 Loss: 0.0053945863619446754  PSNR: 27.659067153930664
[TRAIN] Iter: 984500 Loss: 0.0036699222400784492  PSNR: 29.857013702392578
[TRAIN] Iter: 984600 Loss: 0.0037134839221835136  PSNR: 30.113811492919922
[TRAIN] Iter: 984700 Loss: 0.004442964680492878  PSNR: 28.7662410736084
[TRAIN] Iter: 984800 Loss: 0.0035036341287195683  PSNR: 30.278196334838867
[TRAIN] Iter: 984900 Loss: 0.0033043043222278357  PSNR: 30.586034774780273
[TRAIN] Iter: 985000 Loss: 0.0037498276215046644  PSNR: 30.00886344909668
[TRAIN] Iter: 985100 Loss: 0.004393638111650944  PSNR: 28.746740341186523
[TRAIN] Iter: 985200 Loss: 0.005477059632539749  PSNR: 27.173789978027344
[TRAIN] Iter: 985300 Loss: 0.003952867351472378  PSNR: 29.550128936767578
[TRAIN] Iter: 985400 Loss: 0.005320077762007713  PSNR: 27.194297790527344
[TRAIN] Iter: 985500 Loss: 0.0056683667935431  PSNR: 27.23366928100586
[TRAIN] Iter: 985600 Loss: 0.004161741118878126  PSNR: 28.696632385253906
[TRAIN] Iter: 985700 Loss: 0.0043328688479959965  PSNR: 28.079925537109375
[TRAIN] Iter: 985800 Loss: 0.004752985201776028  PSNR: 27.849613189697266
[TRAIN] Iter: 985900 Loss: 0.004962941166013479  PSNR: 28.279611587524414
[TRAIN] Iter: 986000 Loss: 0.004863687790930271  PSNR: 28.39922523498535
[TRAIN] Iter: 986100 Loss: 0.0038582035340368748  PSNR: 28.702070236206055
[TRAIN] Iter: 986200 Loss: 0.003444214817136526  PSNR: 29.995101928710938
[TRAIN] Iter: 986300 Loss: 0.004283871967345476  PSNR: 28.0218505859375
[TRAIN] Iter: 986400 Loss: 0.00416172668337822  PSNR: 28.708553314208984
[TRAIN] Iter: 986500 Loss: 0.0054276930168271065  PSNR: 27.99163246154785
[TRAIN] Iter: 986600 Loss: 0.0038915053009986877  PSNR: 29.11167335510254
[TRAIN] Iter: 986700 Loss: 0.004824704024940729  PSNR: 28.398731231689453
[TRAIN] Iter: 986800 Loss: 0.005270390305668116  PSNR: 27.206024169921875
[TRAIN] Iter: 986900 Loss: 0.004868374206125736  PSNR: 28.020078659057617
[TRAIN] Iter: 987000 Loss: 0.004618015605956316  PSNR: 28.75326156616211
[TRAIN] Iter: 987100 Loss: 0.0035109580494463444  PSNR: 30.69944953918457
[TRAIN] Iter: 987200 Loss: 0.005326429381966591  PSNR: 27.937408447265625
[TRAIN] Iter: 987300 Loss: 0.004609148483723402  PSNR: 28.258298873901367
[TRAIN] Iter: 987400 Loss: 0.004201315343379974  PSNR: 28.23110008239746
[TRAIN] Iter: 987500 Loss: 0.005135619081556797  PSNR: 27.837345123291016
[TRAIN] Iter: 987600 Loss: 0.005004935897886753  PSNR: 27.706138610839844
[TRAIN] Iter: 987700 Loss: 0.0030372049659490585  PSNR: 29.56560516357422
[TRAIN] Iter: 987800 Loss: 0.004904767498373985  PSNR: 28.01399803161621
[TRAIN] Iter: 987900 Loss: 0.004187053069472313  PSNR: 29.048564910888672
[TRAIN] Iter: 988000 Loss: 0.003878398798406124  PSNR: 29.688549041748047
[TRAIN] Iter: 988100 Loss: 0.0036469013430178165  PSNR: 30.072158813476562
[TRAIN] Iter: 988200 Loss: 0.005377507768571377  PSNR: 27.257587432861328
[TRAIN] Iter: 988300 Loss: 0.004074899014085531  PSNR: 29.63562774658203
[TRAIN] Iter: 988400 Loss: 0.004556568339467049  PSNR: 28.386547088623047
[TRAIN] Iter: 988500 Loss: 0.0046917893923819065  PSNR: 28.190479278564453
[TRAIN] Iter: 988600 Loss: 0.004363782238215208  PSNR: 29.099422454833984
[TRAIN] Iter: 988700 Loss: 0.004399093799293041  PSNR: 28.357343673706055
[TRAIN] Iter: 988800 Loss: 0.005083268973976374  PSNR: 28.15740966796875
[TRAIN] Iter: 988900 Loss: 0.004425880964845419  PSNR: 28.077068328857422
[TRAIN] Iter: 989000 Loss: 0.004649605602025986  PSNR: 27.861032485961914
[TRAIN] Iter: 989100 Loss: 0.004607117734849453  PSNR: 27.97645378112793
[TRAIN] Iter: 989200 Loss: 0.005019021686166525  PSNR: 27.884090423583984
[TRAIN] Iter: 989300 Loss: 0.004749615676701069  PSNR: 27.600597381591797
[TRAIN] Iter: 989400 Loss: 0.004246480762958527  PSNR: 28.701265335083008
[TRAIN] Iter: 989500 Loss: 0.00397917628288269  PSNR: 29.26712417602539
[TRAIN] Iter: 989600 Loss: 0.005621552001684904  PSNR: 27.35965347290039
[TRAIN] Iter: 989700 Loss: 0.003818485187366605  PSNR: 29.715404510498047
[TRAIN] Iter: 989800 Loss: 0.003924339544028044  PSNR: 29.685020446777344
[TRAIN] Iter: 989900 Loss: 0.005364680662751198  PSNR: 27.669952392578125
Saved checkpoints at ./logs/TUT-KE101-nerf/990000.tar
[TRAIN] Iter: 990000 Loss: 0.004370309412479401  PSNR: 28.759422302246094
[TRAIN] Iter: 990100 Loss: 0.005325105972588062  PSNR: 27.676877975463867
[TRAIN] Iter: 990200 Loss: 0.004511256702244282  PSNR: 28.353132247924805
[TRAIN] Iter: 990300 Loss: 0.003975105471909046  PSNR: 29.71378517150879
[TRAIN] Iter: 990400 Loss: 0.005404050461947918  PSNR: 27.681350708007812
[TRAIN] Iter: 990500 Loss: 0.004607826936990023  PSNR: 28.19882583618164
[TRAIN] Iter: 990600 Loss: 0.003753960132598877  PSNR: 29.770540237426758
[TRAIN] Iter: 990700 Loss: 0.004139572381973267  PSNR: 29.58194351196289
[TRAIN] Iter: 990800 Loss: 0.00494262482970953  PSNR: 27.794574737548828
[TRAIN] Iter: 990900 Loss: 0.004717155825346708  PSNR: 28.153900146484375
[TRAIN] Iter: 991000 Loss: 0.005885395221412182  PSNR: 26.57013702392578
[TRAIN] Iter: 991100 Loss: 0.003964449279010296  PSNR: 29.577951431274414
[TRAIN] Iter: 991200 Loss: 0.005639294628053904  PSNR: 26.87318992614746
[TRAIN] Iter: 991300 Loss: 0.004332836251705885  PSNR: 28.41034698486328
[TRAIN] Iter: 991400 Loss: 0.005476575810462236  PSNR: 27.631147384643555
[TRAIN] Iter: 991500 Loss: 0.003372319508343935  PSNR: 30.400514602661133
[TRAIN] Iter: 991600 Loss: 0.0034940524492412806  PSNR: 30.29262351989746
[TRAIN] Iter: 991700 Loss: 0.005758780986070633  PSNR: 27.865306854248047
[TRAIN] Iter: 991800 Loss: 0.004243386909365654  PSNR: 27.854286193847656
[TRAIN] Iter: 991900 Loss: 0.004506463650614023  PSNR: 28.294567108154297
[TRAIN] Iter: 992000 Loss: 0.00493343360722065  PSNR: 27.360246658325195
[TRAIN] Iter: 992100 Loss: 0.004728060215711594  PSNR: 28.46604347229004
[TRAIN] Iter: 992200 Loss: 0.003976812120527029  PSNR: 30.282285690307617
[TRAIN] Iter: 992300 Loss: 0.005176727660000324  PSNR: 27.970888137817383
[TRAIN] Iter: 992400 Loss: 0.004266093950718641  PSNR: 29.03867530822754
[TRAIN] Iter: 992500 Loss: 0.004150438122451305  PSNR: 28.161256790161133
[TRAIN] Iter: 992600 Loss: 0.004737649112939835  PSNR: 27.99854850769043
[TRAIN] Iter: 992700 Loss: 0.004038410726934671  PSNR: 29.23045539855957
[TRAIN] Iter: 992800 Loss: 0.005861605983227491  PSNR: 27.016145706176758
[TRAIN] Iter: 992900 Loss: 0.003680241061374545  PSNR: 29.35677146911621
[TRAIN] Iter: 993000 Loss: 0.004080723039805889  PSNR: 28.60942268371582
[TRAIN] Iter: 993100 Loss: 0.0045823291875422  PSNR: 28.304222106933594
[TRAIN] Iter: 993200 Loss: 0.004092526622116566  PSNR: 28.50044822692871
[TRAIN] Iter: 993300 Loss: 0.004528569523245096  PSNR: 28.52263641357422
[TRAIN] Iter: 993400 Loss: 0.004053290002048016  PSNR: 29.8718204498291
[TRAIN] Iter: 993500 Loss: 0.004528373945504427  PSNR: 28.927215576171875
[TRAIN] Iter: 993600 Loss: 0.0036601084284484386  PSNR: 29.32809829711914
[TRAIN] Iter: 993700 Loss: 0.003588050603866577  PSNR: 30.4044246673584
[TRAIN] Iter: 993800 Loss: 0.005197059363126755  PSNR: 28.01852035522461
[TRAIN] Iter: 993900 Loss: 0.004459306597709656  PSNR: 28.23564910888672
[TRAIN] Iter: 994000 Loss: 0.0034496881999075413  PSNR: 30.109922409057617
[TRAIN] Iter: 994100 Loss: 0.006015429738909006  PSNR: 26.44358253479004
[TRAIN] Iter: 994200 Loss: 0.004841722548007965  PSNR: 27.533294677734375
[TRAIN] Iter: 994300 Loss: 0.003306068480014801  PSNR: 29.976205825805664
[TRAIN] Iter: 994400 Loss: 0.0047896429896354675  PSNR: 27.942100524902344
[TRAIN] Iter: 994500 Loss: 0.0043439497239887714  PSNR: 28.018905639648438
[TRAIN] Iter: 994600 Loss: 0.005237009841948748  PSNR: 27.91666030883789
[TRAIN] Iter: 994700 Loss: 0.0039854203350842  PSNR: 29.37584686279297
[TRAIN] Iter: 994800 Loss: 0.0064053768292069435  PSNR: 26.835309982299805
[TRAIN] Iter: 994900 Loss: 0.0037992503494024277  PSNR: 29.571155548095703
[TRAIN] Iter: 995000 Loss: 0.004081218969076872  PSNR: 28.95938491821289
[TRAIN] Iter: 995100 Loss: 0.004383834544569254  PSNR: 27.86036491394043
[TRAIN] Iter: 995200 Loss: 0.004038945306092501  PSNR: 29.17255401611328
[TRAIN] Iter: 995300 Loss: 0.006662426516413689  PSNR: 26.846227645874023
[TRAIN] Iter: 995400 Loss: 0.0036983522586524487  PSNR: 29.586711883544922
[TRAIN] Iter: 995500 Loss: 0.0045319730415940285  PSNR: 28.78835678100586
[TRAIN] Iter: 995600 Loss: 0.005806936416774988  PSNR: 27.00872230529785
[TRAIN] Iter: 995700 Loss: 0.004362496547400951  PSNR: 28.43206787109375
[TRAIN] Iter: 995800 Loss: 0.0037837885320186615  PSNR: 29.803667068481445
[TRAIN] Iter: 995900 Loss: 0.0038165051955729723  PSNR: 28.79999351501465
[TRAIN] Iter: 996000 Loss: 0.0036521065048873425  PSNR: 29.99565315246582
[TRAIN] Iter: 996100 Loss: 0.00567144900560379  PSNR: 27.30906867980957
[TRAIN] Iter: 996200 Loss: 0.004568065982311964  PSNR: 27.58749771118164
[TRAIN] Iter: 996300 Loss: 0.005091153085231781  PSNR: 28.432222366333008
[TRAIN] Iter: 996400 Loss: 0.004430227912962437  PSNR: 28.28229522705078
[TRAIN] Iter: 996500 Loss: 0.006309593096375465  PSNR: 26.13463020324707
[TRAIN] Iter: 996600 Loss: 0.003974971827119589  PSNR: 29.947677612304688
[TRAIN] Iter: 996700 Loss: 0.0038470628205686808  PSNR: 29.156953811645508
[TRAIN] Iter: 996800 Loss: 0.0053177266381680965  PSNR: 27.12115478515625
[TRAIN] Iter: 996900 Loss: 0.005610884167253971  PSNR: 27.319072723388672
[TRAIN] Iter: 997000 Loss: 0.005322997458279133  PSNR: 28.110374450683594
[TRAIN] Iter: 997100 Loss: 0.005017767660319805  PSNR: 28.212100982666016
[TRAIN] Iter: 997200 Loss: 0.004076685290783644  PSNR: 29.765348434448242
[TRAIN] Iter: 997300 Loss: 0.005433656275272369  PSNR: 27.003009796142578
[TRAIN] Iter: 997400 Loss: 0.0038929313886910677  PSNR: 29.288955688476562
[TRAIN] Iter: 997500 Loss: 0.004925753455609083  PSNR: 28.403564453125
[TRAIN] Iter: 997600 Loss: 0.004688107408583164  PSNR: 29.00096893310547
[TRAIN] Iter: 997700 Loss: 0.005175028461962938  PSNR: 27.857635498046875
[TRAIN] Iter: 997800 Loss: 0.0038568242453038692  PSNR: 29.889556884765625
[TRAIN] Iter: 997900 Loss: 0.003502373117953539  PSNR: 30.10400390625
[TRAIN] Iter: 998000 Loss: 0.00515036191791296  PSNR: 27.850746154785156
[TRAIN] Iter: 998100 Loss: 0.004035057500004768  PSNR: 28.700353622436523
[TRAIN] Iter: 998200 Loss: 0.00419122539460659  PSNR: 28.275217056274414
[TRAIN] Iter: 998300 Loss: 0.0046510049141943455  PSNR: 28.52004051208496
[TRAIN] Iter: 998400 Loss: 0.004141420125961304  PSNR: 30.01374626159668
[TRAIN] Iter: 998500 Loss: 0.004172319546341896  PSNR: 29.932498931884766
[TRAIN] Iter: 998600 Loss: 0.0049422737210989  PSNR: 27.75606918334961
[TRAIN] Iter: 998700 Loss: 0.00375853618606925  PSNR: 29.838302612304688
[TRAIN] Iter: 998800 Loss: 0.005522358231246471  PSNR: 27.893407821655273
[TRAIN] Iter: 998900 Loss: 0.003697770182043314  PSNR: 30.693729400634766
[TRAIN] Iter: 999000 Loss: 0.004704799968749285  PSNR: 28.666881561279297
[TRAIN] Iter: 999100 Loss: 0.004305318463593721  PSNR: 28.658889770507812
[TRAIN] Iter: 999200 Loss: 0.003945299424231052  PSNR: 29.516799926757812
[TRAIN] Iter: 999300 Loss: 0.004664725624024868  PSNR: 28.268564224243164
[TRAIN] Iter: 999400 Loss: 0.003587564919143915  PSNR: 29.72098731994629
[TRAIN] Iter: 999500 Loss: 0.002996536437422037  PSNR: 30.43610382080078
[TRAIN] Iter: 999600 Loss: 0.0036686332896351814  PSNR: 30.307249069213867
[TRAIN] Iter: 999700 Loss: 0.0033078158739954233  PSNR: 30.63739585876465
[TRAIN] Iter: 999800 Loss: 0.003724928479641676  PSNR: 29.274208068847656
[TRAIN] Iter: 999900 Loss: 0.0035875120665878057  PSNR: 29.93067741394043
Saved checkpoints at ./logs/TUT-KE101-nerf/1000000.tar
0 0.000423431396484375
torch.Size([320, 640, 3]) torch.Size([320, 640])
1 15.54685115814209
2 13.274561882019043
3 13.292060852050781
4 15.522327899932861
5 13.118649244308472
6 15.56714415550232
7 13.16643476486206
8 13.417732238769531
9 15.529075622558594
10 13.111157655715942
11 15.627613306045532
12 13.05582880973816
13 13.272685289382935
14 15.759366989135742
15 13.067216396331787
16 15.544896364212036
17 12.828444957733154
18 15.694276332855225
19 13.383524656295776
20 12.874896764755249
21 16.03273320198059
22 12.93149209022522
23 15.455148696899414
24 13.40749454498291
25 12.953137159347534
26 15.58532452583313
27 12.802608966827393
28 15.817705392837524
29 12.931863069534302
30 13.235624551773071
31 15.690284013748169
32 12.914064884185791
33 15.926189422607422
34 12.785559892654419
35 13.191233396530151
36 15.781838178634644
37 13.199250221252441
38 15.43429183959961
39 13.175444841384888
40 15.566369533538818
41 13.289254188537598
42 13.239315271377563
43 15.377277374267578
44 13.236134052276611
45 15.451608419418335
46 13.248921155929565
47 13.2362539768219
48 15.483922958374023
49 13.237184524536133
50 15.478722095489502
51 13.27800178527832
52 13.260695457458496
53 15.459859132766724
54 13.221945524215698
55 15.484383821487427
56 13.295037031173706
57 15.48796820640564
58 13.26120376586914
59 13.246651411056519
60 15.44243049621582
61 13.275638818740845
62 15.487394571304321
63 13.209943294525146
64 13.156744480133057
65 15.6012442111969
66 12.96160364151001
67 15.805688858032227
68 13.44363522529602
69 13.524150609970093
70 14.834129333496094
71 13.118829011917114
72 15.769376516342163
73 13.151268482208252
74 13.32902479171753
75 15.454420804977417
76 13.221259832382202
77 15.542928457260132
78 13.205496549606323
79 15.555392742156982
80 13.206529140472412
81 13.227764129638672
82 15.538436889648438
83 13.19746994972229
84 15.525768041610718
85 13.188209056854248
86 13.277641773223877
87 15.533641576766968
88 13.217297315597534
89 15.538105010986328
90 13.170295715332031
91 13.269761562347412
92 15.468176126480103
93 13.283786058425903
94 15.59292459487915
95 13.121160984039307
96 15.568097114562988
97 13.168766498565674
98 13.277744054794312
99 15.65798830986023
100 12.94196343421936
101 15.734057664871216
102 13.40774941444397
103 12.95967411994934
104 15.483459711074829
105 12.688288688659668
106 16.040253162384033
107 13.066774606704712
108 13.058979272842407
109 16.010804891586304
110 12.930251836776733
111 15.64660358428955
112 12.798382759094238
113 15.692854404449463
114 13.434587478637695
115 12.918041706085205
116 15.81522798538208
117 12.847394943237305
118 15.752387285232544
119 13.010326147079468
Done, saving (120, 320, 640, 3) (120, 320, 640)
extras:{'raw': tensor([[[-1.5432e+00, -8.9838e-01,  6.6631e-01, -7.7924e+01],
         [ 1.9183e-01,  7.8130e-02, -2.5457e-01, -1.6883e+01],
         [ 2.0495e-01,  9.8785e-02, -2.1306e-01, -1.6653e+01],
         ...,
         [-1.7735e+00, -5.6335e+00, -1.4354e+01, -1.8980e+02],
         [-1.7261e+00, -5.6589e+00, -1.4472e+01, -2.2971e+02],
         [-1.3236e+00, -5.0793e+00, -1.3205e+01, -7.7192e+01]],

        [[ 1.7110e+01,  1.8776e+01,  2.3355e+01, -6.6577e+01],
         [ 3.4685e-02, -1.2576e-01, -2.8531e-01,  6.2049e+00],
         [ 4.6748e-02, -9.9936e-02, -2.4704e-01,  4.6945e+00],
         ...,
         [ 5.6706e+01,  5.0454e+01,  4.0923e+01,  7.8223e+01],
         [ 6.8082e+01,  6.2261e+01,  5.8237e+01,  6.1856e+01],
         [ 6.0105e+01,  5.4001e+01,  4.7964e+01,  8.6350e+01]],

        [[-2.2216e+00, -1.1983e+00,  8.6609e-01, -6.3726e+01],
         [-1.7018e+00, -1.5035e+00, -9.4366e-01, -3.4310e+01],
         [ 8.0980e-01,  9.3982e-01,  1.0844e+00, -2.3729e+01],
         ...,
         [-1.7951e+01, -1.7540e+01, -2.0330e+01, -2.5307e+02],
         [-2.0795e+01, -2.0270e+01, -2.2549e+01, -1.7883e+02],
         [-1.9529e+01, -1.8593e+01, -2.0329e+01, -1.8695e+02]],

        ...,

        [[-2.3147e+00, -1.9665e+00, -4.7732e-01, -6.3523e+01],
         [ 2.0031e-01,  2.7890e-02, -1.9261e-01, -3.8375e+01],
         [ 1.9945e-01,  2.7121e-02, -1.9337e-01, -3.8339e+01],
         ...,
         [-3.8588e+00, -1.8026e+00, -8.1883e+00,  6.2488e+02],
         [-3.2018e+00, -1.7152e+00, -8.7877e+00,  7.0851e+02],
         [-3.9376e+00, -2.1394e+00, -8.8509e+00,  7.2927e+02]],

        [[-7.3508e-01, -5.3564e-01, -7.8076e-02, -5.2121e+01],
         [ 1.1964e+00,  9.8708e-01,  7.2112e-01, -1.7584e+01],
         [ 5.2216e-01,  3.7052e-01,  1.2364e-01, -1.5318e+01],
         ...,
         [ 3.8048e+00,  2.5886e+00, -4.1909e-01,  9.6151e+02],
         [ 4.1189e+00,  3.6463e+00,  2.7361e+00,  8.7296e+02],
         [ 2.6349e+00,  1.3788e+00, -1.5469e+00,  1.0036e+03]],

        [[-4.2446e+00, -4.4529e+00, -5.5242e+00, -8.5700e+01],
         [-4.8404e-01, -1.0704e-01,  6.9726e-01, -4.1666e+01],
         [-4.9131e-01, -4.5729e-01, -2.2591e-01, -1.9179e+01],
         ...,
         [-1.8140e+01, -1.7877e+01, -1.8896e+01, -3.7561e+02],
         [-1.9637e+01, -1.9137e+01, -1.9789e+01, -2.8035e+02],
         [-1.8161e+01, -1.7168e+01, -1.6602e+01, -2.5381e+02]]],
       grad_fn=<ReshapeAliasBackward0>), 'rgb0': tensor([[0.5695, 0.5717, 0.6160],
        [0.5418, 0.4981, 0.4538],
        [0.3379, 0.3305, 0.3077],
        ...,
        [0.4585, 0.4088, 0.3503],
        [0.4573, 0.4066, 0.3215],
        [0.3731, 0.3590, 0.3426]], grad_fn=<ReshapeAliasBackward0>), 'disp0': tensor([ 91.6423, 103.4783,  56.8923,  ..., 624.5627,  93.2192,  71.1290],
       grad_fn=<ReshapeAliasBackward0>), 'acc0': tensor([1., 1., 1.,  ..., 1., 1., 1.], grad_fn=<ReshapeAliasBackward0>), 'z_std': tensor([0.0022, 0.0019, 0.0043,  ..., 0.0606, 0.0030, 0.0027])}
0 0.0005204677581787109
torch.Size([320, 640, 3]) torch.Size([320, 640])
1 15.722839832305908
2 13.198784112930298
3 15.584934949874878
4 13.028741359710693
5 13.265312671661377
6 15.529390335083008
7 13.074710607528687
8 15.90924882888794
9 12.845987319946289
10 15.738717079162598
11 13.066330671310425
12 13.334187507629395
13 15.53378939628601
14 12.920831203460693
15 15.77941608428955
16 12.87056016921997
17 13.134799003601074
18 15.716611385345459
19 12.96855115890503
20 16.29839587211609
21 13.351552486419678
22 13.391969203948975
23 14.513716459274292
24 13.399964332580566
25 15.663566589355469
26 13.025108575820923
27 15.565148830413818
28 13.198787689208984
29 13.049525022506714
30 15.589865684509277
31 12.918912410736084
32 15.863871812820435
33 12.939284324645996
34 13.27017593383789
35 15.554355144500732
36 12.925313949584961
37 15.924678564071655
38 12.861720085144043
39 13.491310358047485
40 15.361238479614258
41 13.03657341003418
42 16.00360107421875
43 13.332923412322998
44 13.356165170669556
45 15.177233695983887
46 13.310918807983398
47 14.786193132400513
48 12.048415899276733
49 15.322274923324585
50 13.286571025848389
51 13.358747482299805
52 15.330486536026001
53 13.322303056716919
54 15.371950387954712
55 13.320436716079712
56 13.26478886604309
57 15.34249472618103
58 13.33900761604309
59 15.370857238769531
60 13.299408197402954
61 13.306951761245728
62 15.38082504272461
63 13.21465253829956
64 15.497941255569458
65 13.211668252944946
66 15.469446420669556
67 13.203280448913574
68 13.17709493637085
69 15.666445016860962
70 13.008451223373413
71 15.546562433242798
72 13.32938838005066
73 13.097153186798096
74 15.538361549377441
75 13.169075727462769
76 15.446072578430176
77 13.010152578353882
78 13.338028907775879
79 15.512955904006958
80 12.94585394859314
81 15.825295209884644
82 12.860059976577759
83 13.341527462005615
84 15.50211215019226
85 12.986104488372803
86 15.919819831848145
87 12.73454475402832
88 15.691914796829224
89 13.225112199783325
90 13.183250665664673
91 15.558593034744263
92 13.251128673553467
93 15.419023513793945
94 13.225356101989746
95 13.207477569580078
96 15.54239797592163
97 13.215131044387817
98 15.4644935131073
99 13.230514764785767
100 13.229059219360352
101 15.487060308456421
102 13.214098930358887
103 15.484414100646973
104 13.24260663986206
105 13.190237522125244
106 15.505758047103882
107 13.189082145690918
108 15.505832433700562
109 13.186078786849976
110 15.461264371871948
111 13.20750880241394
112 13.208626985549927
113 15.475080490112305
114 13.242600679397583
115 15.550219058990479
116 13.266278982162476
117 13.21657681465149
118 15.409070491790771
119 13.213638305664062
test poses shape torch.Size([4, 3, 4])
0 0.0007038116455078125
torch.Size([320, 640, 3]) torch.Size([320, 640])
1 13.33897876739502
2 15.503594160079956
3 13.273576974868774
Saved test set
[TRAIN] Iter: 1000000 Loss: 0.006437772884964943  PSNR: 26.242000579833984
